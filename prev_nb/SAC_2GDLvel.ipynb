{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XCEQ6e08PQYe"
      },
      "outputs": [],
      "source": [
        "from gymnasium import Env\n",
        "from gymnasium.spaces.box import Box\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import pickle\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rFCb5ECGR1Lt"
      },
      "outputs": [],
      "source": [
        "class BobinaEnvTest(Env):\n",
        "  \"\"\"\n",
        "  Custom Gym Environment for the Hybrid Electromagnetic Suspension, considering just two degrees of freedom.\n",
        "  \"\"\"\n",
        "  def __init__(self, alive = 20, masa_pod = 250, airgap = 16.3, duration = 10, dist_cm = 0.7, inercia= 70, min_distance = 9, max_distance = 22.5):\n",
        "    # Actions: Voltage applied to the coil\n",
        "    self.action_space = Box(low=np.array([-100, -100]), high = np.array([100, 100]), dtype=np.float32)\n",
        "    # Observations: Airgap to the ceiling, velocity, distance to the objective, current of the coil\n",
        "    self.observation_space = Box(low=np.array([-10, -10, -10, -10, -45, -45]), high = np.array([10, 10, 10, 10, 45, 45]), dtype=np.float32)\n",
        "    # Initial parameters\n",
        "    airgap = random.uniform(15.3, 17.3)\n",
        "    self.state = np.array([airgap - 16.3, airgap - 16.3, airgap - 16.3, airgap - 16.3, 0, 0])\n",
        "    self.airgapinicial = airgap\n",
        "    self.duration = duration\n",
        "    self.timeleft = duration\n",
        "    self.crash = False\n",
        "    self.masa_pod = masa_pod\n",
        "    self.sistema = TwoGDL(masa_pod = self.masa_pod, airgap = self.airgapinicial, dist_cm = dist_cm, inercia= inercia, min_distance = min_distance, max_distance = max_distance)\n",
        "    self.steps = []\n",
        "    self.airgap1 = airgap\n",
        "    self.airgap2 = airgap\n",
        "    self.distancia1 = 0\n",
        "    self.distancia2 = 0\n",
        "    self.velocidad = 0\n",
        "    self.current1 = 0\n",
        "    self.current2 = 0\n",
        "    self.alive = alive\n",
        "    self.vel = 0\n",
        "    self.velangular = 0\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # Apply action\n",
        "    \n",
        "    self.state, self.crash = self.sistema.step(action[0],action[1])\n",
        "    self.distancia11, self.distancia12, self.distancia21, self.distancia22, self.current1, self.current2 = self.state\n",
        "\n",
        "    # Reduce the time of the experiment\n",
        "    self.timeleft -= 0.001\n",
        "\n",
        "    # Calculate reward\n",
        "    reward = -abs(self.distancia12) -abs(self.distancia22) - 10000*self.crash\n",
        "\n",
        "    \"\"\"if self.airgap >= 23: reward = -500\n",
        "    elif self.airgap <= 10: reward = -500\n",
        "    else: reward = -abs(self.current)\"\"\"\n",
        "\n",
        "    # Check if experiment is done\n",
        "    if self.timeleft <= 0:\n",
        "      truncated = True\n",
        "    else: truncated = False\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    # Only if we implemented the crash\n",
        "    \"\"\"if self.crash:\n",
        "      reward = - 100000\n",
        "      terminated = True\n",
        "    else:\n",
        "      terminated = False\"\"\"\n",
        "    terminated = False\n",
        "    self.steps.append(self.state)\n",
        "    return self.state, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "  def render(self, yes = \"yes\", normalize = True, seed = 0):\n",
        "    # create data\n",
        "    x = list(range(0,len(self.steps)))\n",
        "    y1 = [16.3+step[0] for step in self.steps]\n",
        "    y2 = [16.3+step[1] for step in self.steps]\n",
        "    objective = [16.3]*len(self.steps)\n",
        "\n",
        "    # plot lines\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    #ax.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n",
        "    #ax.tick_params(axis='y', colors='white')  #setting up Y-axis tick color to black\n",
        "    if normalize: ax.set_ylim(bottom=8, top=23)\n",
        "    plt.xlabel(\"Time (s)\") #, color='white')\n",
        "    plt.ylabel(\"Airgap (mm)\") #, color='white')\n",
        "    plt.title(\"Airgap evolution\") #, color='white')\n",
        "\n",
        "    plt.plot(x, y1, label = \"Airgap 1\")\n",
        "    plt.plot(x, y2, label = \"Airgap 2\")\n",
        "    plt.plot(x, objective, label = \"Objective\")\n",
        "    plt.legend()\n",
        "    #if self.steps[0][0] < 0: plt.savefig(\"images/top-2gdl8x8nocrash.png\")\n",
        "    #else: plt.savefig(\"images/bottom-2gdl8x8nocrash.png\")\n",
        "    plt.savefig(f\"images/2gdl8x8nocrashsituation{seed}.png\")\n",
        "    plt.show()\n",
        "\n",
        "  def reset(self, seed = 0):\n",
        "    # Reset experiment choosing randomly the start (ceiling or floor)\n",
        "    airgap = random.uniform(9.5, 22)\n",
        "    #self.state = np.array([airgap - 16.3, airgap - 16.3, 0, 0, 0, 0])\n",
        "    self.sistema = TwoGDL(masa_pod = self.masa_pod, airgap = airgap, dist_cm = 0.7, inercia= 70, min_distance = 9, max_distance = 22.5)\n",
        "    angulo = random.uniform(-0.008, 0.008)\n",
        "    self.vel1 = random.uniform(-0.2, 0.2)\n",
        "    self.velangular = random.uniform(-0.3, 0.3)\n",
        "    self.current1 = 0\n",
        "    self.current2 = 0\n",
        "    airgap1, airgap2 = self.sistema.pos2airgap(pos = airgap, theta = angulo)\n",
        "    self.distancia1, self.distancia2 = airgap1-16.3, airgap2-16.3\n",
        "    self.state= self.distancia1, self.distancia2, self.vel, self.velangular, self.current1, self.current2\n",
        "    self.sistema.integral_aceleracion = self.vel\n",
        "    self.sistema.integral_alpha = self.velangular\n",
        "    self.sistema.integral_omega = angulo\n",
        "    self.state, self.crash = self.sistema.step(0,0)\n",
        "    self.distancia11, self.distancia12, self.distancia21, self.distancia22, self.current1, self.current2 = self.state\n",
        "    \n",
        "    \n",
        "    # Reset time\n",
        "    self.timeleft = self.duration\n",
        "    self.crash = False\n",
        "    self.steps = []\n",
        "    return self.state, seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BobinaEnv(Env):\n",
        "  \"\"\"\n",
        "  Custom Gym Environment for the Hybrid Electromagnetic Suspension, considering just two degrees of freedom.\n",
        "  \"\"\"\n",
        "  def __init__(self, alive = 20, masa_pod = 250, airgap = 16.3, duration = 10, dist_cm = 0.7, inercia= 70, min_distance = 9, max_distance = 22.5):\n",
        "    # Actions: Voltage applied to the coil\n",
        "    self.action_space = Box(low=np.array([-100, -100]), high = np.array([100, 100]), dtype=np.float32)\n",
        "    # Observations: Airgap to the ceiling, velocity, distance to the objective, current of the coil\n",
        "    self.observation_space = Box(low=np.array([-10, -10, -np.inf, -np.inf, -45, -45]), high = np.array([10, 10, np.inf, np.inf, 45, 45]), dtype=np.float32)\n",
        "    # Initial parameters\n",
        "    airgap = 16.3\n",
        "    angulo = random.uniform(-0.008, 0.008)\n",
        "    self.airgapinicial = airgap\n",
        "    self.duration = duration\n",
        "    self.timeleft = duration\n",
        "    self.crash = False\n",
        "    self.masa_pod = masa_pod\n",
        "    self.sistema = TwoGDL(masa_pod = self.masa_pod, airgap = self.airgapinicial, dist_cm = dist_cm, inercia= inercia, min_distance = min_distance, max_distance = max_distance)\n",
        "\n",
        "    \n",
        "    #airgap1, airgap2 = self.sistema.pos2airgap(pos = airgap, theta = angulo)\n",
        "    \n",
        "    self.state = np.array([airgap - 16.3, airgap - 16.3, 0, 0, 0, 0])\n",
        "   # self.sistema.integral_omega = angulo\n",
        "\n",
        "    self.steps = []\n",
        "    self.airgap1 = airgap\n",
        "    self.airgap2 = airgap\n",
        "    \"\"\"self.distancia1 = 0\n",
        "    self.distancia2 = 0\"\"\"\n",
        "    self.distancia1, self.distancia2 = airgap-16.3, airgap-16.3\n",
        "    self.velocidad = 0\n",
        "    self.current1 = 0\n",
        "    self.current2 = 0\n",
        "    self.alive = alive\n",
        "    self.vel = 0\n",
        "    self.velangular = 0\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    # Apply action\n",
        "    self.state, self.crash, angulo, error = self.sistema.step(action[0],action[1])\n",
        "    self.distancia1, self.distancia2, self.vel, self.velangular, self.current1, self.current2 = self.state\n",
        "\n",
        "    # Reduce the time of the experiment\n",
        "    self.timeleft -= 0.001\n",
        "\n",
        "    # Calculate reward\n",
        "    reward = -abs(self.distancia1) -abs(self.distancia2) - 10000*self.crash\n",
        "\n",
        "    \"\"\"if self.airgap >= 23: reward = -500\n",
        "    elif self.airgap <= 10: reward = -500\n",
        "    else: reward = -abs(self.current)\"\"\"\n",
        "\n",
        "    # Check if experiment is done\n",
        "    if self.timeleft <= 0:\n",
        "      truncated = True\n",
        "    else: truncated = False\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    # Only if we implemented the crash\n",
        "    \"\"\"if self.crash:\n",
        "      reward = - 100000\n",
        "      terminated = True\n",
        "    else:\n",
        "      terminated = False\"\"\"\n",
        "    terminated = False\n",
        "    self.steps.append(self.state)\n",
        "    return self.state, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "  def render(self, yes = \"yes\", normalize = True, seed = 0):\n",
        "    # create data\n",
        "    x = list(range(0,len(self.steps)))\n",
        "    y1 = [16.3+step[0] for step in self.steps]\n",
        "    y2 = [16.3+step[1] for step in self.steps]\n",
        "    objective = [16.3]*len(self.steps)\n",
        "\n",
        "    # plot lines\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    #ax.tick_params(axis='x', colors='white')    #setting up X-axis tick color to red\n",
        "    #ax.tick_params(axis='y', colors='white')  #setting up Y-axis tick color to black\n",
        "    if normalize: ax.set_ylim(bottom=8, top=23)\n",
        "    plt.xlabel(\"Time (s)\") #, color='white')\n",
        "    plt.ylabel(\"Airgap (mm)\") #, color='white')\n",
        "    plt.title(\"Airgap evolution\") #, color='white')\n",
        "\n",
        "    plt.plot(x, y1, label = \"Airgap 1\")\n",
        "    plt.plot(x, y2, label = \"Airgap 2\")\n",
        "    plt.plot(x, objective, label = \"Objective\")\n",
        "    plt.legend()\n",
        "    #if self.steps[0][0] < 0: plt.savefig(\"images/top-2gdl8x8nocrash.png\")\n",
        "    #else: plt.savefig(\"images/bottom-2gdl8x8nocrash.png\")\n",
        "    plt.savefig(f\"images/2gdl8x8nocrashsituation{seed}.png\")\n",
        "    plt.show()\n",
        "\n",
        "  def reset(self, seed = 0):\n",
        "    # Reset experiment choosing randomly the start (ceiling or floor)\n",
        "    airgap = random.choice([10, 22])\n",
        "    #angulo = random.uniform(-0.005, 0.005)\n",
        "    self.sistema = TwoGDL(masa_pod = self.masa_pod, airgap = airgap, dist_cm = 0.7, inercia= 70, min_distance = 9, max_distance = 22.5)\n",
        "    #airgap1, airgap2 = self.sistema.pos2airgap(pos = airgap, theta = angulo)\n",
        "    #self.distancia1, self.distancia2 = airgap1-16.3, airgap2-16.3\n",
        "    self.state = np.array([airgap - 16.3, airgap - 16.3, 0, 0, 0, 0])\n",
        "    #self.sistema.integral_omega = angulo\n",
        "    \n",
        "    # Reset time\n",
        "    self.timeleft = self.duration\n",
        "    self.crash = False\n",
        "    self.steps = []\n",
        "    return self.state, seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9SgMX9wfPZEl"
      },
      "outputs": [],
      "source": [
        "class Bobina():\n",
        "  \"\"\" \n",
        "  Coil model for the Hybrid Electromagnetic Suspension\n",
        "  \"\"\"\n",
        "  def __init__(self, masa_pod = 250, airgap = 22.5, min_distance = 9, max_distance = 22.5):\n",
        "    self.masa_pod = masa_pod\n",
        "    self.airgap = airgap\n",
        "    self.aceleracion = []\n",
        "    self.velocidad = []\n",
        "    mat = scipy.io.loadmat('./coil/hems.mat')\n",
        "    self.hems = mat[\"hems\"][0][0]\n",
        "    self.airgaps = self.hems[0][0]\n",
        "    self.currents = self.hems[1][0]\n",
        "    self.force_vertical = pickle.load(open(\"./coil/mlp_vertical.pkl\", 'rb'))\n",
        "    self.flux = self.hems[3]\n",
        "    self.force_horizontal = self.hems[4]\n",
        "    self.inductance = pickle.load(open(\"./coil/rf_inductance.pkl\", 'rb'))\n",
        "    self.resistance = self.hems[6][0][0]\n",
        "    self.min_distance = min_distance\n",
        "    self.max_distance = max_distance\n",
        "    self.airgapinicial = airgap\n",
        "    self.crash = False\n",
        "    self.current = 0\n",
        "    self.steps_RL = []\n",
        "\n",
        "  def integral(self, lista, h = 1):\n",
        "    return sum(h*(lista[i]+lista[i+1])/2 for i in range(len(lista)-1))\n",
        "\n",
        "  def RL(self, airgap, target_voltage, temperature):\n",
        "    # Resistance-Inductance circuit\n",
        "\n",
        "    # Get resistance of the coil\n",
        "    R = self.R(current = self.current, temperature = temperature)   \n",
        "    V = target_voltage\n",
        "    I = self.current\n",
        "    L = self.L(airgap = airgap, current = self.current)\n",
        "    dIdt = ((V/R) - I)/(L/R)                                      \n",
        "    self.steps_RL.append(dIdt)                                   \n",
        "    current = self.integral(self.steps_RL, h = 0.001)                          \n",
        "    return current                                                  \n",
        "\n",
        "  def R(self, current, temperature):\n",
        "    return self.resistance                                         \n",
        "\n",
        "  def L(self, airgap, current):\n",
        "    # Return predicted inductance\n",
        "    return float(self.inductance.predict(np.array([[airgap, current]]))[0])\n",
        "\n",
        "  def vertical_force(self, airgap, target_voltage, temperature):\n",
        "    # Get current from RL circuit\n",
        "    I = self.RL(airgap = airgap, target_voltage = target_voltage, temperature = temperature) \n",
        "    \n",
        "    # Clip result\n",
        "    if I > 45:\n",
        "      I = 45\n",
        "    elif I < -45:\n",
        "      I = -45\n",
        "    self.current = I\n",
        "    # Get vertical force from \n",
        "    FI = self.FI_vertical(airgap = airgap, current = I)                    \n",
        "    return FI, I\n",
        "\n",
        "\n",
        "  def FI_vertical(self, airgap, current):\n",
        "    # Return predicted vertical force\n",
        "    return float(self.force_vertical.predict(np.array([[airgap, current]])))    \n",
        "\n",
        "\n",
        "  def next_airgap(self, masa_pod, vertical_force, verbose = False):\n",
        "    # Devide the mass of the pod by 4 coils\n",
        "    masa = masa_pod/4  \n",
        "    # Calculate weight\n",
        "    peso = masa * 9.8    \n",
        "    # Get total force                                                       \n",
        "    fuerza = peso - vertical_force\n",
        "    if verbose: print(\"Fuerza resultante: \", fuerza)   \n",
        "    # Calculate acceleration                                     \n",
        "    aceleracion = fuerza/masa\n",
        "    if verbose: print(\"Aceleración: \", aceleracion)                                         \n",
        "    self.aceleracion.append(aceleracion)       \n",
        "    # Calculate velocity                                \n",
        "    velocidad = self.integral(self.aceleracion, 0.001)\n",
        "    if verbose: print(\"Integral primera (velocidad): \", velocidad)                          \n",
        "    self.velocidad.append(velocidad)               \n",
        "    # Calculate position (x1000 as to transform from m to mm)                            \n",
        "    posicion = self.airgapinicial + self.integral(self.velocidad, 0.001)*1000 \n",
        "    if verbose: print(\"Integral segunda (posición): \", posicion)\n",
        "\n",
        "    # Clip position\n",
        "    if posicion > self.max_distance:\n",
        "      airgap = self.max_distance\n",
        "      self.aceleracion = [0]\n",
        "      self.velocidad = [0]  \n",
        "      #self.crash = True     \n",
        "      self.airgapinicial = self.max_distance                                               \n",
        "    elif posicion < self.min_distance:\n",
        "      airgap = self.min_distance\n",
        "      self.aceleracion = [0]\n",
        "      self.velocidad = [0]\n",
        "      #self.crash = True\n",
        "      self.airgapinicial = self.min_distance\n",
        "    else: airgap = posicion\n",
        "    return airgap, velocidad\n",
        "\n",
        "  def step(self, target_voltage, verbose = False):\n",
        "    # Calculate vertical force and current\n",
        "    vertical_force, self.current = self.vertical_force(airgap = self.airgap, target_voltage = target_voltage, temperature = 40)\n",
        "    if verbose: print(\"Fuerza vertical: \", vertical_force)\n",
        "    # Calculate next airgap and velocity\n",
        "    self.airgap, velocidad = self.next_airgap(self.masa_pod, vertical_force, verbose = verbose)\n",
        "    #noise = np.random.normal(0,0.5)\n",
        "    #self.airgap = self.airgap + noise     \n",
        "    return (np.float32(self.airgap), np.float32(velocidad), np.float32(self.airgap - 16.3), np.float32(self.current)), self.crash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoGDL():\n",
        "    def __init__(self, masa_pod = 250, airgap = 22.5, dist_cm = 0.7, inercia= 70, min_distance = 9, max_distance = 22.5):\n",
        "        self.masa_pod = masa_pod\n",
        "        self.airgap1 = airgap\n",
        "        self.airgap1_list = [airgap]\n",
        "        self.airgap2 = airgap\n",
        "        self.airgap2_list = [airgap]\n",
        "        self.dist_cm = dist_cm\n",
        "        self.inercia = inercia\n",
        "        self.airgapinicial = airgap\n",
        "        self.min_distance = min_distance\n",
        "        self.max_distance = max_distance\n",
        "        self.b1 = Bobina(masa_pod = self.masa_pod, airgap = self.airgap1, min_distance = self.min_distance, max_distance = self.max_distance)\n",
        "        self.b2 = Bobina(masa_pod = self.masa_pod, airgap = self.airgap2, min_distance = self.min_distance, max_distance = self.max_distance)\n",
        "        self.crash = False\n",
        "        self.angulo = [0]\n",
        "        self.alpha = [0]\n",
        "        self.integral_alpha = 0\n",
        "        self.omega = [0]\n",
        "        self.integral_omega = 0\n",
        "        self.altura = [0]\n",
        "        self.aceleracion = [0]\n",
        "        self.integral_aceleracion = 0\n",
        "        self.velocidad = [0]\n",
        "        self.integral_velocidad = 0\n",
        "    \n",
        "    def integral(self, lista, h = 1):\n",
        "        return sum(h*(lista[i]+lista[i+1])/2 for i in range(len(lista)-1))\n",
        "    \n",
        "    def fast_integral(self, last, lista, h = 0.001):\n",
        "        return last + h*(lista[-1]+lista[-2])/2\n",
        "    \n",
        "    def get_angulo(self, f1, f2):\n",
        "        F1 = f1*self.dist_cm\n",
        "        F2 = -f2*self.dist_cm\n",
        "        M = F1+F2\n",
        "        alpha = M/self.inercia\n",
        "        self.alpha.append(alpha)\n",
        "        omega = self.fast_integral(self.integral_alpha, self.alpha, 0.001)\n",
        "        self.integral_alpha = omega\n",
        "        self.omega.append(omega)\n",
        "        angulo = self.fast_integral(self.integral_omega, self.omega, 0.001)\n",
        "        self.integral_omega = angulo\n",
        "        self.angulo.append(angulo)\n",
        "        return angulo\n",
        "    \n",
        "    def get_altura(self, f1, f2):\n",
        "        F = (self.masa_pod/2)*9.8 - f1 - f2\n",
        "        a = F/(self.masa_pod/2)\n",
        "        self.aceleracion.append(a)\n",
        "        v = self.fast_integral(self.integral_aceleracion, self.aceleracion, 0.001)\n",
        "        self.integral_aceleracion = v\n",
        "        self.velocidad.append(v)\n",
        "        zpos = self.fast_integral(self.integral_velocidad,self.velocidad, 0.001)\n",
        "        self.integral_velocidad = zpos\n",
        "        zpos = self.airgapinicial + zpos*1000\n",
        "        self.altura.append(zpos)\n",
        "        return zpos\n",
        "    \n",
        "    def pos2airgap(self, pos, theta):\n",
        "        airgap1 = pos + self.dist_cm*math.sin(theta)*1000\n",
        "        airgap2 = pos - self.dist_cm*math.sin(theta)*1000\n",
        "        return airgap1, airgap2\n",
        "    \n",
        "    def step(self, target_voltage1, target_voltage2, verbose = False):\n",
        "        # Calculate vertical force and current\n",
        "        self.crash = False\n",
        "        vertical_force1, current1 = self.b1.vertical_force(airgap = self.airgap1, target_voltage = target_voltage1, temperature = 40)\n",
        "        vertical_force2, current2 = self.b2.vertical_force(airgap = self.airgap2, target_voltage = target_voltage2, temperature = 40)\n",
        "        if verbose: print(\"Fuerza vertical 1: \", vertical_force1)\n",
        "        if verbose: print(\"Fuerza vertical 2: \", vertical_force2)\n",
        "        angulo = self.get_angulo(f1 = vertical_force1, f2 = vertical_force2)\n",
        "        zpos = self.get_altura(f1 = vertical_force1, f2 = vertical_force2)\n",
        "        self.airgap1, self.airgap2 = self.pos2airgap(pos = zpos, theta = angulo)\n",
        "        self.airgap1_list.append(self.airgap1)\n",
        "        self.airgap2_list.append(self.airgap2)\n",
        "        if verbose: print(\"Airgap 1: \", self.airgap1)\n",
        "        if verbose: print(\"Airgap 2: \", self.airgap2)\n",
        "        if self.airgap1 > self.max_distance or self.airgap2 > self.max_distance:\n",
        "            self.airgap1 = self.max_distance\n",
        "            self.airgap2 = self.max_distance\n",
        "            self.crash = True\n",
        "            self.angulo = [0]\n",
        "            self.alpha = [0]\n",
        "            self.integral_alpha = 0\n",
        "            self.omega = [0]\n",
        "            self.integral_omega = 0\n",
        "            self.altura = [0]\n",
        "            self.aceleracion = [0]\n",
        "            self.integral_aceleracion = 0\n",
        "            self.velocidad = [0]\n",
        "            self.integral_velocidad = 0\n",
        "            self.airgapinicial = self.max_distance\n",
        "        elif self.airgap1 < self.min_distance or self.airgap2 < self.min_distance:\n",
        "            self.airgap1 = self.min_distance\n",
        "            self.airgap2 = self.min_distance\n",
        "            self.crash = True\n",
        "            self.angulo = [0]\n",
        "            self.alpha = [0]\n",
        "            self.integral_alpha = 0\n",
        "            self.omega = [0]\n",
        "            self.integral_omega = 0\n",
        "            self.altura = [0]\n",
        "            self.aceleracion = [0]\n",
        "            self.integral_aceleracion = 0\n",
        "            self.velocidad = [0]\n",
        "            self.integral_velocidad = 0\n",
        "            self.airgapinicial = self.min_distance\n",
        "        #return (np.float32(self.airgap1 -16.3), np.float32(self.airgap2-16.3), np.float32(current1), np.float32(current2)), self.crash\n",
        "        return (np.float32(self.airgap1 -16.3), np.float32(self.airgap2-16.3), np.float32(self.integral_aceleracion), np.float32(self.integral_alpha), np.float32(current1), np.float32(current2)), self.crash, angulo, zpos - 16.3\n",
        "        #return (np.float32(self.airgap1_list[-2] -16.3), np.float32(self.airgap1_list[-1] -16.3), np.float32(self.airgap2_list[-2] -16.3), np.float32(self.airgap2_list[-1] -16.3), np.float32(current1), np.float32(current2)), self.crash\n",
        "        #añadir una que returnee los 2 errores y las 2 velocidades\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "aNE6VBJdTFDL",
        "outputId": "be5ffab5-4f53-48bc-9fcc-fd387eb09d56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:tc4ggih5) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-terrain-36</strong> at: <a href='https://wandb.ai/hyperloopupv/2gdl/runs/tc4ggih5' target=\"_blank\">https://wandb.ai/hyperloopupv/2gdl/runs/tc4ggih5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240129_113843-tc4ggih5\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:tc4ggih5). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\hugoa\\H9\\RL\\GitHub Vrain\\rl_levitation_control\\rl_levitation_control\\wandb\\run-20240129_113945-sc3uf3u7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hyperloopupv/2gdl/runs/sc3uf3u7' target=\"_blank\">copper-frog-37</a></strong> to <a href='https://wandb.ai/hyperloopupv/2gdl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/hyperloopupv/2gdl' target=\"_blank\">https://wandb.ai/hyperloopupv/2gdl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/hyperloopupv/2gdl/runs/sc3uf3u7' target=\"_blank\">https://wandb.ai/hyperloopupv/2gdl/runs/sc3uf3u7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    project=\"2gdl\",\n",
        "    sync_tensorboard=True,\n",
        "    monitor_gym=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oinA6TP-TL36"
      },
      "outputs": [],
      "source": [
        "# Create log dir\n",
        "log_dir = \"2gdl/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = BobinaEnv( duration = 5 )\n",
        "env = Monitor(env, log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22oTxNkmTfCd",
        "outputId": "7872ccfd-4bbe-4c07-9740-0ddcc3fb26bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func\n",
        "\n",
        "# para el critic , qf=[400, 300], para el actor pi=[300, 200]\n",
        "policy_kwargs = dict(net_arch=dict(pi=[8, 8], qf=[256, 256]))\n",
        "model = SAC(\"MlpPolicy\", env,learning_rate = linear_schedule(0.001), policy_kwargs=policy_kwargs, verbose=1, tensorboard_log=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "MODEL_PATH = f\"models/2gdl8x8nocrash.2 copy/model.zip\"\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = BobinaEnv(duration = 5)\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(MODEL_PATH, env=env, learning_rate = linear_schedule(0.0001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NVwBakWxTf2c"
      },
      "outputs": [],
      "source": [
        "# Create checkpoint callback\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=10000, save_path=log_dir, name_prefix=\"2gdl\"\n",
        ")\n",
        "\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-350, verbose=1)\n",
        "eval_callback = EvalCallback(env, callback_on_new_best=callback_on_best, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d75OTj_9TxDJ",
        "outputId": "f5860707-4220-4914-9a00-b3da3cf62a24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to 2gdl/SAC_51\n",
            "Eval num_timesteps=10000, episode_reward=-49521511.76 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.95e+07 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 10000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.8e+05   |\n",
            "|    critic_loss     | 3.91e+05  |\n",
            "|    ent_coef        | 11.6      |\n",
            "|    ent_coef_loss   | -0.177    |\n",
            "|    learning_rate   | 0.000999  |\n",
            "|    n_updates       | 11257     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20000, episode_reward=-934157.42 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.34e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 20000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.15e+05  |\n",
            "|    critic_loss     | 5.9e+06   |\n",
            "|    ent_coef        | 256       |\n",
            "|    ent_coef_loss   | -0.947    |\n",
            "|    learning_rate   | 0.000998  |\n",
            "|    n_updates       | 21257     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.73e+07 |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 1170      |\n",
            "|    total_timesteps | 20000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-39674.42 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.97e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.89e+05  |\n",
            "|    critic_loss     | 1.26e+07  |\n",
            "|    ent_coef        | 211       |\n",
            "|    ent_coef_loss   | -1.39     |\n",
            "|    learning_rate   | 0.000997  |\n",
            "|    n_updates       | 31257     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=-96437.99 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.64e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.16e+05  |\n",
            "|    critic_loss     | 6.39e+06  |\n",
            "|    ent_coef        | 147       |\n",
            "|    ent_coef_loss   | -0.0605   |\n",
            "|    learning_rate   | 0.000996  |\n",
            "|    n_updates       | 41257     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.87e+07 |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 2334      |\n",
            "|    total_timesteps | 40000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-851019.75 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.51e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 8.82e+04  |\n",
            "|    critic_loss     | 5.35e+06  |\n",
            "|    ent_coef        | 132       |\n",
            "|    ent_coef_loss   | -0.118    |\n",
            "|    learning_rate   | 0.000995  |\n",
            "|    n_updates       | 51257     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-48613.79 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.86e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 7.88e+04  |\n",
            "|    critic_loss     | 4.52e+06  |\n",
            "|    ent_coef        | 118       |\n",
            "|    ent_coef_loss   | -1.26     |\n",
            "|    learning_rate   | 0.000994  |\n",
            "|    n_updates       | 61257     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.25e+07 |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 3487      |\n",
            "|    total_timesteps | 60000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-33522.11 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.35e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.56e+04  |\n",
            "|    critic_loss     | 2.77e+06  |\n",
            "|    ent_coef        | 81        |\n",
            "|    ent_coef_loss   | 0.787     |\n",
            "|    learning_rate   | 0.000993  |\n",
            "|    n_updates       | 71257     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=80000, episode_reward=-1763.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.76e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.54e+04  |\n",
            "|    critic_loss     | 2.33e+06  |\n",
            "|    ent_coef        | 55.9      |\n",
            "|    ent_coef_loss   | -0.87     |\n",
            "|    learning_rate   | 0.000992  |\n",
            "|    n_updates       | 81257     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -9.4e+06 |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 17       |\n",
            "|    time_elapsed    | 4642     |\n",
            "|    total_timesteps | 80000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-48470.35 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.85e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.9e+04   |\n",
            "|    critic_loss     | 1.53e+06  |\n",
            "|    ent_coef        | 39.3      |\n",
            "|    ent_coef_loss   | 0.155     |\n",
            "|    learning_rate   | 0.000991  |\n",
            "|    n_updates       | 91257     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-60267.75 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.03e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.59e+04  |\n",
            "|    critic_loss     | 7.08e+05  |\n",
            "|    ent_coef        | 33.6      |\n",
            "|    ent_coef_loss   | 0.706     |\n",
            "|    learning_rate   | 0.00099   |\n",
            "|    n_updates       | 101257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -7.54e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 5797      |\n",
            "|    total_timesteps | 100000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=-172597.54 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.73e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 110000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.75e+04  |\n",
            "|    critic_loss     | 1.88e+06  |\n",
            "|    ent_coef        | 30.7      |\n",
            "|    ent_coef_loss   | -0.4      |\n",
            "|    learning_rate   | 0.000989  |\n",
            "|    n_updates       | 111257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=-53304.25 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.33e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 120000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.73e+04  |\n",
            "|    critic_loss     | 1e+06     |\n",
            "|    ent_coef        | 25.7      |\n",
            "|    ent_coef_loss   | 0.669     |\n",
            "|    learning_rate   | 0.000988  |\n",
            "|    n_updates       | 121257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -6.3e+06 |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 17       |\n",
            "|    time_elapsed    | 6991     |\n",
            "|    total_timesteps | 120000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=-51111.16 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.11e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 130000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 5.24e+04  |\n",
            "|    critic_loss     | 1.74e+06  |\n",
            "|    ent_coef        | 24.2      |\n",
            "|    ent_coef_loss   | -1.03     |\n",
            "|    learning_rate   | 0.000987  |\n",
            "|    n_updates       | 131257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=-42702.26 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 140000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.97e+04  |\n",
            "|    critic_loss     | 1.3e+06   |\n",
            "|    ent_coef        | 19.6      |\n",
            "|    ent_coef_loss   | -0.44     |\n",
            "|    learning_rate   | 0.000986  |\n",
            "|    n_updates       | 141257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -5.4e+06 |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 17       |\n",
            "|    time_elapsed    | 8218     |\n",
            "|    total_timesteps | 140000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=-66270.66 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.63e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 150000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.4e+04   |\n",
            "|    critic_loss     | 2.64e+06  |\n",
            "|    ent_coef        | 18.3      |\n",
            "|    ent_coef_loss   | 0.725     |\n",
            "|    learning_rate   | 0.000985  |\n",
            "|    n_updates       | 151257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=-30825.68 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.08e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 160000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.54e+04  |\n",
            "|    critic_loss     | 9.71e+05  |\n",
            "|    ent_coef        | 18.9      |\n",
            "|    ent_coef_loss   | 0.833     |\n",
            "|    learning_rate   | 0.000984  |\n",
            "|    n_updates       | 161257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.74e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 32        |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 9423      |\n",
            "|    total_timesteps | 160000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=-75364.52 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 170000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.62e+04  |\n",
            "|    critic_loss     | 1.12e+06  |\n",
            "|    ent_coef        | 18.8      |\n",
            "|    ent_coef_loss   | -0.397    |\n",
            "|    learning_rate   | 0.000983  |\n",
            "|    n_updates       | 171257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=-100867.20 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.01e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 180000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.74e+04  |\n",
            "|    critic_loss     | 8.49e+05  |\n",
            "|    ent_coef        | 16.3      |\n",
            "|    ent_coef_loss   | 0.189     |\n",
            "|    learning_rate   | 0.000982  |\n",
            "|    n_updates       | 181257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.22e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 36        |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 10588     |\n",
            "|    total_timesteps | 180000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=-72690.85 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 190000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.78e+04  |\n",
            "|    critic_loss     | 7.34e+05  |\n",
            "|    ent_coef        | 16.4      |\n",
            "|    ent_coef_loss   | 0.791     |\n",
            "|    learning_rate   | 0.000981  |\n",
            "|    n_updates       | 191257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=-42052.46 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.21e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 200000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.31e+04  |\n",
            "|    critic_loss     | 7.6e+05   |\n",
            "|    ent_coef        | 16.7      |\n",
            "|    ent_coef_loss   | 0.912     |\n",
            "|    learning_rate   | 0.00098   |\n",
            "|    n_updates       | 201257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -3.8e+06 |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 17       |\n",
            "|    time_elapsed    | 11754    |\n",
            "|    total_timesteps | 200000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=-56708.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.67e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 210000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.34e+04  |\n",
            "|    critic_loss     | 8.82e+05  |\n",
            "|    ent_coef        | 14.8      |\n",
            "|    ent_coef_loss   | 0.0855    |\n",
            "|    learning_rate   | 0.000979  |\n",
            "|    n_updates       | 211257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=-66802.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.68e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 220000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.63e+04  |\n",
            "|    critic_loss     | 3.32e+05  |\n",
            "|    ent_coef        | 13.2      |\n",
            "|    ent_coef_loss   | 0.125     |\n",
            "|    learning_rate   | 0.000978  |\n",
            "|    n_updates       | 221257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.46e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 44        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 12922     |\n",
            "|    total_timesteps | 220000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=-43263.49 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.33e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 230000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.97e+04  |\n",
            "|    critic_loss     | 7.01e+05  |\n",
            "|    ent_coef        | 12.4      |\n",
            "|    ent_coef_loss   | 0.634     |\n",
            "|    learning_rate   | 0.000977  |\n",
            "|    n_updates       | 231257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=-69010.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -6.9e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 240000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.14e+04 |\n",
            "|    critic_loss     | 3.32e+06 |\n",
            "|    ent_coef        | 13       |\n",
            "|    ent_coef_loss   | -0.852   |\n",
            "|    learning_rate   | 0.000976 |\n",
            "|    n_updates       | 241257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.18e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 48        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 14113     |\n",
            "|    total_timesteps | 240000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=-41456.18 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.15e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 250000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.23e+04  |\n",
            "|    critic_loss     | 7.61e+05  |\n",
            "|    ent_coef        | 12.8      |\n",
            "|    ent_coef_loss   | -1.09     |\n",
            "|    learning_rate   | 0.000975  |\n",
            "|    n_updates       | 251257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=-16584.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.66e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 260000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.57e+04  |\n",
            "|    critic_loss     | 6.41e+05  |\n",
            "|    ent_coef        | 13.2      |\n",
            "|    ent_coef_loss   | -0.0138   |\n",
            "|    learning_rate   | 0.000974  |\n",
            "|    n_updates       | 261257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.94e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 52        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 15290     |\n",
            "|    total_timesteps | 260000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=-17130.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.71e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 270000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.29e+04  |\n",
            "|    critic_loss     | 5.99e+05  |\n",
            "|    ent_coef        | 11.2      |\n",
            "|    ent_coef_loss   | 0.651     |\n",
            "|    learning_rate   | 0.000973  |\n",
            "|    n_updates       | 271257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=-16232.44 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.62e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 280000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.94e+04  |\n",
            "|    critic_loss     | 1.39e+06  |\n",
            "|    ent_coef        | 11.9      |\n",
            "|    ent_coef_loss   | -0.198    |\n",
            "|    learning_rate   | 0.000972  |\n",
            "|    n_updates       | 281257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.73e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 56        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 16467     |\n",
            "|    total_timesteps | 280000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=-38954.77 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -3.9e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 290000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.86e+04 |\n",
            "|    critic_loss     | 8.28e+05 |\n",
            "|    ent_coef        | 11.5     |\n",
            "|    ent_coef_loss   | -0.607   |\n",
            "|    learning_rate   | 0.000971 |\n",
            "|    n_updates       | 291257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=-86296.05 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.63e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 300000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.28e+04  |\n",
            "|    critic_loss     | 5.18e+05  |\n",
            "|    ent_coef        | 11.6      |\n",
            "|    ent_coef_loss   | 0.98      |\n",
            "|    learning_rate   | 0.00097   |\n",
            "|    n_updates       | 301257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.55e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 60        |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 17653     |\n",
            "|    total_timesteps | 300000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=-40709.81 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.07e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 310000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.06e+04  |\n",
            "|    critic_loss     | 6.01e+05  |\n",
            "|    ent_coef        | 11.2      |\n",
            "|    ent_coef_loss   | -0.432    |\n",
            "|    learning_rate   | 0.000969  |\n",
            "|    n_updates       | 311257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=-90552.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.06e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 320000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.35e+04  |\n",
            "|    critic_loss     | 3.49e+05  |\n",
            "|    ent_coef        | 11.2      |\n",
            "|    ent_coef_loss   | 0.318     |\n",
            "|    learning_rate   | 0.000968  |\n",
            "|    n_updates       | 321257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.4e+06 |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 17       |\n",
            "|    time_elapsed    | 18812    |\n",
            "|    total_timesteps | 320000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=-69006.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -6.9e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 330000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.42e+04 |\n",
            "|    critic_loss     | 3.31e+05 |\n",
            "|    ent_coef        | 11.7     |\n",
            "|    ent_coef_loss   | -0.00988 |\n",
            "|    learning_rate   | 0.000967 |\n",
            "|    n_updates       | 331257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=-123067.66 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.23e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 340000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.48e+04  |\n",
            "|    critic_loss     | 1.37e+07  |\n",
            "|    ent_coef        | 10.8      |\n",
            "|    ent_coef_loss   | -0.802    |\n",
            "|    learning_rate   | 0.000966  |\n",
            "|    n_updates       | 341257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.26e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 68        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 19974     |\n",
            "|    total_timesteps | 340000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=-31685.67 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.17e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 350000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.73e+04  |\n",
            "|    critic_loss     | 4.33e+06  |\n",
            "|    ent_coef        | 12.1      |\n",
            "|    ent_coef_loss   | -0.1      |\n",
            "|    learning_rate   | 0.000965  |\n",
            "|    n_updates       | 351257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=-24163.51 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.42e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 360000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.21e+04  |\n",
            "|    critic_loss     | 1.33e+06  |\n",
            "|    ent_coef        | 11.3      |\n",
            "|    ent_coef_loss   | 0.15      |\n",
            "|    learning_rate   | 0.000964  |\n",
            "|    n_updates       | 361257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.13e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 72        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 21127     |\n",
            "|    total_timesteps | 360000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=-8753.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.75e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 370000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.87e+04  |\n",
            "|    critic_loss     | 4.09e+05  |\n",
            "|    ent_coef        | 10.7      |\n",
            "|    ent_coef_loss   | 0.704     |\n",
            "|    learning_rate   | 0.000963  |\n",
            "|    n_updates       | 371257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=-11015.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.1e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 380000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.22e+04 |\n",
            "|    critic_loss     | 3.3e+05  |\n",
            "|    ent_coef        | 11.1     |\n",
            "|    ent_coef_loss   | 0.259    |\n",
            "|    learning_rate   | 0.000962 |\n",
            "|    n_updates       | 381257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.02e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 76        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 22315     |\n",
            "|    total_timesteps | 380000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=-11002.40 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.1e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 390000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.89e+04 |\n",
            "|    critic_loss     | 1.43e+06 |\n",
            "|    ent_coef        | 9.83     |\n",
            "|    ent_coef_loss   | -0.0952  |\n",
            "|    learning_rate   | 0.000961 |\n",
            "|    n_updates       | 391257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=-17278.45 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.73e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 400000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.65e+04  |\n",
            "|    critic_loss     | 3.9e+05   |\n",
            "|    ent_coef        | 9.47      |\n",
            "|    ent_coef_loss   | 0.704     |\n",
            "|    learning_rate   | 0.00096   |\n",
            "|    n_updates       | 401257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.92e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 80        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 23474     |\n",
            "|    total_timesteps | 400000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=-53827.82 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 410000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.1e+04   |\n",
            "|    critic_loss     | 2.11e+06  |\n",
            "|    ent_coef        | 8.19      |\n",
            "|    ent_coef_loss   | -0.883    |\n",
            "|    learning_rate   | 0.000959  |\n",
            "|    n_updates       | 411257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=-41184.43 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.12e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 420000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.45e+04  |\n",
            "|    critic_loss     | 7.11e+05  |\n",
            "|    ent_coef        | 9.24      |\n",
            "|    ent_coef_loss   | -0.594    |\n",
            "|    learning_rate   | 0.000958  |\n",
            "|    n_updates       | 421257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.83e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 84        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 24630     |\n",
            "|    total_timesteps | 420000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=-29329.53 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.93e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 430000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.77e+04  |\n",
            "|    critic_loss     | 5.45e+05  |\n",
            "|    ent_coef        | 7.48      |\n",
            "|    ent_coef_loss   | 0.567     |\n",
            "|    learning_rate   | 0.000957  |\n",
            "|    n_updates       | 431257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=-19610.08 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.96e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 440000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.05e+04  |\n",
            "|    critic_loss     | 1.08e+06  |\n",
            "|    ent_coef        | 8.88      |\n",
            "|    ent_coef_loss   | -0.184    |\n",
            "|    learning_rate   | 0.000956  |\n",
            "|    n_updates       | 441257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.75e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 88        |\n",
            "|    fps             | 17        |\n",
            "|    time_elapsed    | 25812     |\n",
            "|    total_timesteps | 440000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=-182292.60 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.82e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 450000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.1e+04   |\n",
            "|    critic_loss     | 3.98e+05  |\n",
            "|    ent_coef        | 10.2      |\n",
            "|    ent_coef_loss   | 0.599     |\n",
            "|    learning_rate   | 0.000955  |\n",
            "|    n_updates       | 451257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=-184592.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.85e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 460000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.4e+04   |\n",
            "|    critic_loss     | 4.05e+05  |\n",
            "|    ent_coef        | 9.7       |\n",
            "|    ent_coef_loss   | 0.0765    |\n",
            "|    learning_rate   | 0.000954  |\n",
            "|    n_updates       | 461257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.68e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 92        |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 27121     |\n",
            "|    total_timesteps | 460000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=-173735.08 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.74e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 470000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.25e+04  |\n",
            "|    critic_loss     | 2.55e+05  |\n",
            "|    ent_coef        | 9.93      |\n",
            "|    ent_coef_loss   | 0.482     |\n",
            "|    learning_rate   | 0.000953  |\n",
            "|    n_updates       | 471257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=-9997.67 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1e+04   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 480000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.6e+04  |\n",
            "|    critic_loss     | 1.93e+05 |\n",
            "|    ent_coef        | 8.1      |\n",
            "|    ent_coef_loss   | 0.495    |\n",
            "|    learning_rate   | 0.000952 |\n",
            "|    n_updates       | 481257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.61e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 96        |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 28430     |\n",
            "|    total_timesteps | 480000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=-7859.31 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.86e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 490000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.5e+04   |\n",
            "|    critic_loss     | 6.34e+05  |\n",
            "|    ent_coef        | 6.33      |\n",
            "|    ent_coef_loss   | 0.502     |\n",
            "|    learning_rate   | 0.000951  |\n",
            "|    n_updates       | 491257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=-14783.02 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 500000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.77e+04  |\n",
            "|    critic_loss     | 2.45e+05  |\n",
            "|    ent_coef        | 6.49      |\n",
            "|    ent_coef_loss   | -0.701    |\n",
            "|    learning_rate   | 0.00095   |\n",
            "|    n_updates       | 501257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.55e+06 |\n",
            "| time/              |           |\n",
            "|    episodes        | 100       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 29755     |\n",
            "|    total_timesteps | 500000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=-22822.70 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.28e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 510000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.99e+04  |\n",
            "|    critic_loss     | 4.18e+05  |\n",
            "|    ent_coef        | 8.96      |\n",
            "|    ent_coef_loss   | -0.285    |\n",
            "|    learning_rate   | 0.000949  |\n",
            "|    n_updates       | 511257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=-9428.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 520000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.32e+03  |\n",
            "|    critic_loss     | 4.01e+05  |\n",
            "|    ent_coef        | 10.4      |\n",
            "|    ent_coef_loss   | 0.601     |\n",
            "|    learning_rate   | 0.000948  |\n",
            "|    n_updates       | 521257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.85e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 104       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 31035     |\n",
            "|    total_timesteps | 520000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=-112369.99 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.12e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 530000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.09e+04  |\n",
            "|    critic_loss     | 5.29e+05  |\n",
            "|    ent_coef        | 11.7      |\n",
            "|    ent_coef_loss   | 0.337     |\n",
            "|    learning_rate   | 0.000947  |\n",
            "|    n_updates       | 531257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=-3573.47 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.57e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 540000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.29e+04  |\n",
            "|    critic_loss     | 5.9e+05   |\n",
            "|    ent_coef        | 7.47      |\n",
            "|    ent_coef_loss   | 0.131     |\n",
            "|    learning_rate   | 0.000946  |\n",
            "|    n_updates       | 541257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.69e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 108       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 32299     |\n",
            "|    total_timesteps | 540000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=-75341.37 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.53e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 550000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.08e+04  |\n",
            "|    critic_loss     | 5.63e+05  |\n",
            "|    ent_coef        | 8.77      |\n",
            "|    ent_coef_loss   | 0.113     |\n",
            "|    learning_rate   | 0.000945  |\n",
            "|    n_updates       | 551257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=-11277.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.13e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 560000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.56e+04  |\n",
            "|    critic_loss     | 8.31e+05  |\n",
            "|    ent_coef        | 7.32      |\n",
            "|    ent_coef_loss   | -0.276    |\n",
            "|    learning_rate   | 0.000944  |\n",
            "|    n_updates       | 561257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.33e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 112       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 33565     |\n",
            "|    total_timesteps | 560000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=-9125.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 570000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.55e+04  |\n",
            "|    critic_loss     | 6.05e+05  |\n",
            "|    ent_coef        | 7.46      |\n",
            "|    ent_coef_loss   | -0.191    |\n",
            "|    learning_rate   | 0.000943  |\n",
            "|    n_updates       | 571257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=-74963.47 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -7.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 580000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.87e+04 |\n",
            "|    critic_loss     | 6.06e+05 |\n",
            "|    ent_coef        | 7.44     |\n",
            "|    ent_coef_loss   | -0.0418  |\n",
            "|    learning_rate   | 0.000942 |\n",
            "|    n_updates       | 581257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.32e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 116       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 34827     |\n",
            "|    total_timesteps | 580000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=-6306.97 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 590000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.55e+04  |\n",
            "|    critic_loss     | 2.71e+05  |\n",
            "|    ent_coef        | 6.87      |\n",
            "|    ent_coef_loss   | 0.577     |\n",
            "|    learning_rate   | 0.000941  |\n",
            "|    n_updates       | 591257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=-12396.96 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.24e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 600000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.5e+04   |\n",
            "|    critic_loss     | 2.35e+05  |\n",
            "|    ent_coef        | 6.61      |\n",
            "|    ent_coef_loss   | -0.169    |\n",
            "|    learning_rate   | 0.00094   |\n",
            "|    n_updates       | 601257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.88e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 120       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 36087     |\n",
            "|    total_timesteps | 600000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=-5579.74 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 610000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.42e+04  |\n",
            "|    critic_loss     | 6.07e+05  |\n",
            "|    ent_coef        | 7.53      |\n",
            "|    ent_coef_loss   | 0.764     |\n",
            "|    learning_rate   | 0.000939  |\n",
            "|    n_updates       | 611257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=-419535.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -4.2e+05 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 620000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.35e+04 |\n",
            "|    critic_loss     | 2.94e+05 |\n",
            "|    ent_coef        | 10.6     |\n",
            "|    ent_coef_loss   | -0.611   |\n",
            "|    learning_rate   | 0.000938 |\n",
            "|    n_updates       | 621257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.03e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 124       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 37354     |\n",
            "|    total_timesteps | 620000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=-172940.23 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.73e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 630000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.63e+04  |\n",
            "|    critic_loss     | 5.08e+05  |\n",
            "|    ent_coef        | 8.99      |\n",
            "|    ent_coef_loss   | -0.0235   |\n",
            "|    learning_rate   | 0.000937  |\n",
            "|    n_updates       | 631257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=-5921.02 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.92e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 640000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 8.81e+03  |\n",
            "|    critic_loss     | 5.32e+05  |\n",
            "|    ent_coef        | 7.84      |\n",
            "|    ent_coef_loss   | 0.581     |\n",
            "|    learning_rate   | 0.000936  |\n",
            "|    n_updates       | 641257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.61e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 128       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 38627     |\n",
            "|    total_timesteps | 640000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=-9312.09 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 650000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.3e+04   |\n",
            "|    critic_loss     | 5.84e+05  |\n",
            "|    ent_coef        | 7.44      |\n",
            "|    ent_coef_loss   | -0.676    |\n",
            "|    learning_rate   | 0.000935  |\n",
            "|    n_updates       | 651257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=-21312.11 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.13e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 660000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.57e+04  |\n",
            "|    critic_loss     | 2.03e+05  |\n",
            "|    ent_coef        | 7.29      |\n",
            "|    ent_coef_loss   | 0.199     |\n",
            "|    learning_rate   | 0.000934  |\n",
            "|    n_updates       | 661257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.38e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 132       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 39929     |\n",
            "|    total_timesteps | 660000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=-65116.64 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.51e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 670000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.92e+03  |\n",
            "|    critic_loss     | 2.61e+05  |\n",
            "|    ent_coef        | 8.43      |\n",
            "|    ent_coef_loss   | -0.275    |\n",
            "|    learning_rate   | 0.000933  |\n",
            "|    n_updates       | 671257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=-124993.53 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.25e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 680000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.1e+03   |\n",
            "|    critic_loss     | 3.03e+05  |\n",
            "|    ent_coef        | 9.05      |\n",
            "|    ent_coef_loss   | 0.909     |\n",
            "|    learning_rate   | 0.000932  |\n",
            "|    n_updates       | 681257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.25e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 136       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 41262     |\n",
            "|    total_timesteps | 680000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=-13213.02 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.32e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 690000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 7.67e+03  |\n",
            "|    critic_loss     | 1.38e+05  |\n",
            "|    ent_coef        | 7.32      |\n",
            "|    ent_coef_loss   | -0.392    |\n",
            "|    learning_rate   | 0.000931  |\n",
            "|    n_updates       | 691257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=-295426.96 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.95e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 700000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.49e+04  |\n",
            "|    critic_loss     | 2.58e+05  |\n",
            "|    ent_coef        | 9.01      |\n",
            "|    ent_coef_loss   | -0.572    |\n",
            "|    learning_rate   | 0.00093   |\n",
            "|    n_updates       | 701257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -5.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 42544    |\n",
            "|    total_timesteps | 700000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=-65928.45 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 710000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.05e+04  |\n",
            "|    critic_loss     | 3.76e+05  |\n",
            "|    ent_coef        | 8.43      |\n",
            "|    ent_coef_loss   | 0.275     |\n",
            "|    learning_rate   | 0.000929  |\n",
            "|    n_updates       | 711257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=-20276.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.03e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 720000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.08e+04  |\n",
            "|    critic_loss     | 5.33e+05  |\n",
            "|    ent_coef        | 7.89      |\n",
            "|    ent_coef_loss   | 0.108     |\n",
            "|    learning_rate   | 0.000928  |\n",
            "|    n_updates       | 721257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.29e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 144       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 43829     |\n",
            "|    total_timesteps | 720000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=-21055.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.11e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 730000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.41e+04  |\n",
            "|    critic_loss     | 1.81e+05  |\n",
            "|    ent_coef        | 8.18      |\n",
            "|    ent_coef_loss   | 0.126     |\n",
            "|    learning_rate   | 0.000927  |\n",
            "|    n_updates       | 731257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=-25382.05 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 740000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 6.25e+03  |\n",
            "|    critic_loss     | 2.08e+05  |\n",
            "|    ent_coef        | 8.32      |\n",
            "|    ent_coef_loss   | -0.437    |\n",
            "|    learning_rate   | 0.000926  |\n",
            "|    n_updates       | 741257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.14e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 148       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 45109     |\n",
            "|    total_timesteps | 740000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=-26479.11 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.65e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 750000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.25e+04  |\n",
            "|    critic_loss     | 2.88e+05  |\n",
            "|    ent_coef        | 7.08      |\n",
            "|    ent_coef_loss   | -0.284    |\n",
            "|    learning_rate   | 0.000925  |\n",
            "|    n_updates       | 751257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=-42741.74 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 760000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.15e+04  |\n",
            "|    critic_loss     | 8.69e+05  |\n",
            "|    ent_coef        | 7.22      |\n",
            "|    ent_coef_loss   | 0.249     |\n",
            "|    learning_rate   | 0.000924  |\n",
            "|    n_updates       | 761257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.15e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 152       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 46376     |\n",
            "|    total_timesteps | 760000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=-14944.34 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.49e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 770000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.17e+04  |\n",
            "|    critic_loss     | 4.36e+05  |\n",
            "|    ent_coef        | 7.67      |\n",
            "|    ent_coef_loss   | -0.569    |\n",
            "|    learning_rate   | 0.000923  |\n",
            "|    n_updates       | 771257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=-29925.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.99e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 780000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.29e+04  |\n",
            "|    critic_loss     | 4.25e+05  |\n",
            "|    ent_coef        | 6.36      |\n",
            "|    ent_coef_loss   | -0.136    |\n",
            "|    learning_rate   | 0.000922  |\n",
            "|    n_updates       | 781257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.09e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 156       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 47635     |\n",
            "|    total_timesteps | 780000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=-73853.96 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.39e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 790000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.49e+04  |\n",
            "|    critic_loss     | 5.37e+05  |\n",
            "|    ent_coef        | 5.85      |\n",
            "|    ent_coef_loss   | 0.161     |\n",
            "|    learning_rate   | 0.000921  |\n",
            "|    n_updates       | 791257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=-325330.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.25e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 800000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.08e+04  |\n",
            "|    critic_loss     | 5.17e+05  |\n",
            "|    ent_coef        | 7.35      |\n",
            "|    ent_coef_loss   | 1.05      |\n",
            "|    learning_rate   | 0.00092   |\n",
            "|    n_updates       | 801257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.42e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 160       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 48896     |\n",
            "|    total_timesteps | 800000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=-374945.16 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.75e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 810000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 7.8e+03   |\n",
            "|    critic_loss     | 2.07e+05  |\n",
            "|    ent_coef        | 9.83      |\n",
            "|    ent_coef_loss   | -0.389    |\n",
            "|    learning_rate   | 0.000919  |\n",
            "|    n_updates       | 811257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=-8956.24 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.96e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 820000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.34e+04  |\n",
            "|    critic_loss     | 6.91e+05  |\n",
            "|    ent_coef        | 9.24      |\n",
            "|    ent_coef_loss   | -0.821    |\n",
            "|    learning_rate   | 0.000918  |\n",
            "|    n_updates       | 821257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.76e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 164       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 50155     |\n",
            "|    total_timesteps | 820000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=-7198.77 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -7.2e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 830000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.05e+04 |\n",
            "|    critic_loss     | 6.86e+05 |\n",
            "|    ent_coef        | 6.39     |\n",
            "|    ent_coef_loss   | -0.0129  |\n",
            "|    learning_rate   | 0.000917 |\n",
            "|    n_updates       | 831257   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=-10422.85 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 840000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 8.26e+03  |\n",
            "|    critic_loss     | 2.55e+05  |\n",
            "|    ent_coef        | 13        |\n",
            "|    ent_coef_loss   | 0.873     |\n",
            "|    learning_rate   | 0.000916  |\n",
            "|    n_updates       | 841257    |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -5.7e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 51422    |\n",
            "|    total_timesteps | 840000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=-7214.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 850000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.16e+04  |\n",
            "|    critic_loss     | 1.69e+05  |\n",
            "|    ent_coef        | 7.24      |\n",
            "|    ent_coef_loss   | -0.683    |\n",
            "|    learning_rate   | 0.000915  |\n",
            "|    n_updates       | 851257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=-4188.35 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.19e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 860000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.37e+04  |\n",
            "|    critic_loss     | 2.03e+05  |\n",
            "|    ent_coef        | 6.11      |\n",
            "|    ent_coef_loss   | 0.0205    |\n",
            "|    learning_rate   | 0.000914  |\n",
            "|    n_updates       | 861257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.56e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 172       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 52698     |\n",
            "|    total_timesteps | 860000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=-8550.97 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 870000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.45e+04  |\n",
            "|    critic_loss     | 3.47e+05  |\n",
            "|    ent_coef        | 5.02      |\n",
            "|    ent_coef_loss   | -0.297    |\n",
            "|    learning_rate   | 0.000913  |\n",
            "|    n_updates       | 871257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=-2567.60 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.57e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 880000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.1e+04   |\n",
            "|    critic_loss     | 4.31e+05  |\n",
            "|    ent_coef        | 6.12      |\n",
            "|    ent_coef_loss   | 0.116     |\n",
            "|    learning_rate   | 0.000912  |\n",
            "|    n_updates       | 881257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.55e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 176       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 53976     |\n",
            "|    total_timesteps | 880000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=-15573.08 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.56e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 890000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.28e+03  |\n",
            "|    critic_loss     | 1.68e+05  |\n",
            "|    ent_coef        | 5.36      |\n",
            "|    ent_coef_loss   | -0.213    |\n",
            "|    learning_rate   | 0.000911  |\n",
            "|    n_updates       | 891257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=-14311.67 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 900000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.36e+04  |\n",
            "|    critic_loss     | 4.76e+05  |\n",
            "|    ent_coef        | 4.88      |\n",
            "|    ent_coef_loss   | 0.0775    |\n",
            "|    learning_rate   | 0.00091   |\n",
            "|    n_updates       | 901257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.73e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 180       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 55265     |\n",
            "|    total_timesteps | 900000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=-86239.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.62e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 910000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.8e+03   |\n",
            "|    critic_loss     | 1.96e+05  |\n",
            "|    ent_coef        | 4.55      |\n",
            "|    ent_coef_loss   | 0.213     |\n",
            "|    learning_rate   | 0.000909  |\n",
            "|    n_updates       | 911257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=-27368.10 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.74e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 920000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.23e+04  |\n",
            "|    critic_loss     | 1.57e+05  |\n",
            "|    ent_coef        | 5.4       |\n",
            "|    ent_coef_loss   | 0.0462    |\n",
            "|    learning_rate   | 0.000908  |\n",
            "|    n_updates       | 921257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.74e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 184       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 56585     |\n",
            "|    total_timesteps | 920000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=-16090.75 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 930000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.74e+04  |\n",
            "|    critic_loss     | 1.86e+05  |\n",
            "|    ent_coef        | 4.72      |\n",
            "|    ent_coef_loss   | -0.756    |\n",
            "|    learning_rate   | 0.000907  |\n",
            "|    n_updates       | 931257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=-11823.62 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.18e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 940000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 7.66e+03  |\n",
            "|    critic_loss     | 2.78e+05  |\n",
            "|    ent_coef        | 4.7       |\n",
            "|    ent_coef_loss   | 0.0261    |\n",
            "|    learning_rate   | 0.000906  |\n",
            "|    n_updates       | 941257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.99e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 188       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 57863     |\n",
            "|    total_timesteps | 940000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=-25219.24 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.52e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 950000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 9.9e+03   |\n",
            "|    critic_loss     | 3.7e+05   |\n",
            "|    ent_coef        | 4.8       |\n",
            "|    ent_coef_loss   | 0.326     |\n",
            "|    learning_rate   | 0.000905  |\n",
            "|    n_updates       | 951257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=-26838.92 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.68e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 960000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.55e+04  |\n",
            "|    critic_loss     | 2.25e+05  |\n",
            "|    ent_coef        | 4.68      |\n",
            "|    ent_coef_loss   | -0.176    |\n",
            "|    learning_rate   | 0.000904  |\n",
            "|    n_updates       | 961257    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -6.03e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 192       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 59132     |\n",
            "|    total_timesteps | 960000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=-30425.82 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 970000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.46e+04  |\n",
            "|    critic_loss     | 2.32e+05  |\n",
            "|    ent_coef        | 5.17      |\n",
            "|    ent_coef_loss   | -0.241    |\n",
            "|    learning_rate   | 0.000903  |\n",
            "|    n_updates       | 971257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=-25003.53 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -2.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 980000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.08e+04 |\n",
            "|    critic_loss     | 3.33e+05 |\n",
            "|    ent_coef        | 4.87     |\n",
            "|    ent_coef_loss   | 0.0372   |\n",
            "|    learning_rate   | 0.000902 |\n",
            "|    n_updates       | 981257   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.94e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 196       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 60414     |\n",
            "|    total_timesteps | 980000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=-11105.73 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.11e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 990000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.34e+04  |\n",
            "|    critic_loss     | 2.95e+05  |\n",
            "|    ent_coef        | 4.75      |\n",
            "|    ent_coef_loss   | -0.0837   |\n",
            "|    learning_rate   | 0.000901  |\n",
            "|    n_updates       | 991257    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=-12789.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.28e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.05e+04  |\n",
            "|    critic_loss     | 1.67e+05  |\n",
            "|    ent_coef        | 4.99      |\n",
            "|    ent_coef_loss   | -0.188    |\n",
            "|    learning_rate   | 0.0009    |\n",
            "|    n_updates       | 1001257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.88e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 200       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 61675     |\n",
            "|    total_timesteps | 1000000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1010000, episode_reward=-21772.97 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.18e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1010000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.76e+03  |\n",
            "|    critic_loss     | 5.63e+04  |\n",
            "|    ent_coef        | 4.66      |\n",
            "|    ent_coef_loss   | 0.514     |\n",
            "|    learning_rate   | 0.000899  |\n",
            "|    n_updates       | 1011257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1020000, episode_reward=-63905.12 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.39e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1020000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.71e+03  |\n",
            "|    critic_loss     | 5.24e+04  |\n",
            "|    ent_coef        | 4.34      |\n",
            "|    ent_coef_loss   | -0.434    |\n",
            "|    learning_rate   | 0.000898  |\n",
            "|    n_updates       | 1021257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.89e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 204       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 62940     |\n",
            "|    total_timesteps | 1020000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1030000, episode_reward=-17716.63 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.77e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1030000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 4.31e+03  |\n",
            "|    critic_loss     | 5.47e+04  |\n",
            "|    ent_coef        | 3.87      |\n",
            "|    ent_coef_loss   | -0.618    |\n",
            "|    learning_rate   | 0.000897  |\n",
            "|    n_updates       | 1031257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1040000, episode_reward=-20030.79 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -2e+04   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1040000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 4.13e+03 |\n",
            "|    critic_loss     | 4.55e+04 |\n",
            "|    ent_coef        | 3.71     |\n",
            "|    ent_coef_loss   | -0.12    |\n",
            "|    learning_rate   | 0.000896 |\n",
            "|    n_updates       | 1041257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.51e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 208       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 64207     |\n",
            "|    total_timesteps | 1040000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1050000, episode_reward=-13415.54 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.34e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1050000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.98e+03  |\n",
            "|    critic_loss     | 4.3e+04   |\n",
            "|    ent_coef        | 3.27      |\n",
            "|    ent_coef_loss   | 0.65      |\n",
            "|    learning_rate   | 0.000895  |\n",
            "|    n_updates       | 1051257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1060000, episode_reward=-9757.84 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.76e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1060000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.89e+03  |\n",
            "|    critic_loss     | 6.95e+04  |\n",
            "|    ent_coef        | 3.13      |\n",
            "|    ent_coef_loss   | -0.0342   |\n",
            "|    learning_rate   | 0.000894  |\n",
            "|    n_updates       | 1061257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.42e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 212       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 65479     |\n",
            "|    total_timesteps | 1060000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1070000, episode_reward=-10618.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.06e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1070000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.72e+03  |\n",
            "|    critic_loss     | 3.55e+04  |\n",
            "|    ent_coef        | 3.43      |\n",
            "|    ent_coef_loss   | -0.101    |\n",
            "|    learning_rate   | 0.000893  |\n",
            "|    n_updates       | 1071257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1080000, episode_reward=-12033.21 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.2e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1080000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.68e+03 |\n",
            "|    critic_loss     | 8.31e+04 |\n",
            "|    ent_coef        | 2.91     |\n",
            "|    ent_coef_loss   | -0.0026  |\n",
            "|    learning_rate   | 0.000892 |\n",
            "|    n_updates       | 1081257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.29e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 216       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 66763     |\n",
            "|    total_timesteps | 1080000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1090000, episode_reward=-15092.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.51e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1090000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.46e+03  |\n",
            "|    critic_loss     | 4.15e+04  |\n",
            "|    ent_coef        | 3.21      |\n",
            "|    ent_coef_loss   | -0.00688  |\n",
            "|    learning_rate   | 0.000891  |\n",
            "|    n_updates       | 1091257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1100000, episode_reward=-16888.74 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.69e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1100000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.35e+03  |\n",
            "|    critic_loss     | 5.3e+04   |\n",
            "|    ent_coef        | 3.02      |\n",
            "|    ent_coef_loss   | 0.438     |\n",
            "|    learning_rate   | 0.00089   |\n",
            "|    n_updates       | 1101257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.31e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 220       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 68052     |\n",
            "|    total_timesteps | 1100000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1110000, episode_reward=-14688.77 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.47e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1110000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.22e+03  |\n",
            "|    critic_loss     | 8.28e+04  |\n",
            "|    ent_coef        | 2.77      |\n",
            "|    ent_coef_loss   | -0.541    |\n",
            "|    learning_rate   | 0.000889  |\n",
            "|    n_updates       | 1111257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1120000, episode_reward=-9830.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1120000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.06e+03  |\n",
            "|    critic_loss     | 4.54e+05  |\n",
            "|    ent_coef        | 3.11      |\n",
            "|    ent_coef_loss   | 0.0143    |\n",
            "|    learning_rate   | 0.000888  |\n",
            "|    n_updates       | 1121257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -5.02e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 224       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 69347     |\n",
            "|    total_timesteps | 1120000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1130000, episode_reward=-56021.92 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -5.6e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1130000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.29e+03 |\n",
            "|    critic_loss     | 1.77e+05 |\n",
            "|    ent_coef        | 2.58     |\n",
            "|    ent_coef_loss   | -0.293   |\n",
            "|    learning_rate   | 0.000887 |\n",
            "|    n_updates       | 1131257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1140000, episode_reward=-164958.20 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.65e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1140000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.26e+03  |\n",
            "|    critic_loss     | 8.6e+04   |\n",
            "|    ent_coef        | 2.7       |\n",
            "|    ent_coef_loss   | -0.366    |\n",
            "|    learning_rate   | 0.000886  |\n",
            "|    n_updates       | 1141257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -4.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 228      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 70639    |\n",
            "|    total_timesteps | 1140000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1150000, episode_reward=-14308.63 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1150000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.33e+03  |\n",
            "|    critic_loss     | 3.71e+05  |\n",
            "|    ent_coef        | 2.74      |\n",
            "|    ent_coef_loss   | 0.0269    |\n",
            "|    learning_rate   | 0.000885  |\n",
            "|    n_updates       | 1151257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1160000, episode_reward=-11566.03 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.16e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1160000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.38e+03  |\n",
            "|    critic_loss     | 1.55e+05  |\n",
            "|    ent_coef        | 2.42      |\n",
            "|    ent_coef_loss   | 0.124     |\n",
            "|    learning_rate   | 0.000884  |\n",
            "|    n_updates       | 1161257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.36e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 232       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 71925     |\n",
            "|    total_timesteps | 1160000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1170000, episode_reward=-20174.49 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.02e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1170000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.16e+03  |\n",
            "|    critic_loss     | 2.69e+04  |\n",
            "|    ent_coef        | 2.31      |\n",
            "|    ent_coef_loss   | 0.115     |\n",
            "|    learning_rate   | 0.000883  |\n",
            "|    n_updates       | 1171257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1180000, episode_reward=-15020.93 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1180000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.43e+03 |\n",
            "|    critic_loss     | 4.17e+04 |\n",
            "|    ent_coef        | 2.36     |\n",
            "|    ent_coef_loss   | -0.268   |\n",
            "|    learning_rate   | 0.000882 |\n",
            "|    n_updates       | 1181257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.35e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 236       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 73205     |\n",
            "|    total_timesteps | 1180000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1190000, episode_reward=-14777.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1190000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.57e+03  |\n",
            "|    critic_loss     | 7.74e+04  |\n",
            "|    ent_coef        | 2.24      |\n",
            "|    ent_coef_loss   | 0.0162    |\n",
            "|    learning_rate   | 0.000881  |\n",
            "|    n_updates       | 1191257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1200000, episode_reward=-4910.56 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.91e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1200000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.22e+03  |\n",
            "|    critic_loss     | 4.17e+04  |\n",
            "|    ent_coef        | 2.29      |\n",
            "|    ent_coef_loss   | -0.109    |\n",
            "|    learning_rate   | 0.00088   |\n",
            "|    n_updates       | 1201257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -4.09e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 240       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 74482     |\n",
            "|    total_timesteps | 1200000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1210000, episode_reward=-15562.70 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.56e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1210000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.09e+03  |\n",
            "|    critic_loss     | 1.42e+05  |\n",
            "|    ent_coef        | 2.43      |\n",
            "|    ent_coef_loss   | -0.391    |\n",
            "|    learning_rate   | 0.000879  |\n",
            "|    n_updates       | 1211257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1220000, episode_reward=-15864.86 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1220000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 3.13e+03  |\n",
            "|    critic_loss     | 3.53e+04  |\n",
            "|    ent_coef        | 2.56      |\n",
            "|    ent_coef_loss   | -0.102    |\n",
            "|    learning_rate   | 0.000878  |\n",
            "|    n_updates       | 1221257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.96e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 244       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 75761     |\n",
            "|    total_timesteps | 1220000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1230000, episode_reward=-13763.34 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1230000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.72e+03  |\n",
            "|    critic_loss     | 3.03e+04  |\n",
            "|    ent_coef        | 2.04      |\n",
            "|    ent_coef_loss   | 0.0431    |\n",
            "|    learning_rate   | 0.000877  |\n",
            "|    n_updates       | 1231257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1240000, episode_reward=-109418.09 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.09e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1240000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.79e+03  |\n",
            "|    critic_loss     | 1.94e+05  |\n",
            "|    ent_coef        | 2.48      |\n",
            "|    ent_coef_loss   | -0.136    |\n",
            "|    learning_rate   | 0.000876  |\n",
            "|    n_updates       | 1241257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.78e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 248       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 76991     |\n",
            "|    total_timesteps | 1240000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1250000, episode_reward=-23614.33 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.36e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1250000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.76e+03  |\n",
            "|    critic_loss     | 2.96e+04  |\n",
            "|    ent_coef        | 2.28      |\n",
            "|    ent_coef_loss   | 0.0033    |\n",
            "|    learning_rate   | 0.000875  |\n",
            "|    n_updates       | 1251257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1260000, episode_reward=-14901.58 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.49e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1260000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.61e+03  |\n",
            "|    critic_loss     | 1.09e+05  |\n",
            "|    ent_coef        | 1.9       |\n",
            "|    ent_coef_loss   | 0.00523   |\n",
            "|    learning_rate   | 0.000874  |\n",
            "|    n_updates       | 1261257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.64e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 252       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 78207     |\n",
            "|    total_timesteps | 1260000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1270000, episode_reward=-19198.07 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.92e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1270000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.71e+03  |\n",
            "|    critic_loss     | 4.82e+04  |\n",
            "|    ent_coef        | 2.43      |\n",
            "|    ent_coef_loss   | -0.117    |\n",
            "|    learning_rate   | 0.000873  |\n",
            "|    n_updates       | 1271257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1280000, episode_reward=-21354.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.14e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1280000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.76e+03  |\n",
            "|    critic_loss     | 8.4e+04   |\n",
            "|    ent_coef        | 2.54      |\n",
            "|    ent_coef_loss   | -0.00128  |\n",
            "|    learning_rate   | 0.000872  |\n",
            "|    n_updates       | 1281257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.61e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 256       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 79437     |\n",
            "|    total_timesteps | 1280000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1290000, episode_reward=-15387.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1290000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.88e+03  |\n",
            "|    critic_loss     | 3.05e+04  |\n",
            "|    ent_coef        | 2.79      |\n",
            "|    ent_coef_loss   | 0.0274    |\n",
            "|    learning_rate   | 0.000871  |\n",
            "|    n_updates       | 1291257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1300000, episode_reward=-23752.66 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1300000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.94e+03  |\n",
            "|    critic_loss     | 2.42e+05  |\n",
            "|    ent_coef        | 2.48      |\n",
            "|    ent_coef_loss   | 0.183     |\n",
            "|    learning_rate   | 0.00087   |\n",
            "|    n_updates       | 1301257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.24e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 260       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 80674     |\n",
            "|    total_timesteps | 1300000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1310000, episode_reward=-19057.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.91e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1310000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.77e+03  |\n",
            "|    critic_loss     | 2.23e+04  |\n",
            "|    ent_coef        | 2.16      |\n",
            "|    ent_coef_loss   | -0.122    |\n",
            "|    learning_rate   | 0.000869  |\n",
            "|    n_updates       | 1311257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1320000, episode_reward=-13235.04 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.32e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1320000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.77e+03  |\n",
            "|    critic_loss     | 2.04e+04  |\n",
            "|    ent_coef        | 2.07      |\n",
            "|    ent_coef_loss   | 0.351     |\n",
            "|    learning_rate   | 0.000868  |\n",
            "|    n_updates       | 1321257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.79e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 264       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 81914     |\n",
            "|    total_timesteps | 1320000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1330000, episode_reward=-14201.63 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.42e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1330000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.73e+03  |\n",
            "|    critic_loss     | 1.29e+05  |\n",
            "|    ent_coef        | 1.88      |\n",
            "|    ent_coef_loss   | -0.00293  |\n",
            "|    learning_rate   | 0.000867  |\n",
            "|    n_updates       | 1331257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1340000, episode_reward=-14493.34 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.45e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1340000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.82e+03  |\n",
            "|    critic_loss     | 1.29e+05  |\n",
            "|    ent_coef        | 1.96      |\n",
            "|    ent_coef_loss   | 0.185     |\n",
            "|    learning_rate   | 0.000866  |\n",
            "|    n_updates       | 1341257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.73e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 268       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 83156     |\n",
            "|    total_timesteps | 1340000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1350000, episode_reward=-16441.07 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.64e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1350000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.7e+03   |\n",
            "|    critic_loss     | 4.2e+04   |\n",
            "|    ent_coef        | 1.83      |\n",
            "|    ent_coef_loss   | 0.0304    |\n",
            "|    learning_rate   | 0.000865  |\n",
            "|    n_updates       | 1351257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1360000, episode_reward=-14043.48 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.4e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1360000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.62e+03 |\n",
            "|    critic_loss     | 1.52e+04 |\n",
            "|    ent_coef        | 1.79     |\n",
            "|    ent_coef_loss   | -0.00156 |\n",
            "|    learning_rate   | 0.000864 |\n",
            "|    n_updates       | 1361257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.75e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 272       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 84399     |\n",
            "|    total_timesteps | 1360000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1370000, episode_reward=-15882.83 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1370000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.66e+03  |\n",
            "|    critic_loss     | 2.54e+04  |\n",
            "|    ent_coef        | 1.74      |\n",
            "|    ent_coef_loss   | 0.152     |\n",
            "|    learning_rate   | 0.000863  |\n",
            "|    n_updates       | 1371257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1380000, episode_reward=-10137.80 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.01e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1380000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.69e+03  |\n",
            "|    critic_loss     | 1.41e+05  |\n",
            "|    ent_coef        | 2.07      |\n",
            "|    ent_coef_loss   | -0.724    |\n",
            "|    learning_rate   | 0.000862  |\n",
            "|    n_updates       | 1381257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.75e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 276       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 85635     |\n",
            "|    total_timesteps | 1380000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1390000, episode_reward=-19439.29 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.94e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1390000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.62e+03  |\n",
            "|    critic_loss     | 2.13e+04  |\n",
            "|    ent_coef        | 1.93      |\n",
            "|    ent_coef_loss   | 0.0594    |\n",
            "|    learning_rate   | 0.000861  |\n",
            "|    n_updates       | 1391257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1400000, episode_reward=-989080.30 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.89e+05 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1400000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.43e+03  |\n",
            "|    critic_loss     | 8.5e+04   |\n",
            "|    ent_coef        | 2.1       |\n",
            "|    ent_coef_loss   | -0.617    |\n",
            "|    learning_rate   | 0.00086   |\n",
            "|    n_updates       | 1401257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.57e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 280       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 86860     |\n",
            "|    total_timesteps | 1400000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1410000, episode_reward=-18950.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.9e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1410000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.41e+03 |\n",
            "|    critic_loss     | 3.06e+04 |\n",
            "|    ent_coef        | 2.29     |\n",
            "|    ent_coef_loss   | -0.0915  |\n",
            "|    learning_rate   | 0.000859 |\n",
            "|    n_updates       | 1411257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1420000, episode_reward=-15762.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.58e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1420000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.26e+03  |\n",
            "|    critic_loss     | 3.14e+04  |\n",
            "|    ent_coef        | 2.17      |\n",
            "|    ent_coef_loss   | 0.0249    |\n",
            "|    learning_rate   | 0.000858  |\n",
            "|    n_updates       | 1421257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.51e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 284       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 88083     |\n",
            "|    total_timesteps | 1420000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1430000, episode_reward=-29414.17 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.94e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1430000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.46e+03  |\n",
            "|    critic_loss     | 6.65e+04  |\n",
            "|    ent_coef        | 2.38      |\n",
            "|    ent_coef_loss   | 0.00145   |\n",
            "|    learning_rate   | 0.000857  |\n",
            "|    n_updates       | 1431257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1440000, episode_reward=-19052.42 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.91e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1440000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.24e+03  |\n",
            "|    critic_loss     | 2.26e+04  |\n",
            "|    ent_coef        | 2.04      |\n",
            "|    ent_coef_loss   | 0.196     |\n",
            "|    learning_rate   | 0.000856  |\n",
            "|    n_updates       | 1441257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.08e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 288       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 89296     |\n",
            "|    total_timesteps | 1440000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1450000, episode_reward=-20287.63 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.03e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1450000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.29e+03  |\n",
            "|    critic_loss     | 1.61e+04  |\n",
            "|    ent_coef        | 2         |\n",
            "|    ent_coef_loss   | 0.239     |\n",
            "|    learning_rate   | 0.000855  |\n",
            "|    n_updates       | 1451257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1460000, episode_reward=-16320.97 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.63e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1460000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.26e+03  |\n",
            "|    critic_loss     | 3.44e+04  |\n",
            "|    ent_coef        | 2.05      |\n",
            "|    ent_coef_loss   | -2.13     |\n",
            "|    learning_rate   | 0.000854  |\n",
            "|    n_updates       | 1461257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.87e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 292       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 90505     |\n",
            "|    total_timesteps | 1460000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1470000, episode_reward=-18063.98 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.81e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1470000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.06e+03  |\n",
            "|    critic_loss     | 1.86e+04  |\n",
            "|    ent_coef        | 1.83      |\n",
            "|    ent_coef_loss   | 0.239     |\n",
            "|    learning_rate   | 0.000853  |\n",
            "|    n_updates       | 1471257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1480000, episode_reward=-21868.27 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.19e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1480000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.05e+03  |\n",
            "|    critic_loss     | 1.14e+04  |\n",
            "|    ent_coef        | 2.15      |\n",
            "|    ent_coef_loss   | 0.175     |\n",
            "|    learning_rate   | 0.000852  |\n",
            "|    n_updates       | 1481257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.78e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 296       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 91747     |\n",
            "|    total_timesteps | 1480000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1490000, episode_reward=-22930.01 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.29e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1490000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.16e+03  |\n",
            "|    critic_loss     | 2.65e+04  |\n",
            "|    ent_coef        | 1.88      |\n",
            "|    ent_coef_loss   | 0.109     |\n",
            "|    learning_rate   | 0.000851  |\n",
            "|    n_updates       | 1491257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1500000, episode_reward=-6095.44 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -6.1e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1500000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.06e+03 |\n",
            "|    critic_loss     | 2.82e+04 |\n",
            "|    ent_coef        | 2.17     |\n",
            "|    ent_coef_loss   | 0.142    |\n",
            "|    learning_rate   | 0.00085  |\n",
            "|    n_updates       | 1501257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.76e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 300       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 92983     |\n",
            "|    total_timesteps | 1500000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1510000, episode_reward=-14401.63 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.44e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1510000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.36e+03  |\n",
            "|    critic_loss     | 1.37e+04  |\n",
            "|    ent_coef        | 2.14      |\n",
            "|    ent_coef_loss   | -0.056    |\n",
            "|    learning_rate   | 0.000849  |\n",
            "|    n_updates       | 1511257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1520000, episode_reward=-16229.58 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.62e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1520000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.2e+03   |\n",
            "|    critic_loss     | 1.29e+04  |\n",
            "|    ent_coef        | 2.1       |\n",
            "|    ent_coef_loss   | 0.101     |\n",
            "|    learning_rate   | 0.000848  |\n",
            "|    n_updates       | 1521257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.72e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 304       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 94215     |\n",
            "|    total_timesteps | 1520000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1530000, episode_reward=-14478.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.45e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1530000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.09e+03  |\n",
            "|    critic_loss     | 1.08e+04  |\n",
            "|    ent_coef        | 1.86      |\n",
            "|    ent_coef_loss   | 0.00163   |\n",
            "|    learning_rate   | 0.000847  |\n",
            "|    n_updates       | 1531257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1540000, episode_reward=-12103.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.21e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1540000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.07e+03  |\n",
            "|    critic_loss     | 9.03e+03  |\n",
            "|    ent_coef        | 1.8       |\n",
            "|    ent_coef_loss   | 0.247     |\n",
            "|    learning_rate   | 0.000846  |\n",
            "|    n_updates       | 1541257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.71e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 308       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 95452     |\n",
            "|    total_timesteps | 1540000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1550000, episode_reward=-16132.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1550000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.99e+03  |\n",
            "|    critic_loss     | 9.46e+03  |\n",
            "|    ent_coef        | 1.79      |\n",
            "|    ent_coef_loss   | -0.0456   |\n",
            "|    learning_rate   | 0.000845  |\n",
            "|    n_updates       | 1551257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1560000, episode_reward=-10732.96 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.07e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1560000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.13e+03  |\n",
            "|    critic_loss     | 1.5e+04   |\n",
            "|    ent_coef        | 1.51      |\n",
            "|    ent_coef_loss   | 0.00242   |\n",
            "|    learning_rate   | 0.000844  |\n",
            "|    n_updates       | 1561257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.69e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 312       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 96694     |\n",
            "|    total_timesteps | 1560000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1570000, episode_reward=-19715.12 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.97e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1570000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.15e+03  |\n",
            "|    critic_loss     | 1.24e+04  |\n",
            "|    ent_coef        | 1.86      |\n",
            "|    ent_coef_loss   | 0.354     |\n",
            "|    learning_rate   | 0.000843  |\n",
            "|    n_updates       | 1571257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1580000, episode_reward=-19154.49 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.92e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1580000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.05e+03  |\n",
            "|    critic_loss     | 1.09e+04  |\n",
            "|    ent_coef        | 1.9       |\n",
            "|    ent_coef_loss   | 0.207     |\n",
            "|    learning_rate   | 0.000842  |\n",
            "|    n_updates       | 1581257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.71e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 316       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 97932     |\n",
            "|    total_timesteps | 1580000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1590000, episode_reward=-12415.92 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.24e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1590000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.07e+03  |\n",
            "|    critic_loss     | 1.01e+04  |\n",
            "|    ent_coef        | 1.39      |\n",
            "|    ent_coef_loss   | 0.0712    |\n",
            "|    learning_rate   | 0.000841  |\n",
            "|    n_updates       | 1591257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1600000, episode_reward=-18114.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.81e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1600000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.15e+03  |\n",
            "|    critic_loss     | 5.09e+04  |\n",
            "|    ent_coef        | 1.56      |\n",
            "|    ent_coef_loss   | 0.0616    |\n",
            "|    learning_rate   | 0.00084   |\n",
            "|    n_updates       | 1601257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.71e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 320       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 99162     |\n",
            "|    total_timesteps | 1600000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1610000, episode_reward=-23078.64 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.31e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1610000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.11e+03  |\n",
            "|    critic_loss     | 3.63e+04  |\n",
            "|    ent_coef        | 2.26      |\n",
            "|    ent_coef_loss   | 0.464     |\n",
            "|    learning_rate   | 0.000839  |\n",
            "|    n_updates       | 1611257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1620000, episode_reward=-15658.02 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.57e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1620000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.29e+03  |\n",
            "|    critic_loss     | 2.88e+05  |\n",
            "|    ent_coef        | 1.9       |\n",
            "|    ent_coef_loss   | -0.19     |\n",
            "|    learning_rate   | 0.000838  |\n",
            "|    n_updates       | 1621257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.12e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 324       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 100396    |\n",
            "|    total_timesteps | 1620000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1630000, episode_reward=-14537.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.45e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1630000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.01e+03  |\n",
            "|    critic_loss     | 7.22e+03  |\n",
            "|    ent_coef        | 1.55      |\n",
            "|    ent_coef_loss   | 0.0308    |\n",
            "|    learning_rate   | 0.000837  |\n",
            "|    n_updates       | 1631257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1640000, episode_reward=-16970.14 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.7e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1640000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.26e+03 |\n",
            "|    critic_loss     | 1.15e+04 |\n",
            "|    ent_coef        | 1.65     |\n",
            "|    ent_coef_loss   | 0.134    |\n",
            "|    learning_rate   | 0.000836 |\n",
            "|    n_updates       | 1641257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.11e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 328       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 101610    |\n",
            "|    total_timesteps | 1640000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1650000, episode_reward=-14270.64 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1650000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.99e+03  |\n",
            "|    critic_loss     | 8.62e+03  |\n",
            "|    ent_coef        | 1.6       |\n",
            "|    ent_coef_loss   | -0.423    |\n",
            "|    learning_rate   | 0.000835  |\n",
            "|    n_updates       | 1651257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1660000, episode_reward=-16697.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.67e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1660000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.96e+03  |\n",
            "|    critic_loss     | 1.38e+04  |\n",
            "|    ent_coef        | 1.74      |\n",
            "|    ent_coef_loss   | 0.0774    |\n",
            "|    learning_rate   | 0.000834  |\n",
            "|    n_updates       | 1661257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.08e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 332       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 102821    |\n",
            "|    total_timesteps | 1660000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1670000, episode_reward=-13831.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.38e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1670000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.83e+03  |\n",
            "|    critic_loss     | 1.2e+04   |\n",
            "|    ent_coef        | 1.3       |\n",
            "|    ent_coef_loss   | 0.0889    |\n",
            "|    learning_rate   | 0.000833  |\n",
            "|    n_updates       | 1671257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1680000, episode_reward=-16178.99 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.62e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1680000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.89e+03  |\n",
            "|    critic_loss     | 1.15e+04  |\n",
            "|    ent_coef        | 1.13      |\n",
            "|    ent_coef_loss   | -0.0237   |\n",
            "|    learning_rate   | 0.000832  |\n",
            "|    n_updates       | 1681257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.08e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 336       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 104041    |\n",
            "|    total_timesteps | 1680000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1690000, episode_reward=-20439.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1690000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.83e+03  |\n",
            "|    critic_loss     | 8.96e+03  |\n",
            "|    ent_coef        | 1.4       |\n",
            "|    ent_coef_loss   | -0.0382   |\n",
            "|    learning_rate   | 0.000831  |\n",
            "|    n_updates       | 1691257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1700000, episode_reward=-14777.49 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1700000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.86e+03  |\n",
            "|    critic_loss     | 8.67e+03  |\n",
            "|    ent_coef        | 1.07      |\n",
            "|    ent_coef_loss   | 0.0159    |\n",
            "|    learning_rate   | 0.00083   |\n",
            "|    n_updates       | 1701257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.1e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 340      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 105255   |\n",
            "|    total_timesteps | 1700000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1710000, episode_reward=-13511.17 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.35e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1710000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.28e+03  |\n",
            "|    critic_loss     | 3.47e+04  |\n",
            "|    ent_coef        | 1.11      |\n",
            "|    ent_coef_loss   | -0.00727  |\n",
            "|    learning_rate   | 0.000829  |\n",
            "|    n_updates       | 1711257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1720000, episode_reward=-20398.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1720000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.65e+03  |\n",
            "|    critic_loss     | 1.73e+04  |\n",
            "|    ent_coef        | 1.89      |\n",
            "|    ent_coef_loss   | 0.0934    |\n",
            "|    learning_rate   | 0.000828  |\n",
            "|    n_updates       | 1721257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.1e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 344      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 106467   |\n",
            "|    total_timesteps | 1720000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1730000, episode_reward=-13557.27 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.36e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1730000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.57e+03  |\n",
            "|    critic_loss     | 9.82e+03  |\n",
            "|    ent_coef        | 1.54      |\n",
            "|    ent_coef_loss   | -0.0353   |\n",
            "|    learning_rate   | 0.000827  |\n",
            "|    n_updates       | 1731257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1740000, episode_reward=-25463.79 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.55e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1740000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.47e+03  |\n",
            "|    critic_loss     | 3.57e+04  |\n",
            "|    ent_coef        | 1.56      |\n",
            "|    ent_coef_loss   | 0.104     |\n",
            "|    learning_rate   | 0.000826  |\n",
            "|    n_updates       | 1741257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.15e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 348       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 107691    |\n",
            "|    total_timesteps | 1740000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1750000, episode_reward=-18622.17 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.86e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1750000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.59e+03  |\n",
            "|    critic_loss     | 1.84e+04  |\n",
            "|    ent_coef        | 1.3       |\n",
            "|    ent_coef_loss   | 0.0387    |\n",
            "|    learning_rate   | 0.000825  |\n",
            "|    n_updates       | 1751257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1760000, episode_reward=-15878.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1760000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.71e+03  |\n",
            "|    critic_loss     | 1.97e+04  |\n",
            "|    ent_coef        | 1.02      |\n",
            "|    ent_coef_loss   | 0.00507   |\n",
            "|    learning_rate   | 0.000824  |\n",
            "|    n_updates       | 1761257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.15e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 352       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 108942    |\n",
            "|    total_timesteps | 1760000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1770000, episode_reward=-32887.80 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.29e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1770000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.79e+03  |\n",
            "|    critic_loss     | 5.16e+04  |\n",
            "|    ent_coef        | 1.39      |\n",
            "|    ent_coef_loss   | -0.23     |\n",
            "|    learning_rate   | 0.000823  |\n",
            "|    n_updates       | 1771257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1780000, episode_reward=-20045.46 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -2e+04   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1780000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.59e+03 |\n",
            "|    critic_loss     | 9.34e+03 |\n",
            "|    ent_coef        | 1.42     |\n",
            "|    ent_coef_loss   | 0.134    |\n",
            "|    learning_rate   | 0.000822 |\n",
            "|    n_updates       | 1781257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.15e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 356       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 110174    |\n",
            "|    total_timesteps | 1780000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1790000, episode_reward=-14318.84 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1790000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.68e+03  |\n",
            "|    critic_loss     | 8.05e+03  |\n",
            "|    ent_coef        | 1.02      |\n",
            "|    ent_coef_loss   | 0.00518   |\n",
            "|    learning_rate   | 0.000821  |\n",
            "|    n_updates       | 1791257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1800000, episode_reward=-16629.51 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.66e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1800000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.68e+03  |\n",
            "|    critic_loss     | 8.33e+03  |\n",
            "|    ent_coef        | 0.941     |\n",
            "|    ent_coef_loss   | 0.0166    |\n",
            "|    learning_rate   | 0.00082   |\n",
            "|    n_updates       | 1801257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.12e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 360       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 111437    |\n",
            "|    total_timesteps | 1800000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1810000, episode_reward=-17982.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.8e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1810000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.62e+03 |\n",
            "|    critic_loss     | 5.27e+03 |\n",
            "|    ent_coef        | 0.851    |\n",
            "|    ent_coef_loss   | 0.00888  |\n",
            "|    learning_rate   | 0.000819 |\n",
            "|    n_updates       | 1811257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1820000, episode_reward=-17689.28 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.77e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1820000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.61e+03  |\n",
            "|    critic_loss     | 6.79e+03  |\n",
            "|    ent_coef        | 0.915     |\n",
            "|    ent_coef_loss   | -0.00804  |\n",
            "|    learning_rate   | 0.000818  |\n",
            "|    n_updates       | 1821257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.11e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 364       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 112748    |\n",
            "|    total_timesteps | 1820000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1830000, episode_reward=-14986.76 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1830000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.52e+03 |\n",
            "|    critic_loss     | 9.05e+03 |\n",
            "|    ent_coef        | 1.39     |\n",
            "|    ent_coef_loss   | 0.0187   |\n",
            "|    learning_rate   | 0.000817 |\n",
            "|    n_updates       | 1831257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1840000, episode_reward=-11727.98 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.17e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1840000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.51e+03  |\n",
            "|    critic_loss     | 1.66e+04  |\n",
            "|    ent_coef        | 1.62      |\n",
            "|    ent_coef_loss   | 0.173     |\n",
            "|    learning_rate   | 0.000816  |\n",
            "|    n_updates       | 1841257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.24e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 368       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 114023    |\n",
            "|    total_timesteps | 1840000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1850000, episode_reward=-19431.44 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.94e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1850000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.62e+03  |\n",
            "|    critic_loss     | 1.78e+04  |\n",
            "|    ent_coef        | 1.05      |\n",
            "|    ent_coef_loss   | -0.00341  |\n",
            "|    learning_rate   | 0.000815  |\n",
            "|    n_updates       | 1851257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1860000, episode_reward=-18442.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.84e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1860000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.67e+03  |\n",
            "|    critic_loss     | 8.27e+03  |\n",
            "|    ent_coef        | 0.874     |\n",
            "|    ent_coef_loss   | -0.0236   |\n",
            "|    learning_rate   | 0.000814  |\n",
            "|    n_updates       | 1861257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.35e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 372       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 115304    |\n",
            "|    total_timesteps | 1860000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1870000, episode_reward=-14074.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.41e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1870000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.68e+03  |\n",
            "|    critic_loss     | 6.21e+03  |\n",
            "|    ent_coef        | 0.95      |\n",
            "|    ent_coef_loss   | -0.0087   |\n",
            "|    learning_rate   | 0.000813  |\n",
            "|    n_updates       | 1871257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1880000, episode_reward=-11577.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.16e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1880000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.95e+03  |\n",
            "|    critic_loss     | 4.45e+04  |\n",
            "|    ent_coef        | 1.33      |\n",
            "|    ent_coef_loss   | -0.0075   |\n",
            "|    learning_rate   | 0.000812  |\n",
            "|    n_updates       | 1881257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.36e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 376       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 116590    |\n",
            "|    total_timesteps | 1880000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1890000, episode_reward=-17593.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.76e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1890000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.97e+03  |\n",
            "|    critic_loss     | 8.25e+03  |\n",
            "|    ent_coef        | 0.923     |\n",
            "|    ent_coef_loss   | -0.004    |\n",
            "|    learning_rate   | 0.000811  |\n",
            "|    n_updates       | 1891257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1900000, episode_reward=-10681.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.07e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1900000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.63e+03  |\n",
            "|    critic_loss     | 7.59e+03  |\n",
            "|    ent_coef        | 0.958     |\n",
            "|    ent_coef_loss   | -0.0196   |\n",
            "|    learning_rate   | 0.00081   |\n",
            "|    n_updates       | 1901257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.36e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 380       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 117863    |\n",
            "|    total_timesteps | 1900000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1910000, episode_reward=-14394.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.44e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1910000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.79e+03  |\n",
            "|    critic_loss     | 5.42e+04  |\n",
            "|    ent_coef        | 1.14      |\n",
            "|    ent_coef_loss   | -0.0089   |\n",
            "|    learning_rate   | 0.000809  |\n",
            "|    n_updates       | 1911257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1920000, episode_reward=-18663.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.87e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1920000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.75e+03  |\n",
            "|    critic_loss     | 9.83e+03  |\n",
            "|    ent_coef        | 0.803     |\n",
            "|    ent_coef_loss   | -0.0844   |\n",
            "|    learning_rate   | 0.000808  |\n",
            "|    n_updates       | 1921257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.42e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 384       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 119139    |\n",
            "|    total_timesteps | 1920000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1930000, episode_reward=-13926.97 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.39e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1930000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.82e+03  |\n",
            "|    critic_loss     | 6.13e+03  |\n",
            "|    ent_coef        | 1.21      |\n",
            "|    ent_coef_loss   | 0.0292    |\n",
            "|    learning_rate   | 0.000807  |\n",
            "|    n_updates       | 1931257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1940000, episode_reward=-9149.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1940000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.72e+03  |\n",
            "|    critic_loss     | 5.84e+03  |\n",
            "|    ent_coef        | 0.828     |\n",
            "|    ent_coef_loss   | 0.094     |\n",
            "|    learning_rate   | 0.000806  |\n",
            "|    n_updates       | 1941257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.41e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 388       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 120419    |\n",
            "|    total_timesteps | 1940000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1950000, episode_reward=-15462.14 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.55e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1950000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.78e+03  |\n",
            "|    critic_loss     | 3.94e+03  |\n",
            "|    ent_coef        | 0.809     |\n",
            "|    ent_coef_loss   | 0.0212    |\n",
            "|    learning_rate   | 0.000805  |\n",
            "|    n_updates       | 1951257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1960000, episode_reward=-17620.43 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.76e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1960000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.74e+03  |\n",
            "|    critic_loss     | 4.13e+03  |\n",
            "|    ent_coef        | 0.808     |\n",
            "|    ent_coef_loss   | -0.0379   |\n",
            "|    learning_rate   | 0.000804  |\n",
            "|    n_updates       | 1961257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -3.4e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 392      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 121699   |\n",
            "|    total_timesteps | 1960000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1970000, episode_reward=-15358.56 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1970000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.87e+03  |\n",
            "|    critic_loss     | 1.05e+04  |\n",
            "|    ent_coef        | 0.925     |\n",
            "|    ent_coef_loss   | 0.0191    |\n",
            "|    learning_rate   | 0.000803  |\n",
            "|    n_updates       | 1971257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1980000, episode_reward=-13615.31 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.36e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1980000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.77e+03  |\n",
            "|    critic_loss     | 4.96e+04  |\n",
            "|    ent_coef        | 0.826     |\n",
            "|    ent_coef_loss   | -0.0238   |\n",
            "|    learning_rate   | 0.000802  |\n",
            "|    n_updates       | 1981257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.39e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 396       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 122981    |\n",
            "|    total_timesteps | 1980000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1990000, episode_reward=-18815.39 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.88e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1990000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.73e+03  |\n",
            "|    critic_loss     | 8.88e+03  |\n",
            "|    ent_coef        | 0.646     |\n",
            "|    ent_coef_loss   | -0.0151   |\n",
            "|    learning_rate   | 0.000801  |\n",
            "|    n_updates       | 1991257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2000000, episode_reward=-17833.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.78e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2000000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.66e+03  |\n",
            "|    critic_loss     | 4.34e+03  |\n",
            "|    ent_coef        | 0.643     |\n",
            "|    ent_coef_loss   | -0.0106   |\n",
            "|    learning_rate   | 0.0008    |\n",
            "|    n_updates       | 2001257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.39e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 400       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 124262    |\n",
            "|    total_timesteps | 2000000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2010000, episode_reward=-13698.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.37e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2010000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.6e+03   |\n",
            "|    critic_loss     | 4.23e+03  |\n",
            "|    ent_coef        | 1.25      |\n",
            "|    ent_coef_loss   | 0.0799    |\n",
            "|    learning_rate   | 0.000799  |\n",
            "|    n_updates       | 2011257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2020000, episode_reward=-20730.77 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.07e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2020000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.61e+03  |\n",
            "|    critic_loss     | 1.44e+04  |\n",
            "|    ent_coef        | 0.716     |\n",
            "|    ent_coef_loss   | 0.224     |\n",
            "|    learning_rate   | 0.000798  |\n",
            "|    n_updates       | 2021257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.58e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 404       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 125545    |\n",
            "|    total_timesteps | 2020000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2030000, episode_reward=-19352.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.94e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2030000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.63e+03  |\n",
            "|    critic_loss     | 4.67e+04  |\n",
            "|    ent_coef        | 0.758     |\n",
            "|    ent_coef_loss   | -0.094    |\n",
            "|    learning_rate   | 0.000797  |\n",
            "|    n_updates       | 2031257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2040000, episode_reward=-12399.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.24e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2040000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.48e+03  |\n",
            "|    critic_loss     | 7.65e+03  |\n",
            "|    ent_coef        | 0.625     |\n",
            "|    ent_coef_loss   | 0.0429    |\n",
            "|    learning_rate   | 0.000796  |\n",
            "|    n_updates       | 2041257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.58e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 408       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 126874    |\n",
            "|    total_timesteps | 2040000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2050000, episode_reward=-13729.38 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.37e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2050000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.49e+03  |\n",
            "|    critic_loss     | 3.86e+03  |\n",
            "|    ent_coef        | 0.769     |\n",
            "|    ent_coef_loss   | -0.0899   |\n",
            "|    learning_rate   | 0.000795  |\n",
            "|    n_updates       | 2051257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2060000, episode_reward=-15990.84 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.6e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2060000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.57e+03 |\n",
            "|    critic_loss     | 5.67e+04 |\n",
            "|    ent_coef        | 0.812    |\n",
            "|    ent_coef_loss   | -0.0406  |\n",
            "|    learning_rate   | 0.000794 |\n",
            "|    n_updates       | 2061257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.58e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 412       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 128147    |\n",
            "|    total_timesteps | 2060000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2070000, episode_reward=-14091.29 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.41e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2070000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.38e+03  |\n",
            "|    critic_loss     | 4.52e+03  |\n",
            "|    ent_coef        | 1.22      |\n",
            "|    ent_coef_loss   | 0.0226    |\n",
            "|    learning_rate   | 0.000793  |\n",
            "|    n_updates       | 2071257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2080000, episode_reward=-14033.52 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.4e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2080000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.33e+03 |\n",
            "|    critic_loss     | 3.41e+03 |\n",
            "|    ent_coef        | 0.868    |\n",
            "|    ent_coef_loss   | -0.00494 |\n",
            "|    learning_rate   | 0.000792 |\n",
            "|    n_updates       | 2081257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.57e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 416       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 129416    |\n",
            "|    total_timesteps | 2080000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2090000, episode_reward=-15504.27 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.55e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2090000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.32e+03  |\n",
            "|    critic_loss     | 4.58e+03  |\n",
            "|    ent_coef        | 0.75      |\n",
            "|    ent_coef_loss   | 0.106     |\n",
            "|    learning_rate   | 0.000791  |\n",
            "|    n_updates       | 2091257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2100000, episode_reward=-13466.22 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.35e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2100000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.34e+03  |\n",
            "|    critic_loss     | 3.96e+03  |\n",
            "|    ent_coef        | 1.06      |\n",
            "|    ent_coef_loss   | 0.0265    |\n",
            "|    learning_rate   | 0.00079   |\n",
            "|    n_updates       | 2101257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.56e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 420       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 130691    |\n",
            "|    total_timesteps | 2100000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2110000, episode_reward=-15378.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2110000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.57e+03  |\n",
            "|    critic_loss     | 7.22e+03  |\n",
            "|    ent_coef        | 0.805     |\n",
            "|    ent_coef_loss   | -0.119    |\n",
            "|    learning_rate   | 0.000789  |\n",
            "|    n_updates       | 2111257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2120000, episode_reward=-14254.91 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2120000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.28e+03  |\n",
            "|    critic_loss     | 3.61e+03  |\n",
            "|    ent_coef        | 0.629     |\n",
            "|    ent_coef_loss   | -0.0904   |\n",
            "|    learning_rate   | 0.000788  |\n",
            "|    n_updates       | 2121257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.11e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 424       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 131991    |\n",
            "|    total_timesteps | 2120000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2130000, episode_reward=-27332.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.73e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2130000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.38e+03  |\n",
            "|    critic_loss     | 1.07e+04  |\n",
            "|    ent_coef        | 0.742     |\n",
            "|    ent_coef_loss   | -0.0585   |\n",
            "|    learning_rate   | 0.000787  |\n",
            "|    n_updates       | 2131257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2140000, episode_reward=-16903.87 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.69e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2140000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.33e+03  |\n",
            "|    critic_loss     | 1.26e+04  |\n",
            "|    ent_coef        | 0.771     |\n",
            "|    ent_coef_loss   | 0.0805    |\n",
            "|    learning_rate   | 0.000786  |\n",
            "|    n_updates       | 2141257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.12e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 428       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 133303    |\n",
            "|    total_timesteps | 2140000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2150000, episode_reward=-18057.71 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.81e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2150000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.35e+03  |\n",
            "|    critic_loss     | 3.19e+03  |\n",
            "|    ent_coef        | 0.896     |\n",
            "|    ent_coef_loss   | 0.218     |\n",
            "|    learning_rate   | 0.000785  |\n",
            "|    n_updates       | 2151257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2160000, episode_reward=-19429.51 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.94e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2160000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.28e+03  |\n",
            "|    critic_loss     | 3.29e+03  |\n",
            "|    ent_coef        | 0.819     |\n",
            "|    ent_coef_loss   | 0.0391    |\n",
            "|    learning_rate   | 0.000784  |\n",
            "|    n_updates       | 2161257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -3.2e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 432      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 134552   |\n",
            "|    total_timesteps | 2160000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2170000, episode_reward=-16736.14 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.67e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2170000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.42e+03  |\n",
            "|    critic_loss     | 1.06e+04  |\n",
            "|    ent_coef        | 0.695     |\n",
            "|    ent_coef_loss   | -0.0324   |\n",
            "|    learning_rate   | 0.000783  |\n",
            "|    n_updates       | 2171257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2180000, episode_reward=-15046.60 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2180000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.27e+03 |\n",
            "|    critic_loss     | 4.56e+03 |\n",
            "|    ent_coef        | 0.815    |\n",
            "|    ent_coef_loss   | -0.0114  |\n",
            "|    learning_rate   | 0.000782 |\n",
            "|    n_updates       | 2181257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.21e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 436       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 135804    |\n",
            "|    total_timesteps | 2180000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2190000, episode_reward=-15620.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.56e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2190000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.29e+03  |\n",
            "|    critic_loss     | 4.12e+03  |\n",
            "|    ent_coef        | 0.769     |\n",
            "|    ent_coef_loss   | 0.0996    |\n",
            "|    learning_rate   | 0.000781  |\n",
            "|    n_updates       | 2191257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2200000, episode_reward=-11980.25 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.2e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2200000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.2e+03  |\n",
            "|    critic_loss     | 1.3e+04  |\n",
            "|    ent_coef        | 0.756    |\n",
            "|    ent_coef_loss   | 0.114    |\n",
            "|    learning_rate   | 0.00078  |\n",
            "|    n_updates       | 2201257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.31e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 440       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 137053    |\n",
            "|    total_timesteps | 2200000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2210000, episode_reward=-22676.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2210000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.22e+03  |\n",
            "|    critic_loss     | 1.1e+04   |\n",
            "|    ent_coef        | 0.785     |\n",
            "|    ent_coef_loss   | 0.0113    |\n",
            "|    learning_rate   | 0.000779  |\n",
            "|    n_updates       | 2211257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2220000, episode_reward=-12748.17 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2220000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.23e+03  |\n",
            "|    critic_loss     | 4.2e+03   |\n",
            "|    ent_coef        | 0.708     |\n",
            "|    ent_coef_loss   | -0.00294  |\n",
            "|    learning_rate   | 0.000778  |\n",
            "|    n_updates       | 2221257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.55e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 444       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 138300    |\n",
            "|    total_timesteps | 2220000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2230000, episode_reward=-14498.98 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.45e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2230000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.51e+03  |\n",
            "|    critic_loss     | 1.28e+04  |\n",
            "|    ent_coef        | 0.861     |\n",
            "|    ent_coef_loss   | 0.0655    |\n",
            "|    learning_rate   | 0.000777  |\n",
            "|    n_updates       | 2231257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2240000, episode_reward=-10146.07 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.01e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2240000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.41e+03  |\n",
            "|    critic_loss     | 1.67e+04  |\n",
            "|    ent_coef        | 1.16      |\n",
            "|    ent_coef_loss   | 0.0411    |\n",
            "|    learning_rate   | 0.000776  |\n",
            "|    n_updates       | 2241257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -3.5e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 448      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 139554   |\n",
            "|    total_timesteps | 2240000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2250000, episode_reward=-16134.12 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2250000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.41e+03  |\n",
            "|    critic_loss     | 7.3e+03   |\n",
            "|    ent_coef        | 0.699     |\n",
            "|    ent_coef_loss   | 0.171     |\n",
            "|    learning_rate   | 0.000775  |\n",
            "|    n_updates       | 2251257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2260000, episode_reward=-38832.87 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.88e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2260000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.38e+03  |\n",
            "|    critic_loss     | 2.77e+04  |\n",
            "|    ent_coef        | 0.822     |\n",
            "|    ent_coef_loss   | 0.0862    |\n",
            "|    learning_rate   | 0.000774  |\n",
            "|    n_updates       | 2261257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.47e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 452       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 140806    |\n",
            "|    total_timesteps | 2260000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2270000, episode_reward=-13329.21 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.33e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2270000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.66e+03  |\n",
            "|    critic_loss     | 4.88e+04  |\n",
            "|    ent_coef        | 0.706     |\n",
            "|    ent_coef_loss   | 0.0695    |\n",
            "|    learning_rate   | 0.000773  |\n",
            "|    n_updates       | 2271257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2280000, episode_reward=-16112.65 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2280000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.52e+03  |\n",
            "|    critic_loss     | 6.05e+03  |\n",
            "|    ent_coef        | 0.702     |\n",
            "|    ent_coef_loss   | 0.0304    |\n",
            "|    learning_rate   | 0.000772  |\n",
            "|    n_updates       | 2281257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.46e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 456       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 142064    |\n",
            "|    total_timesteps | 2280000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2290000, episode_reward=-10830.56 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.08e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2290000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.5e+03   |\n",
            "|    critic_loss     | 5.75e+03  |\n",
            "|    ent_coef        | 0.798     |\n",
            "|    ent_coef_loss   | 0.0435    |\n",
            "|    learning_rate   | 0.000771  |\n",
            "|    n_updates       | 2291257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2300000, episode_reward=-4636.26 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2300000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.51e+03  |\n",
            "|    critic_loss     | 2.6e+04   |\n",
            "|    ent_coef        | 0.6       |\n",
            "|    ent_coef_loss   | 0.0309    |\n",
            "|    learning_rate   | 0.00077   |\n",
            "|    n_updates       | 2301257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.45e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 460       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 143318    |\n",
            "|    total_timesteps | 2300000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2310000, episode_reward=-17625.46 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.76e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2310000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.96e+03  |\n",
            "|    critic_loss     | 4.89e+04  |\n",
            "|    ent_coef        | 0.627     |\n",
            "|    ent_coef_loss   | 0.0783    |\n",
            "|    learning_rate   | 0.000769  |\n",
            "|    n_updates       | 2311257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2320000, episode_reward=-2507.18 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2320000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.53e+03  |\n",
            "|    critic_loss     | 1.18e+04  |\n",
            "|    ent_coef        | 0.769     |\n",
            "|    ent_coef_loss   | -0.0759   |\n",
            "|    learning_rate   | 0.000768  |\n",
            "|    n_updates       | 2321257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -3.43e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 464       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 144597    |\n",
            "|    total_timesteps | 2320000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2330000, episode_reward=-15089.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.51e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2330000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.43e+03  |\n",
            "|    critic_loss     | 1.47e+04  |\n",
            "|    ent_coef        | 0.694     |\n",
            "|    ent_coef_loss   | 0.137     |\n",
            "|    learning_rate   | 0.000767  |\n",
            "|    n_updates       | 2331257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2340000, episode_reward=-22555.82 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.26e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2340000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.34e+03  |\n",
            "|    critic_loss     | 9.25e+03  |\n",
            "|    ent_coef        | 0.776     |\n",
            "|    ent_coef_loss   | 0.0294    |\n",
            "|    learning_rate   | 0.000766  |\n",
            "|    n_updates       | 2341257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.29e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 468       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 145844    |\n",
            "|    total_timesteps | 2340000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2350000, episode_reward=-14962.22 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2350000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.28e+03 |\n",
            "|    critic_loss     | 1.11e+04 |\n",
            "|    ent_coef        | 0.948    |\n",
            "|    ent_coef_loss   | 0.00895  |\n",
            "|    learning_rate   | 0.000765 |\n",
            "|    n_updates       | 2351257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2360000, episode_reward=-12494.33 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.25e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2360000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.26e+03  |\n",
            "|    critic_loss     | 6.38e+03  |\n",
            "|    ent_coef        | 0.833     |\n",
            "|    ent_coef_loss   | -0.0479   |\n",
            "|    learning_rate   | 0.000764  |\n",
            "|    n_updates       | 2361257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.18e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 472       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 147090    |\n",
            "|    total_timesteps | 2360000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2370000, episode_reward=-15218.26 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.52e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2370000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.21e+03  |\n",
            "|    critic_loss     | 1.16e+04  |\n",
            "|    ent_coef        | 0.544     |\n",
            "|    ent_coef_loss   | 0.0299    |\n",
            "|    learning_rate   | 0.000763  |\n",
            "|    n_updates       | 2371257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2380000, episode_reward=-14633.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.46e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2380000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.48e+03  |\n",
            "|    critic_loss     | 8.18e+04  |\n",
            "|    ent_coef        | 0.691     |\n",
            "|    ent_coef_loss   | 0.0413    |\n",
            "|    learning_rate   | 0.000762  |\n",
            "|    n_updates       | 2381257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.16e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 476       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 148341    |\n",
            "|    total_timesteps | 2380000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2390000, episode_reward=-14985.88 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2390000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.23e+03 |\n",
            "|    critic_loss     | 1.25e+04 |\n",
            "|    ent_coef        | 0.917    |\n",
            "|    ent_coef_loss   | 0.00268  |\n",
            "|    learning_rate   | 0.000761 |\n",
            "|    n_updates       | 2391257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2400000, episode_reward=-15155.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.52e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2400000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.26e+03  |\n",
            "|    critic_loss     | 7.32e+03  |\n",
            "|    ent_coef        | 0.799     |\n",
            "|    ent_coef_loss   | -0.0552   |\n",
            "|    learning_rate   | 0.00076   |\n",
            "|    n_updates       | 2401257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.16e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 480       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 149586    |\n",
            "|    total_timesteps | 2400000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2410000, episode_reward=-12647.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.26e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2410000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.28e+03  |\n",
            "|    critic_loss     | 6.44e+03  |\n",
            "|    ent_coef        | 0.75      |\n",
            "|    ent_coef_loss   | 0.0292    |\n",
            "|    learning_rate   | 0.000759  |\n",
            "|    n_updates       | 2411257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2420000, episode_reward=-13094.98 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.31e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2420000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.39e+03  |\n",
            "|    critic_loss     | 8.26e+03  |\n",
            "|    ent_coef        | 0.69      |\n",
            "|    ent_coef_loss   | 0.0708    |\n",
            "|    learning_rate   | 0.000758  |\n",
            "|    n_updates       | 2421257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.07e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 484       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 150835    |\n",
            "|    total_timesteps | 2420000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2430000, episode_reward=-4023.67 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.02e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2430000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.6e+03   |\n",
            "|    critic_loss     | 1.7e+05   |\n",
            "|    ent_coef        | 0.637     |\n",
            "|    ent_coef_loss   | 0.136     |\n",
            "|    learning_rate   | 0.000757  |\n",
            "|    n_updates       | 2431257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2440000, episode_reward=-11526.33 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.15e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2440000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.28e+03  |\n",
            "|    critic_loss     | 6.36e+03  |\n",
            "|    ent_coef        | 0.895     |\n",
            "|    ent_coef_loss   | -0.0075   |\n",
            "|    learning_rate   | 0.000756  |\n",
            "|    n_updates       | 2441257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.1e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 488      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 152097   |\n",
            "|    total_timesteps | 2440000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2450000, episode_reward=-13737.64 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.37e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2450000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.11e+03  |\n",
            "|    critic_loss     | 3.12e+03  |\n",
            "|    ent_coef        | 0.563     |\n",
            "|    ent_coef_loss   | 0.217     |\n",
            "|    learning_rate   | 0.000755  |\n",
            "|    n_updates       | 2451257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2460000, episode_reward=-4535.22 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2460000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.1e+03   |\n",
            "|    critic_loss     | 1.13e+05  |\n",
            "|    ent_coef        | 0.62      |\n",
            "|    ent_coef_loss   | -0.0492   |\n",
            "|    learning_rate   | 0.000754  |\n",
            "|    n_updates       | 2461257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.16e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 492       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 153362    |\n",
            "|    total_timesteps | 2460000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2470000, episode_reward=-13138.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.31e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2470000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.45e+03  |\n",
            "|    critic_loss     | 5.97e+04  |\n",
            "|    ent_coef        | 0.666     |\n",
            "|    ent_coef_loss   | 0.0308    |\n",
            "|    learning_rate   | 0.000753  |\n",
            "|    n_updates       | 2471257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2480000, episode_reward=-9558.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2480000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.15e+03  |\n",
            "|    critic_loss     | 6.76e+04  |\n",
            "|    ent_coef        | 1.17      |\n",
            "|    ent_coef_loss   | -0.0792   |\n",
            "|    learning_rate   | 0.000752  |\n",
            "|    n_updates       | 2481257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.15e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 496       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 154625    |\n",
            "|    total_timesteps | 2480000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2490000, episode_reward=-3693.86 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.69e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2490000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.21e+03  |\n",
            "|    critic_loss     | 7.87e+03  |\n",
            "|    ent_coef        | 1.06      |\n",
            "|    ent_coef_loss   | 0.0034    |\n",
            "|    learning_rate   | 0.000751  |\n",
            "|    n_updates       | 2491257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2500000, episode_reward=-9730.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.73e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2500000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.58e+03  |\n",
            "|    critic_loss     | 2.01e+04  |\n",
            "|    ent_coef        | 0.803     |\n",
            "|    ent_coef_loss   | 0.0332    |\n",
            "|    learning_rate   | 0.00075   |\n",
            "|    n_updates       | 2501257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.13e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 500       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 155888    |\n",
            "|    total_timesteps | 2500000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2510000, episode_reward=-3134.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2510000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.4e+03   |\n",
            "|    critic_loss     | 1.66e+04  |\n",
            "|    ent_coef        | 0.813     |\n",
            "|    ent_coef_loss   | -0.0169   |\n",
            "|    learning_rate   | 0.000749  |\n",
            "|    n_updates       | 2511257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2520000, episode_reward=-3426.37 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2520000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.39e+03  |\n",
            "|    critic_loss     | 4.05e+03  |\n",
            "|    ent_coef        | 0.706     |\n",
            "|    ent_coef_loss   | 0.05      |\n",
            "|    learning_rate   | 0.000748  |\n",
            "|    n_updates       | 2521257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.91e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 504       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 157150    |\n",
            "|    total_timesteps | 2520000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2530000, episode_reward=-2118.22 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.12e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2530000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.26e+03  |\n",
            "|    critic_loss     | 5.74e+03  |\n",
            "|    ent_coef        | 0.876     |\n",
            "|    ent_coef_loss   | -0.0333   |\n",
            "|    learning_rate   | 0.000747  |\n",
            "|    n_updates       | 2531257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2540000, episode_reward=-12845.69 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.28e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2540000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.35e+03  |\n",
            "|    critic_loss     | 7.23e+03  |\n",
            "|    ent_coef        | 0.837     |\n",
            "|    ent_coef_loss   | 0.00841   |\n",
            "|    learning_rate   | 0.000746  |\n",
            "|    n_updates       | 2541257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -1.9e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 508      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 158402   |\n",
            "|    total_timesteps | 2540000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2550000, episode_reward=-21949.85 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.19e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2550000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.3e+03   |\n",
            "|    critic_loss     | 1.75e+04  |\n",
            "|    ent_coef        | 1.1       |\n",
            "|    ent_coef_loss   | -0.00708  |\n",
            "|    learning_rate   | 0.000745  |\n",
            "|    n_updates       | 2551257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2560000, episode_reward=-13352.61 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.34e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2560000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.18e+03  |\n",
            "|    critic_loss     | 6.61e+03  |\n",
            "|    ent_coef        | 0.695     |\n",
            "|    ent_coef_loss   | -0.0242   |\n",
            "|    learning_rate   | 0.000744  |\n",
            "|    n_updates       | 2561257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.95e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 512       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 159659    |\n",
            "|    total_timesteps | 2560000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2570000, episode_reward=-16486.03 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.65e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2570000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.36e+03  |\n",
            "|    critic_loss     | 6.73e+04  |\n",
            "|    ent_coef        | 0.708     |\n",
            "|    ent_coef_loss   | 0.138     |\n",
            "|    learning_rate   | 0.000743  |\n",
            "|    n_updates       | 2571257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2580000, episode_reward=-59949.81 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.99e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2580000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.63e+03  |\n",
            "|    critic_loss     | 9.51e+03  |\n",
            "|    ent_coef        | 0.587     |\n",
            "|    ent_coef_loss   | 0.577     |\n",
            "|    learning_rate   | 0.000742  |\n",
            "|    n_updates       | 2581257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.98e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 516       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 160910    |\n",
            "|    total_timesteps | 2580000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2590000, episode_reward=-18768.09 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.88e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2590000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.48e+03  |\n",
            "|    critic_loss     | 8.34e+03  |\n",
            "|    ent_coef        | 0.695     |\n",
            "|    ent_coef_loss   | -0.0779   |\n",
            "|    learning_rate   | 0.000741  |\n",
            "|    n_updates       | 2591257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2600000, episode_reward=-71556.00 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.16e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2600000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.31e+03  |\n",
            "|    critic_loss     | 2.62e+04  |\n",
            "|    ent_coef        | 0.963     |\n",
            "|    ent_coef_loss   | 0.00606   |\n",
            "|    learning_rate   | 0.00074   |\n",
            "|    n_updates       | 2601257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.99e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 520       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 162178    |\n",
            "|    total_timesteps | 2600000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2610000, episode_reward=-16333.24 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.63e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2610000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.7e+03   |\n",
            "|    critic_loss     | 3.27e+04  |\n",
            "|    ent_coef        | 0.749     |\n",
            "|    ent_coef_loss   | -0.0408   |\n",
            "|    learning_rate   | 0.000739  |\n",
            "|    n_updates       | 2611257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2620000, episode_reward=-7862.94 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.86e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2620000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.78e+03  |\n",
            "|    critic_loss     | 2.77e+04  |\n",
            "|    ent_coef        | 0.493     |\n",
            "|    ent_coef_loss   | 0.116     |\n",
            "|    learning_rate   | 0.000738  |\n",
            "|    n_updates       | 2621257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.05e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 524       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 163349    |\n",
            "|    total_timesteps | 2620000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2630000, episode_reward=-12259.54 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.23e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2630000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.52e+03  |\n",
            "|    critic_loss     | 3.4e+05   |\n",
            "|    ent_coef        | 0.586     |\n",
            "|    ent_coef_loss   | -0.0178   |\n",
            "|    learning_rate   | 0.000737  |\n",
            "|    n_updates       | 2631257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2640000, episode_reward=-13599.55 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.36e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2640000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.52e+03  |\n",
            "|    critic_loss     | 6.1e+03   |\n",
            "|    ent_coef        | 0.718     |\n",
            "|    ent_coef_loss   | -0.0695   |\n",
            "|    learning_rate   | 0.000736  |\n",
            "|    n_updates       | 2641257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.03e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 528       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 164528    |\n",
            "|    total_timesteps | 2640000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2650000, episode_reward=-16343.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.63e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2650000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.57e+03  |\n",
            "|    critic_loss     | 1.05e+04  |\n",
            "|    ent_coef        | 0.722     |\n",
            "|    ent_coef_loss   | -0.085    |\n",
            "|    learning_rate   | 0.000735  |\n",
            "|    n_updates       | 2651257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2660000, episode_reward=-1844.36 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2660000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.75e+03  |\n",
            "|    critic_loss     | 1.35e+04  |\n",
            "|    ent_coef        | 1.12      |\n",
            "|    ent_coef_loss   | 0.038     |\n",
            "|    learning_rate   | 0.000734  |\n",
            "|    n_updates       | 2661257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.97e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 532       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 165702    |\n",
            "|    total_timesteps | 2660000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2670000, episode_reward=-1271.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.27e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2670000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.49e+03  |\n",
            "|    critic_loss     | 6.5e+03   |\n",
            "|    ent_coef        | 1.08      |\n",
            "|    ent_coef_loss   | -0.0163   |\n",
            "|    learning_rate   | 0.000733  |\n",
            "|    n_updates       | 2671257   |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2680000, episode_reward=-1084.29 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.08e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2680000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.71e+03  |\n",
            "|    critic_loss     | 2.03e+04  |\n",
            "|    ent_coef        | 1.27      |\n",
            "|    ent_coef_loss   | 0.00194   |\n",
            "|    learning_rate   | 0.000732  |\n",
            "|    n_updates       | 2681257   |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.93e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 536       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 166873    |\n",
            "|    total_timesteps | 2680000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2690000, episode_reward=-1423.85 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2690000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.33e+03  |\n",
            "|    critic_loss     | 6.97e+03  |\n",
            "|    ent_coef        | 1.07      |\n",
            "|    ent_coef_loss   | 0.0211    |\n",
            "|    learning_rate   | 0.000731  |\n",
            "|    n_updates       | 2691257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2700000, episode_reward=-14460.02 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.45e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2700000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.33e+03  |\n",
            "|    critic_loss     | 8.23e+03  |\n",
            "|    ent_coef        | 0.881     |\n",
            "|    ent_coef_loss   | -0.0274   |\n",
            "|    learning_rate   | 0.00073   |\n",
            "|    n_updates       | 2701257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.79e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 540       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 168042    |\n",
            "|    total_timesteps | 2700000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2710000, episode_reward=-17659.37 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.77e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2710000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.07e+03  |\n",
            "|    critic_loss     | 1.36e+05  |\n",
            "|    ent_coef        | 0.924     |\n",
            "|    ent_coef_loss   | -0.00185  |\n",
            "|    learning_rate   | 0.000729  |\n",
            "|    n_updates       | 2711257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2720000, episode_reward=-19953.08 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -2e+04   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2720000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.34e+03 |\n",
            "|    critic_loss     | 1.56e+04 |\n",
            "|    ent_coef        | 0.96     |\n",
            "|    ent_coef_loss   | -0.0121  |\n",
            "|    learning_rate   | 0.000728 |\n",
            "|    n_updates       | 2721257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.55e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 544       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 169208    |\n",
            "|    total_timesteps | 2720000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2730000, episode_reward=-20284.41 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.03e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2730000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.07e+03  |\n",
            "|    critic_loss     | 5.17e+03  |\n",
            "|    ent_coef        | 1.08      |\n",
            "|    ent_coef_loss   | 0.00995   |\n",
            "|    learning_rate   | 0.000727  |\n",
            "|    n_updates       | 2731257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2740000, episode_reward=-8882.75 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.88e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2740000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.06e+03  |\n",
            "|    critic_loss     | 4.77e+05  |\n",
            "|    ent_coef        | 1.15      |\n",
            "|    ent_coef_loss   | 0.00648   |\n",
            "|    learning_rate   | 0.000726  |\n",
            "|    n_updates       | 2741257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.56e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 548       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 170379    |\n",
            "|    total_timesteps | 2740000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2750000, episode_reward=-556.18 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -556     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2750000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 984      |\n",
            "|    critic_loss     | 1.09e+04 |\n",
            "|    ent_coef        | 0.926    |\n",
            "|    ent_coef_loss   | -0.0192  |\n",
            "|    learning_rate   | 0.000725 |\n",
            "|    n_updates       | 2751257  |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2760000, episode_reward=-1427.09 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2760000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 979       |\n",
            "|    critic_loss     | 6.05e+03  |\n",
            "|    ent_coef        | 0.849     |\n",
            "|    ent_coef_loss   | 0.0436    |\n",
            "|    learning_rate   | 0.000724  |\n",
            "|    n_updates       | 2761257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 552       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 171556    |\n",
            "|    total_timesteps | 2760000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2770000, episode_reward=-1748.85 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.75e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2770000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.06e+03  |\n",
            "|    critic_loss     | 6.37e+03  |\n",
            "|    ent_coef        | 0.826     |\n",
            "|    ent_coef_loss   | 0.00757   |\n",
            "|    learning_rate   | 0.000723  |\n",
            "|    n_updates       | 2771257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2780000, episode_reward=-948.38 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -948     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2780000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.1e+03  |\n",
            "|    critic_loss     | 4.12e+03 |\n",
            "|    ent_coef        | 0.734    |\n",
            "|    ent_coef_loss   | 0.144    |\n",
            "|    learning_rate   | 0.000722 |\n",
            "|    n_updates       | 2781257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.51e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 556       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 172737    |\n",
            "|    total_timesteps | 2780000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2790000, episode_reward=-2670.19 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2790000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.28e+03  |\n",
            "|    critic_loss     | 3.52e+04  |\n",
            "|    ent_coef        | 0.786     |\n",
            "|    ent_coef_loss   | -0.0608   |\n",
            "|    learning_rate   | 0.000721  |\n",
            "|    n_updates       | 2791257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2800000, episode_reward=-1052.10 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.05e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2800000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.26e+03  |\n",
            "|    critic_loss     | 1.31e+04  |\n",
            "|    ent_coef        | 0.773     |\n",
            "|    ent_coef_loss   | -0.0749   |\n",
            "|    learning_rate   | 0.00072   |\n",
            "|    n_updates       | 2801257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.47e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 560       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 173911    |\n",
            "|    total_timesteps | 2800000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2810000, episode_reward=-16105.31 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2810000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.45e+03  |\n",
            "|    critic_loss     | 1.87e+04  |\n",
            "|    ent_coef        | 1.39      |\n",
            "|    ent_coef_loss   | -0.129    |\n",
            "|    learning_rate   | 0.000719  |\n",
            "|    n_updates       | 2811257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2820000, episode_reward=-16115.92 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.61e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2820000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.42e+03  |\n",
            "|    critic_loss     | 1.39e+04  |\n",
            "|    ent_coef        | 0.997     |\n",
            "|    ent_coef_loss   | -0.00031  |\n",
            "|    learning_rate   | 0.000718  |\n",
            "|    n_updates       | 2821257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.46e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 564       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 175082    |\n",
            "|    total_timesteps | 2820000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2830000, episode_reward=-18824.62 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.88e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2830000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.31e+03  |\n",
            "|    critic_loss     | 1.25e+04  |\n",
            "|    ent_coef        | 1.74      |\n",
            "|    ent_coef_loss   | 0.179     |\n",
            "|    learning_rate   | 0.000717  |\n",
            "|    n_updates       | 2831257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2840000, episode_reward=-21851.08 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.19e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2840000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 995       |\n",
            "|    critic_loss     | 1.34e+04  |\n",
            "|    ent_coef        | 1.82      |\n",
            "|    ent_coef_loss   | 0.108     |\n",
            "|    learning_rate   | 0.000716  |\n",
            "|    n_updates       | 2841257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.47e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 568       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 176248    |\n",
            "|    total_timesteps | 2840000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2850000, episode_reward=-16523.72 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.65e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2850000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.18e+03  |\n",
            "|    critic_loss     | 6.97e+03  |\n",
            "|    ent_coef        | 1.02      |\n",
            "|    ent_coef_loss   | -0.00394  |\n",
            "|    learning_rate   | 0.000715  |\n",
            "|    n_updates       | 2851257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2860000, episode_reward=-15041.96 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.5e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2860000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.11e+03 |\n",
            "|    critic_loss     | 7.47e+03 |\n",
            "|    ent_coef        | 1.04     |\n",
            "|    ent_coef_loss   | -0.0119  |\n",
            "|    learning_rate   | 0.000714 |\n",
            "|    n_updates       | 2861257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 572       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 177412    |\n",
            "|    total_timesteps | 2860000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2870000, episode_reward=-2324.60 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.32e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2870000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.47e+03  |\n",
            "|    critic_loss     | 2.39e+05  |\n",
            "|    ent_coef        | 0.91      |\n",
            "|    ent_coef_loss   | 0.0157    |\n",
            "|    learning_rate   | 0.000713  |\n",
            "|    n_updates       | 2871257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2880000, episode_reward=-13050.93 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.31e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2880000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.02e+03  |\n",
            "|    critic_loss     | 9.22e+03  |\n",
            "|    ent_coef        | 0.972     |\n",
            "|    ent_coef_loss   | 0.0044    |\n",
            "|    learning_rate   | 0.000712  |\n",
            "|    n_updates       | 2881257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.49e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 576       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 178572    |\n",
            "|    total_timesteps | 2880000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2890000, episode_reward=-16693.93 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.67e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2890000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 949       |\n",
            "|    critic_loss     | 2.21e+04  |\n",
            "|    ent_coef        | 0.607     |\n",
            "|    ent_coef_loss   | -0.14     |\n",
            "|    learning_rate   | 0.000711  |\n",
            "|    n_updates       | 2891257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2900000, episode_reward=-16544.45 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.65e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2900000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 886       |\n",
            "|    critic_loss     | 1.97e+03  |\n",
            "|    ent_coef        | 0.719     |\n",
            "|    ent_coef_loss   | -0.122    |\n",
            "|    learning_rate   | 0.00071   |\n",
            "|    n_updates       | 2901257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.49e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 580       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 179732    |\n",
            "|    total_timesteps | 2900000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2910000, episode_reward=-11451.78 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.15e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2910000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 962       |\n",
            "|    critic_loss     | 3.46e+03  |\n",
            "|    ent_coef        | 0.866     |\n",
            "|    ent_coef_loss   | -0.0471   |\n",
            "|    learning_rate   | 0.000709  |\n",
            "|    n_updates       | 2911257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2920000, episode_reward=-9426.37 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2920000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.08e+03  |\n",
            "|    critic_loss     | 6.15e+03  |\n",
            "|    ent_coef        | 0.587     |\n",
            "|    ent_coef_loss   | -0.0517   |\n",
            "|    learning_rate   | 0.000708  |\n",
            "|    n_updates       | 2921257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 584       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 180897    |\n",
            "|    total_timesteps | 2920000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2930000, episode_reward=-16625.30 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.66e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2930000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.22e+03  |\n",
            "|    critic_loss     | 9.19e+04  |\n",
            "|    ent_coef        | 0.647     |\n",
            "|    ent_coef_loss   | -0.0212   |\n",
            "|    learning_rate   | 0.000707  |\n",
            "|    n_updates       | 2931257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2940000, episode_reward=-14788.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2940000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.09e+03  |\n",
            "|    critic_loss     | 3.81e+03  |\n",
            "|    ent_coef        | 0.513     |\n",
            "|    ent_coef_loss   | -0.18     |\n",
            "|    learning_rate   | 0.000706  |\n",
            "|    n_updates       | 2941257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 588       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 182063    |\n",
            "|    total_timesteps | 2940000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2950000, episode_reward=-15620.16 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.56e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2950000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.05e+03  |\n",
            "|    critic_loss     | 7.15e+03  |\n",
            "|    ent_coef        | 0.514     |\n",
            "|    ent_coef_loss   | -0.0764   |\n",
            "|    learning_rate   | 0.000705  |\n",
            "|    n_updates       | 2951257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2960000, episode_reward=-14421.35 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.44e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2960000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.05e+03  |\n",
            "|    critic_loss     | 4.18e+03  |\n",
            "|    ent_coef        | 0.477     |\n",
            "|    ent_coef_loss   | -0.0147   |\n",
            "|    learning_rate   | 0.000704  |\n",
            "|    n_updates       | 2961257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.37e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 592       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 183231    |\n",
            "|    total_timesteps | 2960000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2970000, episode_reward=-16014.32 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.6e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2970000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.22e+03 |\n",
            "|    critic_loss     | 4.71e+04 |\n",
            "|    ent_coef        | 0.491    |\n",
            "|    ent_coef_loss   | -0.00239 |\n",
            "|    learning_rate   | 0.000703 |\n",
            "|    n_updates       | 2971257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2980000, episode_reward=-17493.73 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.75e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2980000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.06e+03  |\n",
            "|    critic_loss     | 3.4e+03   |\n",
            "|    ent_coef        | 0.56      |\n",
            "|    ent_coef_loss   | -0.123    |\n",
            "|    learning_rate   | 0.000702  |\n",
            "|    n_updates       | 2981257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.39e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 596       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 184403    |\n",
            "|    total_timesteps | 2980000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2990000, episode_reward=-10052.68 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.01e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 2990000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.22e+03  |\n",
            "|    critic_loss     | 1e+04     |\n",
            "|    ent_coef        | 0.511     |\n",
            "|    ent_coef_loss   | 0.25      |\n",
            "|    learning_rate   | 0.000701  |\n",
            "|    n_updates       | 2991257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3000000, episode_reward=-15395.99 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3000000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 924       |\n",
            "|    critic_loss     | 1.9e+03   |\n",
            "|    ent_coef        | 0.298     |\n",
            "|    ent_coef_loss   | 0.355     |\n",
            "|    learning_rate   | 0.0007    |\n",
            "|    n_updates       | 3001257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.41e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 600       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 185581    |\n",
            "|    total_timesteps | 3000000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3010000, episode_reward=-15845.11 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.58e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3010000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 937       |\n",
            "|    critic_loss     | 1.46e+04  |\n",
            "|    ent_coef        | 0.309     |\n",
            "|    ent_coef_loss   | 0.463     |\n",
            "|    learning_rate   | 0.000699  |\n",
            "|    n_updates       | 3011257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3020000, episode_reward=-3666.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3020000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 899       |\n",
            "|    critic_loss     | 4.54e+03  |\n",
            "|    ent_coef        | 0.453     |\n",
            "|    ent_coef_loss   | 0.0811    |\n",
            "|    learning_rate   | 0.000698  |\n",
            "|    n_updates       | 3021257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 604       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 186758    |\n",
            "|    total_timesteps | 3020000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3030000, episode_reward=-3963.95 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.96e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3030000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 942       |\n",
            "|    critic_loss     | 3.98e+03  |\n",
            "|    ent_coef        | 1.34      |\n",
            "|    ent_coef_loss   | -0.0315   |\n",
            "|    learning_rate   | 0.000697  |\n",
            "|    n_updates       | 3031257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3040000, episode_reward=-25911.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3040000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 762       |\n",
            "|    critic_loss     | 9.11e+03  |\n",
            "|    ent_coef        | 0.892     |\n",
            "|    ent_coef_loss   | -0.0191   |\n",
            "|    learning_rate   | 0.000696  |\n",
            "|    n_updates       | 3041257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 608       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 187928    |\n",
            "|    total_timesteps | 3040000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3050000, episode_reward=-12615.09 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.26e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3050000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 441       |\n",
            "|    critic_loss     | 2.91e+03  |\n",
            "|    ent_coef        | 0.979     |\n",
            "|    ent_coef_loss   | -0.00339  |\n",
            "|    learning_rate   | 0.000695  |\n",
            "|    n_updates       | 3051257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3060000, episode_reward=-9882.91 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.88e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3060000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 457       |\n",
            "|    critic_loss     | 3.08e+03  |\n",
            "|    ent_coef        | 0.784     |\n",
            "|    ent_coef_loss   | 0.0106    |\n",
            "|    learning_rate   | 0.000694  |\n",
            "|    n_updates       | 3061257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.39e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 612       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 189092    |\n",
            "|    total_timesteps | 3060000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3070000, episode_reward=-9720.76 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.72e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3070000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 826       |\n",
            "|    critic_loss     | 3.92e+03  |\n",
            "|    ent_coef        | 0.627     |\n",
            "|    ent_coef_loss   | -0.0943   |\n",
            "|    learning_rate   | 0.000693  |\n",
            "|    n_updates       | 3071257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3080000, episode_reward=-986.15 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -986     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3080000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 921      |\n",
            "|    critic_loss     | 4.82e+03 |\n",
            "|    ent_coef        | 0.575    |\n",
            "|    ent_coef_loss   | -0.102   |\n",
            "|    learning_rate   | 0.000692 |\n",
            "|    n_updates       | 3081257  |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.33e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 616       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 190250    |\n",
            "|    total_timesteps | 3080000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3090000, episode_reward=-9463.60 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -9.46e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3090000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 992       |\n",
            "|    critic_loss     | 4.18e+03  |\n",
            "|    ent_coef        | 0.888     |\n",
            "|    ent_coef_loss   | 0.00939   |\n",
            "|    learning_rate   | 0.000691  |\n",
            "|    n_updates       | 3091257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3100000, episode_reward=-12672.45 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.27e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3100000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 944       |\n",
            "|    critic_loss     | 5.87e+03  |\n",
            "|    ent_coef        | 0.649     |\n",
            "|    ent_coef_loss   | -0.183    |\n",
            "|    learning_rate   | 0.00069   |\n",
            "|    n_updates       | 3101257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -1.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 620      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 191410   |\n",
            "|    total_timesteps | 3100000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3110000, episode_reward=-16048.87 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 5e+03    |\n",
            "|    mean_reward     | -1.6e+04 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3110000  |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 961      |\n",
            "|    critic_loss     | 2.8e+03  |\n",
            "|    ent_coef        | 0.539    |\n",
            "|    ent_coef_loss   | -0.11    |\n",
            "|    learning_rate   | 0.000689 |\n",
            "|    n_updates       | 3111257  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3120000, episode_reward=-20419.13 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.04e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3120000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 980       |\n",
            "|    critic_loss     | 1.23e+04  |\n",
            "|    ent_coef        | 0.959     |\n",
            "|    ent_coef_loss   | -0.00756  |\n",
            "|    learning_rate   | 0.000688  |\n",
            "|    n_updates       | 3121257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.23e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 624       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 192670    |\n",
            "|    total_timesteps | 3120000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3130000, episode_reward=-16199.73 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.62e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3130000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.06e+03  |\n",
            "|    critic_loss     | 9.08e+03  |\n",
            "|    ent_coef        | 0.769     |\n",
            "|    ent_coef_loss   | 0.0821    |\n",
            "|    learning_rate   | 0.000687  |\n",
            "|    n_updates       | 3131257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3140000, episode_reward=-13909.79 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.39e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3140000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.07e+03  |\n",
            "|    critic_loss     | 1.78e+04  |\n",
            "|    ent_coef        | 0.82      |\n",
            "|    ent_coef_loss   | 0.0814    |\n",
            "|    learning_rate   | 0.000686  |\n",
            "|    n_updates       | 3141257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -1.47e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 628       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 193951    |\n",
            "|    total_timesteps | 3140000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3150000, episode_reward=-14239.68 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.42e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3150000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.02e+03  |\n",
            "|    critic_loss     | 1.54e+04  |\n",
            "|    ent_coef        | 0.898     |\n",
            "|    ent_coef_loss   | -0.0503   |\n",
            "|    learning_rate   | 0.000685  |\n",
            "|    n_updates       | 3151257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3160000, episode_reward=-14259.70 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.43e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3160000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.03e+03  |\n",
            "|    critic_loss     | 7.1e+03   |\n",
            "|    ent_coef        | 1.3       |\n",
            "|    ent_coef_loss   | 0.102     |\n",
            "|    learning_rate   | 0.000684  |\n",
            "|    n_updates       | 3161257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.25e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 632       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 195311    |\n",
            "|    total_timesteps | 3160000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3170000, episode_reward=-14176.40 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.42e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3170000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 543       |\n",
            "|    critic_loss     | 1.76e+04  |\n",
            "|    ent_coef        | 1.28      |\n",
            "|    ent_coef_loss   | -0.00451  |\n",
            "|    learning_rate   | 0.000683  |\n",
            "|    n_updates       | 3171257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3180000, episode_reward=-20669.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.07e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3180000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 732       |\n",
            "|    critic_loss     | 9.24e+03  |\n",
            "|    ent_coef        | 0.94      |\n",
            "|    ent_coef_loss   | 0.00916   |\n",
            "|    learning_rate   | 0.000682  |\n",
            "|    n_updates       | 3181257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.28e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 636       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 196522    |\n",
            "|    total_timesteps | 3180000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3190000, episode_reward=-21286.52 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.13e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3190000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 843       |\n",
            "|    critic_loss     | 8.23e+03  |\n",
            "|    ent_coef        | 0.647     |\n",
            "|    ent_coef_loss   | -0.0335   |\n",
            "|    learning_rate   | 0.000681  |\n",
            "|    n_updates       | 3191257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3200000, episode_reward=-8834.75 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3200000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.12e+03  |\n",
            "|    critic_loss     | 2.71e+04  |\n",
            "|    ent_coef        | 0.772     |\n",
            "|    ent_coef_loss   | 0.00685   |\n",
            "|    learning_rate   | 0.00068   |\n",
            "|    n_updates       | 3201257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 640      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 197704   |\n",
            "|    total_timesteps | 3200000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3210000, episode_reward=-15901.86 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.59e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3210000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 751       |\n",
            "|    critic_loss     | 1.3e+04   |\n",
            "|    ent_coef        | 1.02      |\n",
            "|    ent_coef_loss   | 0.00647   |\n",
            "|    learning_rate   | 0.000679  |\n",
            "|    n_updates       | 3211257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3220000, episode_reward=-19309.90 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.93e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3220000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 708       |\n",
            "|    critic_loss     | 5.91e+03  |\n",
            "|    ent_coef        | 0.791     |\n",
            "|    ent_coef_loss   | -0.00833  |\n",
            "|    learning_rate   | 0.000678  |\n",
            "|    n_updates       | 3221257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 644      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 198886   |\n",
            "|    total_timesteps | 3220000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3230000, episode_reward=-22556.62 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.26e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3230000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 473       |\n",
            "|    critic_loss     | 2.74e+03  |\n",
            "|    ent_coef        | 0.808     |\n",
            "|    ent_coef_loss   | -0.0503   |\n",
            "|    learning_rate   | 0.000677  |\n",
            "|    n_updates       | 3231257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3240000, episode_reward=-6515.61 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3240000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 492       |\n",
            "|    critic_loss     | 1.16e+04  |\n",
            "|    ent_coef        | 1.14      |\n",
            "|    ent_coef_loss   | 0.0164    |\n",
            "|    learning_rate   | 0.000676  |\n",
            "|    n_updates       | 3241257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.28e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 648       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 200077    |\n",
            "|    total_timesteps | 3240000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3250000, episode_reward=-5592.54 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -5.59e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3250000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 561       |\n",
            "|    critic_loss     | 2.37e+04  |\n",
            "|    ent_coef        | 1.31      |\n",
            "|    ent_coef_loss   | 0.0216    |\n",
            "|    learning_rate   | 0.000675  |\n",
            "|    n_updates       | 3251257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3260000, episode_reward=-7678.34 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.68e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3260000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 604       |\n",
            "|    critic_loss     | 1.87e+04  |\n",
            "|    ent_coef        | 1.17      |\n",
            "|    ent_coef_loss   | -0.0338   |\n",
            "|    learning_rate   | 0.000674  |\n",
            "|    n_updates       | 3261257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.28e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 652       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 201278    |\n",
            "|    total_timesteps | 3260000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3270000, episode_reward=-8193.06 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.19e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3270000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 716       |\n",
            "|    critic_loss     | 1.54e+04  |\n",
            "|    ent_coef        | 0.563     |\n",
            "|    ent_coef_loss   | 0.072     |\n",
            "|    learning_rate   | 0.000673  |\n",
            "|    n_updates       | 3271257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3280000, episode_reward=-11786.54 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.18e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3280000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 713       |\n",
            "|    critic_loss     | 9.02e+03  |\n",
            "|    ent_coef        | 0.506     |\n",
            "|    ent_coef_loss   | -0.0333   |\n",
            "|    learning_rate   | 0.000672  |\n",
            "|    n_updates       | 3281257   |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 5e+03    |\n",
            "|    ep_rew_mean     | -2.3e+04 |\n",
            "| time/              |          |\n",
            "|    episodes        | 656      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 202475   |\n",
            "|    total_timesteps | 3280000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3290000, episode_reward=-11117.58 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.11e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3290000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 728       |\n",
            "|    critic_loss     | 8.73e+03  |\n",
            "|    ent_coef        | 0.436     |\n",
            "|    ent_coef_loss   | -0.11     |\n",
            "|    learning_rate   | 0.000671  |\n",
            "|    n_updates       | 3291257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3300000, episode_reward=-20066.57 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -2.01e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3300000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 841       |\n",
            "|    critic_loss     | 1.54e+04  |\n",
            "|    ent_coef        | 0.46      |\n",
            "|    ent_coef_loss   | -0.0749   |\n",
            "|    learning_rate   | 0.00067   |\n",
            "|    n_updates       | 3301257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.34e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 660       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 203681    |\n",
            "|    total_timesteps | 3300000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3310000, episode_reward=-8708.74 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -8.71e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3310000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 778       |\n",
            "|    critic_loss     | 7.36e+03  |\n",
            "|    ent_coef        | 0.577     |\n",
            "|    ent_coef_loss   | 0.0277    |\n",
            "|    learning_rate   | 0.000669  |\n",
            "|    n_updates       | 3311257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3320000, episode_reward=-12642.98 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.26e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3320000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 783       |\n",
            "|    critic_loss     | 1.09e+04  |\n",
            "|    ent_coef        | 0.542     |\n",
            "|    ent_coef_loss   | -0.191    |\n",
            "|    learning_rate   | 0.000668  |\n",
            "|    n_updates       | 3321257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.35e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 664       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 204884    |\n",
            "|    total_timesteps | 3320000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3330000, episode_reward=-7857.39 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -7.86e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3330000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 706       |\n",
            "|    critic_loss     | 6.23e+03  |\n",
            "|    ent_coef        | 0.471     |\n",
            "|    ent_coef_loss   | -0.0262   |\n",
            "|    learning_rate   | 0.000667  |\n",
            "|    n_updates       | 3331257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3340000, episode_reward=-15361.16 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.54e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3340000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 734       |\n",
            "|    critic_loss     | 1.31e+04  |\n",
            "|    ent_coef        | 0.431     |\n",
            "|    ent_coef_loss   | 0.0193    |\n",
            "|    learning_rate   | 0.000666  |\n",
            "|    n_updates       | 3341257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.32e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 668       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 206064    |\n",
            "|    total_timesteps | 3340000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3350000, episode_reward=-11943.99 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.19e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3350000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 833       |\n",
            "|    critic_loss     | 2.89e+04  |\n",
            "|    ent_coef        | 0.556     |\n",
            "|    ent_coef_loss   | -0.167    |\n",
            "|    learning_rate   | 0.000665  |\n",
            "|    n_updates       | 3351257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3360000, episode_reward=-4047.17 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.05e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3360000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 785       |\n",
            "|    critic_loss     | 2.91e+03  |\n",
            "|    ent_coef        | 0.678     |\n",
            "|    ent_coef_loss   | -0.132    |\n",
            "|    learning_rate   | 0.000664  |\n",
            "|    n_updates       | 3361257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.29e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 672       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 207244    |\n",
            "|    total_timesteps | 3360000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3370000, episode_reward=-3175.23 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -3.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3370000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 779       |\n",
            "|    critic_loss     | 2.19e+04  |\n",
            "|    ent_coef        | 0.371     |\n",
            "|    ent_coef_loss   | 0.0707    |\n",
            "|    learning_rate   | 0.000663  |\n",
            "|    n_updates       | 3371257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3380000, episode_reward=-6132.27 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3380000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 720       |\n",
            "|    critic_loss     | 2.82e+03  |\n",
            "|    ent_coef        | 0.381     |\n",
            "|    ent_coef_loss   | -0.225    |\n",
            "|    learning_rate   | 0.000662  |\n",
            "|    n_updates       | 3381257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.26e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 676       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 208422    |\n",
            "|    total_timesteps | 3380000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3390000, episode_reward=-14818.31 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.48e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3390000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 743       |\n",
            "|    critic_loss     | 1.63e+03  |\n",
            "|    ent_coef        | 0.299     |\n",
            "|    ent_coef_loss   | -0.125    |\n",
            "|    learning_rate   | 0.000661  |\n",
            "|    n_updates       | 3391257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3400000, episode_reward=-6382.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3400000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 722       |\n",
            "|    critic_loss     | 1.76e+03  |\n",
            "|    ent_coef        | 0.322     |\n",
            "|    ent_coef_loss   | -0.419    |\n",
            "|    learning_rate   | 0.00066   |\n",
            "|    n_updates       | 3401257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.25e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 680       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 209625    |\n",
            "|    total_timesteps | 3400000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3410000, episode_reward=-6629.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -6.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3410000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 617       |\n",
            "|    critic_loss     | 1.68e+04  |\n",
            "|    ent_coef        | 0.412     |\n",
            "|    ent_coef_loss   | -0.165    |\n",
            "|    learning_rate   | 0.000659  |\n",
            "|    n_updates       | 3411257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3420000, episode_reward=-12203.03 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.22e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3420000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 776       |\n",
            "|    critic_loss     | 2.67e+03  |\n",
            "|    ent_coef        | 0.321     |\n",
            "|    ent_coef_loss   | 0.179     |\n",
            "|    learning_rate   | 0.000658  |\n",
            "|    n_updates       | 3421257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.26e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 684       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 210821    |\n",
            "|    total_timesteps | 3420000   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3430000, episode_reward=-4835.50 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -4.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3430000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 813       |\n",
            "|    critic_loss     | 4.31e+03  |\n",
            "|    ent_coef        | 0.4       |\n",
            "|    ent_coef_loss   | 0.26      |\n",
            "|    learning_rate   | 0.000657  |\n",
            "|    n_updates       | 3431257   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3440000, episode_reward=-11938.70 +/- 0.00\n",
            "Episode length: 5000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 5e+03     |\n",
            "|    mean_reward     | -1.19e+04 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 3440000   |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 875       |\n",
            "|    critic_loss     | 2.33e+03  |\n",
            "|    ent_coef        | 0.372     |\n",
            "|    ent_coef_loss   | -0.0499   |\n",
            "|    learning_rate   | 0.000656  |\n",
            "|    n_updates       | 3441257   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 5e+03     |\n",
            "|    ep_rew_mean     | -2.26e+04 |\n",
            "| time/              |           |\n",
            "|    episodes        | 688       |\n",
            "|    fps             | 16        |\n",
            "|    time_elapsed    | 212029    |\n",
            "|    total_timesteps | 3440000   |\n",
            "----------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgradient_save_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/2gdl8x8nocrash.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_save_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\stable_baselines3\\sac\\sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:331\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\stable_baselines3\\sac\\sac.py:267\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Optimize the critic\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 267\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# Alternative: actor_loss = th.mean(log_prob - qf1_pi)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Min over all critic networks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=10000000,\n",
        "    callback=[\n",
        "        checkpoint_callback,\n",
        "        eval_callback,\n",
        "        WandbCallback(\n",
        "            gradient_save_freq=10000,\n",
        "            model_save_path=f\"models/2gdl8x8nocrash.2\",\n",
        "            model_save_freq=10000,\n",
        "            verbose=2,\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHXGbAZET2JT",
        "outputId": "b97fc6c4-ec5b-4df7-d0d1-33c8be525e88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:1 Score:-57087.66697450366\n",
            "Episode:2 Score:-57169.60609087738\n",
            "Episode:3 Score:-57424.86711319699\n"
          ]
        }
      ],
      "source": [
        "episodes = 3\n",
        "state, seed = env.reset()\n",
        "for episode in range(1, episodes+1):\n",
        "    state, seed = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = Bobina()\n",
        "b.airgap = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((20.00214, -0.10582279, 3.7021403, 45.0), False)"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b.step(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "agkJzsGwT_Uc",
        "outputId": "53ab7a6a-04fc-4968-e6b5-f07eb32b5f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "[-6.3 -6.3  0.   0.   0.   0. ]\n",
            "Time predict 10.924832820892334\n",
            "Time model 67.07681846618652\n",
            "Episode reward -21742.932198846713\n",
            "[(0.0, 0.0), (-3.2980323, 3.2852898), (-6.4622445, 6.4871492), (-9.34559, 6.130542), (-11.946186, 2.1916382), (-14.27443, -1.5946237), (-16.39189, -4.9274845), (-18.371971, -7.931929), (-20.23514, -10.657313), (-21.990543, -13.097919), (-23.648859, -15.301554), (-25.22713, -17.3456), (-26.736614, -19.266346), (-28.184608, -21.073292), (-29.577755, -22.77825), (-30.921392, -24.400772), (-32.219673, -25.952799), (-33.47646, -27.434887), (-34.69315, -28.856543), (-35.870956, -30.226986), (-37.011204, -31.549433), (-38.1155, -32.827183), (-39.18497, -34.063858), (-40.220905, -35.261074), (-41.22466, -36.420116), (-42.197384, -37.542416), (-43.139988, -38.62894), (-44.053383, -39.680595), (-44.938683, -40.698597), (-45.0, -41.68392), (-45.0, -42.637135), (-45.0, -43.55853), (-45.0, -44.44816), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -45.0), (-45.0, -44.398064), (-45.0, -41.64586), (-45.0, -38.725246), (-45.0, -35.769833), (-45.0, -32.81485), (-45.0, -29.882448), (-45.0, -27.842367), (-45.0, -27.68194), (-45.0, -28.547312), (-45.0, -29.060562), (-42.418858, -28.826666), (-38.968204, -27.20791), (-35.69049, -24.39749), (-32.79572, -21.4395), (-30.664515, -18.602444), (-29.631819, -16.19616), (-29.565016, -15.189027), (-29.9869, -15.830047), (-30.51827, -16.978561), (-30.977867, -17.706741), (-31.335619, -17.906538), (-31.63864, -18.0008), (-31.926245, -18.29094), (-32.19268, -18.663939), (-32.41611, -18.908596), (-32.594574, -18.999855), (-32.870174, -19.183931), (-33.382256, -19.675215), (-34.087616, -20.417097), (-34.909462, -21.291822), (-35.720985, -22.240831), (-35.758137, -23.202955), (-34.11562, -23.696133), (-31.23463, -22.444645), (-28.120535, -19.656221), (-25.378656, -16.642225), (-23.769194, -13.696571), (-23.73643, -11.222573), (-24.787146, -11.221255), (-25.975733, -13.349668), (-25.502169, -15.436803), (-22.991686, -16.069782), (-19.926971, -14.254096), (-16.920712, -11.350106), (-14.176157, -8.659716), (-12.225167, -6.6794057), (-11.187581, -6.0615582), (-10.253017, -6.4125214), (-8.860231, -6.345825), (-7.3820767, -5.3609295), (-6.172109, -3.9874737), (-5.0741262, -3.0417533), (-4.109023, -2.8274198), (-3.6002116, -2.949852), (-3.2847748, -3.0972886), (-2.5147288, -3.1326172), (-1.5760727, -3.143696), (-1.16366, -3.331484), (-0.96538395, -3.5677068), (-0.24457909, -3.5991206), (0.68310434, -3.5378911), (1.0475138, -3.6457355), (1.1959299, -3.8116813), (2.026235, -3.5405254), (3.608219, -2.1140163), (5.364977, 0.33933526), (6.336996, 2.8249304), (6.246197, 3.9870021), (6.5494075, 3.734015), (7.5638356, 3.5046613), (7.5606284, 4.108347), (7.092091, 5.135597), (8.1080675, 5.7193246), (9.493352, 6.177152), (8.931809, 7.2096124), (8.26509, 8.207098), (9.748133, 8.366994), (11.039448, 8.594413), (9.8736925, 9.720666), (9.270924, 10.612294), (11.217859, 10.45852), (12.098817, 10.648885), (10.372449, 11.878719), (10.133428, 12.502039), (12.530642, 12.021513), (12.755283, 12.323453), (10.463102, 13.679143), (10.641528, 13.908801), (13.405733, 13.040993), (13.398084, 13.446458), (10.7388525, 15.004661), (10.904081, 14.858014), (13.75327, 13.902844), (13.984459, 15.034097), (11.49152, 17.078909), (11.548633, 16.08707), (14.207395, 14.230977), (14.239547, 15.633053), (11.755653, 18.180449), (11.875729, 18.921457), (14.599568, 17.770771), (14.495623, 17.813736), (11.748047, 19.679255), (11.965176, 20.289402), (14.911298, 18.910315), (14.836021, 18.697693), (11.782943, 20.531778), (11.756726, 21.279272), (14.742353, 19.767088), (14.8397455, 19.053953), (11.899029, 20.724562), (11.810477, 22.152353), (14.762524, 21.009962), (14.742212, 18.8033), (11.615691, 19.232819), (11.315303, 21.98332), (14.1118965, 21.959051), (15.137547, 19.262968), (12.660872, 19.27019), (10.658794, 21.861605), (12.172929, 21.790297), (14.908378, 18.724625), (14.071483, 18.33525), (10.321139, 20.963957), (10.115396, 21.80051), (13.101965, 19.242714), (13.340669, 17.967451), (10.463112, 20.155836), (10.157937, 21.087444), (12.915148, 18.489162), (12.734732, 17.131351), (9.371391, 19.31117), (9.021318, 20.591122), (11.887735, 18.248064), (11.880598, 15.762146), (8.651244, 16.88468), (7.985972, 19.6994), (10.547615, 18.886736), (11.186484, 14.653039), (8.555069, 14.024636), (6.350748, 16.94904), (7.414599, 18.760984), (10.329122, 16.883976), (10.130224, 12.928215), (6.684543, 12.511049), (4.833812, 15.398007), (6.2905226, 17.152567), (9.181868, 15.289784), (8.914846, 11.534598), (5.4469285, 11.243478), (3.4619987, 14.120579), (4.7421026, 14.80482), (7.567424, 12.056021), (7.2412543, 9.755747), (3.8441908, 10.78786), (2.791213, 13.314277), (4.865651, 12.593901), (6.1343265, 9.005368), (4.3123693, 8.314894), (1.4218959, 10.711783), (1.5518004, 10.712347), (4.2200027, 7.6980844), (4.0868444, 6.810938), (1.1261986, 8.778622), (0.71562356, 8.693478), (2.9668288, 5.89919), (2.6869805, 5.24551), (-0.19305022, 7.199507), (-0.5659636, 6.934696), (1.5871049, 4.1506047), (1.308492, 3.631944), (-1.3900933, 5.569475), (-1.7289593, 5.2132154), (0.27814138, 2.4799213), (-7.4136675e-05, 1.9947402), (-2.5802758, 3.8178778), (-2.9565163, 3.4835618), (-1.0747668, 0.9269341), (-1.3358734, 0.38920212), (-3.7951403, 1.9915652), (-4.1626797, 1.7130947), (-2.379756, -0.62297773), (-2.6295695, -1.1749071), (-4.956019, 0.28849787), (-5.3638105, 0.1100234), (-3.7804904, -1.9829171), (-3.9811614, -2.6590729), (-6.066559, -1.4715549), (-6.4529495, -1.4430594), (-5.0130854, -3.1970317), (-5.1234517, -4.0939527), (-6.948686, -3.1878667), (-7.34291, -2.7731988), (-6.0954285, -4.072764), (-6.0030313, -5.4431906), (-7.4447536, -4.998174), (-7.954611, -3.7894042), (-7.012867, -4.3333893), (-6.7637196, -5.9449716), (-7.805977, -6.5327506), (-8.284415, -5.5357904), (-7.6750574, -5.3266683), (-7.4140253, -6.822982), (-7.6321406, -8.13978), (-6.990019, -7.4949994), (-6.48628, -6.4353824), (-7.499046, -7.481444), (-7.990685, -9.155667), (-6.4451265, -8.446347), (-5.639774, -5.765529), (-7.1050534, -5.510853), (-8.642255, -7.82247), (-8.016251, -9.773949), (-5.509351, -9.005143), (-5.005623, -6.047832), (-7.0053186, -5.0498843), (-8.486635, -6.8135767), (-7.9672008, -9.269574), (-5.7019744, -9.125579), (-4.676481, -6.3251104), (-6.1051497, -4.7225733), (-7.7008705, -5.8730264), (-7.6312985, -8.374415), (-5.7687426, -9.19308), (-4.2510977, -7.314078), (-5.095716, -4.719932), (-6.8685026, -4.8443494), (-7.155835, -7.3617086), (-5.722421, -9.373007), (-3.980929, -8.622165), (-4.370638, -5.665988), (-6.158235, -4.7475686), (-6.4954314, -6.550181), (-5.265521, -8.767223), (-3.87667, -8.304804), (-4.2975335, -5.438964), (-5.637022, -4.716559), (-5.5963535, -6.6813574), (-4.615937, -8.52606), (-3.9156158, -7.683129), (-4.3982697, -4.966867), (-4.9861765, -4.762534), (-4.513541, -7.0570235), (-3.9495919, -7.5910826), (-4.1548924, -5.6027265), (-4.4191527, -4.649465), (-4.0061193, -6.148842), (-3.6786582, -7.0529394), (-4.009585, -5.6324997), (-4.1563244, -4.668532), (-3.5526547, -5.928376), (-3.3121252, -6.6066484), (-3.8956244, -5.140274), (-3.931775, -4.581915), (-3.0606048, -6.1004324), (-2.9968283, -6.249648), (-3.8631837, -4.335795), (-3.695618, -4.335518), (-2.5676124, -6.4064646), (-2.6391478, -6.065502), (-3.73116, -3.5286238), (-3.5572946, -3.5274901), (-2.2743106, -5.902982), (-2.2747686, -5.81832), (-3.4632695, -3.296079), (-3.3292007, -3.2332656), (-1.9520273, -5.620954), (-1.9369903, -5.518771), (-3.2253788, -2.9442813), (-3.1254303, -2.8716009), (-1.6803882, -5.28887), (-1.6367891, -5.1937075), (-2.9821465, -2.5951529), (-2.9157429, -2.5232165), (-1.4361081, -4.972477), (-1.3926188, -4.9054685), (-2.791071, -2.3164215), (-2.7267585, -2.2694612), (-1.194765, -4.7666855), (-1.1469231, -4.686566), (-2.5961945, -2.0319848), (-2.5518212, -1.9663888), (-0.9953297, -4.4922814), (-0.9350389, -4.4149194), (-2.4032676, -1.7291929), (-2.3732958, -1.6610459), (-0.8071366, -4.2182245), (-0.746474, -4.157523), (-2.2324698, -1.4653813), (-2.227029, -1.420905), (-0.6678956, -4.0293183), (-0.5765238, -3.9664621), (-2.0436158, -1.2261531), (-2.0579877, -1.1075742), (-0.51983064, -3.6468399), (-0.43421733, -3.6063151), (-1.9144958, -0.94093424), (-1.9269863, -0.8276435), (-0.3648576, -3.3027978), (-0.30138853, -3.244953), (-1.8127134, -0.62532747), (-1.8044382, -0.5127641), (-0.2189264, -2.9360914), (-0.16988066, -2.9056683), (-1.7047404, -0.3804924), (-1.6965159, -0.27144405), (-0.10017792, -2.6224375), (-0.052449115, -2.5985384), (-1.5997039, -0.15095931), (-1.596822, -0.0031084202), (0.00198518, -2.232093), (0.048492055, -2.2500873), (-1.505015, 0.04809342), (-1.5053887, 0.2326758), (0.09317802, -1.8371656), (0.15572359, -1.8273691), (-1.3821446, 0.3815153), (-1.403922, 0.5746252), (0.20064421, -1.3329283), (0.28318137, -1.3148459), (-1.2870257, 0.71264243), (-1.3284436, 1.1927905), (0.3270209, -0.21468943), (0.43574002, -0.6349306), (-1.2110378, 0.65826863), (-1.3425733, 1.746762), (0.32208848, 1.1962293), (0.49683398, -0.018416941), (-1.1216586, 0.41291693), (-1.2794954, 2.1933434), (0.32252127, 2.3621044), (0.43813494, 0.6469492), (-1.1350352, 0.40578887), (-1.2070477, 2.2575371), (0.37565213, 2.8835561), (0.43475607, 1.6369804), (-1.2282355, 0.77891153), (-1.2710477, 1.7179368), (0.5004661, 3.0845804), (0.53416103, 2.7731273), (-1.3597407, 1.5861619), (-1.3734899, 1.4785793), (0.58520114, 2.487285), (0.5163332, 2.5454543), (-1.5746305, 1.5187544), (-1.5806885, 1.5081301), (0.47369435, 2.5292256), (0.485192, 2.4907827), (-1.5794746, 1.4181035), (-1.6707227, 1.3261592), (0.29817328, 2.2735744), (0.3112532, 2.2553904), (-1.7204314, 1.2007879), (-1.7920556, 1.1240008), (0.16777655, 2.05762), (0.15177487, 2.0169768), (-1.8990555, 0.9712358), (-1.9623724, 0.8637464), (0.014607895, 1.7393632), (-0.00627675, 1.6007787), (-2.0814254, 0.3824946), (-2.1250534, 0.45362094), (-0.11068852, 1.7025307), (-0.1436807, 1.6123452), (-2.242324, 0.2970418), (-2.331233, 0.04658164), (-0.33819798, 1.0582402), (-0.3502536, 0.87979746), (-2.4553218, -0.6828568), (-2.5806165, -0.5209496), (-0.5795512, 1.23585), (-0.48051235, 1.2244973), (-2.5043857, -0.4046942), (-2.7163498, -0.88559407), (-0.8769106, 0.18787943), (-0.8095628, 0.11442589), (-2.7253435, -1.5513716), (-2.9623857, -1.4115553), (-1.1913655, 0.49340558), (-0.99288076, 0.39870527), (-2.780113, -1.6128266), (-3.1020486, -1.8748153), (-1.5118871, -0.19484995), (-1.2737668, -0.26713356), (-2.896485, -2.3431206), (-3.1756117, -2.372377), (-1.6833403, -0.29130405), (-1.5297092, -0.37838975), (-3.0639517, -2.6665332), (-3.2382753, -2.7910745), (-1.7758708, -0.6684931), (-1.70384, -0.644328), (-3.2446382, -2.859644), (-3.3594935, -3.0305676), (-1.8684636, -0.9862459), (-1.8248943, -0.9645512), (-3.384605, -3.1310608), (-3.478201, -3.2656822), (-1.9657993, -1.222503), (-1.9295458, -1.2185135), (-3.4999537, -3.385163), (-3.5833113, -3.4986494), (-2.064235, -1.437336), (-2.0300777, -1.4314568), (-3.5918865, -3.5991523), (-3.673222, -3.7180355), (-2.152651, -1.6630875), (-2.1040654, -1.6144037), (-3.6581755, -3.7156155), (-3.747957, -3.8525686), (-2.2317436, -1.8617811), (-2.1666412, -1.81533), (-3.7031696, -3.8744788), (-3.7966049, -3.9842951), (-2.2913408, -1.9952908), (-2.2204974, -1.9557327), (-3.7401147, -3.9960399), (-3.8277073, -4.087702), (-2.329224, -2.1072912), (-2.2580302, -2.0656843), (-3.76483, -4.0653486), (-3.8457582, -4.1450043), (-2.3526466, -2.2048316), (-2.2787766, -2.1485522), (-3.772483, -4.0839996), (-3.8519287, -4.1754436), (-2.3661523, -2.3004444), (-2.2837799, -2.2206264), (-3.760878, -4.072584), (-3.8445156, -4.1780615), (-2.3715062, -2.3825154), (-2.276851, -2.27952), (-3.7320104, -4.038503), (-3.822622, -4.1583533), (-2.36792, -2.4500499), (-2.2597733, -2.3227627), (-3.6886938, -3.982668), (-3.7874165, -4.120228), (-2.3550675, -2.5058732), (-2.2329452, -2.340209), (-3.6333473, -3.8924775), (-3.742337, -4.0737185), (-2.3341584, -2.5676458), (-2.1943681, -2.3482194), (-3.5643806, -3.7773705), (-3.6892488, -4.0108232), (-2.3090308, -2.6280806), (-2.1461298, -2.3416214), (-3.481882, -3.6333027), (-3.6226733, -3.9400525), (-2.2737942, -2.6982007), (-2.092511, -2.3130248), (-3.3938692, -3.4480152), (-3.5584342, -3.869777), (-2.2419438, -2.7908921), (-2.0240347, -2.2567503), (-3.2875772, -3.2088768), (-3.49825, -3.8210135), (-2.2115774, -2.9399924), (-1.9497448, -2.2130034), (-3.1851976, -2.9460402), (-3.4352958, -3.7431407), (-2.1806278, -3.08888), (-1.8794672, -2.1765068), (-3.0761988, -2.670924), (-3.362222, -3.6377425), (-2.149005, -3.22354), (-1.810433, -2.148827), (-2.9601123, -2.396903), (-3.281977, -3.5038335), (-2.1190314, -3.3341613), (-1.7357733, -2.058763), (-2.8323038, -2.05894), (-3.2110875, -3.3927376), (-2.1042588, -3.4702291), (-1.6491615, -1.9721553), (-2.6827698, -1.7222859), (-3.1487458, -3.2362568), (-2.116353, -3.5733187), (-1.5566632, -2.0073516), (-2.4963136, -1.4822685), (-3.0777807, -2.98227), (-2.1630685, -3.6135008), (-1.474644, -2.1668723), (-2.3086336, -1.385434), (-3.027089, -2.694477), (-2.2131906, -3.5480268), (-1.3795162, -2.366966), (-2.102347, -1.3901167), (-2.96773, -2.3908167), (-2.27519, -3.4149225), (-1.2942365, -2.5901897), (-1.889106, -1.473035), (-2.8946807, -2.0477831), (-2.3371792, -3.179707), (-1.2111801, -2.6930573), (-1.6621562, -1.4917526), (-2.8215423, -1.7759233), (-2.4164221, -2.9573002), (-1.1379172, -2.7065482), (-1.4297819, -1.4829625), (-2.7499392, -1.5724883), (-2.5115914, -2.7492244), (-1.0990833, -2.6760266), (-1.2249873, -1.4828104), (-2.6424067, -1.4164472), (-2.5635378, -2.5644243), (-1.1095434, -2.6809242), (-1.0953035, -1.5302474), (-2.5134256, -1.2689177), (-2.5567207, -2.3545918), (-1.139275, -2.6887937), (-1.0248051, -1.628727), (-2.3923504, -1.1850047), (-2.5105267, -2.1629062), (-1.1521143, -2.6683557), (-0.98743963, -1.7291051), (-2.2962708, -1.1277589), (-2.4566321, -1.9771934), (-1.1489434, -2.6366363), (-0.9524088, -1.8330148), (-2.2237418, -1.0845017), (-2.4131973, -1.794674), (-1.1467128, -2.5992754), (-0.92783815, -1.9408027), (-2.1601546, -1.0550941), (-2.3706012, -1.6197727), (-1.1453129, -2.5568595), (-0.91097814, -2.0465143), (-2.1051552, -1.0398817), (-2.3306904, -1.4592211), (-1.1448402, -2.5124311), (-0.9002036, -2.1477056), (-2.0587244, -1.0328509), (-2.2950878, -1.3103422), (-1.1458306, -2.4587977), (-0.8939158, -2.2245858), (-2.018633, -1.0318743), (-2.26397, -1.1953871), (-1.1491543, -2.4040399), (-0.89100355, -2.2802682), (-1.9842558, -1.0482426), (-2.2373798, -1.1107625), (-1.1545066, -2.3399894), (-0.891015, -2.3098657), (-1.9551151, -1.0761578), (-2.2156496, -1.0574201), (-1.1624494, -2.2742438), (-0.8937264, -2.3173447), (-1.9309148, -1.1095874), (-2.1992311, -1.0301096), (-1.1736867, -2.2134676), (-0.89901984, -2.3107727), (-1.9112478, -1.1446178), (-2.1884146, -1.0217525), (-1.1888679, -2.1612952), (-0.9067518, -2.297659), (-1.8954781, -1.1796787), (-2.1832392, -1.0263824), (-1.2085085, -2.1186635), (-0.9166928, -2.283134), (-1.8827735, -1.2147045), (-2.1835463, -1.0399032), (-1.2330105, -2.0849059), (-0.92851, -2.2700675), (-1.8721489, -1.250305), (-2.1890783, -1.0597707), (-1.2627329, -2.0586386), (-0.94185406, -2.2586265), (-1.8625402, -1.2859242), (-2.1995409, -1.0855516), (-1.2980568, -2.0399113), (-0.95618623, -2.2489302), (-1.8527377, -1.3213785), (-2.2147703, -1.1167088), (-1.339502, -2.0282154), (-0.9707821, -2.2409756), (-1.8412937, -1.3568375), (-2.2347457, -1.1526821), (-1.387831, -2.0225449), (-0.98474944, -2.2341828), (-1.826422, -1.3922983), (-2.259604, -1.1933818), (-1.4441909, -2.0220757), (-0.996978, -2.2275472), (-1.8058349, -1.4275492), (-2.2897012, -1.2391374), (-1.5103135, -2.0262215), (-1.0037404, -2.2196689), (-1.7739913, -1.4619402), (-2.327969, -1.2908896), (-1.591805, -2.0348182), (-1.0070658, -2.2083287), (-1.7296834, -1.4951503), (-2.3715882, -1.3502954), (-1.6887968, -2.047703), (-1.0103204, -2.1843314), (-1.6735579, -1.5189388), (-2.4166381, -1.4194167), (-1.7996613, -2.067682), (-1.0161675, -2.1375988), (-1.6065478, -1.5212833), (-2.460004, -1.5218142), (-1.9212798, -2.1142642), (-1.0207957, -2.0476902), (-1.525562, -1.4905621), (-2.5032775, -1.6705809), (-2.0646226, -2.2035177), (-1.0314599, -1.9093301), (-1.4202234, -1.4096936), (-2.54447, -1.81478), (-2.2290819, -2.2953618), (-1.0406283, -1.782498), (-1.2990799, -1.3297175), (-2.5730171, -1.9401981), (-2.3939018, -2.3880732), (-1.0691887, -1.6907248), (-1.1894022, -1.262745), (-2.5766752, -2.038357), (-2.5343432, -2.4806712), (-1.133463, -1.6475905), (-1.1244472, -1.2144296), (-2.5499227, -2.1082501), (-2.6250498, -2.5767565), (-1.2106091, -1.6637641), (-1.0996557, -1.196951), (-2.5179222, -2.1431305), (-2.6810246, -2.6552062), (-1.288617, -1.7125516), (-1.1046901, -1.2013534), (-2.49097, -2.162578), (-2.7061014, -2.7171736), (-1.3483702, -1.7742087), (-1.1280091, -1.2116787), (-2.480636, -2.1622856), (-2.7272706, -2.7726343), (-1.4003055, -1.8486047), (-1.1495049, -1.2184001), (-2.472742, -2.138911), (-2.7493222, -2.8275108), (-1.4492327, -1.9424942), (-1.1673748, -1.2223973), (-2.4639885, -2.0915294), (-2.772803, -2.880585), (-1.4980015, -2.0566516), (-1.1840391, -1.2332761), (-2.45435, -2.0298538), (-2.7941976, -2.921263), (-1.5450765, -2.1761549), (-1.18945, -1.2491418), (-2.429622, -1.9560475), (-2.82021, -2.949439), (-1.6059169, -2.2982907), (-1.1979178, -1.2698728), (-2.3972473, -1.8778182), (-2.8400269, -2.9662614), (-1.669726, -2.413906), (-1.206149, -1.2944475), (-2.3561401, -1.8019719), (-2.8557115, -2.9707997), (-1.7377524, -2.514795), (-1.2118664, -1.3269461), (-2.3034647, -1.7400078), (-2.8680317, -2.9591255), (-1.8125976, -2.5889397), (-1.2137038, -1.3602268), (-2.2356656, -1.6927311), (-2.8786848, -2.939965), (-1.898223, -2.6407459), (-1.2096977, -1.3918625), (-2.1480393, -1.6588553), (-2.889689, -2.9173145), (-2.000247, -2.6748137), (-1.1971081, -1.3984951), (-2.0342784, -1.612275), (-2.9062805, -2.9144075), (-2.1275356, -2.7237148), (-1.1721072, -1.3593556), (-1.8862005, -1.5161884), (-2.9332304, -2.9457629), (-2.2905767, -2.82401), (-1.1356874, -1.2980896), (-1.6964922, -1.3674656), (-2.9602432, -2.9578607), (-2.489052, -2.9399545), (-1.1059624, -1.2627956), (-1.4765406, -1.215259), (-2.9718552, -2.8929625), (-2.7087104, -2.9928648), (-1.1292646, -1.2995774), (-1.2873733, -1.1569195), (-2.9122875, -2.807372), (-2.84536, -2.9906297), (-1.203962, -1.3730546), (-1.1951206, -1.1764477), (-2.8230994, -2.743365), (-2.8866875, -2.9629228), (-1.2829394, -1.4405633), (-1.1780516, -1.2246895), (-2.7465358, -2.7067235), (-2.8756583, -2.9304512), (-1.3387545, -1.4863807), (-1.1916225, -1.2722306), (-2.6949775, -2.6879332), (-2.8481097, -2.9005742), (-1.3695123, -1.5122747), (-1.2110528, -1.2966466), (-2.665374, -2.6642797), (-2.8214312, -2.8872478), (-1.3817544, -1.5420158), (-1.2242925, -1.3135493), (-2.647985, -2.6396863), (-2.8009577, -2.8750339), (-1.3848872, -1.5681132), (-1.2307225, -1.3237038), (-2.635995, -2.6114628), (-2.7862797, -2.8646417), (-1.3845254, -1.5949502), (-1.2314354, -1.3285954), (-2.6250181, -2.5776577), (-2.77615, -2.85623), (-1.3842474, -1.6258252), (-1.2278724, -1.3293792), (-2.6123726, -2.5365808), (-2.7693143, -2.8500917), (-1.3862545, -1.6635593), (-1.2212623, -1.3264223), (-2.5964482, -2.4862165), (-2.764892, -2.8420627), (-1.3922422, -1.7055967), (-1.213037, -1.324295), (-2.5761814, -2.430788), (-2.7613623, -2.8398376), (-1.402299, -1.7603133), (-1.2034392, -1.3151793), (-2.551441, -2.3598533), (-2.7598054, -2.8459682), (-1.417535, -1.8339262), (-1.1911929, -1.2955782), (-2.5206156, -2.2651732), (-2.7619565, -2.8650444), (-1.4404049, -1.9373657), (-1.1794338, -1.2754747), (-2.486056, -2.1509182), (-2.7581928, -2.8866708), (-1.4600283, -2.0637252), (-1.176633, -1.2655253), (-2.461104, -2.0283842), (-2.7559173, -2.8972414), (-1.4793867, -2.1913366), (-1.1726665, -1.2620981), (-2.4351237, -1.9031019), (-2.7539692, -2.8968196), (-1.5002483, -2.3141377), (-1.158294, -1.2658497), (-2.3938458, -1.779377), (-2.7585912, -2.8824093), (-1.5383099, -2.4265254), (-1.1489543, -1.2796681), (-2.3450704, -1.6686746), (-2.7578793, -2.8538458), (-1.5822664, -2.5169978), (-1.1421018, -1.3085568), (-2.2879765, -1.5847164), (-2.7528174, -2.807071), (-1.6327332, -2.5731013), (-1.1363685, -1.3429456), (-2.2213411, -1.5255855), (-2.745518, -2.7541494), (-1.6914915, -2.6037312), (-1.1297487, -1.378938), (-2.143211, -1.486581), (-2.7384453, -2.713937), (-1.7609422, -2.6318126), (-1.1201639, -1.4042192), (-2.0519657, -1.4484081), (-2.7353635, -2.6704795), (-1.8460991, -2.6462014), (-1.104558, -1.4302408), (-1.9400722, -1.4196405), (-2.737408, -2.6270728), (-1.9542197, -2.6543317), (-1.0804275, -1.4175328), (-1.8001394, -1.3591584), (-2.7508144, -2.6236835), (-2.094874, -2.7002027), (-1.0452263, -1.3641648), (-1.6254281, -1.2550794), (-2.7726858, -2.6280053), (-2.2717142, -2.7604904), (-1.0084848, -1.3099817), (-1.4178537, -1.1402928), (-2.7935412, -2.5933492), (-2.4825406, -2.792493), (-0.9981423, -1.3025608), (-1.2044803, -1.0698841), (-2.7642293, -2.5300505), (-2.6579375, -2.7996366), (-1.0637898, -1.3559994), (-1.0840714, -1.0705227), (-2.6809285, -2.4688337), (-2.7297082, -2.7805364), (-1.149903, -1.4231007), (-1.0490552, -1.113077), (-2.5988247, -2.4291735), (-2.7378883, -2.7592168), (-1.2227496, -1.4873469), (-1.0572913, -1.1604384), (-2.5379515, -2.3984063), (-2.7224934, -2.7411704), (-1.274798, -1.5432245), (-1.0782722, -1.1913706), (-2.4982758, -2.360737), (-2.6970854, -2.738548), (-1.3005357, -1.607752), (-1.1016732, -1.209813), (-2.4819052, -2.3128188), (-2.6842859, -2.7449312), (-1.3193685, -1.6791837), (-1.1142753, -1.2136881), (-2.4691386, -2.2485662), (-2.682538, -2.7631135), (-1.3391309, -1.7689085), (-1.1171798, -1.2014705), (-2.452937, -2.1570253), (-2.6910641, -2.7976503), (-1.3662498, -1.8922693), (-1.1188226, -1.1944271), (-2.4358795, -2.0510864), (-2.7013245, -2.827107), (-1.3959749, -2.0288794), (-1.1215224, -1.1961203), (-2.4179556, -1.9377631), (-2.710531, -2.8457854), (-1.4270277, -2.1671658), (-1.1248745, -1.2045922), (-2.3977103, -1.8221071), (-2.718399, -2.852965), (-1.4601438, -2.299274), (-1.1284536, -1.2204622), (-2.3736165, -1.7121885), (-2.7244418, -2.8461761), (-1.4961046, -2.414856), (-1.1320066, -1.2450951), (-2.3440437, -1.6176996), (-2.7280447, -2.824303), (-1.5357376, -2.5046499), (-1.1269494, -1.2835952), (-2.2966497, -1.549105), (-2.7347708, -2.783283), (-1.5926785, -2.560812), (-1.1262989, -1.3267285), (-2.2383847, -1.5049032), (-2.7360227, -2.7377987), (-1.6574612, -2.59248), (-1.1264172, -1.3696011), (-2.168918, -1.4794924), (-2.7355587, -2.7064998), (-1.7312129, -2.6222897), (-1.1242083, -1.3996211), (-2.08761, -1.4535799), (-2.7378473, -2.6732292), (-1.8180842, -2.6386151), (-1.1160647, -1.4280841), (-1.9876374, -1.4360378), (-2.7444563, -2.641237), (-1.9247341, -2.6482682), (-1.0994494, -1.4278923), (-1.8618381, -1.3975283), (-2.7604475, -2.638287), (-2.0602055, -2.6832457), (-1.0710474, -1.3809282), (-1.7025788, -1.3125827), (-2.78783, -2.6735244), (-2.2316196, -2.762339), (-1.034813, -1.3086299), (-1.5075374, -1.1883585), (-2.8112528, -2.6638467), (-2.4309404, -2.812096), (-1.0159855, -1.2801517), (-1.2987001, -1.0990782), (-2.813467, -2.6197457), (-2.6338916, -2.8341746), (-1.0597496, -1.3126909), (-1.1474321, -1.0818088), (-2.7485867, -2.566207), (-2.7409306, -2.827504), (-1.1406554, -1.3757564), (-1.0875454, -1.1133119), (-2.669433, -2.5261104), (-2.7700691, -2.8096998), (-1.2179469, -1.4381596), (-1.0856942, -1.1610861), (-2.6063144, -2.501221), (-2.762115, -2.790558), (-1.2745073, -1.4876834), (-1.1055936, -1.2066693), (-2.5652127, -2.4863045), (-2.743905, -2.7676296), (-1.3113326, -1.5171784), (-1.1300471, -1.2396204), (-2.5425444, -2.474221), (-2.7263849, -2.7605674), (-1.3325343, -1.550245), (-1.1501653, -1.2641853), (-2.531488, -2.4610152), (-2.7142482, -2.756653), (-1.3452326, -1.580766), (-1.1647462, -1.2811776), (-2.5264137, -2.444299), (-2.70753, -2.7561688), (-1.3540492, -1.6123827), (-1.1741325, -1.2916784), (-2.5232928, -2.421981), (-2.7054625, -2.7592602), (-1.3622459, -1.6482954), (-1.1790786, -1.2964103), (-2.5192788, -2.3918695), (-2.707178, -2.7663286), (-1.3721404, -1.6917343), (-1.1802709, -1.2953944), (-2.5122783, -2.3511221), (-2.7120528, -2.778385), (-1.3855395, -1.7466874), (-1.178041, -1.2876105), (-2.500491, -2.2953982), (-2.7199595, -2.797487), (-1.4042292, -1.8190556), (-1.1720932, -1.2705424), (-2.4818306, -2.2174172), (-2.7245786, -2.8272977), (-1.4221003, -1.91939), (-1.1706967, -1.2517879), (-2.467734, -2.118802), (-2.7347212, -2.8597982), (-1.4452839, -2.0405638), (-1.169195, -1.2433773), (-2.4517598, -2.0128329), (-2.7437632, -2.8800216), (-1.4701382, -2.1639004), (-1.1680274, -1.242885), (-2.4325228, -1.9036747), (-2.751035, -2.8887818), (-1.4973593, -2.2833736), (-1.1671699, -1.2496333), (-2.408908, -1.7976333), (-2.7560976, -2.8852127), (-1.5274973, -2.3908637), (-1.1572226, -1.2639853), (-2.3682182, -1.6999485), (-2.765551, -2.8675714), (-1.5749003, -2.4812768), (-1.1520798, -1.2869133), (-2.3181336, -1.6199092), (-2.768895, -2.8387659), (-1.6287149, -2.5477104), (-1.1487007, -1.3211894), (-2.2580945, -1.5665904), (-2.7679746, -2.7971568), (-1.6897755, -2.5831485), (-1.1451517, -1.3551611), (-2.1867993, -1.5327204), (-2.7658188, -2.7557304), (-1.7605255, -2.6003058), (-1.1388017, -1.385669), (-2.1015067, -1.5119117), (-2.7653694, -2.718031), (-1.845162, -2.6068964), (-1.1272031, -1.41111), (-1.9973092, -1.4976345), (-2.7691467, -2.6993918), (-1.9491438, -2.624852), (-1.1071099, -1.3886658), (-1.8694566, -1.4406973), (-2.7848053, -2.7138648), (-2.081839, -2.6761308), (-1.0741202, -1.3253828), (-1.7082988, -1.3353312), (-2.8121116, -2.7554557), (-2.2510924, -2.7713034), (-1.0341178, -1.2498742), (-1.5104012, -1.1912453), (-2.8337474, -2.7373905), (-2.449188, -2.8320699), (-1.0133511, -1.2287081), (-1.2978774, -1.0899305), (-2.8317688, -2.6776292), (-2.650408, -2.8557286), (-1.0573822, -1.2712318), (-1.1448708, -1.0700258), (-2.7615297, -2.6113753), (-2.752887, -2.8453307), (-1.1382059, -1.3410547), (-1.0855327, -1.1030658), (-2.6787484, -2.5635982), (-2.7767735, -2.8226628), (-1.2136065, -1.4062449), (-1.0841213, -1.152443), (-2.6139312, -2.5341082), (-2.7646196, -2.7991846), (-1.2671446, -1.4563631), (-1.1035917, -1.1987892), (-2.572135, -2.5158517), (-2.743383, -2.7790644), (-1.3003852, -1.4931388), (-1.1260895, -1.2362335), (-2.5483532, -2.5029998), (-2.7237842, -2.7634256), (-1.3193457, -1.5212942), (-1.1453408, -1.2537652), (-2.5369613, -2.4802139), (-2.7104228, -2.757183), (-1.3299968, -1.5529104), (-1.1580796, -1.2707336), (-2.5306761, -2.4597156), (-2.702728, -2.7547593), (-1.3376113, -1.5869282), (-1.165806, -1.2813377), (-2.5259128, -2.432729), (-2.6998975, -2.7562945), (-1.3454404, -1.6267), (-1.1692932, -1.2861918), (-2.5198336, -2.3968318), (-2.7010531, -2.7623346), (-1.3557819, -1.6757089), (-1.169197, -1.2850231), (-2.51034, -2.3487642), (-2.7056231, -2.7742498), (-1.3704537, -1.7384742), (-1.1657255, -1.276248), (-2.495552, -2.2833447), (-2.706297, -2.7947657), (-1.3823783, -1.822887), (-1.1630915, -1.2538582), (-2.4827788, -2.188497), (-2.7171638, -2.8311462), (-1.4038395, -1.9418343), (-1.1597859, -1.2401024), (-2.467461, -2.0816405), (-2.7280967, -2.8586583), (-1.4284426, -2.0706642), (-1.1578028, -1.2360294), (-2.4495354, -1.9684392), (-2.7369668, -2.8745432), (-1.4552429, -2.2002237), (-1.156931, -1.2394975), (-2.4281225, -1.8539003), (-2.743567, -2.8786354), (-1.4845698, -2.3230493), (-1.1567465, -1.2505816), (-2.402248, -1.7456077), (-2.747648, -2.8692734), (-1.5168508, -2.4297063), (-1.1476268, -1.2700181), (-2.359154, -1.6496556), (-2.755923, -2.8447998), (-1.5664828, -2.5149946), (-1.143518, -1.3038307), (-2.30633, -1.580501), (-2.7571945, -2.8039973), (-1.6224496, -2.5669267), (-1.1417321, -1.3409084), (-2.2438874, -1.5345284), (-2.7547917, -2.7594306), (-1.6856009, -2.5952103), (-1.1393375, -1.3770638), (-2.1706014, -1.5065391), (-2.751868, -2.715975), (-1.7585263, -2.6073892), (-1.1337667, -1.4094208), (-2.0833519, -1.489725), (-2.7512934, -2.6902864), (-1.8450452, -2.6259468), (-1.1226314, -1.4271418), (-1.978804, -1.4658418), (-2.7566214, -2.664442), (-1.9516236, -2.6378155), (-1.1017787, -1.407848), (-1.8480943, -1.4119726), (-2.7733202, -2.6758833), (-2.088218, -2.6853695), (-1.0685848, -1.3462298), (-1.6834317, -1.3108572), (-2.800613, -2.7105849), (-2.261151, -2.7683105), (-1.0298563, -1.2736045), (-1.4832191, -1.1788738), (-2.8307762, -2.693731), (-2.473357, -2.8188808), (-1.0089729, -1.2509131), (-1.2625173, -1.0882294), (-2.8152034, -2.6373506), (-2.6651142, -2.8372767), (-1.0652833, -1.2949909), (-1.125255, -1.0780073), (-2.7389865, -2.5794914), (-2.7531688, -2.82695), (-1.1488721, -1.3621844), (-1.0788598, -1.1136601), (-2.658224, -2.538521), (-2.7694595, -2.806505), (-1.2219787, -1.4236106), (-1.083101, -1.1617368), (-2.5977075, -2.5131464), (-2.7555895, -2.7856066), (-1.2727807, -1.470797), (-1.1041454, -1.2057782), (-2.559634, -2.497241), (-2.7351446, -2.7613647), (-1.3045928, -1.498316), (-1.1276441, -1.2475246), (-2.5386014, -2.4950316), (-2.715542, -2.7425215), (-1.3220218, -1.5160527), (-1.1486242, -1.2689438), (-2.5304506, -2.4845002), (-2.7015157, -2.7395976), (-1.329968, -1.5422413), (-1.1627388, -1.2832433), (-2.5283601, -2.4697638), (-2.6942143, -2.7396507), (-1.3346236, -1.570304), (-1.1710119, -1.2918198), (-2.5277777, -2.4490588), (-2.692269, -2.7427473), (-1.3395343, -1.6032451), (-1.1747189, -1.2956805), (-2.525743, -2.420662), (-2.6943362, -2.7491083), (-1.3469697, -1.6438007), (-1.174963, -1.2950953), (-2.5203285, -2.3824084), (-2.6993997, -2.7595005), (-1.3584372, -1.6951258), (-1.1724144, -1.2892992), (-2.5101254, -2.3309646), (-2.7070096, -2.7756414), (-1.3752126, -1.7617915), (-1.1670754, -1.276112), (-2.493661, -2.2606497), (-2.710224, -2.8007205), (-1.3900629, -1.8522782), (-1.1638563, -1.2521157), (-2.4794223, -2.162086), (-2.7222607, -2.8388963), (-1.4138306, -1.9754696), (-1.1613953, -1.2404888), (-2.463641, -2.0552418), (-2.732757, -2.86386), (-1.4392134, -2.102711), (-1.1600001, -1.237721), (-2.4451022, -1.9434088), (-2.7412214, -2.8774092), (-1.4667264, -2.228815), (-1.1594504, -1.2422749), (-2.4228737, -1.8320447), (-2.7474148, -2.8790169), (-1.4967641, -2.346054), (-1.1593776, -1.2544258), (-2.3959262, -1.7288793), (-2.7510538, -2.8673007), (-1.5298083, -2.4454095), (-1.1504215, -1.2745556), (-2.3517838, -1.6394329), (-2.7587922, -2.841396), (-1.5800312, -2.522886), (-1.1461433, -1.3081756), (-2.2976992, -1.57688), (-2.7598171, -2.8006692), (-1.6367989, -2.5680413), (-1.1438603, -1.3437276), (-2.2337377, -1.5362086), (-2.7575874, -2.7578974), (-1.7011434, -2.5916078), (-1.1405821, -1.3772572), (-2.1585052, -1.511778), (-2.755319, -2.7174861), (-1.7758915, -2.601075), (-1.1336981, -1.4062696), (-2.068591, -1.4969577), (-2.7558613, -2.6954882), (-1.865155, -2.6184354), (-1.1208435, -1.420373), (-1.9603035, -1.4738572), (-2.762606, -2.673796), (-1.9756399, -2.6305416), (-1.0976706, -1.3863263), (-1.8246986, -1.4089935), (-2.7822838, -2.700177), (-2.1178596, -2.691712), (-1.0618956, -1.3142922), (-1.6542586, -1.2950066), (-2.8109586, -2.7328534), (-2.2965002, -2.777469), (-1.0237265, -1.2463006), (-1.4492649, -1.1611555), (-2.8379946, -2.7091525), (-2.5108747, -2.8284364), (-1.0202543, -1.2364645), (-1.2458285, -1.0817107), (-2.8085785, -2.652304), (-2.6798773, -2.8441648), (-1.0761493, -1.2857016), (-1.1207477, -1.0773531), (-2.733934, -2.5939884), (-2.7580853, -2.831634), (-1.1567003, -1.353462), (-1.0810275, -1.1153604), (-2.6568084, -2.5540216), (-2.770216, -2.810136), (-1.2258165, -1.4132671), (-1.0873055, -1.1637408), (-2.5998688, -2.5298903), (-2.7553515, -2.7887414), (-1.2732071, -1.4584105), (-1.1084036, -1.2074332), (-2.5645487, -2.5151784), (-2.7350116, -2.7707412), (-1.3020791, -1.4914991), (-1.1304587, -1.2423425), (-2.5453708, -2.5048459), (-2.7172925, -2.7569792), (-1.3183101, -1.5170338), (-1.1488049, -1.2578604), (-2.5371013, -2.4840434), (-2.7058544, -2.752289), (-1.3274039, -1.5466206), (-1.160718, -1.2735019), (-2.5329125, -2.4654753), (-2.6998024, -2.7510128), (-1.3342047, -1.578824), (-1.1678904, -1.2834536), (-2.5296075, -2.4406426), (-2.698222, -2.7532022), (-1.3416138, -1.6165692), (-1.1711695, -1.2883495), (-2.5246758, -2.4074876), (-2.7001855, -2.7592769), (-1.3516213, -1.6628413), (-1.1712542, -1.2880108), (-2.5163195, -2.363252), (-2.705078, -2.7703817), (-1.3657452, -1.7214645), (-1.168413, -1.281101), (-2.5029807, -2.3035665), (-2.7128563, -2.7888422), (-1.3855504, -1.798343), (-1.1622075, -1.2646462), (-2.4827223, -2.2208986), (-2.7172346, -2.8187532), (-1.4046559, -1.904364), (-1.1610776, -1.2466671), (-2.4675074, -2.1174107), (-2.727188, -2.851511), (-1.4290324, -2.0314999), (-1.1600759, -1.2390702), (-2.45079, -2.0065737), (-2.7360785, -2.8719473), (-1.4549116, -2.1606872), (-1.1595082, -1.2393596), (-2.4311354, -1.8926221), (-2.7432945, -2.8806655), (-1.4830215, -2.2855613), (-1.1592863, -1.2471222), (-2.407333, -1.782302), (-2.7483642, -2.8763611), (-1.5139605, -2.3973079), (-1.1498737, -1.2630734), (-2.3664048, -1.6815034), (-2.757882, -2.8569381), (-1.5622883, -2.4901776), (-1.1454618, -1.288363), (-2.3161514, -1.6001107), (-2.7610877, -2.8254714), (-1.6169269, -2.5570762), (-1.1429883, -1.3256998), (-2.256036, -1.5472587), (-2.7598338, -2.7806528), (-1.6786673, -2.59146), (-1.1405172, -1.362823), (-2.1848483, -1.5147789), (-2.7572424, -2.7361407), (-1.7499098, -2.6070797), (-1.1353648, -1.3963794), (-2.0999398, -1.4956555), (-2.7563426, -2.709111), (-1.834077, -2.627017), (-1.1250505, -1.4153037), (-1.9984038, -1.4709674), (-2.7608862, -2.681974), (-1.9373548, -2.638944), (-1.1054919, -1.4030168), (-1.8716031, -1.4228389), (-2.7757707, -2.6862204), (-2.0693798, -2.6797523), (-1.0738237, -1.3468765), (-1.7115271, -1.3276658), (-2.8023078, -2.7255812), (-2.237779, -2.7672968), (-1.0345441, -1.2706561), (-1.5150775, -1.1909708), (-2.8246632, -2.71148), (-2.4353647, -2.8229856), (-1.0122654, -1.2434455), (-1.3022531, -1.0919235), (-2.8254788, -2.658319), (-2.6387835, -2.8463387), (-1.054169, -1.2800617), (-1.1467582, -1.0708984), (-2.7577868, -2.5964134), (-2.745204, -2.8371854), (-1.1342903, -1.3462781), (-1.0846947, -1.1023191), (-2.6758337, -2.551093), (-2.772135, -2.8158941), (-1.2104386, -1.4098169), (-1.0819516, -1.1508709), (-2.610903, -2.5233283), (-2.761638, -2.7935152), (-1.2651839, -1.4590781), (-1.1012121, -1.1970536), (-2.5688703, -2.5067174), (-2.7412195, -2.7742088), (-1.2995166, -1.4950305), (-1.1240712, -1.2346879), (-2.545076, -2.4957438), (-2.722026, -2.7527962), (-1.3195941, -1.514834), (-1.1448253, -1.2588766), (-2.534096, -2.4840782), (-2.707369, -2.7478738), (-1.3296198, -1.5425726), (-1.1594107, -1.2756181), (-2.5298448, -2.4686906), (-2.69902, -2.7462378), (-1.335699, -1.5716958), (-1.1684176, -1.2862207), (-2.5277631, -2.4476097), (-2.695931, -2.7479398), (-1.341445, -1.6053479), (-1.1728762, -1.2916615), (-2.524783, -2.4189382), (-2.6969445, -2.7532043), (-1.3492603, -1.6464314), (-1.1737454, -1.2922088), (-2.5188227, -2.3803458), (-2.701154, -2.7628088), (-1.3608168, -1.6982915), (-1.1716146, -1.2871053), (-2.508303, -2.328325), (-2.7081585, -2.7784972), (-1.3775579, -1.7657341), (-1.1664525, -1.2741673), (-2.4915814, -2.2569647), (-2.7110708, -2.803516), (-1.3924556, -1.8575475), (-1.1634706, -1.2509309), (-2.477308, -2.1577222), (-2.722705, -2.8410113), (-1.4160234, -1.9814222), (-1.161133, -1.2398694), (-2.4615035, -2.050129), (-2.732909, -2.8653884), (-1.4412261, -2.109295), (-1.1597743, -1.2375519), (-2.4429123, -1.9375972), (-2.7411566, -2.8783255), (-1.4686102, -2.235822), (-1.1592026, -1.242574), (-2.4205635, -1.8257619), (-2.7471685, -2.879157), (-1.4985955, -2.3530698), (-1.159077, -1.255312), (-2.393394, -1.722549), (-2.7506242, -2.8664694), (-1.5316832, -2.4518964), (-1.1501095, -1.2761388), (-2.3489702, -1.6335464), (-2.7581387, -2.8395007), (-1.5819957, -2.5283792), (-1.1457853, -1.3105305), (-2.2944777, -1.5718273), (-2.75896, -2.7977333), (-1.6389856, -2.5723515), (-1.1434374, -1.3467288), (-2.2299716, -1.53198), (-2.756579, -2.7541342), (-1.7037281, -2.5949566), (-1.1400548, -1.3807932), (-2.1540246, -1.5081401), (-2.754247, -2.7130506), (-1.7791126, -2.6038165), (-1.133, -1.4103011), (-2.0631452, -1.493598), (-2.7548282, -2.6904938), (-1.869338, -2.6209617), (-1.1198877, -1.4245789), (-1.9535562, -1.4702197), (-2.7617412, -2.668497), (-1.9811913, -2.6333594), (-1.0963334, -1.3892552), (-1.8162676, -1.4035357), (-2.7817855, -2.6959777), (-2.1251857, -2.6962411), (-1.0602342, -1.3163671), (-1.6439809, -1.2880707), (-2.8102493, -2.7255423), (-2.305332, -2.7790308), (-1.0226967, -1.2505033), (-1.4378791, -1.1571221), (-2.8358786, -2.700454), (-2.5196314, -2.8276005), (-1.0223316, -1.2433169), (-1.2374142, -1.0822558), (-2.8035913, -2.6438968), (-2.684237, -2.8413937), (-1.0802604, -1.2932875), (-1.1171597, -1.0805482), (-2.728194, -2.5870762), (-2.7583416, -2.8284984), (-1.160645, -1.3605578), (-1.0804958, -1.1190848), (-2.6519356, -2.5481098), (-2.7686536, -2.8072934), (-1.2287639, -1.4197203), (-1.0878056, -1.1669692), (-2.596061, -2.5242364), (-2.7534559, -2.786363), (-1.2753412, -1.4646069), (-1.1089714, -1.209924), (-2.5614448, -2.509246), (-2.7333508, -2.768831), (-1.3038112, -1.4978815), (-1.1307831, -1.2441384), (-2.5425723, -2.4982886), (-2.7160149, -2.7491384), (-1.3203474, -1.5167845), (-1.1497104, -1.2654686), (-2.534373, -2.485537), (-2.7034469, -2.7457254), (-1.3286599, -1.5449802), (-1.1624783, -1.2799393), (-2.5312989, -2.468304), (-2.6969724, -2.7454288), (-1.3343464, -1.5756475), (-1.1699325, -1.2887796), (-2.5292535, -2.4447544), (-2.695413, -2.748348), (-1.3406198, -1.6117467), (-1.1731635, -1.2927685), (-2.5255184, -2.4129899), (-2.6976123, -2.7548573), (-1.3495885, -1.6561677), (-1.1730798, -1.2918823), (-2.518252, -2.3704867), (-2.7027655, -2.7659943), (-1.3627563, -1.712494), (-1.170129, -1.2849658), (-2.5059662, -2.313248), (-2.71066, -2.7839036), (-1.3815643, -1.7861646), (-1.1640441, -1.2692826), (-2.4869142, -2.2343743), (-2.714773, -2.8124108), (-1.3993026, -1.8872524), (-1.1621633, -1.2492021), (-2.4720163, -2.1323981), (-2.7254448, -2.8469877), (-1.4234673, -2.0131068), (-1.1606396, -1.2401382), (-2.4556, -2.022763), (-2.734879, -2.868987), (-1.4491671, -2.1417043), (-1.1597723, -1.2392539), (-2.4363186, -1.9093595), (-2.7425158, -2.879402), (-1.4770511, -2.2670887), (-1.1594199, -1.2457819), (-2.413059, -1.798501), (-2.7479656, -2.8771405), (-1.5076487, -2.380776), (-1.1498557, -1.2603372), (-2.3727503, -1.6958213), (-2.7579412, -2.8599389), (-1.5556253, -2.4769416), (-1.1454608, -1.2842802), (-2.323397, -1.6115149), (-2.7615209, -2.8303955), (-1.6096665, -2.5476952), (-1.143134, -1.3205467), (-2.2644956, -1.5554262), (-2.760509, -2.7868898), (-1.6704651, -2.5855877), (-1.1409532, -1.3572035), (-2.1948905, -1.5203416), (-2.7579217, -2.7428913), (-1.7402692, -2.6036289), (-1.1363049, -1.3907303), (-2.112098, -1.4995726), (-2.7567275, -2.7157927), (-1.822293, -2.6248024), (-1.1267498, -1.409911), (-2.0134473, -1.4741615), (-2.7606997, -2.6883867), (-1.9225118, -2.6371133), (-1.108392, -1.403898), (-1.8904822, -1.4316299), (-2.7740133, -2.686378), (-2.0501645, -2.6714318), (-1.0782636, -1.3525152), (-1.7351408, -1.3426452), (-2.7995572, -2.7249696), (-2.213772, -2.7560463), (-1.0390445, -1.275708), (-1.5430309, -1.2079711), (-2.8240457, -2.718667), (-2.4091656, -2.8172972), (-1.0119904, -1.240574), (-1.3285033, -1.1008567), (-2.830831, -2.6692538), (-2.618112, -2.845434), (-1.0457921, -1.2700478), (-1.1617163, -1.0698941), (-2.7687638, -2.6066859), (-2.736786, -2.8396058), (-1.1232754, -1.3344501), (-1.088797, -1.0964187), (-2.6865687, -2.558942), (-2.7716165, -2.819183), (-1.2012159, -1.3991437), (-1.0808121, -1.144115), (-2.6189516, -2.5294728), (-2.7639992, -2.7966244), (-1.2586374, -1.4500232), (-1.098578, -1.1911297), (-2.5743487, -2.5120945), (-2.7439926, -2.776775), (-1.2950624, -1.487079), (-1.121485, -1.2300012), (-2.5487728, -2.501045), (-2.724338, -2.761096), (-1.316094, -1.5145365), (-1.1418985, -1.2490094), (-2.5366845, -2.4808369), (-2.7105045, -2.7546358), (-1.3277773, -1.5444213), (-1.1560005, -1.2674793), (-2.5306423, -2.463816), (-2.7022374, -2.751819), (-1.3355402, -1.5755907), (-1.165094, -1.2796087), (-2.526906, -2.441279), (-2.698838, -2.7526596), (-1.3427643, -1.6112248), (-1.1699103, -1.2862346), (-2.5225303, -2.4111166), (-2.699403, -2.7574291), (-1.3518076, -1.6543875), (-1.1711818, -1.2874866), (-2.5154092, -2.3707192), (-2.7032232, -2.767013), (-1.3644187, -1.7087274), (-1.1693181, -1.2824454), (-2.5038335, -2.3161676), (-2.7100623, -2.7833347), (-1.3822161, -1.7795683), (-1.1641239, -1.2687012), (-2.4859269, -2.2408795), (-2.7132719, -2.8099217), (-1.3985521, -1.8765697), (-1.1620424, -1.2478068), (-2.4714718, -2.1403005), (-2.7241313, -2.8454647), (-1.4222612, -2.001612), (-1.160327, -1.238322), (-2.4555035, -2.0316281), (-2.733781, -2.8682637), (-1.4475752, -2.1300216), (-1.159328, -1.2371906), (-2.436672, -1.9186081), (-2.741635, -2.8795576), (-1.4751272, -2.2560413), (-1.1589137, -1.2434293), (-2.413897, -1.8073376), (-2.7473185, -2.8783798), (-1.5054182, -2.3713331), (-1.1493049, -1.2575912), (-2.3740902, -1.703352), (-2.7575707, -2.8623505), (-1.5531285, -2.469954), (-1.1449218, -1.2811773), (-2.3252993, -1.6170629), (-2.7614155, -2.8337374), (-1.6068846, -2.5435011), (-1.1426591, -1.3173124), (-2.2670112, -1.5587919), (-2.7606113, -2.790687), (-1.667344, -2.5839314), (-1.1406087, -1.3542596), (-2.1980681, -1.521897), (-2.758105, -2.7465599), (-1.7367013, -2.6038418), (-1.1361934, -1.3884254), (-2.1160316, -1.4998634), (-2.7568324, -2.7189066), (-1.8180993, -2.6262324), (-1.1269922, -1.408493), (-2.0183191, -1.4736683), (-2.7605789, -2.6906374), (-1.9174391, -2.6392124), (-1.1091839, -1.4060358), (-1.8965281, -1.4331442), (-2.7731955, -2.685037), (-2.0437891, -2.6710515), (-1.079795, -1.3576342), (-1.7426172, -1.3466463), (-2.7981026, -2.7211328), (-2.2059047, -2.752998), (-1.0409217, -1.2822275), (-1.5518721, -1.214319), (-2.8228827, -2.7167583), (-2.4004276, -2.815035), (-1.0128922, -1.2451022), (-1.3373935, -1.1059093), (-2.8312428, -2.6687422), (-2.610481, -2.8440807), (-1.0439804, -1.2718337), (-1.1672657, -1.071811), (-2.7712631, -2.6063511), (-2.7331316, -2.8395076), (-1.1203686, -1.3354543), (-1.0906181, -1.0963628), (-2.6891515, -2.5576086), (-2.7708716, -2.8196473), (-1.1989033, -1.4008455), (-1.0805968, -1.1433799), (-2.6205273, -2.5269094), (-2.764514, -2.797285), (-1.2574725, -1.4530041), (-1.0976197, -1.1903532), (-2.5747318, -2.5082989), (-2.744874, -2.777475), (-1.2950286, -1.4914609), (-1.1203579, -1.2293729), (-2.5480754, -2.496027), (-2.7252388, -2.7554047), (-1.3173952, -1.5130775), (-1.1416905, -1.2548821), (-2.5351508, -2.483535), (-2.7098129, -2.749888), (-1.3289078, -1.5421221), (-1.1569936, -1.2727383), (-2.529664, -2.4675968), (-2.7006981, -2.7477157), (-1.3358899, -1.5721346), (-1.1666653, -1.2841958), (-2.5268552, -2.446133), (-2.696958, -2.7489944), (-1.3421323, -1.6064025), (-1.1716613, -1.2902282), (-2.523491, -2.4171498), (-2.6974788, -2.7539766), (-1.3501834, -1.6479604), (-1.1729109, -1.2911173), (-2.5173523, -2.378223), (-2.7013638, -2.7634552), (-1.3618387, -1.7002869), (-1.1710038, -1.2861236), (-2.5067468, -2.325742), (-2.7082088, -2.7791967), (-1.3786447, -1.7683359), (-1.1659106, -1.2730578), (-2.4899302, -2.253652), (-2.7111368, -2.80449), (-1.3936923, -1.8610978), (-1.1631026, -1.2502187), (-2.475723, -2.1541197), (-2.722639, -2.8416576), (-1.4172004, -1.9852383), (-1.1608678, -1.2394233), (-2.4599729, -2.0462704), (-2.7327673, -2.8657656), (-1.4423608, -2.1132765), (-1.1595589, -1.2373137), (-2.4414034, -1.9335812), (-2.740974, -2.8784084), (-1.4697353, -2.239785), (-1.1590003, -1.2425689), (-2.4190211, -1.8217863), (-2.7469542, -2.878856), (-1.4997578, -2.356735), (-1.1588707, -1.2556045), (-2.391749, -1.7188898), (-2.7503698, -2.865709), (-1.5329366, -2.4549654), (-1.1499261, -1.2767575), (-2.3471932, -1.6304702), (-2.757818, -2.8383002), (-1.5833554, -2.5306556), (-1.1455975, -1.3114545), (-2.2925014, -1.5694786), (-2.7585928, -2.7961879), (-1.6405213, -2.5738301), (-1.1432194, -1.3478359), (-2.2277198, -1.5302771), (-2.7562096, -2.7524226), (-1.7055367, -2.5958412), (-1.1397684, -1.3819779), (-2.151397, -1.506894), (-2.7539344, -2.711301), (-1.781331, -2.604353), (-1.1325935, -1.4115076), (-2.059993, -1.492602), (-2.7546384, -2.688786), (-1.8721524, -2.6213841), (-1.1192595, -1.4245707), (-1.9496831, -1.4681615), (-2.7619107, -2.6680813), (-1.9848973, -2.6351123), (-1.0953498, -1.3882346), (-1.8114204, -1.4001938), (-2.7823393, -2.696439), (-2.1300254, -2.6993384), (-1.0589973, -1.314855), (-1.6379848, -1.2835398), (-2.810756, -2.7241414), (-2.3112032, -2.7808897), (-1.02187, -1.2505736), (-1.4311382, -1.1539183), (-2.8355079, -2.697724), (-2.5255837, -2.8282518), (-1.0234598, -1.245486), (-1.2323599, -1.081688), (-2.8013887, -2.6408281), (-2.6876154, -2.8408468), (-1.0827614, -1.296358), (-1.115027, -1.0817713), (-2.7254167, -2.5846388), (-2.7591896, -2.827556), (-1.1631498, -1.3635381), (-1.080315, -1.1208924), (-2.6496341, -2.5462806), (-2.7682867, -2.8063576), (-1.2306697, -1.4222789), (-1.0883678, -1.1686902), (-2.5944476, -2.522744), (-2.752777, -2.7855759), (-1.2766688, -1.4668111), (-1.109667, -1.2113456), (-2.5603702, -2.5078702), (-2.7327275, -2.7682283), (-1.3047476, -1.4998966), (-1.131393, -1.2452426), (-2.5418353, -2.4968812), (-2.7155576, -2.748738), (-1.3210706, -1.5187889), (-1.150173, -1.266317), (-2.5338037, -2.4840145), (-2.7031674, -2.7454698), (-1.3293104, -1.5470375), (-1.1628011, -1.2805597), (-2.5307777, -2.466604), (-2.6968474, -2.7453222), (-1.3350153, -1.5778468), (-1.1701361, -1.2891877), (-2.5287006, -2.4428144), (-2.6954143, -2.748407), (-1.3413659, -1.6141784), (-1.1732652, -1.2929529), (-2.5248773, -2.4107234), (-2.697723, -2.7551224), (-1.3504581, -1.6589528), (-1.1730825, -1.2917974), (-2.517471, -2.3677566), (-2.7029865, -2.7665417), (-1.3637974, -1.7158167), (-1.1700156, -1.284516), (-2.5049858, -2.3098204), (-2.7110174, -2.7848651), (-1.382847, -1.7903373), (-1.1637658, -1.2683016), (-2.485634, -2.229839), (-2.7153523, -2.814), (-1.4009807, -1.8928177), (-1.1620475, -1.248733), (-2.4707038, -2.1275764), (-2.7258718, -2.8480618), (-1.4251667, -2.018796), (-1.1606306, -1.2399977), (-2.4542353, -2.0177698), (-2.7351916, -2.8696294), (-1.450895, -2.1473093), (-1.1598142, -1.2393664), (-2.4348643, -1.9044031), (-2.7427363, -2.8795807), (-1.4788294, -2.272296), (-1.1594743, -1.2461724), (-2.41146, -1.7939012), (-2.7480972, -2.8767757), (-1.5095129, -2.3851979), (-1.1499355, -1.2610443), (-2.3709857, -1.6919463), (-2.757958, -2.859015), (-1.5575746, -2.48024), (-1.1455204, -1.2852681), (-2.321394, -1.6086177), (-2.761447, -2.8290393), (-1.6117651, -2.549779), (-1.1431373, -1.321708), (-2.262175, -1.5535408), (-2.7603884, -2.785315), (-1.6728038, -2.5866203), (-1.1408577, -1.3583493), (-2.1921608, -1.5192488), (-2.7578247, -2.7413466), (-1.7429771, -2.6039577), (-1.136049, -1.3917234), (-2.1088216, -1.4989781), (-2.7567408, -2.714449), (-1.8255571, -2.624773), (-1.1262621, -1.4106703), (-2.009422, -1.4737966), (-2.7609012, -2.6873174), (-1.9265751, -2.6369934), (-1.1075469, -1.4026593), (-1.8854576, -1.4296259), (-2.7746992, -2.68734), (-2.055392, -2.67328), (-1.0769533, -1.3497531), (-1.7288606, -1.3387693), (-2.800621, -2.727066), (-2.2203207, -2.759773), (-1.0376253, -1.2726002), (-1.5355686, -1.2026086), (-2.824663, -2.7182534), (-2.4164271, -2.8195827), (-1.0115792, -1.2397205), (-1.3210621, -1.0972953), (-2.8299599, -2.6673994), (-2.624328, -2.8464963), (-1.0477642, -1.2716873), (-1.1571821, -1.0692865), (-2.7661045, -2.6045964), (-2.739643, -2.839516), (-1.1262152, -1.3369427), (-1.0873709, -1.0974834), (-2.6837883, -2.5574007), (-2.772117, -2.8186681), (-1.203774, -1.4014454), (-1.0810081, -1.1456527), (-2.6168628, -2.528437), (-2.763553, -2.7960377), (-1.2604554, -1.4518582), (-1.0993046, -1.1925807), (-2.5730026, -2.5113723), (-2.7433405, -2.776259), (-1.2962444, -1.4885154), (-1.1222649, -1.2311983), (-2.5479877, -2.500477), (-2.7237601, -2.7607014), (-1.316834, -1.5157189), (-1.1425654, -1.2499543), (-2.5362644, -2.4803298), (-2.7100816, -2.7543733), (-1.3282459, -1.5454713), (-1.1565123, -1.2681752), (-2.5304308, -2.4632654), (-2.7019787, -2.7516897), (-1.3358693, -1.5766269), (-1.1654524, -1.2800891), (-2.5267847, -2.4406075), (-2.6987271, -2.7526624), (-1.343052, -1.6123526), (-1.1701328, -1.2865231), (-2.5224104, -2.4102478), (-2.6994176, -2.7575736), (-1.3521274, -1.6557124), (-1.1712865, -1.287584), (-2.5152242, -2.369558), (-2.7033443, -2.7673278), (-1.3648273, -1.7103791), (-1.1693132, -1.2823217), (-2.5035233, -2.314572), (-2.7102854, -2.7838826), (-1.3827716, -1.7817369), (-1.164, -1.2682773), (-2.485423, -2.2386146), (-2.7136264, -2.8108165), (-1.3993437, -1.8795648), (-1.1619998, -1.2476691), (-2.470929, -2.1378236), (-2.7244012, -2.8460746), (-1.4230787, -2.0047314), (-1.1603392, -1.2383629), (-2.4549124, -2.0289965), (-2.7339826, -2.8686397), (-1.4484181, -2.1331599), (-1.1593689, -1.2373652), (-2.436018, -1.9159282), (-2.7417798, -2.8796842), (-1.4760038, -2.2590282), (-1.1589624, -1.2437468), (-2.4131548, -1.8047806), (-2.7474072, -2.8782053), (-1.5063416, -2.3739493), (-1.149365, -1.2580811), (-2.3732483, -1.7011278), (-2.75759, -2.8618457), (-1.5541004, -2.47199), (-1.1449716, -1.2818358), (-2.3243186, -1.6153362), (-2.7613747, -2.8329527), (-1.6079364, -2.5448704), (-1.142682, -1.3180931), (-2.2658517, -1.5576138), (-2.760528, -2.7897284), (-1.6685194, -2.5846972), (-1.1405855, -1.3550631), (-2.1966858, -1.5211651), (-2.7580156, -2.7455637), (-1.7380629, -2.6041813), (-1.1360922, -1.3891734), (-2.11436, -1.4994227), (-2.7567825, -2.7179728), (-1.81974, -2.6263368), (-1.1267757, -1.4091359), (-2.0162594, -1.4733729), (-2.7606122, -2.6898131), (-1.9194825, -2.639226), (-1.1087861, -1.405691), (-1.8939526, -1.4320579), (-2.7734652, -2.6852052), (-2.0464213, -2.6720045), (-1.0791571, -1.3565176), (-1.7393965, -1.3446531), (-2.798565, -2.7218862), (-2.2092104, -2.7548482), (-1.0402093, -1.2808752), (-1.548031, -1.2115991), (-2.8231366, -2.716285), (-2.4041183, -2.8161116), (-1.0126612, -1.2448264), (-1.3335197, -1.1041095), (-2.8307586, -2.6675804), (-2.6136813, -2.8445256), (-1.0449681, -1.2727934), (-1.1648544, -1.0715241), (-2.7698414, -2.6050925), (-2.7346041, -2.8393617), (-1.1218761, -1.3368353), (-1.0898252, -1.0969332), (-2.6876485, -2.556652), (-2.7711043, -2.8192925), (-1.2002227, -1.4021177), (-1.0806594, -1.1441855), (-2.6193764, -2.526223), (-2.7642448, -2.796904), (-1.2584114, -1.4540286), (-1.0979623, -1.1911013), (-2.5739627, -2.507775), (-2.7444954, -2.7771397), (-1.295641, -1.4922779), (-1.1207293, -1.2299796), (-2.5475948, -2.4955816), (-2.7249007, -2.755147), (-1.3177819, -1.5137734), (-1.142001, -1.2553482), (-2.5348551, -2.4831088), (-2.7095585, -2.7496963), (-1.3291619, -1.5427502), (-1.1572247, -1.273076), (-2.529471, -2.4671495), (-2.700529, -2.747594), (-1.3360782, -1.5727538), (-1.1668199, -1.2844207), (-2.5267081, -2.4456282), (-2.6968663, -2.7489445), (-1.3423017, -1.607064), (-1.1717476, -1.2903512), (-2.5233452, -2.41655), (-2.697452, -2.7540033), (-1.3503715, -1.6487172), (-1.1729403, -1.2911385), (-2.5171757, -2.3774812), (-2.7013922, -2.7635765), (-1.3620718, -1.7012048), (-1.1709787, -1.2860255), (-2.5065098, -2.324786), (-2.7082918, -2.7794456), (-1.378953, -1.7695128), (-1.1658258, -1.2728002), (-2.489598, -2.2523613), (-2.7112908, -2.804925), (-1.3941222, -1.8626926), (-1.1630638, -1.2501154), (-2.4753785, -2.1527424), (-2.7227492, -2.841943), (-1.4176397, -1.9868776), (-1.1608608, -1.2394122), (-2.4596121, -2.0448322), (-2.732844, -2.8659344), (-1.4428079, -2.1149101), (-1.1595677, -1.2373673), (-2.4410186, -1.9321328), (-2.7410245, -2.8784568), (-1.4701955, -2.2413328), (-1.1590136, -1.2426924), (-2.4185994, -1.8204112), (-2.7469807, -2.8787572), (-1.5002397, -2.3580875), (-1.1492616, -1.2558135), (-2.379198, -1.7149566), (-2.7575564, -2.8643851), (-1.5477031, -2.4592557), (-1.1448847, -1.2783586), (-2.3310437, -1.6263462), (-2.7616594, -2.8372564), (-1.6010144, -2.5358782), (-1.1427315, -1.3136312), (-2.273652, -1.5654081), (-2.761024, -2.7952354), (-1.6607602, -2.579201), (-1.1408993, -1.3502033), (-2.205898, -1.5262882), (-2.7585104, -2.7514715), (-1.7290232, -2.6011622), (-1.1368638, -1.3843788), (-2.125456, -1.5027885), (-2.756998, -2.7103016), (-1.8095121, -2.60983), (-1.1282294, -1.413946), (-2.02802, -1.4879808), (-2.7591236, -2.6878119), (-1.9071579, -2.6276884), (-1.1122239, -1.4129909), (-1.9092314, -1.4490921), (-2.770334, -2.681199), (-2.0300992, -2.6577713), (-1.0842869, -1.365383), (-1.7595658, -1.3647918), (-2.7945096, -2.7177444), (-2.1879182, -2.7380075), (-1.0457143, -1.2885796), (-1.5732783, -1.2337041), (-2.820869, -2.721994), (-2.3794825, -2.806534), (-1.0148404, -1.2444562), (-1.3600545, -1.1184161), (-2.8338184, -2.6787283), (-2.5915768, -2.8408918), (-1.038648, -1.2639667), (-1.1822168, -1.0744932), (-2.7797782, -2.616811), (-2.7239907, -2.840498), (-1.1114537, -1.3251294), (-1.09605, -1.093302), (-2.698637, -2.5659719), (-2.769268, -2.8221452), (-1.190729, -1.3912281), (-1.0806942, -1.1385812), (-2.6281796, -2.5333993), (-2.7661273, -2.799996), (-1.2513943, -1.444916), (-1.0958701, -1.1857581), (-2.5802212, -2.5136638), (-2.7472608, -2.779884), (-1.2908137, -1.4846116), (-1.1183541, -1.2255934), (-2.55191, -2.5008972), (-2.7274377, -2.7637172), (-1.3141108, -1.5141551), (-1.1391194, -1.2456565), (-2.5378306, -2.4794204), (-2.7130258, -2.756733), (-1.3273836, -1.5456357), (-1.1537843, -1.2650504), (-2.530409, -2.4614408), (-2.7041116, -2.753453), (-1.3362099, -1.5779917), (-1.1634377, -1.2778584), (-2.5257587, -2.4381275), (-2.7001355, -2.75396), (-1.344114, -1.6145507), (-1.1687143, -1.2848854), (-2.5207922, -2.4072468), (-2.7002594, -2.7585745), (-1.3535864, -1.6585382), (-1.1702995, -1.2862442), (-2.5132747, -2.36605), (-2.7038035, -2.7682292), (-1.3664995, -1.7137948), (-1.1685776, -1.280993), (-2.5013697, -2.3104274), (-2.7105525, -2.7849138), (-1.3845997, -1.7859064), (-1.16333, -1.2666656), (-2.4830647, -2.233506), (-2.713923, -2.8122518), (-1.4014319, -1.8849179), (-1.1615684, -1.2465897), (-2.468615, -2.13247), (-2.7244856, -2.8469818), (-1.4250963, -2.0101688), (-1.1600436, -1.237623), (-2.4526296, -2.0235052), (-2.733935, -2.8691082), (-1.4503831, -2.1384866), (-1.1591295, -1.2368903), (-2.433722, -1.9105015), (-2.7416487, -2.8796911), (-1.4779572, -2.2639494), (-1.1587279, -1.2435644), (-2.410775, -1.7997291), (-2.7472105, -2.8776736), (-1.5083448, -2.3780925), (-1.1491551, -1.2582375), (-2.3707564, -1.6968205), (-2.757301, -2.8607626), (-1.556166, -2.4750237), (-1.1447337, -1.2823031), (-2.3216176, -1.6120287), (-2.7610283, -2.831444), (-1.6101599, -2.546705), (-1.1423781, -1.3187702), (-2.262837, -1.5553464), (-2.7601717, -2.7880118), (-1.6710211, -2.5855012), (-1.1401727, -1.3557601), (-2.193236, -1.5197144), (-2.757725, -2.7438958), (-1.7410084, -2.6043172), (-1.135507, -1.3897558), (-2.110306, -1.4984845), (-2.7566519, -2.7165294), (-1.8233545, -2.626171), (-1.1259385, -1.4095266), (-2.011351, -1.4726586), (-2.7607262, -2.6886592), (-1.924063, -2.6390438), (-1.1075547, -1.4038548), (-1.8878845, -1.4294169), (-2.7741659, -2.6863623), (-2.0524073, -2.6741765), (-1.0774045, -1.3530176), (-1.7318487, -1.3398275), (-2.7997267, -2.7242966), (-2.2168055, -2.7592475), (-1.0383496, -1.2770797), (-1.5390652, -1.2050518), (-2.8237922, -2.7156422), (-2.412632, -2.8187866), (-1.0120096, -1.2438043), (-1.3245265, -1.0997382), (-2.8296616, -2.6651518), (-2.621045, -2.8457353), (-1.0471946, -1.2748356), (-1.1592835, -1.07071), (-2.7665875, -2.6023238), (-2.7380066, -2.8392022), (-1.1253339, -1.3400075), (-1.0879911, -1.0981644), (-2.6842158, -2.5544863), (-2.7716916, -2.818638), (-1.203292, -1.405171), (-1.0808061, -1.1460177), (-2.6167498, -2.5246053), (-2.7637005, -2.796184), (-1.2606484, -1.45663), (-1.0987711, -1.1928501), (-2.5722075, -2.5064664), (-2.743719, -2.77653), (-1.2971706, -1.4945134), (-1.1216174, -1.2314341), (-2.5464878, -2.4943757), (-2.7242324, -2.754731), (-1.3188396, -1.5158536), (-1.1427609, -1.256505), (-2.5341516, -2.4818642), (-2.709098, -2.7494442), (-1.329962, -1.5447932), (-1.1578084, -1.2739493), (-2.528977, -2.4657574), (-2.70028, -2.7475216), (-1.3367778, -1.5749041), (-1.1672316, -1.2850311), (-2.52628, -2.4439864), (-2.6968071, -2.7490673), (-1.3430152, -1.6094588), (-1.172002, -1.2907017), (-2.5228755, -2.41454), (-2.6975594, -2.7543554), (-1.3511859, -1.6515152), (-1.1730479, -1.2911977), (-2.516576, -2.3749392), (-2.7016559, -2.7642286), (-1.3630662, -1.7046231), (-1.170933, -1.2857064), (-2.5056913, -2.3214536), (-2.7087233, -2.7805305), (-1.3802176, -1.7738919), (-1.1655895, -1.271934), (-2.488445, -2.2478025), (-2.7119644, -2.806667), (-1.3958197, -1.8686054), (-1.1630014, -1.2498099), (-2.4741836, -2.1478367), (-2.7232587, -2.8431406), (-1.4193659, -1.9929848), (-1.1609159, -1.2394539), (-2.4583573, -2.0396793), (-2.7332258, -2.8666956), (-1.4445646, -2.1210198), (-1.1596836, -1.2376567), (-2.439673, -1.9269209), (-2.7413046, -2.8787603), (-1.4719977, -2.2471373), (-1.1591448, -1.2432438), (-2.4171104, -1.8154509), (-2.7471652, -2.8785076), (-1.5021191, -2.3631816), (-1.1494154, -1.2566841), (-2.3775437, -1.7106433), (-2.75762, -2.8635201), (-1.5496613, -2.4632363), (-1.1450151, -1.2795483), (-2.329143, -1.6229994), (-2.7616189, -2.8358612), (-1.6031158, -2.5385697), (-1.142806, -1.3150607), (-2.271422, -1.5631394), (-2.76091, -2.793506), (-1.6630907, -2.5807054), (-1.1408819, -1.3516765), (-2.203249, -1.5249104), (-2.7583892, -2.74968), (-1.7317076, -2.6018236), (-1.1366973, -1.385739), (-2.122261, -1.5020014), (-2.756956, -2.7086341), (-1.8127263, -2.6100125), (-1.1278408, -1.4151119), (-2.0240862, -1.4874911), (-2.7592452, -2.6863682), (-1.9111412, -2.6277053), (-1.1114837, -1.4122139), (-1.9043144, -1.4470532), (-2.7709272, -2.6817417), (-2.0352187, -2.6596544), (-1.0830609, -1.3630503), (-1.7533925, -1.3609427), (-2.7955172, -2.719505), (-2.1943774, -2.7417245), (-1.0442839, -1.2857138), (-1.5658294, -1.2283171), (-2.8215568, -2.7213757), (-2.3868275, -2.8088303), (-1.0142492, -1.2437019), (-1.3523266, -1.1147182), (-2.833115, -2.6766136), (-2.5981917, -2.8419228), (-1.0404738, -1.2657428), (-1.1771948, -1.0738114), (-2.7771137, -2.614392), (-2.7272308, -2.8402948), (-1.1144501, -1.3278471), (-1.0943089, -1.094433), (-2.6956882, -2.5641441), (-2.7699134, -2.8214905), (-1.1934258, -1.3937755), (-1.0807829, -1.1402472), (-2.6258998, -2.5321293), (-2.7656732, -2.799275), (-1.2533326, -1.4469695), (-1.0965805, -1.1873306), (-2.5787127, -2.5127347), (-2.746547, -2.7792509), (-1.2920804, -1.4862385), (-1.1191401, -1.2268808), (-2.5509918, -2.500127), (-2.7267902, -2.7632227), (-1.3149102, -1.5155141), (-1.1397902, -1.2466639), (-2.5372949, -2.4787073), (-2.7125375, -2.7563858), (-1.327897, -1.5468581), (-1.154291, -1.2657789), (-2.5300908, -2.460669), (-2.7037976, -2.753256), (-1.33658, -1.5792066), (-1.1637809, -1.2783425), (-2.5255306, -2.4372137), (-2.6999803, -2.7539155), (-1.3444448, -1.6158748), (-1.1689104, -1.2851487), (-2.520561, -2.4061048), (-2.7002394, -2.7586944), (-1.3539566, -1.660091), (-1.1703676, -1.2862856), (-2.512967, -2.3645701), (-2.7039018, -2.768551), (-1.3669729, -1.7157264), (-1.168522, -1.280771), (-2.5009186, -2.308442), (-2.7107697, -2.785515), (-1.3852446, -1.7884425), (-1.1631384, -1.2660867), (-2.4823923, -2.230738), (-2.7142975, -2.8132675), (-1.402354, -1.8884267), (-1.1614718, -1.246348), (-2.4678986, -2.1294565), (-2.7247643, -2.8476632), (-1.4260514, -2.0138226), (-1.1600131, -1.2375978), (-2.4518578, -2.0203197), (-2.734137, -2.86951), (-1.4513731, -2.1421509), (-1.1591346, -1.2370303), (-2.432876, -1.907277), (-2.7417858, -2.8797927), (-1.4789915, -2.2674174), (-1.1587435, -1.2438848), (-2.4098222, -1.7966752), (-2.7472837, -2.8774154), (-1.5094436, -2.381103), (-1.1491898, -1.2587699), (-2.3696852, -1.694187), (-2.7572944, -2.8601203), (-1.5573285, -2.4773352), (-1.1447589, -1.2830347), (-2.3203826, -1.6100006), (-2.760956, -2.8304846), (-1.6114236, -2.5482302), (-1.1423719, -1.3196412), (-2.2613885, -1.5539702), (-2.7600567, -2.7868655), (-1.6724418, -2.5863261), (-1.1401099, -1.3566515), (-2.1915174, -1.5188603), (-2.757612, -2.742724), (-1.742667, -2.6046584), (-1.13535, -1.3905777), (-2.1082327, -1.497968), (-2.7565963, -2.7154474), (-1.825367, -2.6262536), (-1.125642, -1.4102207), (-2.0087998, -1.4723055), (-2.7607794, -2.6877196), (-1.9265791, -2.6390378), (-1.1070393, -1.4033344), (-1.8847003, -1.4280782), (-2.7745113, -2.6866493), (-2.0556536, -2.6753447), (-1.0766045, -1.3515614), (-1.7278769, -1.337368), (-2.8002985, -2.7252817), (-2.2208707, -2.7615225), (-1.0374868, -1.2753837), (-1.5343654, -1.2017252), (-2.8240767, -2.7150989), (-2.417119, -2.8201063), (-1.0117812, -1.2434598), (-1.3198711, -1.0975721), (-2.8290212, -2.6637864), (-2.6248496, -2.8462918), (-1.0484445, -1.2759893), (-1.1564679, -1.0703787), (-2.7648532, -2.6008596), (-2.7397106, -2.839056), (-1.127166, -1.3416524), (-1.0871049, -1.0988481), (-2.6824234, -2.553382), (-2.7719443, -2.8182433), (-1.2048774, -1.4066778), (-1.0809155, -1.1469743), (-2.615394, -2.523821), (-2.7633786, -2.7957568), (-1.2617741, -1.4578394), (-1.0992011, -1.1937402), (-2.5713124, -2.5058799), (-2.7432785, -2.7761557), (-1.2979064, -1.4954716), (-1.1220769, -1.2321573), (-2.545939, -2.4938872), (-2.7238438, -2.7544487), (-1.3193066, -1.5166645), (-1.1431483, -1.2570636), (-2.5338268, -2.481406), (-2.7088099, -2.74924), (-1.330268, -1.5455179), (-1.1581018, -1.2743568), (-2.5287795, -2.4652808), (-2.7000952, -2.7473998), (-1.3370029, -1.5756128), (-1.1674334, -1.2853082), (-2.5261383, -2.4434483), (-2.696714, -2.7490292), (-1.3432177, -1.6102127), (-1.1721253, -1.2908608), (-2.5227387, -2.4138968), (-2.6975427, -2.7544072), (-1.3514066, -1.6523747), (-1.1731025, -1.2912396), (-2.5164056, -2.3741374), (-2.701705, -2.7643871), (-1.363338, -1.7056612), (-1.1709247, -1.2856135), (-2.5054538, -2.3204126), (-2.7088363, -2.780832), (-1.3805735, -1.7752196), (-1.1655114, -1.2716609), (-2.4881003, -2.246385), (-2.7121592, -2.8071783), (-1.3963126, -1.8704023), (-1.1629715, -1.2497061), (-2.4738224, -2.146319), (-2.7234056, -2.8434842), (-1.4198711, -1.9948338), (-1.1609204, -1.239456), (-2.4579737, -2.0380921), (-2.733334, -2.8669047), (-1.4450815, -2.1228607), (-1.159706, -1.2377338), (-2.4392588, -1.9253237), (-2.7413816, -2.8788302), (-1.4725302, -2.2488768), (-1.1591712, -1.2434009), (-2.416651, -1.8139398), (-2.7472136, -2.8784087), (-1.5026778, -2.3646958), (-1.1494493, -1.2569375), (-2.377031, -1.7093362), (-2.7576308, -2.8632374), (-1.5502456, -2.464408), (-1.1450425, -1.2798964), (-2.3285542, -1.621991), (-2.7615974, -2.8354223), (-1.603746, -2.5393488), (-1.1428182, -1.3154787), (-2.2707324, -1.5624598), (-2.760867, -2.7929718), (-1.6637927, -2.581128), (-1.1408662, -1.3521047), (-2.2024322, -1.5244995), (-2.7583456, -2.7491333), (-1.7325175, -2.6019943), (-1.1366363, -1.3861285), (-2.1212792, -1.5017675), (-2.756938, -2.7081316), (-1.8136985, -2.6100435), (-1.1277124, -1.4154402), (-2.022881, -1.4873452), (-2.75928, -2.685938), (-1.9123486, -2.6276903), (-1.1112474, -1.4119465), (-1.90281, -1.4464321), (-2.771106, -2.6819215), (-2.036773, -2.6602135), (-1.0826782, -1.3623087), (-1.7515073, -1.3597673), (-2.7958224, -2.7200556), (-2.1963384, -2.742846), (-1.043843, -1.2848175), (-1.5635611, -1.2266752), (-2.8217595, -2.721199), (-2.3890505, -2.8095222), (-1.014073, -1.2434542), (-1.3499887, -1.113594), (-2.8328903, -2.6759858), (-2.600178, -2.8422334), (-1.0410309, -1.2662632), (-1.1756896, -1.0736014), (-2.7763014, -2.6136754), (-2.7281926, -2.8402362), (-1.1153536, -1.3286507), (-1.0937928, -1.0947669), (-2.6948, -2.5636053), (-2.7701004, -2.821296), (-1.1942338, -1.3945258), (-1.0808114, -1.1407409), (-2.625215, -2.531758), (-2.7655337, -2.7990613), (-1.2539126, -1.4475726), (-1.0967951, -1.1877971), (-2.578262, -2.512467), (-2.7463322, -2.7790637), (-1.2924583, -1.4867133), (-1.1193776, -1.2272627), (-2.5507207, -2.499909), (-2.726596, -2.7566357), (-1.3154967, -1.508603), (-1.1408826, -1.2532227), (-2.537243, -2.487401), (-2.7108974, -2.7508407), (-1.3273747, -1.5377294), (-1.1564344, -1.2715319), (-2.5314693, -2.4716713), (-2.7014844, -2.7483428), (-1.3344954, -1.5675435), (-1.1663618, -1.2834388), (-2.528579, -2.4506412), (-2.6974576, -2.7492347), (-1.3406986, -1.6013191), (-1.171605, -1.289954), (-2.5252979, -2.4223647), (-2.6976993, -2.7537303), (-1.3485603, -1.642031), (-1.1731087, -1.2914273), (-2.5193973, -2.3845286), (-2.70129, -2.7625465), (-1.359869, -1.6930137), (-1.1714964, -1.287229), (-2.5092075, -2.333732), (-2.7077758, -2.7773178), (-1.3761255, -1.7589415), (-1.1668029, -1.2753465), (-2.4930634, -2.264317), (-2.710153, -2.8011217), (-1.3902484, -1.8482802), (-1.1635922, -1.2512337), (-2.478878, -2.1655223), (-2.721995, -2.8394523), (-1.4137217, -1.9719278), (-1.1610806, -1.2396488), (-2.463191, -2.0582376), (-2.7323747, -2.8644648), (-1.4388338, -2.0998442), (-1.1596211, -1.2369837), (-2.444754, -1.9457451), (-2.7407634, -2.8780453), (-1.4661149, -2.2268572), (-1.1590129, -1.2416458), (-2.4226243, -1.833496), (-2.7469091, -2.8796644), (-1.4959624, -2.3451948), (-1.1588942, -1.2539327), (-2.3957682, -1.7292897), (-2.7505136, -2.8678615), (-1.5288568, -2.445694), (-1.1498994, -1.2742999), (-2.3516839, -1.6387976), (-2.7582166, -2.841665), (-1.5789816, -2.5241606), (-1.145613, -1.308341), (-2.2976224, -1.5753852), (-2.7591712, -2.8003895), (-1.6356896, -2.5700033), (-1.1433635, -1.3444862), (-2.2336488, -1.5341035), (-2.7568243, -2.756867), (-1.7000006, -2.5939767), (-1.1401637, -1.3787452), (-2.1583822, -1.5092726), (-2.7543976, -2.7155612), (-1.7747349, -2.6036541), (-1.1333985, -1.4085912), (-2.068423, -1.4941859), (-2.754753, -2.6926131), (-1.8639963, -2.6211438), (-1.1206933, -1.423561), (-1.9600958, -1.4708943), (-2.7613034, -2.6698637), (-1.9744872, -2.633221), (-1.0977094, -1.3913248), (-1.8244436, -1.4069064), (-2.7806547, -2.6943047), (-2.116654, -2.6929913), (-1.0621643, -1.3205472), (-1.6539916, -1.2942803), (-2.809014, -2.7257586), (-2.2951436, -2.7767866), (-1.0241408, -1.2528186), (-1.449105, -1.162119), (-2.8359075, -2.7024255), (-2.5092647, -2.826489), (-1.020628, -1.2422253), (-1.2458568, -1.0834632), (-2.8066528, -2.6465542), (-2.6781967, -2.841943), (-1.0763221, -1.290602), (-1.1207206, -1.0789896), (-2.7322273, -2.5887814), (-2.7566285, -2.829627), (-1.1567695, -1.3579943), (-1.0807414, -1.1165676), (-2.6551294, -2.5487714), (-2.769075, -2.8084402), (-1.2260182, -1.4179616), (-1.0867747, -1.1645571), (-2.5980089, -2.5241992), (-2.7544732, -2.787323), (-1.2737259, -1.4636271), (-1.1077056, -1.2079571), (-2.5623684, -2.508814), (-2.7343283, -2.7695565), (-1.3030294, -1.4974557), (-1.1296594, -1.242643), (-2.5427926, -2.4976697), (-2.7167583, -2.7496395), (-1.3201048, -1.5166881), (-1.1488299, -1.2643728), (-2.5341628, -2.4848735), (-2.7039254, -2.7460334), (-1.3287259, -1.5450416), (-1.1618415, -1.2791584), (-2.5308623, -2.4676845), (-2.697217, -2.7455757), (-1.3345555, -1.5757384), (-1.1695026, -1.2882311), (-2.5287337, -2.4442382), (-2.6954715, -2.7483704), (-1.3408583, -1.6117793), (-1.1728942, -1.2923846), (-2.5250115, -2.4126167), (-2.697538, -2.7547858), (-1.3497871, -1.6560811), (-1.1729252, -1.2916162), (-2.5178146, -2.3702888), (-2.7026048, -2.7658467), (-1.3628751, -1.7122386), (-1.1700507, -1.2847944), (-2.5056307, -2.3132617), (-2.7104473, -2.7836795), (-1.3815815, -1.7856876), (-1.1640201, -1.2692137), (-2.4867013, -2.2346604), (-2.7145245, -2.812087), (-1.3992034, -1.8864743), (-1.1621174, -1.2490566), (-2.4718666, -2.1328082), (-2.7252326, -2.846753), (-1.4233248, -2.0122511), (-1.1605747, -1.2399508), (-2.4555073, -2.0232675), (-2.7347045, -2.8688266), (-1.4489924, -2.1408174), (-1.1596931, -1.2390419), (-2.4362755, -1.9099176), (-2.7423759, -2.8793194), (-1.4768549, -2.2662332), (-1.1593367, -1.2455473), (-2.4130642, -1.7990552), (-2.7478583, -2.8771508), (-1.5074357, -2.3800259), (-1.1497719, -1.2600744), (-2.3728015, -1.696306), (-2.7578661, -2.8600492), (-1.5554005, -2.4763606), (-1.1453837, -1.2839917), (-2.3234978, -1.6118796), (-2.761475, -2.830592), (-1.609429, -2.5473137), (-1.1430676, -1.3202474), (-2.2646503, -1.5556518), (-2.76049, -2.787139), (-1.6702099, -2.5853896), (-1.1409013, -1.3569223), (-2.1951027, -1.5204527), (-2.7579195, -2.743153), (-1.7399884, -2.6035686), (-1.1362748, -1.3904896), (-2.1123736, -1.4996064), (-2.7567306, -2.7160385), (-1.821974, -2.6248348), (-1.1267502, -1.4097271), (-2.0137992, -1.4741504), (-2.760696, -2.6885967), (-1.922134, -2.6372023), (-1.1084368, -1.4039541), (-1.89093, -1.4317645), (-2.7739706, -2.686363), (-2.049697, -2.67136), (-1.0783643, -1.3527722), (-1.7357004, -1.3429549), (-2.7994804, -2.7248025), (-2.2132037, -2.7558084), (-1.0391717, -1.2760606), (-1.5436925, -1.2084342), (-2.824001, -2.7186608), (-2.408546, -2.8171568), (-1.0120484, -1.240786), (-1.3291662, -1.1012026), (-2.8309093, -2.669345), (-2.6175876, -2.84538), (-1.0456563, -1.2700746), (-1.1621327, -1.0700088), (-2.7689974, -2.606784), (-2.7365606, -2.8396473), (-1.1230682, -1.3344296), (-1.0889453, -1.0963997), (-2.6868076, -2.5589738), (-2.7716017, -2.819264), (-1.2010565, -1.3991727), (-1.0808213, -1.144059), (-2.6191225, -2.5294328), (-2.7640738, -2.7967172), (-1.2585596, -1.4501313), (-1.0985404, -1.19108), (-2.5744421, -2.511989), (-2.7440927, -2.7768676), (-1.2950615, -1.4872677), (-1.1214427, -1.2299722), (-2.5488007, -2.5008824), (-2.7244363, -2.7611837), (-1.3161536, -1.5147957), (-1.1418656, -1.249005), (-2.5366611, -2.4806283), (-2.710592, -2.7547183), (-1.3278822, -1.5447415), (-1.1559792, -1.2674881), (-2.5305786, -2.4635572), (-2.702314, -2.7518992), (-1.335681, -1.5759679), (-1.1650823, -1.2796212), (-2.5268097, -2.4409666), (-2.6989055, -2.752745), (-1.3429343, -1.6116623), (-1.1699055, -1.2862386), (-2.5224066, -2.4107404), (-2.6994655, -2.7575312), (-1.3520026, -1.6548955), (-1.1711766, -1.287462), (-2.5152583, -2.3702579), (-2.7032888, -2.7671504), (-1.3646408, -1.7093341), (-1.1693009, -1.2823696), (-2.5036469, -2.31558), (-2.7101417, -2.783534), (-1.3824773, -1.7803259), (-1.1640834, -1.26854), (-2.4856887, -2.240095), (-2.7133825, -2.8102214), (-1.3988767, -1.8775707), (-1.1620337, -1.2477388), (-2.4712317, -2.139461), (-2.7242105, -2.8456712), (-1.4225825, -2.0026388), (-1.1603354, -1.2383102), (-2.4552565, -2.0307498), (-2.7338393, -2.8683941), (-1.4478968, -2.131042), (-1.1593441, -1.237222), (-2.4364116, -1.917725), (-2.7416766, -2.879606), (-1.4754534, -2.2570014), (-1.1589303, -1.2435071), (-2.413613, -1.8065048), (-2.7473433, -2.8783314), (-1.505755, -2.3721645), (-1.1493233, -1.2577254), (-2.3737776, -1.702636), (-2.7575753, -2.8621979), (-1.553478, -2.4705923), (-1.144934, -1.2813638), (-2.324942, -1.6165124), (-2.761402, -2.833498), (-1.6072602, -2.5439224), (-1.1426611, -1.3175362), (-2.2665954, -1.5584208), (-2.7605863, -2.790398), (-1.6677618, -2.5841618), (-1.1405946, -1.3544886), (-2.1975782, -1.5216681), (-2.7580807, -2.7462626), (-1.7371832, -2.6039386), (-1.1361505, -1.3886354), (-2.1154432, -1.4997268), (-2.7568257, -2.718634), (-1.8186785, -2.626259), (-1.1269066, -1.408668), (-2.0175943, -1.4735743), (-2.7606027, -2.6904037), (-1.9181615, -2.639218), (-1.1090347, -1.4058654), (-1.8956215, -1.432763), (-2.7733042, -2.6851513), (-2.044721, -2.6713982), (-1.0795605, -1.3571978), (-1.7414817, -1.3459378), (-2.7982807, -2.7214465), (-2.2070782, -2.7536705), (-1.0406619, -1.2817199), (-1.5505133, -1.2133478), (-2.8229856, -2.7166224), (-2.4017417, -2.815436), (-1.012805, -1.2449923), (-1.3360181, -1.1052641), (-2.8310816, -2.668346), (-2.611627, -2.844255), (-1.0443288, -1.272175), (-1.166405, -1.0717052), (-2.7707653, -2.6059093), (-2.733666, -2.8394694), (-1.1209061, -1.3359563), (-1.0903331, -1.0965656), (-2.6886199, -2.5572653), (-2.7709627, -2.819533), (-1.1993784, -1.4013201), (-1.0806181, -1.1436702), (-2.6201172, -2.5266533), (-2.764425, -2.79716), (-1.2578148, -1.4534), (-1.0977411, -1.1906244), (-2.574454, -2.5080914), (-2.7447455, -2.7773664), (-1.2952588, -1.4917947), (-1.1204906, -1.2295954), (-2.5478973, -2.495837), (-2.7251248, -2.7553234), (-1.3175478, -1.5133795), (-1.1418006, -1.2550548), (-2.5350342, -2.4833398), (-2.7097285, -2.7498298), (-1.3290176, -1.5424119), (-1.1570756, -1.2728657), (-2.5295792, -2.4673824), (-2.7006435, -2.7476847), (-1.3359804, -1.5724354), (-1.1667197, -1.2842795), (-2.526783, -2.4458826), (-2.6969328, -2.748993), (-1.3422205, -1.6067351), (-1.1716877, -1.29027), (-2.5234118, -2.4168441), (-2.6974792, -2.7540076), (-1.3502864, -1.6483498), (-1.1729149, -1.2911147), (-2.5172539, -2.3778403), (-2.7013867, -2.7635295), (-1.3619658, -1.7007643), (-1.1709836, -1.2860647), (-2.5066166, -2.325244), (-2.708256, -2.7793338), (-1.3788099, -1.7689527), (-1.1658611, -1.2729192), (-2.48975, -2.2529752), (-2.7112188, -2.8047218), (-1.3939211, -1.8619376), (-1.1630789, -1.2501627), (-2.4755359, -2.1533928), (-2.7226965, -2.841811), (-1.4174342, -1.986107), (-1.1608617, -1.2394159), (-2.459777, -2.0455067), (-2.7328064, -2.8658576), (-1.4425983, -2.1141481), (-1.159561, -1.2373422), (-2.441194, -1.9328078), (-2.7409985, -2.8784356), (-1.4699802, -2.2406151), (-1.159006, -1.2426353), (-2.4187934, -1.8210479), (-2.746966, -2.8788047), (-1.5000134, -2.3574655), (-1.1492504, -1.2557164), (-2.379412, -1.715505), (-2.757556, -2.8645065), (-1.5474668, -2.4587774), (-1.1448764, -1.2782236), (-2.3312879, -1.6267682), (-2.761671, -2.8374407), (-1.6007593, -2.535565), (-1.1427296, -1.3134693), (-2.2739367, -1.5656912), (-2.761044, -2.7954557), (-1.6604764, -2.5790367), (-1.1409081, -1.3500396), (-2.206233, -1.5264574), (-2.7585304, -2.7516937), (-1.7286963, -2.6011035), (-1.1368914, -1.3842324), (-2.1258566, -1.5028816), (-2.757007, -2.7105026), (-1.8091193, -2.6098297), (-1.1282834, -1.4138267), (-2.0285103, -1.4880359), (-2.7591107, -2.6879797), (-1.9066699, -2.6277075), (-1.1123222, -1.413118), (-1.9098415, -1.4493402), (-2.7702596, -2.6811118), (-2.029471, -2.6575546), (-1.0844463, -1.365708), (-1.7603296, -1.3652673), (-2.7943797, -2.7175), (-2.1871216, -2.7375567), (-1.045901, -1.2889731), (-1.5742022, -1.2343761), (-2.8207753, -2.7220397), (-2.3785708, -2.806248), (-1.0149255, -1.2445866), (-1.3610189, -1.1188879), (-2.8338938, -2.6789658), (-2.5907462, -2.8407583), (-1.0384313, -1.2637763), (-1.1828511, -1.0745914), (-2.780101, -2.6170914), (-2.7235777, -2.8405178), (-1.1110866, -1.3248165), (-1.0962735, -1.093171), (-2.6989975, -2.5661795), (-2.7691803, -2.8222218), (-1.1903969, -1.3909333), (-1.0806869, -1.1383793), (-2.6284575, -2.5335357), (-2.766179, -2.800081), (-1.2511559, -1.4446819), (-1.0957817, -1.1855645), (-2.5804014, -2.5137541), (-2.7473462, -2.7799592), (-1.2906601, -1.4844334), (-1.1182541, -1.2254335), (-2.5520139, -2.5009637), (-2.7275155, -2.763776), (-1.3140168, -1.5140138), (-1.1390327, -1.2455308), (-2.5378852, -2.4794762), (-2.7130835, -2.7567725), (-1.3273257, -1.5455139), (-1.1537172, -1.2649579), (-2.5304344, -2.4614995), (-2.7041476, -2.7534752), (-1.3361721, -1.5778748), (-1.1633899, -1.2777936), (-2.5257704, -2.438198), (-2.7001536, -2.7539647), (-1.344083, -1.6144255), (-1.1686834, -1.2848464), (-2.520802, -2.40734), (-2.7002606, -2.7585604), (-1.3535525, -1.6583911), (-1.1702833, -1.2862289), (-2.5132904, -2.3661754), (-2.703791, -2.768195), (-1.3664557, -1.7136124), (-1.168574, -1.2810036), (-2.5013983, -2.3106005), (-2.7105272, -2.7848525), (-1.3845406, -1.785667), (-1.1633416, -1.2667112), (-2.4831152, -2.233754), (-2.7138817, -2.812151), (-1.4013461, -1.884586), (-1.1615719, -1.2466038), (-2.4686697, -2.1327422), (-2.7244527, -2.846913), (-1.4250066, -2.0098233), (-1.1600407, -1.2376175), (-2.4526904, -2.0237942), (-2.73391, -2.8690658), (-1.45029, -2.13814), (-1.1591243, -1.2368698), (-2.4337914, -1.9107949), (-2.74163, -2.8796763), (-1.4778594, -2.26362), (-1.1587214, -1.2435273), (-2.4108543, -1.800008), (-2.7471974, -2.8776932), (-1.5082414, -2.3778064), (-1.149148, -1.2581819), (-2.3708477, -1.6970613), (-2.757296, -2.8608181), (-1.5560563, -2.474803), (-1.1447275, -1.282229), (-2.3217263, -1.6122133), (-2.7610312, -2.8315287), (-1.6100394, -2.5465567), (-1.142374, -1.3186829), (-2.262966, -1.5554707), (-2.760179, -2.7881155), (-1.6708872, -2.58542), (-1.1401743, -1.3556716), (-2.1933904, -1.5197898), (-2.7577329, -2.7440026), (-1.740853, -2.604282), (-1.1355166, -1.3896744), (-2.110492, -1.4985286), (-2.7566547, -2.7166283), (-1.8231685, -2.6261623), (-1.1259626, -1.4094586), (-2.0115817, -1.4726881), (-2.7607193, -2.688745), (-1.9238304, -2.6390433), (-1.1076, -1.4038996), (-1.8881742, -1.4295367), (-2.7741318, -2.6863353), (-2.052108, -2.6740692), (-1.0774767, -1.3531492), (-1.7322116, -1.3400503), (-2.7996724, -2.724205), (-2.2164307, -2.7590394), (-1.0384272, -1.2772357), (-1.5394951, -1.2053552), (-2.823764, -2.715689), (-2.412218, -2.8186646), (-1.0120311, -1.243838), (-1.3249547, -1.0999374), (-2.8297174, -2.665275), (-2.6206923, -2.8456836), (-1.0470812, -1.2747315), (-1.1595446, -1.0707414), (-2.766745, -2.602457), (-2.7378469, -2.8392148), (-1.1251662, -1.3398576), (-1.0880749, -1.0981022), (-2.6843803, -2.5545871), (-2.771666, -2.8186743), (-1.2031456, -1.405033), (-1.0807976, -1.1459298), (-2.6168752, -2.5246773), (-2.7637293, -2.7962239), (-1.2605443, -1.4565196), (-1.0987325, -1.1927688), (-2.572291, -2.5065212), (-2.74376, -2.7765656), (-1.2971029, -1.4944257), (-1.1215756, -1.2313677), (-2.546539, -2.4944215), (-2.724269, -2.7547586), (-1.3187968, -1.5157794), (-1.1427264, -1.2564528), (-2.5341837, -2.4819078), (-2.709125, -2.7494655), (-1.329933, -1.5447268), (-1.1577824, -1.273911), (-2.5289974, -2.4658024), (-2.7002983, -2.747536), (-1.3367571, -1.574841), (-1.1672132, -1.2850049), (-2.5262945, -2.4440353), (-2.6968179, -2.749075), (-1.342998, -1.6093941), (-1.1719912, -1.2906858), (-2.522889, -2.4145963), (-2.6975627, -2.7543547), (-1.3511674, -1.6514432), (-1.1730429, -1.2911925), (-2.5165913, -2.3750074), (-2.7016532, -2.764218), (-1.3630441, -1.7045367), (-1.1709346, -1.2857134), (-2.5057135, -2.321542), (-2.7087154, -2.780507), (-1.380188, -1.7737813), (-1.1655961, -1.2719574), (-2.4884748, -2.2479224), (-2.7119503, -2.8066256), (-1.3957803, -1.8684558), (-1.1630044, -1.2498198), (-2.4742146, -2.1479657), (-2.7232482, -2.8431127), (-1.4193252, -1.9928297), (-1.1609164, -1.2394545), (-2.4583905, -2.0398142), (-2.733218, -2.866679), (-1.4445223, -2.120865), (-1.1596828, -1.2376513), (-2.4397092, -1.9270574), (-2.7412999, -2.878756), (-1.4719534, -2.2469904), (-1.1591427, -1.2432306), (-2.417151, -1.8155805), (-2.747164, -2.8785176), (-1.502073, -2.3630538), (-1.149412, -1.2566628), (-2.3775878, -1.7107551), (-2.757622, -2.8635464), (-1.5496138, -2.463139), (-1.1450125, -1.2795193), (-2.3291922, -1.6230854), (-2.7616224, -2.8358998), (-1.6030656, -2.5385048), (-1.142806, -1.3150258), (-2.2714794, -1.5631984), (-2.760915, -2.7935529), (-1.663035, -2.5806708), (-1.1408839, -1.3516414), (-2.203316, -1.5249459), (-2.7583938, -2.7497268), (-1.7316432, -2.6018105), (-1.1367034, -1.3857079), (-2.1223414, -1.5020221), (-2.7569587, -2.7086773), (-1.8126488, -2.6100109), (-1.1278521, -1.4150854), (-2.0241854, -1.487504), (-2.7592444, -2.6864057), (-1.9110446, -2.627709), (-1.1115035, -1.4122355), (-1.9044375, -1.4471036), (-2.7709138, -2.681729), (-2.0350935, -2.6596112), (-1.0830922, -1.3631104), (-1.7535461, -1.3610374), (-2.795494, -2.7194617), (-2.1942196, -2.7416356), (-1.0443199, -1.2857875), (-1.5660139, -1.2284509), (-2.821542, -2.7213907), (-2.3866484, -2.8087747), (-1.0142639, -1.2437224), (-1.352516, -1.1148105), (-2.8331332, -2.6766655), (-2.5980313, -2.8418975), (-1.0404301, -1.2657013), (-1.1773185, -1.0738302), (-2.777179, -2.6144512), (-2.7271514, -2.8403), (-1.1143771, -1.3277829), (-1.0943525, -1.0944065), (-2.6957617, -2.5641885), (-2.7698982, -2.821507), (-1.1933597, -1.3937144), (-1.0807812, -1.1402066), (-2.6259565, -2.53216), (-2.7656846, -2.7992945), (-1.2532849, -1.4469216), (-1.0965636, -1.1872914), (-2.5787508, -2.5127554), (-2.7465649, -2.7792683), (-1.2920494, -1.4862019), (-1.1191212, -1.2268475), (-2.5510142, -2.5001433), (-2.7268062, -2.7632375), (-1.314891, -1.5154848), (-1.139773, -1.2466376), (-2.537306, -2.4787204), (-2.7125497, -2.756396), (-1.3278867, -1.546833), (-1.1542785, -1.26576), (-2.5300965, -2.4606829), (-2.7038057, -2.7532625), (-1.3365731, -1.5791827), (-1.1637722, -1.2783298), (-2.5255337, -2.4372303), (-2.6999838, -2.7539175), (-1.3444393, -1.6158482), (-1.1689057, -1.2851411), (-2.5205638, -2.4061263), (-2.7002394, -2.7586925), (-1.3539495, -1.6600596), (-1.1703655, -1.2862829), (-2.5129719, -2.3645985), (-2.7038994, -2.7685442), (-1.3669635, -1.7156876), (-1.1685233, -1.2807755), (-2.5009274, -2.3084815), (-2.710764, -2.785501), (-1.3852301, -1.7883898), (-1.1631413, -1.2660981), (-2.4824054, -2.2307937), (-2.7142894, -2.8132458), (-1.4023347, -1.8883542), (-1.161473, -1.2463521), (-2.4679115, -2.1295173), (-2.7247572, -2.8476477), (-1.426032, -2.0137467), (-1.1600139, -1.237599), (-2.4518726, -2.020385), (-2.734131, -2.8695006), (-1.4513526, -2.142075), (-1.159135, -1.2370278), (-2.4328928, -1.907343), (-2.741781, -2.8797882), (-1.4789697, -2.2673442), (-1.1587441, -1.243879), (-2.4098418, -1.796739), (-2.7472804, -2.877419), (-1.5094199, -2.3810387), (-1.1491895, -1.2587593), (-2.369707, -1.694242), (-2.7572927, -2.8601317), (-1.557302, -2.4772859), (-1.1447586, -1.2830203), (-2.3204098, -1.6100428), (-2.7609556, -2.830502), (-1.6113938, -2.5481958), (-1.1423725, -1.3196236), (-2.2614207, -1.5539998), (-2.7600567, -2.7868876), (-1.6724081, -2.5863063), (-1.1401125, -1.3566325), (-2.191557, -1.5188788), (-2.7576118, -2.7427473), (-1.7426261, -2.6046488), (-1.1353549, -1.3905598), (-2.1082826, -1.4979795), (-2.756595, -2.7154686), (-1.825316, -2.6262488), (-1.1256495, -1.410206), (-2.008862, -1.4723145), (-2.7607763, -2.687738), (-1.9265157, -2.6390352), (-1.1070513, -1.4033488), (-1.8847779, -1.4281106), (-2.7745018, -2.6866374), (-2.0555725, -2.675313), (-1.076623, -1.3515993), (-1.7279729, -1.3374285), (-2.8002832, -2.7252529), (-2.2207704, -2.7614648), (-1.0375072, -1.2754276), (-1.5344793, -1.2018067), (-2.8240693, -2.7151084), (-2.417009, -2.8200715), (-1.011786, -1.2434688), (-1.3199831, -1.097625), (-2.8290355, -2.6638174), (-2.6247563, -2.8462763), (-1.0484139, -1.2759614), (-1.1565354, -1.0703875), (-2.7648945, -2.6008942), (-2.7396696, -2.839058), (-1.1271211, -1.3416117), (-1.0871253, -1.0988315), (-2.6824663, -2.5534086), (-2.7719383, -2.818252), (-1.2048389, -1.4066406), (-1.0809115, -1.1469514), (-2.6154256, -2.5238402), (-2.7633867, -2.7957654), (-1.2617475, -1.4578089), (-1.0991902, -1.1937196), (-2.5713327, -2.505895), (-2.7432888, -2.7761638), (-1.2978891, -1.4954472), (-1.1220663, -1.2321402), (-2.545952, -2.4939003), (-2.723852, -2.7544546), (-1.3192946, -1.5166428), (-1.1431392, -1.2570498), (-2.5338347, -2.4814184), (-2.7088158, -2.7492445), (-1.3302597, -1.5454992), (-1.1580952, -1.274347), (-2.528785, -2.4652927), (-2.700099, -2.7474031), (-1.3369967, -1.5755953), (-1.1674286, -1.285301), (-2.526142, -2.4434612), (-2.696716, -2.7490306), (-1.3432124, -1.610195), (-1.1721213, -1.290856), (-2.5227418, -2.4139109), (-2.6975436, -2.7544062), (-1.3514014, -1.6523558), (-1.1731008, -1.2912382), (-2.5164096, -2.3741546), (-2.7017043, -2.7643838), (-1.3633314, -1.7056397), (-1.1709241, -1.2856152), (-2.5054593, -2.3204331), (-2.7088342, -2.7808259), (-1.3805658, -1.7751925), (-1.1655129, -1.2716666), (-2.488108, -2.2464147), (-2.7121546, -2.807168), (-1.3963013, -1.870366), (-1.1629726, -1.2497083), (-2.473831, -2.1463494), (-2.7234018, -2.8434777), (-1.419859, -1.9947973), (-1.1609213, -1.2394563), (-2.4579835, -2.0381238), (-2.7333307, -2.8669004), (-1.4450692, -2.1228244), (-1.1597066, -1.2377329), (-2.4392695, -1.9253554), (-2.7413797, -2.878828), (-1.4725178, -2.248842), (-1.1591718, -1.2433989), (-2.4166625, -1.8139709), (-2.7472117, -2.87841), (-1.5026643, -2.3646646), (-1.1494495, -1.256933), (-2.3770442, -1.7093637), (-2.7576296, -2.863243), (-1.5502307, -2.4643834), (-1.1450424, -1.2798893), (-2.3285694, -1.6220121), (-2.7615976, -2.835431), (-1.6037295, -2.5393314), (-1.142818, -1.3154683), (-2.2707508, -1.5624743), (-2.7608683, -2.7929847), (-1.6637741, -2.5811186), (-1.1408664, -1.3520932), (-2.2024534, -1.524508), (-2.7583463, -2.749147), (-1.7324965, -2.6019914), (-1.1366377, -1.3861194), (-2.1213036, -1.5017718), (-2.756938, -2.7081428), (-1.8136743, -2.610043), (-1.1277156, -1.4154321), (-2.0229104, -1.4873487), (-2.759278, -2.685949), (-1.9123187, -2.6276906), (-1.1112537, -1.4119517), (-1.9028469, -1.4464468), (-2.7711012, -2.6819181), (-2.0367343, -2.6602004), (-1.0826875, -1.3623264), (-1.7515544, -1.3597955), (-2.795815, -2.720042), (-2.1962893, -2.7428179), (-1.043854, -1.2848386), (-1.5636176, -1.2267164), (-2.8217545, -2.7212045), (-2.3889952, -2.8095047), (-1.0140771, -1.2434593), (-1.3500473, -1.1136222), (-2.832896, -2.676003), (-2.6001272, -2.8422253), (-1.0410165, -1.2662483), (-1.1757282, -1.0736066), (-2.7763226, -2.6136947), (-2.7281675, -2.8402371), (-1.1153296, -1.3286287), (-1.0938064, -1.0947583), (-2.6948233, -2.5636203), (-2.770095, -2.821301), (-1.1942121, -1.394505), (-1.0808107, -1.1407278), (-2.625234, -2.5317688), (-2.765537, -2.7990665), (-1.2538958, -1.4475554), (-1.09679, -1.1877847), (-2.5782762, -2.5124748), (-2.7463377, -2.7790687), (-1.292447, -1.4866998), (-1.1193717, -1.2272522), (-2.550729, -2.4999154), (-2.7266011, -2.7566395), (-1.3154896, -1.5085918), (-1.1408772, -1.2532147), (-2.5372472, -2.4874065), (-2.710901, -2.7508435), (-1.3273706, -1.5377204), (-1.1564299, -1.2715263), (-2.531471, -2.4716763), (-2.701487, -2.7483437), (-1.3344928, -1.5675354), (-1.1663585, -1.2834353), (-2.5285802, -2.4506466), (-2.6974592, -2.7492356), (-1.3406966, -1.6013113), (-1.1716027, -1.2899511), (-2.5252988, -2.422371), (-2.6977005, -2.7537305), (-1.3485587, -1.6420219), (-1.1731081, -1.2914257), (-2.5193987, -2.3845367), (-2.7012894, -2.7625456), (-1.3598659, -1.6930026), (-1.1714958, -1.287229), (-2.5092096, -2.3337429), (-2.7077746, -2.7773147), (-1.3761212, -1.7589269), (-1.1668025, -1.275349), (-2.4930663, -2.264332), (-2.7101517, -2.8011167), (-1.3902439, -1.8482611), (-1.1635906, -1.2512337), (-2.4788795, -2.1655376), (-2.7219954, -2.839449), (-1.4137189, -1.9719083), (-1.1610786, -1.2396479), (-2.4631908, -2.0582533), (-2.7323742, -2.864463), (-1.438832, -2.0998263), (-1.15962, -1.2369823), (-2.4447541, -1.9457598), (-2.7407632, -2.8780448), (-1.4661136, -2.2268414), (-1.1590132, -1.2416443), (-2.4226263, -1.83351), (-2.7469082, -2.8796656), (-1.4959592, -2.3451805), (-1.1588943, -1.2539297), (-2.395771, -1.7293019), (-2.7505128, -2.8678653), (-1.5288527, -2.4456844), (-1.1498988, -1.274296), (-2.3516877, -1.6388059), (-2.7582176, -2.84167), (-1.5789778, -2.5241551), (-1.1456122, -1.3083371), (-2.2976267, -1.5753908), (-2.759172, -2.8003948), (-1.6356848, -2.57), (-1.143363, -1.3444817), (-2.233654, -1.5341069), (-2.7568254, -2.7568724), (-1.6999959, -2.593975), (-1.1401633, -1.378741), (-2.1583884, -1.5092747), (-2.754399, -2.7155669), (-1.77473, -2.6036541), (-1.133399, -1.4085866), (-2.0684304, -1.4941866), (-2.7547543, -2.692619), (-1.8639904, -2.6211455), (-1.1206942, -1.4235572), (-1.9601035, -1.4708948), (-2.761304, -2.669869), (-1.9744802, -2.633223), (-1.0977105, -1.3913251), (-1.8244524, -1.406909), (-2.7806544, -2.6943052), (-2.1166458, -2.6929898), (-1.062166, -1.3205509), (-1.6540018, -1.2942863), (-2.809014, -2.7257602), (-2.2951345, -2.7767856), (-1.0241418, -1.2528208), (-1.4491153, -1.162123), (-2.8359087, -2.7024274), (-2.509257, -2.8264892), (-1.0206273, -1.2422252), (-1.245864, -1.0834645), (-2.8066554, -2.6465564), (-2.678192, -2.8419433), (-1.0763197, -1.2906008), (-1.1207246, -1.0789899), (-2.73223, -2.5887833), (-2.7566266, -2.8296278), (-1.1567675, -1.3579931), (-1.0807432, -1.1165668), (-2.6551325, -2.548772), (-2.7690747, -2.8084412), (-1.2260157, -1.4179602), (-1.0867747, -1.1645551), (-2.598011, -2.5241992), (-2.7544737, -2.7873247), (-1.2737246, -1.4636277), (-1.1077055, -1.2079561), (-2.5623698, -2.5088134), (-2.734329, -2.7695575), (-1.303029, -1.4974562), (-1.129659, -1.2426423), (-2.542793, -2.4976692), (-2.7167587, -2.74964), (-1.3201047, -1.5166888), (-1.1488293, -1.264373), (-2.5341625, -2.4848726), (-2.703926, -2.7460325), (-1.3287259, -1.5450424), (-1.1618403, -1.2791594), (-2.5308619, -2.4676833), (-2.6972182, -2.7455742), (-1.3345566, -1.5757388), (-1.1695024, -1.2882328), (-2.5287328, -2.4442384), (-2.6954718, -2.7483687), (-1.3408601, -1.6117785), (-1.1728954, -1.2923867), (-2.525011, -2.4126186), (-2.6975374, -2.7547834), (-1.3497874, -1.6560774), (-1.172926, -1.2916181), (-2.5178149, -2.370293), (-2.7026043, -2.765844), (-1.3628745, -1.7122333), (-1.1700519, -1.2847967), (-2.5056317, -2.3132682), (-2.7104456, -2.7836764), (-1.3815802, -1.7856793), (-1.1640215, -1.2692171), (-2.4867032, -2.2346704), (-2.714523, -2.812082), (-1.3992012, -1.8864604), (-1.162118, -1.2490579), (-2.4718683, -2.1328201), (-2.7252314, -2.84675), (-1.4233221, -2.0122368), (-1.1605743, -1.23995), (-2.455509, -2.0232797), (-2.734704, -2.8688264), (-1.4489899, -2.1408036), (-1.1596931, -1.2390397), (-2.436278, -1.909929), (-2.742376, -2.879321), (-1.476852, -2.266222), (-1.159336, -1.2455449), (-2.4130666, -1.7990654), (-2.7478588, -2.877153), (-1.507433, -2.3800156), (-1.1497719, -1.2600716), (-2.3728046, -1.696315), (-2.7578666, -2.8600519), (-1.5553974, -2.4763522), (-1.1453834, -1.2839891), (-2.3235016, -1.6118864), (-2.7614758, -2.8305943), (-1.6094251, -2.5473077), (-1.1430668, -1.3202448), (-2.2646542, -1.5556564), (-2.7604911, -2.7871423), (-1.6702067, -2.5853868), (-1.1409014, -1.3569192), (-2.195107, -1.5204555), (-2.7579203, -2.7431574), (-1.7399846, -2.6035688), (-1.1362747, -1.3904871), (-2.1123786, -1.4996066), (-2.7567308, -2.7160425), (-1.8219689, -2.6248372), (-1.1267508, -1.4097257), (-2.0138059, -1.4741493), (-2.7606957, -2.6885986), (-1.922128, -2.6372044), (-1.1084383, -1.4039562), (-1.8909383, -1.4317656), (-2.7739701, -2.6863608), (-2.0496886, -2.6713595), (-1.0783663, -1.3527777), (-1.7357111, -1.3429589), (-2.7994792, -2.7247975), (-2.2131932, -2.7558036), (-1.0391738, -1.2760674), (-1.5437047, -1.2084421), (-2.8240004, -2.7186599), (-2.4085338, -2.8171537), (-1.0120485, -1.2407889), (-1.3291783, -1.1012086), (-2.8309119, -2.669346), (-2.6175787, -2.8453777), (-1.0456525, -1.2700726), (-1.1621385, -1.0700092), (-2.769001, -2.6067855), (-2.7365565, -2.8396475), (-1.1230636, -1.334428), (-1.0889468, -1.096398), (-2.6868112, -2.5589738), (-2.771601, -2.8192644), (-1.2010534, -1.3991718), (-1.0808218, -1.1440572), (-2.6191251, -2.5294316), (-2.764074, -2.7967181), (-1.2585576, -1.4501323), (-1.0985408, -1.1910794), (-2.5744448, -2.5119882), (-2.7440925, -2.7768674), (-1.2950584, -1.4872677), (-1.1214423, -1.2299712), (-2.5488036, -2.500881), (-2.724437, -2.7611837), (-1.316151, -1.5147966), (-1.1418636, -1.2490036), (-2.5366616, -2.4806244), (-2.7105937, -2.7547176), (-1.3278824, -1.5447438), (-1.1559778, -1.2674874), (-2.5305781, -2.4635537), (-2.7023144, -2.7518985), (-1.3356811, -1.5759699), (-1.1650819, -1.2796208), (-2.5268092, -2.440964), (-2.698906, -2.7527452), (-1.3429343, -1.6116644), (-1.1699038, -1.2862368), (-2.522405, -2.4107363), (-2.699467, -2.757532), (-1.3520042, -1.6549), (-1.1711744, -1.2874608), (-2.515255, -2.370252), (-2.7032897, -2.7671516), (-1.3646442, -1.7093402), (-1.1693, -1.2823683), (-2.5036433, -2.3155732), (-2.7101426, -2.7835352), (-1.3824809, -1.7803338), (-1.1640818, -1.2685393), (-2.485684, -2.240086), (-2.7133832, -2.8102229), (-1.3988817, -1.8775815), (-1.1620327, -1.2477391), (-2.471226, -2.139452), (-2.7242115, -2.8456721), (-1.4225881, -2.0026488), (-1.1603351, -1.2383106), (-2.4552512, -2.030741), (-2.7338395, -2.8683946), (-1.4479021, -2.1310518), (-1.1593441, -1.237223), (-2.4364061, -1.9177166), (-2.7416766, -2.879606), (-1.4754592, -2.2570102), (-1.1589297, -1.2435077), (-2.413606, -1.8064966), (-2.747343, -2.878331), (-1.5057611, -2.372172), (-1.1493235, -1.2577251), (-2.3737705, -1.7026283), (-2.7575738, -2.8621974), (-1.5534844, -2.4705992), (-1.1449353, -1.2813646), (-2.3249366, -1.6165062), (-2.7614012, -2.8334966), (-1.6072655, -2.543927), (-1.1426615, -1.3175375), (-2.2665899, -1.5584167), (-2.7605858, -2.790395), (-1.6677665, -2.5841625), (-1.1405935, -1.3544894), (-2.1975722, -1.5216662), (-2.7580814, -2.7462611), (-1.7371888, -2.6039393), (-1.136149, -1.3886356), (-2.1154366, -1.4997251), (-2.756826, -2.7186325), (-1.8186848, -2.6262586), (-1.1269056, -1.4086682), (-2.0175867, -1.4735739), (-2.760603, -2.6904025), (-1.9181687, -2.639217), (-1.1090329, -1.4058619), (-1.8956121, -1.4327599), (-2.7733054, -2.6851542), (-2.0447304, -2.6714013), (-1.0795586, -1.3571912), (-1.7414703, -1.3459312), (-2.7982812, -2.7214525), (-2.2070885, -2.7536771), (-1.0406594, -1.2817123), (-1.550501, -1.213338), (-2.8229868, -2.716624), (-2.4017541, -2.81544), (-1.0128037, -1.2449877), (-1.3360054, -1.1052576), (-2.8310807, -2.668346), (-2.6116369, -2.844257), (-1.0443306, -1.2721741), (-1.1663972, -1.0717026), (-2.7707615, -2.6059084), (-2.7336702, -2.83947), (-1.1209103, -1.335958), (-1.0903308, -1.0965668), (-2.6886156, -2.557265), (-2.7709632, -2.8195326), (-1.1993817, -1.401321), (-1.0806186, -1.1436721), (-2.620115, -2.5266545), (-2.764424, -2.7971597), (-1.2578166, -1.4533998), (-1.0977428, -1.1906263), (-2.5744543, -2.508094), (-2.7447453, -2.7773657), (-1.2952584, -1.4917916), (-1.1204911, -1.2295958), (-2.5478985, -2.4958405), (-2.7251248, -2.7553241), (-1.3175468, -1.5133767), (-1.1418008, -1.2550547), (-2.5350354, -2.4833431), (-2.709729, -2.7498314), (-1.3290168, -1.54241), (-1.1570753, -1.2728639), (-2.5295808, -2.4673839), (-2.700645, -2.7476864), (-1.3359793, -1.5724343), (-1.1667181, -1.2842788), (-2.5267832, -2.4458835), (-2.6969337, -2.7489939), (-1.3422207, -1.6067345), (-1.1716875, -1.2902694), (-2.5234127, -2.4168456), (-2.69748, -2.754009), (-1.3502856, -1.6483496), (-1.1729143, -1.291114), (-2.5172546, -2.3778408), (-2.7013874, -2.7635307), (-1.3619659, -1.7007651), (-1.1709837, -1.2860644), (-2.506616, -2.3252442), (-2.708256, -2.7793357), (-1.3788108, -1.7689542), (-1.1658617, -1.2729182), (-2.4897501, -2.2529743), (-2.7112193, -2.804724), (-1.3939213, -1.8619398), (-1.1630789, -1.2501621), (-2.4755366, -2.1533918), (-2.7226968, -2.8418114), (-1.417434, -1.9861088), (-1.1608618, -1.2394166), (-2.4597776, -2.0455055), (-2.732807, -2.865858), (-1.4425988, -2.11415), (-1.1595614, -1.2373426), (-2.4411945, -1.9328064), (-2.7409983, -2.8784359), (-1.46998, -2.2406185), (-1.159007, -1.2426366), (-2.4187949, -1.8210459), (-2.7469654, -2.8788042), (-1.5000122, -2.3574677), (-1.1492515, -1.2557175), (-2.379414, -1.7155036), (-2.7575562, -2.8645062), (-1.5474657, -2.4587793), (-1.1448762, -1.278224), (-2.3312893, -1.626767), (-2.761672, -2.8374405), (-1.6007589, -2.5355659), (-1.1427294, -1.3134696), (-2.2739375, -1.5656903), (-2.7610443, -2.7954557), (-1.6604754, -2.5790384), (-1.1409081, -1.3500394), (-2.2062345, -1.5264559), (-2.7585304, -2.7516937), (-1.7286946, -2.6011045), (-1.1368916, -1.3842317), (-2.1258588, -1.5028801), (-2.7570062, -2.7105029), (-1.809117, -2.6098309), (-1.1282845, -1.4138265), (-2.0285134, -1.488035), (-2.75911, -2.68798), (-1.9066665, -2.6277082), (-1.112323, -1.4131192), (-1.9098456, -1.4493412), (-2.7702591, -2.6811109), (-2.0294664, -2.657553), (-1.0844471, -1.3657101), (-1.7603351, -1.3652705), (-2.7943795, -2.7174978), (-2.1871157, -2.737552), (-1.0459017, -1.288975), (-1.574209, -1.2343819), (-2.820775, -2.7220411), (-2.3785644, -2.806245), (-1.014925, -1.2445859), (-1.3610245, -1.1188908), (-2.8338952, -2.678968), (-2.590742, -2.840757), (-1.0384295, -1.2637736), (-1.1828537, -1.0745924), (-2.7801027, -2.6170948), (-2.7235756, -2.8405168), (-1.1110845, -1.3248117), (-1.096275, -1.0931703), (-2.6990001, -2.5661829), (-2.7691798, -2.8222213), (-1.1903942, -1.3909292), (-1.0806856, -1.1383786), (-2.6284585, -2.5335386), (-2.7661798, -2.800081), (-1.2511547, -1.4446781), (-1.0957804, -1.1855638), (-2.580402, -2.513758), (-2.7473469, -2.7799585), (-1.2906593, -1.4844276), (-1.1182537, -1.2254331), (-2.5520144, -2.500969), (-2.7275152, -2.7637756), (-1.3140155, -1.5140076), (-1.1390328, -1.2455305), (-2.5378864, -2.4794822), (-2.7130828, -2.756772), (-1.3273237, -1.5455065), (-1.1537169, -1.2649577), (-2.5304363, -2.4615064), (-2.7041473, -2.7534742), (-1.3361695, -1.5778666), (-1.1633905, -1.2777945), (-2.525773, -2.4382064), (-2.700152, -2.7539632), (-1.3440796, -1.6144158), (-1.1686848, -1.2848464), (-2.520806, -2.4073498), (-2.7002597, -2.7585597), (-1.3535483, -1.6583794), (-1.1702847, -1.2862289), (-2.5132952, -2.3661876), (-2.7037892, -2.7681935), (-1.3664504, -1.7135981), (-1.1685756, -1.2810049), (-2.5014045, -2.3106148), (-2.710526, -2.784849), (-1.384534, -1.7856494), (-1.1633431, -1.2667142), (-2.4831223, -2.2337728), (-2.7138796, -2.8121457), (-1.4013375, -1.884563), (-1.1615725, -1.2466046), (-2.4686778, -2.1327617), (-2.724452, -2.8469093), (-1.4249977, -2.0097995), (-1.1600401, -1.2376173), (-2.452698, -2.023815), (-2.73391, -2.8690634), (-1.4502816, -2.1381168), (-1.1591227, -1.2368693), (-2.4337974, -1.9108149), (-2.7416303, -2.8796751), (-1.4778527, -2.263599), (-1.1587207, -1.2435263), (-2.4108608, -1.8000267), (-2.7471986, -2.877694), (-1.5082349, -2.3777883), (-1.1491466, -1.2581793), (-2.3708532, -1.6970766), (-2.7572966, -2.8608215), (-1.5560505, -2.4747899), (-1.1447275, -1.282225), (-2.3217323, -1.6122246), (-2.7610312, -2.8315346), (-1.6100339, -2.5465498), (-1.1423751, -1.3186792), (-2.2629738, -1.5554781), (-2.7601793, -2.7881212), (-1.6708794, -2.585416), (-1.140175, -1.3556683), (-2.1934001, -1.5197946), (-2.7577333, -2.7440083), (-1.7408438, -2.6042807), (-1.135518, -1.3896712), (-2.1105032, -1.4985309), (-2.7566547, -2.7166328), (-1.823158, -2.626162), (-1.1259644, -1.4094563), (-2.011595, -1.47269), (-2.7607188, -2.6887496), (-1.9238172, -2.6390438), (-1.1076028, -1.403903), (-1.888191, -1.4295435), (-2.7741296, -2.6863327), (-2.0520911, -2.674063), (-1.0774815, -1.3531576), (-1.7322323, -1.3400635), (-2.7996686, -2.724199), (-2.2164087, -2.7590272), (-1.0384327, -1.2772459), (-1.5395212, -1.205373), (-2.8237622, -2.7156906), (-2.4121935, -2.8186574), (-1.0120324, -1.2438413), (-1.3249797, -1.0999492), (-2.8297203, -2.6652806), (-2.620672, -2.8456802), (-1.0470748, -1.2747271), (-1.1595601, -1.0707432), (-2.7667546, -2.6024628), (-2.7378383, -2.8392153), (-1.1251563, -1.3398505), (-1.0880787, -1.0980986), (-2.6843894, -2.554591), (-2.7716656, -2.818676), (-1.2031382, -1.4050276), (-1.0807956, -1.1459255), (-2.6168807, -2.5246792), (-2.7637317, -2.7962255), (-1.2605402, -1.4565158), (-1.0987297, -1.1927649), (-2.5722935, -2.5065217), (-2.7437623, -2.7765667), (-1.2971004, -1.4944229), (-1.1215731, -1.2313645), (-2.5465407, -2.4944217), (-2.724271, -2.75476), (-1.3187957, -1.515778), (-1.1427243, -1.2564505), (-2.5341837, -2.4819074), (-2.7091265, -2.7494657), (-1.3299332, -1.5447257), (-1.1577817, -1.2739099), (-2.5289977, -2.4658024), (-2.7002985, -2.7475348), (-1.3367567, -1.5748389), (-1.1672128, -1.2850053), (-2.5262945, -2.4440367), (-2.696817, -2.7490723), (-1.3429973, -1.6093909), (-1.1719913, -1.2906871), (-2.522889, -2.4145992), (-2.6975625, -2.7543526), (-1.3511674, -1.6514387), (-1.1730443, -1.2911947), (-2.5165927, -2.3750136), (-2.7016525, -2.764215), (-1.3630426, -1.7045285), (-1.1709353, -1.285716), (-2.505715, -2.3215504), (-2.7087147, -2.7805035), (-1.3801861, -1.7737701), (-1.1655974, -1.27196), (-2.488478, -2.247935), (-2.7119484, -2.806621), (-1.3957757, -1.8684392), (-1.1630045, -1.2498201), (-2.474218, -2.1479795), (-2.7232473, -2.8431106), (-1.4193214, -1.9928137), (-1.1609163, -1.2394539), (-2.458393, -2.0398278), (-2.7332175, -2.866678), (-1.444519, -2.1208498), (-1.1596829, -1.23765), (-2.4397125, -1.9270709), (-2.7412994, -2.8787556), (-1.4719498, -2.246976), (-1.1591432, -1.2432299), (-2.4171548, -1.8155941), (-2.7471626, -2.8785183), (-1.5020685, -2.36304), (-1.1494128, -1.2566612), (-2.377592, -1.7107675), (-2.7576213, -2.863548), (-1.5496087, -2.463127), (-1.1450129, -1.2795162), (-2.3291984, -1.6230955), (-2.7616224, -2.8359034), (-1.6030588, -2.5384967), (-1.1428058, -1.3150223), (-2.2714868, -1.5632054), (-2.7609153, -2.7935567), (-1.663027, -2.5806653), (-1.1408842, -1.3516381), (-2.2033257, -1.5249515), (-2.7583945, -2.7497318), (-1.7316334, -2.6018064), (-1.1367036, -1.385703), (-2.1223533, -1.5020261), (-2.7569592, -2.708683), (-1.8126366, -2.610009), (-1.1278532, -1.4150809), (-2.0241997, -1.4875067), (-2.7592442, -2.686411), (-1.9110295, -2.6277068), (-1.1115053, -1.412237), (-1.9044552, -1.447112), (-2.7709126, -2.6817276), (-2.0350754, -2.659603), (-1.0830965, -1.3631184), (-1.7535689, -1.3610522), (-2.7954903, -2.7194555), (-2.1941953, -2.741621), (-1.0443257, -1.2857976), (-1.5660422, -1.2284714), (-2.8215384, -2.7213933), (-2.38662, -2.808766), (-1.014267, -1.2437259), (-1.3525463, -1.1148249), (-2.8331351, -2.676673), (-2.598004, -2.841893), (-1.0404228, -1.265695), (-1.1773381, -1.0738332), (-2.777189, -2.6144598), (-2.727139, -2.8403), (-1.1143658, -1.3277725), (-1.0943589, -1.0944024), (-2.6957726, -2.564195), (-2.7698956, -2.821509), (-1.1933491, -1.3937049), (-1.0807806, -1.1402005), (-2.6259646, -2.5321636), (-2.7656858, -2.799296), (-1.2532774, -1.4469147), (-1.0965606, -1.1872863), (-2.5787559, -2.5127578), (-2.7465677, -2.77927), (-1.2920451, -1.4861975), (-1.1191181, -1.2268442), (-2.5510173, -2.500144), (-2.7268088, -2.763238), (-1.3148884, -1.5154816), (-1.1397702, -1.2466351), (-2.5373082, -2.4787216), (-2.7125528, -2.756396), (-1.3278854, -1.54683), (-1.1542758, -1.2657584), (-2.5300965, -2.4606843), (-2.703807, -2.7532613), (-1.3365725, -1.5791796), (-1.1637703, -1.2783301), (-2.5255334, -2.4372327), (-2.699985, -2.753916), (-1.3444391, -1.6158441), (-1.1689045, -1.285142), (-2.5205626, -2.4061303), (-2.7002392, -2.7586913), (-1.35395, -1.6600547), (-1.1703651, -1.2862841), (-2.5129707, -2.3646035), (-2.7038987, -2.7685425), (-1.3669642, -1.7156818), (-1.1685237, -1.2807777), (-2.5009263, -2.3084886), (-2.7107635, -2.7854989), (-1.3852308, -1.7883816), (-1.1631427, -1.2661015), (-2.4824052, -2.2308037), (-2.714287, -2.813241), (-1.4023339, -1.8883415), (-1.1614747, -1.2463537), (-2.4679127, -2.1295283), (-2.724756, -2.8476453), (-1.4260303, -2.0137336), (-1.1600139, -1.2375991), (-2.4518733, -2.0203967), (-2.7341301, -2.8694987), (-1.4513509, -2.1420608), (-1.1591345, -1.2370274), (-2.432893, -1.9073557), (-2.7417808, -2.8797884), (-1.4789684, -2.2673314), (-1.1587439, -1.2438768), (-2.4098432, -1.7967501), (-2.7472804, -2.8774202), (-1.5094174, -2.381027), (-1.1491882, -1.2587563), (-2.3697083, -1.6942515), (-2.7572932, -2.8601346), (-1.5573007, -2.477277), (-1.1447581, -1.2830163), (-2.3204112, -1.6100504), (-2.760956, -2.8305073), (-1.6113919, -2.5481908), (-1.1423719, -1.3196187), (-2.261423, -1.5540042), (-2.7600582, -2.7868934), (-1.6724061, -2.5863035), (-1.1401112, -1.356628), (-2.191559, -1.5188823), (-2.757614, -2.742753), (-1.7426246, -2.6046484), (-1.1353534, -1.3905557), (-2.1082838, -1.4979804), (-2.7565966, -2.7154734), (-1.8253158, -2.6262505), (-1.1256493, -1.4102031), (-2.0088637, -1.472314), (-2.760777, -2.6877413), (-1.9265146, -2.6390364), (-1.1070527, -1.403347), (-1.8847809, -1.4281116), (-2.7745013, -2.6866407), (-2.05557, -2.6753137), (-1.0766244, -1.3515983), (-1.7279774, -1.3374298), (-2.800283, -2.7252548), (-2.2207665, -2.7614646), (-1.0375085, -1.2754279), (-1.5344847, -1.2018093), (-2.8240695, -2.7151105), (-2.4170046, -2.8200717), (-1.0117857, -1.2434692), (-1.3199883, -1.0976268), (-2.8290374, -2.6638188), (-2.624753, -2.8462765), (-1.048412, -1.2759607), (-1.1565377, -1.0703872), (-2.7648962, -2.600895), (-2.7396681, -2.8390589), (-1.1271201, -1.341612), (-1.0871271, -1.0988309), (-2.6824687, -2.5534081), (-2.7719376, -2.8182523), (-1.2048366, -1.4066408), (-1.0809121, -1.1469506), (-2.6154277, -2.5238392), (-2.7633862, -2.795766), (-1.2617456, -1.4578096), (-1.0991905, -1.1937183), (-2.5713344, -2.5058935), (-2.7432885, -2.7761648), (-1.2978876, -1.4954487), (-1.1220651, -1.2321393), (-2.5459523, -2.4938972), (-2.7238538, -2.7544549), (-1.3192958, -1.5166463), (-1.1431382, -1.2570499), (-2.533833, -2.4814148), (-2.7088165, -2.7492447), (-1.3302617, -1.5455034), (-1.1580944, -1.2743477), (-2.5287828, -2.4652894), (-2.7000995, -2.7474022), (-1.3369985, -1.5755986), (-1.1674284, -1.285302), (-2.52614, -2.4434583), (-2.6967149, -2.7490296), (-1.3432134, -1.610197), (-1.1721222, -1.2908566), (-2.5227408, -2.4139094), (-2.6975427, -2.754406), (-1.3514023, -1.6523566), (-1.1731012, -1.2912378), (-2.5164087, -2.3741536), (-2.7017043, -2.7643838), (-1.3633324, -1.7056396), (-1.1709241, -1.2856147), (-2.505458, -2.3204334), (-2.708834, -2.7808263), (-1.3805664, -1.7751929), (-1.1655132, -1.271666), (-2.4881067, -2.2464135), (-2.7121544, -2.8071678), (-1.3963023, -1.8703661), (-1.1629727, -1.249708), (-2.4738297, -2.1463492), (-2.723402, -2.8434772), (-1.4198601, -1.9947971), (-1.1609206, -1.2394562), (-2.4579823, -2.0381231), (-2.733331, -2.8669), (-1.4450698, -2.1228244), (-1.1597061, -1.237733), (-2.4392684, -1.925355), (-2.7413793, -2.8788276), (-1.4725183, -2.2488425), (-1.1591719, -1.2433989), (-2.416662, -1.81397), (-2.7472117, -2.8784099), (-1.5026643, -2.364665), (-1.1494492, -1.2569332), (-2.3770444, -1.709363), (-2.7576299, -2.863242), (-1.5502303, -2.4643836), (-1.1450418, -1.2798905), (-2.32857, -1.6220113), (-2.7615988, -2.8354292), (-1.6037297, -2.539332), (-1.1428171, -1.3154703), (-2.2707496, -1.5624737), (-2.7608678, -2.7929826), (-1.6637746, -2.581119), (-1.1408669, -1.3520956), (-2.2024531, -1.5245078), (-2.758346, -2.7491443), (-1.7324965, -2.6019912), (-1.136638, -1.3861216), (-2.121304, -1.5017719), (-2.7569387, -2.7081401), (-1.8136742, -2.6100423), (-1.1277152, -1.4154344), (-2.0229106, -1.4873487), (-2.7592785, -2.6859465), (-1.9123186, -2.6276906), (-1.1112534, -1.4119544), (-1.9028468, -1.4464471), (-2.7711012, -2.6819155), (-2.0367343, -2.6601994), (-1.0826879, -1.3623284), (-1.7515544, -1.3597963), (-2.7958148, -2.7200403), (-2.1962893, -2.7428167), (-1.043854, -1.2848397), (-1.563618, -1.2267169), (-2.821755, -2.7212036), (-2.388995, -2.8095043), (-1.0140764, -1.24346), (-1.3500466, -1.1136221), (-2.8328962, -2.676001), (-2.6001282, -2.842225), (-1.0410168, -1.2662499), (-1.1757278, -1.073607), (-2.776322, -2.6136937), (-2.7281678, -2.8402374), (-1.11533, -1.3286303), (-1.0938059, -1.0947584), (-2.6948225, -2.563619), (-2.7700953, -2.8213012), (-1.1942128, -1.3945074), (-1.0808109, -1.140728), (-2.6252334, -2.531767), (-2.7655373, -2.799067), (-1.2538972, -1.447558), (-1.0967904, -1.1877849), (-2.578275, -2.5124722), (-2.746337, -2.7790682), (-1.2924477, -1.4867028), (-1.1193722, -1.227253), (-2.550728, -2.499913), (-2.7266002, -2.7566397), (-1.3154904, -1.5085949), (-1.140878, -1.2532148), (-2.5372472, -2.4874039), (-2.710901, -2.7508435), (-1.3273709, -1.5377231), (-1.1564298, -1.2715261), (-2.5314705, -2.4716735), (-2.7014873, -2.7483437), (-1.3344938, -1.567538), (-1.1663586, -1.2834355), (-2.5285792, -2.4506443), (-2.6974595, -2.7492359), (-1.3406979, -1.6013137), (-1.1716025, -1.289951), (-2.5252972, -2.4223683), (-2.6977003, -2.7537313), (-1.3485603, -1.6420261), (-1.1731077, -1.2914253), (-2.5193965, -2.384532), (-2.7012897, -2.7625465), (-1.3598682, -1.6930084), (-1.1714956, -1.2872275), (-2.5092072, -2.3337362), (-2.7077749, -2.777317), (-1.3761238, -1.7589347), (-1.1668026, -1.275346), (-2.4930646, -2.2643232), (-2.7101526, -2.8011208), (-1.3902459, -1.8482717), (-1.1635908, -1.2512318), (-2.478879, -2.1655283), (-2.7219956, -2.8394518), (-1.4137195, -1.9719193), (-1.1610786, -1.2396467), (-2.4631906, -2.0582433), (-2.7323742, -2.864465), (-1.4388324, -2.0998378), (-1.1596204, -1.2369822), (-2.444754, -1.9457498), (-2.7407622, -2.8780456), (-1.4661131, -2.2268515), (-1.1590137, -1.2416441), (-2.4226272, -1.833501), (-2.7469084, -2.8796656), (-1.4959581, -2.3451893), (-1.1588938, -1.2539303), (-2.395772, -1.7292933), (-2.750514, -2.8678637), (-1.5288525, -2.4456909), (-1.1498979, -1.274297), (-2.3516874, -1.6387991), (-2.7582178, -2.8416672), (-1.5789782, -2.5241594), (-1.1456119, -1.3083391), (-2.2976258, -1.575386), (-2.7591715, -2.800391), (-1.635685, -2.5700026), (-1.1433632, -1.3444848), (-2.2336538, -1.5341045), (-2.7568247, -2.7568684), (-1.699996, -2.5939755), (-1.140164, -1.3787436), (-2.158388, -1.509274), (-2.7543974, -2.7155633), (-1.774729, -2.6036537), (-1.1334002, -1.4085892), (-2.0684311, -1.4941871), (-2.7547522, -2.6926157), (-1.8639882, -2.621144), (-1.1206951, -1.4235597), (-1.9601059, -1.4708952), (-2.7613027, -2.6698654), (-1.9744774, -2.633221), (-1.0977117, -1.3913289), (-1.8244549, -1.4069114), (-2.780652, -2.6943007), (-2.1166413, -2.6929865), (-1.0621678, -1.3205554), (-1.6540062, -1.2942897), (-2.8090115, -2.7257571), (-2.2951288, -2.7767832), (-1.0241437, -1.2528231), (-1.4491211, -1.1621261), (-2.8359067, -2.7024276), (-2.50925, -2.826487), (-1.0206268, -1.2422242), (-1.24587, -1.0834658), (-2.8066566, -2.6465585), (-2.6781871, -2.8419433), (-1.076317, -1.2905983), (-1.1207272, -1.0789886), (-2.7322328, -2.5887845), (-2.756625, -2.8296285), (-1.1567643, -1.3579907), (-1.0807433, -1.1165644), (-2.6551347, -2.548773), (-2.7690744, -2.8084426), (-1.2260134, -1.4179583), (-1.0867741, -1.1645528), (-2.5980124, -2.5241997), (-2.754474, -2.7873254), (-1.2737231, -1.4636253), (-1.1077048, -1.2079537), (-2.562371, -2.5088146), (-2.73433, -2.7695587), (-1.3030276, -1.4974542), (-1.1296586, -1.2426398), (-2.5427942, -2.4976702), (-2.7167587, -2.7496421), (-1.3201032, -1.5166875), (-1.148829, -1.2643702), (-2.5341637, -2.4848726), (-2.7039263, -2.7460344), (-1.3287247, -1.5450418), (-1.1618394, -1.279156), (-2.5308623, -2.4676821), (-2.6972182, -2.7455764), (-1.3345548, -1.5757406), (-1.1695008, -1.2882303), (-2.5287337, -2.444235), (-2.6954727, -2.7483706), (-1.3408581, -1.6117814), (-1.1728929, -1.292383), (-2.5250113, -2.4126136), (-2.6975393, -2.754787), (-1.3497875, -1.6560841), (-1.1729239, -1.2916144), (-2.5178142, -2.370285), (-2.7026062, -2.7658489), (-1.3628758, -1.7122427), (-1.1700494, -1.2847921), (-2.5056295, -2.313257), (-2.7104483, -2.7836819), (-1.3815827, -1.785693), (-1.1640178, -1.2692113), (-2.4866986, -2.2346535), (-2.7145264, -2.8120892), (-1.3992066, -1.8864819), (-1.1621158, -1.2490553), (-2.471863, -2.1328015), (-2.725234, -2.846755), (-1.4233284, -2.012259), (-1.1605736, -1.2399491), (-2.4555032, -2.0232599), (-2.7347057, -2.8688295), (-1.4489973, -2.1408274), (-1.1596925, -1.2390407), (-2.4362707, -1.9099078), (-2.742377, -2.8793216), (-1.4768602, -2.2662444), (-1.1593355, -1.2455469), (-2.4130583, -1.799045), (-2.7478588, -2.8771508), (-1.5074419, -2.3800359), (-1.1497719, -1.2600758), (-2.3727958, -1.6962973), (-2.7578669, -2.8600483), (-1.5554072, -2.4763699), (-1.1453834, -1.2839944), (-2.323491, -1.6118715), (-2.7614753, -2.830588), (-1.6094358, -2.54732), (-1.143067, -1.3202509), (-2.2646425, -1.5556464), (-2.7604902, -2.787135), (-1.6702178, -2.585394), (-1.1409014, -1.3569264), (-2.1950948, -1.5204492), (-2.7579195, -2.7431486), (-1.7399969, -2.6035714), (-1.1362741, -1.3904934), (-2.1123633, -1.4996035), (-2.7567303, -2.7160351), (-1.8219838, -2.6248379), (-1.1267489, -1.4097314), (-2.0137875, -1.4741477), (-2.760696, -2.6885917), (-1.9221462, -2.637204), (-1.1084347, -1.4039533), (-1.8909144, -1.4317567), (-2.7739723, -2.6863632), (-2.0497134, -2.671368), (-1.0783608, -1.3527673), (-1.7356807, -1.3429415), (-2.7994835, -2.7248056), (-2.213225, -2.7558208), (-1.0391679, -1.2760543), (-1.543669, -1.2084177), (-2.8240027, -2.7186568), (-2.408569, -2.8171637), (-1.0120473, -1.2407861), (-1.3291425, -1.1011925), (-2.8309062, -2.6693363), (-2.6176076, -2.8453817), (-1.0456634, -1.2700819), (-1.1621176, -1.0700083), (-2.7689877, -2.6067755), (-2.7365696, -2.839646), (-1.1230781, -1.3344395), (-1.0889404, -1.0964035), (-2.6867979, -2.5589669), (-2.7716036, -2.8192623), (-1.2010654, -1.3991828), (-1.0808218, -1.1440642), (-2.619115, -2.529426), (-2.764073, -2.796715), (-1.258567, -1.4501408), (-1.0985435, -1.1910858), (-2.574437, -2.511984), (-2.7440898, -2.776865), (-1.2950658, -1.4872755), (-1.1214454, -1.229977), (-2.5487974, -2.5008774), (-2.7244337, -2.761181), (-1.3161565, -1.5148021), (-1.1418674, -1.249009), (-2.5366583, -2.4806235), (-2.7105906, -2.7547157), (-1.3278848, -1.5447468), (-1.1559807, -1.2674919), (-2.5305767, -2.463554), (-2.7023125, -2.7518976), (-1.3356832, -1.5759728), (-1.1650841, -1.2796241), (-2.5268078, -2.4409633), (-2.6989045, -2.7527444), (-1.3429366, -1.6116673), (-1.1699065, -1.2862396), (-2.5224044, -2.4107363), (-2.699466, -2.757532), (-1.3520058, -1.6549016), (-1.1711763, -1.2874625), (-2.5152552, -2.3702528), (-2.7032895, -2.7671518), (-1.3646442, -1.7093406), (-1.1693014, -1.282369), (-2.503645, -2.315574), (-2.7101421, -2.7835355), (-1.3824794, -1.7803335), (-1.1640825, -1.2685384), (-2.4856858, -2.2400863), (-2.7133842, -2.8102248), (-1.3988811, -1.8775816), (-1.162033, -1.2477372), (-2.471228, -2.1394517), (-2.724212, -2.845674), (-1.4225874, -2.0026495), (-1.1603353, -1.238309), (-2.455252, -2.03074), (-2.7338398, -2.8683968), (-1.4479022, -2.1310542), (-1.1593448, -1.2372216), (-2.4364076, -1.917715), (-2.7416768, -2.879608), (-1.4754583, -2.2570121), (-1.1589304, -1.2435069), (-2.413608, -1.8064959), (-2.747343, -2.8783324), (-1.5057598, -2.3721733), (-1.1493239, -1.2577251), (-2.3737729, -1.7026283), (-2.7575748, -2.8621976), (-1.553483, -2.4705987), (-1.1449349, -1.2813643), (-2.324938, -1.6165067), (-2.761402, -2.8334966), (-1.6072645, -2.5439262), (-1.1426609, -1.317537), (-2.266591, -1.5584165), (-2.7605867, -2.7903955), (-1.667766, -2.5841632), (-1.1405932, -1.3544888), (-2.1975727, -1.5216656), (-2.7580817, -2.7462614), (-1.7371885, -2.6039393), (-1.1361485, -1.3886347), (-2.115436, -1.4997247), (-2.7568264, -2.7186337), (-1.8186857, -2.62626), (-1.1269048, -1.4086674), (-2.0175853, -1.4735721), (-2.7606044, -2.6904032), (-1.9181708, -2.6392188), (-1.1090318, -1.4058609), (-1.89561, -1.4327569), (-2.7733068, -2.6851547), (-2.0447335, -2.6714046), (-1.0795575, -1.357191), (-1.7414676, -1.3459275), (-2.7982836, -2.7214513), (-2.2070928, -2.7536805), (-1.0406584, -1.2817138), (-1.5504962, -1.2133343), (-2.8229873, -2.71662), (-2.4017582, -2.8154414), (-1.0128033, -1.2449913), (-1.3360009, -1.1052561), (-2.8310797, -2.668341), (-2.611641, -2.8442566), (-1.0443327, -1.2721789), (-1.1663944, -1.071704), (-2.7707593, -2.6059036), (-2.7336724, -2.8394687), (-1.1209128, -1.3359629), (-1.0903296, -1.0965685), (-2.6886137, -2.5572615), (-2.7709641, -2.8195317), (-1.1993837, -1.4013253), (-1.080618, -1.1436739), (-2.6201127, -2.5266511), (-2.7644238, -2.7971585), (-1.2578187, -1.4534042), (-1.097743, -1.1906282), (-2.5744517, -2.50809), (-2.7447438, -2.7773652), (-1.2952609, -1.4917973), (-1.1204928, -1.2295976), (-2.5478961, -2.4958365), (-2.7251232, -2.7553236), (-1.3175489, -1.5133822), (-1.1418015, -1.2550557), (-2.535033, -2.483338), (-2.7097285, -2.7498305), (-1.3290192, -1.542415), (-1.1570756, -1.2728657), (-2.529578, -2.46738), (-2.7006438, -2.7476854), (-1.3359822, -1.5724391), (-1.1667193, -1.28428), (-2.5267808, -2.4458792), (-2.6969326, -2.7489932), (-1.3422226, -1.6067396), (-1.171689, -1.2902702), (-2.5234108, -2.4168403), (-2.6974783, -2.7540085), (-1.3502871, -1.6483551), (-1.1729155, -1.2911143), (-2.5172532, -2.377835), (-2.7013865, -2.7635317), (-1.3619667, -1.7007715), (-1.1709838, -1.2860626), (-2.5066154, -2.3252368), (-2.7082565, -2.7793376), (-1.3788115, -1.7689618), (-1.1658604, -1.2729151), (-2.4897478, -2.2529647), (-2.7112198, -2.8047273), (-1.3939241, -1.8619506), (-1.1630787, -1.2501603), (-2.4755332, -2.1533823), (-2.7226973, -2.8418143), (-1.4174376, -1.9861199), (-1.1608615, -1.2394155), (-2.459774, -2.0454957), (-2.732807, -2.8658595), (-1.4426023, -2.1141608), (-1.1595619, -1.2373424), (-2.441192, -1.9327967), (-2.740999, -2.8784366), (-1.4699832, -2.2406273), (-1.1590061, -1.2426354), (-2.418791, -1.8210367), (-2.7469661, -2.878804), (-1.5000163, -2.3574755), (-1.1492506, -1.2557178), (-2.3794096, -1.715496), (-2.7575564, -2.8645053), (-1.5474696, -2.458786), (-1.1448755, -1.2782248), (-2.3312843, -1.6267601), (-2.761672, -2.8374388), (-1.6007637, -2.5355713), (-1.1427286, -1.3134714), (-2.2739315, -1.5656854), (-2.7610447, -2.7954526), (-1.6604818, -2.5790408), (-1.1409067, -1.3500417), (-2.206226, -1.5264535), (-2.7585309, -2.7516906), (-1.7287033, -2.6011057), (-1.1368899, -1.3842341), (-2.1258478, -1.5028785), (-2.7570066, -2.7104998), (-1.8091282, -2.6098316), (-1.1282831, -1.4138293), (-2.0284998, -1.4880337), (-2.7591102, -2.6879764), (-1.9066799, -2.6277087), (-1.1123203, -1.4131165), (-1.9098285, -1.4493344), (-2.7702608, -2.681113), (-2.029484, -2.6575599), (-1.0844434, -1.3657025), (-1.7603137, -1.3652574), (-2.794382, -2.7175033), (-2.1871383, -2.7375655), (-1.0458975, -1.2889663), (-1.5741827, -1.2343631), (-2.8207765, -2.722038), (-2.3785896, -2.8062534), (-1.0149239, -1.244585), (-1.3609987, -1.1188786), (-2.8338928, -2.6789596), (-2.590764, -2.8407598), (-1.0384351, -1.26378), (-1.1828368, -1.0745893), (-2.7800946, -2.617086), (-2.7235873, -2.8405173), (-1.111094, -1.3248223), (-1.0962679, -1.0931735), (-2.6989896, -2.5661752), (-2.769183, -2.8222198), (-1.1904044, -1.3909391), (-1.0806863, -1.1383839), (-2.6284509, -2.533533), (-2.7661786, -2.800079), (-1.2511612, -1.4446869), (-1.0957823, -1.1855692), (-2.5803964, -2.5137522), (-2.7473445, -2.7799563), (-1.2906641, -1.4844359), (-1.1182568, -1.2254382), (-2.552012, -2.500964), (-2.7275128, -2.763773), (-1.3140178, -1.5140144), (-1.1390352, -1.2455345), (-2.5378847, -2.4794774), (-2.7130811, -2.7567706), (-1.3273253, -1.5455134), (-1.1537188, -1.2649604), (-2.5304358, -2.4615006), (-2.7041466, -2.7534733), (-1.3361704, -1.5778736), (-1.1633908, -1.2777958), (-2.5257723, -2.4382), (-2.7001529, -2.7539637), (-1.3440813, -1.6144239), (-1.1686839, -1.2848467), (-2.5208037, -2.4073408), (-2.7002606, -2.7585604), (-1.3535519, -1.6583909), (-1.170284, -1.2862295), (-2.5132914, -2.3661764), (-2.7037907, -2.768195), (-1.3664552, -1.7136122), (-1.1685749, -1.2810042), (-2.5013995, -2.310601), (-2.7105272, -2.7848525), (-1.3845401, -1.7856672), (-1.1633418, -1.266712), (-2.4831154, -2.233754), (-2.7138817, -2.8121507), (-1.4013458, -1.8845863), (-1.1615725, -1.2466047), (-2.4686708, -2.132742), (-2.7244537, -2.8469129), (-1.4250066, -2.0098238), (-1.1600409, -1.2376179), (-2.452691, -2.0237944), (-2.733911, -2.8690662), (-1.45029, -2.1381407), (-1.1591237, -1.2368695), (-2.4337912, -1.9107941), (-2.7416308, -2.8796766), (-1.4778601, -2.263621), (-1.1587212, -1.2435275), (-2.4108546, -1.8000073), (-2.7471986, -2.877694), (-1.5082417, -2.3778083), (-1.1491476, -1.2581822), (-2.3708475, -1.69706), (-2.7572966, -2.8608181), (-1.5560571, -2.4748044), (-1.1447281, -1.2822297), (-2.3217263, -1.612213), (-2.7610307, -2.8315291), (-1.6100402, -2.546558), (-1.1423752, -1.3186834), (-2.2629657, -1.5554701), (-2.760178, -2.788115), (-1.6708877, -2.5854208), (-1.1401756, -1.3556734), (-2.1933901, -1.51979), (-2.7577317, -2.744001), (-1.7408537, -2.604282), (-1.1355182, -1.3896768), (-2.1104913, -1.4985296), (-2.7566535, -2.7166266), (-1.823169, -2.626161), (-1.1259636, -1.4094597), (-2.0115817, -1.4726892), (-2.7607188, -2.6887445), (-1.9238305, -2.6390421), (-1.1075999, -1.403899), (-1.8881738, -1.4295375), (-2.774132, -2.6863363), (-2.0521083, -2.6740682), (-1.0774765, -1.3531477), (-1.7322111, -1.3400509), (-2.7996724, -2.7242067), (-2.216431, -2.7590392), (-1.0384277, -1.277234), (-1.5394957, -1.2053553), (-2.823764, -2.7156909), (-2.4122179, -2.8186648), (-1.0120311, -1.2438366), (-1.3249553, -1.0999376), (-2.8297184, -2.6652763), (-2.6206925, -2.8456833), (-1.0470805, -1.27473), (-1.1595446, -1.0707412), (-2.766746, -2.602458), (-2.7378476, -2.8392146), (-1.1251657, -1.3398556), (-1.0880743, -1.098102), (-2.6843808, -2.5545888), (-2.771667, -2.818674), (-1.2031457, -1.405032), (-1.080797, -1.1459308), (-2.6168752, -2.524679), (-2.7637298, -2.7962227), (-1.2605444, -1.4565177), (-1.0987324, -1.1927702), (-2.5722904, -2.506523), (-2.7437594, -2.7765646), (-1.2971025, -1.4944239), (-1.121576, -1.2313684), (-2.54654, -2.4944234), (-2.7242692, -2.754758), (-1.3187965, -1.5157771), (-1.1427264, -1.256454), (-2.534184, -2.4819102), (-2.7091248, -2.7494643), (-1.3299323, -1.5447242), (-1.1577826, -1.2739117), (-2.5289986, -2.4658048), (-2.700298, -2.7475345), (-1.3367554, -1.5748376), (-1.1672131, -1.2850059), (-2.5262957, -2.4440382), (-2.6968172, -2.7490726), (-1.3429962, -1.6093904), (-1.1719917, -1.2906876), (-2.522891, -2.4146), (-2.6975627, -2.754353), (-1.3511657, -1.6514391), (-1.1730437, -1.2911947), (-2.5165937, -2.3750126), (-2.7016528, -2.7642152), (-1.3630421, -1.7045304), (-1.1709354, -1.2857168), (-2.505716, -2.3215497), (-2.7087147, -2.780504), (-1.3801851, -1.7737726), (-1.165598, -1.2719607), (-2.4884794, -2.2479327), (-2.7119482, -2.8066213), (-1.395775, -1.8684433), (-1.163005, -1.2498211), (-2.4742193, -2.147976), (-2.7232482, -2.8431106), (-1.419321, -1.9928181), (-1.1609159, -1.2394553), (-2.458394, -2.0398257), (-2.7332196, -2.8666782), (-1.4445195, -2.1208525), (-1.1596812, -1.2376504), (-2.4397116, -1.9270681), (-2.741301, -2.8787558), (-1.4719512, -2.2469795), (-1.1591421, -1.2432305), (-2.4171536, -1.8155913), (-2.7471642, -2.8785183), (-1.5020705, -2.363044), (-1.1494124, -1.2566624), (-2.377591, -1.7107648), (-2.7576213, -2.8635478), (-1.5496095, -2.4631305), (-1.1450127, -1.2795179), (-2.3291974, -1.6230934), (-2.7616227, -2.8359025), (-1.6030607, -2.5384994), (-1.1428059, -1.3150243), (-2.2714844, -1.5632036), (-2.7609155, -2.7935555), (-1.66303, -2.5806673), (-1.1408843, -1.351639), (-2.2033226, -1.5249501), (-2.7583942, -2.7497313), (-1.7316364, -2.6018083), (-1.1367038, -1.385704), (-2.12235, -1.5020245), (-2.756959, -2.7086825), (-1.8126404, -2.6100106), (-1.1278539, -1.4150815), (-2.0241964, -1.487506), (-2.7592437, -2.6864107), (-1.9110335, -2.6277077), (-1.1115054, -1.4122361), (-1.904451, -1.4471104), (-2.7709134, -2.6817296), (-2.0350804, -2.6596053), (-1.0830947, -1.3631147), (-1.7535625, -1.3610483), (-2.7954924, -2.7194588), (-2.1942024, -2.7416246), (-1.0443228, -1.2857927), (-1.5660335, -1.2284653), (-2.821541, -2.721394), (-2.3866298, -2.8087687), (-1.0142648, -1.2437229), (-1.3525361, -1.1148195), (-2.833136, -2.676672), (-2.5980144, -2.8418949), (-1.0404246, -1.2656956), (-1.177331, -1.0738316), (-2.7771866, -2.614458), (-2.727144, -2.8403008), (-1.1143694, -1.3277749), (-1.0943562, -1.0944029), (-2.6957688, -2.5641932), (-2.7698972, -2.8215086), (-1.1933535, -1.3937076), (-1.0807806, -1.1402024), (-2.625961, -2.5321631), (-2.7656853, -2.7992952), (-1.2532809, -1.4469157), (-1.096562, -1.1872878), (-2.5787537, -2.5127587), (-2.746566, -2.779269), (-1.2920464, -1.4861971), (-1.1191198, -1.2268454), (-2.5510173, -2.5001452), (-2.7268074, -2.763238), (-1.3148881, -1.5154814), (-1.1397715, -1.246635), (-2.537309, -2.478722), (-2.7125516, -2.7563972), (-1.3278846, -1.5468304), (-1.1542768, -1.2657579), (-2.5300977, -2.4606843), (-2.7038064, -2.7532628), (-1.3365713, -1.5791798), (-1.1637706, -1.2783283), (-2.5255344, -2.4372313), (-2.6999846, -2.7539175), (-1.3444382, -1.6158463), (-1.168905, -1.28514), (-2.5205648, -2.4061275), (-2.7002397, -2.7586923), (-1.3539484, -1.6600573), (-1.1703641, -1.2862823), (-2.5129716, -2.3645997), (-2.7039, -2.7685444), (-1.3669633, -1.7156858), (-1.1685226, -1.2807752), (-2.5009277, -2.3084831), (-2.7107646, -2.7855005), (-1.3852305, -1.7883874), (-1.1631414, -1.2660991), (-2.4824047, -2.2307968), (-2.7142882, -2.8132436), (-1.4023343, -1.8883492), (-1.1614736, -1.2463531), (-2.467912, -2.1295214), (-2.724757, -2.8476458), (-1.4260309, -2.013741), (-1.1600137, -1.2375993), (-2.4518726, -2.0203896), (-2.7341301, -2.869499), (-1.4513509, -2.1420684), (-1.1591347, -1.2370278), (-2.4328938, -1.9073486), (-2.7417812, -2.8797874), (-1.478968, -2.2673376), (-1.1587431, -1.2438782), (-2.409843, -1.7967442), (-2.7472804, -2.8774195), (-1.5094177, -2.3810337), (-1.1491882, -1.2587577), (-2.369708, -1.6942456), (-2.757293, -2.8601325), (-1.5573008, -2.4772818), (-1.1447582, -1.2830184), (-2.3204107, -1.6100457), (-2.7609558, -2.8305042), (-1.6113929, -2.5481946), (-1.1423719, -1.3196214), (-2.2614222, -1.5540006), (-2.7600584, -2.7868903), (-1.6724074, -2.5863068), (-1.1401109, -1.3566308), (-2.1915567, -1.5188783), (-2.7576134, -2.7427492), (-1.7426265, -2.6046507), (-1.1353534, -1.3905593), (-2.108282, -1.4979784), (-2.7565963, -2.7154696), (-1.825317, -2.6262512), (-1.1256491, -1.4102063), (-2.0088618, -1.4723129), (-2.7607765, -2.687737), (-1.9265162, -2.6390364), (-1.107052, -1.4033499), (-1.8847786, -1.4281102), (-2.7745016, -2.6866372), (-2.0555716, -2.675314), (-1.0766237, -1.3515998), (-1.727975, -1.3374287), (-2.8002832, -2.725253), (-2.2207687, -2.7614644), (-1.0375077, -1.2754279), (-1.5344814, -1.2018074), (-2.824069, -2.7151082), (-2.4170072, -2.8200717), (-1.0117862, -1.2434698), (-1.3199855, -1.0976255), (-2.8290365, -2.6638172), (-2.6247554, -2.8462763), (-1.0484129, -1.2759612), (-1.1565357, -1.0703871), (-2.7648952, -2.600894), (-2.739669, -2.839058), (-1.1271206, -1.3416116), (-1.087125, -1.0988315), (-2.6824665, -2.5534086), (-2.7719386, -2.8182518), (-1.2048388, -1.4066404), (-1.0809113, -1.1469514), (-2.6154256, -2.52384), (-2.7633865, -2.7957652), (-1.2617469, -1.4578084), (-1.0991905, -1.1937186), (-2.571334, -2.5058944), (-2.7432888, -2.7761643), (-1.297888, -1.4954478), (-1.1220653, -1.2321396), (-2.545952, -2.4938986), (-2.7238529, -2.7544556), (-1.3192948, -1.516645), (-1.143138, -1.2570488), (-2.5338333, -2.481416), (-2.7088163, -2.7492454), (-1.3302612, -1.5455017), (-1.1580948, -1.2743467), (-2.5287833, -2.4652905), (-2.7000988, -2.7474034), (-1.3369976, -1.5755975), (-1.1674286, -1.2853005), (-2.5261416, -2.4434588), (-2.6967163, -2.7490306), (-1.3432126, -1.610197), (-1.1721208, -1.2908554), (-2.5227408, -2.4139082), (-2.6975439, -2.754407), (-1.351403, -1.6523589), (-1.1731001, -1.2912378), (-2.5164068, -2.3741512), (-2.7017038, -2.764385), (-1.363334, -1.7056439), (-1.1709243, -1.285614), (-2.505456, -2.320429), (-2.708833, -2.7808273), (-1.3805684, -1.7751979), (-1.1655134, -1.2716653), (-2.4881048, -2.2464087), (-2.7121546, -2.8071697), (-1.3963046, -1.8703727), (-1.1629725, -1.2497078), (-2.4738274, -2.1463437), (-2.7234018, -2.8434787), (-1.4198627, -1.9948035), (-1.1609216, -1.2394559), (-2.4579804, -2.0381184), (-2.733331, -2.8669014), (-1.4450724, -2.12283), (-1.1597065, -1.2377324), (-2.4392664, -1.9253505), (-2.7413797, -2.8788283), (-1.4725207, -2.2488468), (-1.1591723, -1.2433985), (-2.4166598, -1.8139664), (-2.7472112, -2.8784099), (-1.5026667, -2.3646686), (-1.1494497, -1.256933), (-2.377042, -1.7093599), (-2.7576299, -2.8632424), (-1.5502328, -2.464386), (-1.1450422, -1.2798895), (-2.3285677, -1.6220092), (-2.7615983, -2.8354304), (-1.6037316, -2.5393343), (-1.1428175, -1.31547), (-2.270748, -1.5624716), (-2.7608683, -2.792982), (-1.663777, -2.58112), (-1.1408665, -1.3520958), (-2.2024508, -1.5245074), (-2.7583468, -2.7491443), (-1.7324992, -2.601991), (-1.1366372, -1.386121), (-2.1213007, -1.5017725), (-2.7569387, -2.708141), (-1.8136772, -2.6100419), (-1.127715, -1.4154332), (-2.022907, -1.4873486), (-2.7592783, -2.685947), (-1.9123218, -2.6276898), (-1.1112536, -1.4119519), (-1.902844, -1.4464462), (-2.7711017, -2.681918), (-2.0367382, -2.6602006), (-1.0826871, -1.3623247), (-1.7515501, -1.3597933), (-2.7958152, -2.7200432), (-2.1962929, -2.7428198), (-1.0438528, -1.2848368), (-1.5636135, -1.226713), (-2.8217554, -2.7212043), (-2.3889992, -2.8095064), (-1.014076, -1.2434583), (-1.3500421, -1.1136192), (-2.8328965, -2.676002), (-2.6001334, -2.8422272), (-1.0410178, -1.2662493), (-1.1757236, -1.0736048), (-2.7763205, -2.613693), (-2.7281706, -2.8402383), (-1.1153322, -1.3286315), (-1.0938041, -1.094758), (-2.6948202, -2.5636177), (-2.770096, -2.821301), (-1.1942152, -1.3945084), (-1.0808114, -1.1407292), (-2.6252325, -2.5317676), (-2.7655373, -2.7990668), (-1.2538979, -1.4475573), (-1.0967906, -1.1877855), (-2.5782745, -2.5124743), (-2.7463365, -2.7790687), (-1.2924479, -1.4867018), (-1.1193728, -1.2272534), (-2.5507288, -2.4999144), (-2.7266004, -2.7566395), (-1.3154899, -1.5085934), (-1.1408772, -1.2532152), (-2.5372465, -2.4874055), (-2.7109008, -2.7508438), (-1.3273714, -1.5377222), (-1.1564312, -1.2715267), (-2.531472, -2.4716756), (-2.701487, -2.7483444), (-1.3344923, -1.5675368), (-1.166359, -1.2834353), (-2.528581, -2.4506457), (-2.6974592, -2.7492363), (-1.3406961, -1.6013126), (-1.1716028, -1.2899511), (-2.5252995, -2.4223697), (-2.6977005, -2.7537308), (-1.3485582, -1.6420245), (-1.1731074, -1.2914257), (-2.5193982, -2.3845336), (-2.7012906, -2.7625463), (-1.3598676, -1.6930069), (-1.1714948, -1.2872281), (-2.5092077, -2.3337379), (-2.7077756, -2.777317), (-1.3761238, -1.7589345), (-1.166803, -1.2753474), (-2.4930646, -2.2643247), (-2.7101521, -2.80112), (-1.390246, -1.8482707), (-1.1635919, -1.2512326), (-2.4788797, -2.1655295), (-2.721995, -2.839451), (-1.4137193, -1.9719186), (-1.1610799, -1.2396487), (-2.4631917, -2.058245), (-2.7323735, -2.8644636), (-1.4388319, -2.099836), (-1.1596214, -1.2369835), (-2.4447544, -1.9457517), (-2.7407622, -2.8780444), (-1.4661132, -2.2268503), (-1.1590137, -1.2416458), (-2.4226267, -1.8335029), (-2.746908, -2.879665), (-1.4959594, -2.3451877), (-1.158895, -1.2539314), (-2.3957715, -1.7292958), (-2.7505126, -2.8678632), (-1.5288525, -2.4456887), (-1.1498992, -1.2742982), (-2.3516881, -1.6388024), (-2.758217, -2.8416667), (-1.5789769, -2.5241559), (-1.1456126, -1.308339), (-2.2976272, -1.5753893), (-2.759171, -2.8003914), (-1.6356839, -2.5700006), (-1.1433635, -1.3444847), (-2.2336545, -1.5341058), (-2.7568243, -2.7568688), (-1.6999952, -2.593975), (-1.1401644, -1.3787435), (-2.1583889, -1.5092742), (-2.7543972, -2.7155635), (-1.774728, -2.6036534), (-1.1333998, -1.4085888), (-2.0684316, -1.4941866), (-2.7547526, -2.692616), (-1.863988, -2.6211443), (-1.1206952, -1.4235597), (-1.9601064, -1.4708952), (-2.7613025, -2.6698654), (-1.9744765, -2.6332211), (-1.0977119, -1.3913289), (-1.8244562, -1.4069117), (-2.7806516, -2.6943014), (-2.1166399, -2.6929862), (-1.0621678, -1.3205553), (-1.6540079, -1.2942905), (-2.8090117, -2.7257576), (-2.2951274, -2.7767828), (-1.0241435, -1.2528234), (-1.4491228, -1.1621271), (-2.8359072, -2.7024276), (-2.509248, -2.8264866), (-1.020626, -1.2422245), (-1.2458711, -1.0834664), (-2.8066573, -2.6465583), (-2.6781864, -2.8419425), (-1.0763164, -1.2905973), (-1.1207274, -1.0789882), (-2.7322338, -2.5887854), (-2.7566254, -2.829629), (-1.1567632, -1.357989), (-1.0807424, -1.116563), (-2.6551352, -2.5487735), (-2.7690752, -2.8084426), (-1.2260128, -1.4179574), (-1.0867734, -1.1645523), (-2.598013, -2.5242), (-2.7544746, -2.7873259), (-1.2737226, -1.4636258), (-1.1077036, -1.2079533), (-2.5623705, -2.5088134), (-2.7343307, -2.7695587), (-1.3030281, -1.4974545), (-1.1296571, -1.2426394), (-2.5427926, -2.4976695), (-2.7167597, -2.7496421), (-1.320105, -1.5166878), (-1.1488284, -1.2643701), (-2.5341613, -2.4848728), (-2.703926, -2.7460344), (-1.3287265, -1.5450411), (-1.1618396, -1.2791573), (-2.530861, -2.4676843), (-2.6972177, -2.745575), (-1.3345567, -1.5757374), (-1.1695024, -1.288232), (-2.5287323, -2.4442391), (-2.6954706, -2.7483685), (-1.3408589, -1.6117772), (-1.1728956, -1.2923858), (-2.5250115, -2.4126196), (-2.6975367, -2.754784), (-1.3497865, -1.6560769), (-1.1729257, -1.2916178), (-2.5178154, -2.3702934), (-2.7026038, -2.7658446), (-1.3628736, -1.7122328), (-1.1700517, -1.2847965), (-2.5056324, -2.313269), (-2.7104456, -2.783677), (-1.3815794, -1.7856784), (-1.1640209, -1.2692153), (-2.4867032, -2.2346697), (-2.7145233, -2.8120832), (-1.3992012, -1.8864619), (-1.1621177, -1.2490574), (-2.471868, -2.1328192), (-2.7252314, -2.8467505), (-1.4233223, -2.0122375), (-1.1605746, -1.2399497), (-2.4555087, -2.0232794), (-2.734704, -2.8688266), (-1.4489903, -2.1408038), (-1.1596932, -1.2390398), (-2.4362776, -1.9099286), (-2.742376, -2.87932), (-1.4768527, -2.2662218), (-1.1593356, -1.245546), (-2.4130652, -1.7990657), (-2.7478583, -2.8771517), (-1.507434, -2.3800154), (-1.1497718, -1.2600731), (-2.3728037, -1.6963158), (-2.757867, -2.8600514), (-1.5553986, -2.4763517), (-1.145383, -1.2839893), (-2.3235, -1.611887), (-2.7614756, -2.830594), (-1.6094271, -2.5473075), (-1.1430681, -1.3202447), (-2.2646532, -1.5556575), (-2.7604904, -2.787143), (-1.6702073, -2.5853858), (-1.1409013, -1.3569189), (-2.1951063, -1.5204562), (-2.7579203, -2.7431579), (-1.7399855, -2.603568), (-1.1362749, -1.3904867), (-2.112377, -1.4996074), (-2.7567303, -2.7160428), (-1.8219707, -2.6248362), (-1.1267512, -1.409725), (-2.013804, -1.4741505), (-2.7606955, -2.6885998), (-1.9221293, -2.6372037), (-1.1084387, -1.4039545), (-1.8909367, -1.4317659), (-2.7739694, -2.686363), (-2.0496902, -2.6713593), (-1.0783664, -1.3527751), (-1.7357092, -1.3429586), (-2.79948, -2.7248006), (-2.2131956, -2.7558045), (-1.0391734, -1.2760638), (-1.5437027, -1.2084403), (-2.8240016, -2.7186623), (-2.4085367, -2.8171551), (-1.0120479, -1.2407867), (-1.3291755, -1.1012067), (-2.8309116, -2.6693473), (-2.6175814, -2.8453786), (-1.0456536, -1.2700722), (-1.162137, -1.0700091), (-2.769, -2.6067863), (-2.7365575, -2.8396478), (-1.1230645, -1.3344274), (-1.0889466, -1.0963975), (-2.686811, -2.5589743), (-2.7716014, -2.8192654), (-1.2010537, -1.3991718), (-1.0808213, -1.1440563), (-2.6191254, -2.5294318), (-2.7640743, -2.7967184), (-1.2585574, -1.4501321), (-1.0985407, -1.1910791), (-2.5744445, -2.5119874), (-2.7440927, -2.776868), (-1.2950592, -1.4872695), (-1.1214417, -1.2299708), (-2.548802, -2.500879), (-2.7244375, -2.761184), (-1.3161532, -1.5147982), (-1.1418637, -1.2490038), (-2.53666, -2.4806237), (-2.710593, -2.7547185), (-1.3278831, -1.5447454), (-1.1559783, -1.2674872), (-2.5305772, -2.4635525), (-2.702314, -2.7518997), (-1.335682, -1.5759723), (-1.1650823, -1.2796208), (-2.5268083, -2.440962), (-2.6989052, -2.7527454), (-1.3429357, -1.6116669), (-1.1699048, -1.2862369), (-2.5224042, -2.4107344), (-2.6994667, -2.757533), (-1.3520054, -1.6549029), (-1.1711749, -1.2874603), (-2.5152547, -2.3702497), (-2.7032905, -2.7671523), (-1.364645, -1.7093433), (-1.1693, -1.2823682), (-2.503643, -2.3155704), (-2.7101426, -2.783536), (-1.3824809, -1.7803375), (-1.164082, -1.268538), (-2.4856844, -2.2400823), (-2.7133832, -2.810225), (-1.3988812, -1.8775864), (-1.162033, -1.2477378), (-2.4712272, -2.139447), (-2.7242117, -2.8456736), (-1.4225876, -2.0026546), (-1.160335, -1.2383099), (-2.4552512, -2.0307353), (-2.7338395, -2.8683956), (-1.447902, -2.1310577), (-1.1593442, -1.2372226), (-2.4364066, -1.9177108), (-2.7416763, -2.8796067), (-1.4754586, -2.2570164), (-1.1589304, -1.2435081), (-2.4136071, -1.8064911), (-2.7473428, -2.8783302), (-1.5057602, -2.3721771), (-1.1493238, -1.2577263), (-2.3737721, -1.702624), (-2.7575736, -2.862195), (-1.5534831, -2.4706016), (-1.1449355, -1.2813665), (-2.3249376, -1.6165041), (-2.761401, -2.8334944), (-1.6072646, -2.5439281), (-1.1426617, -1.317539), (-2.2665906, -1.5584157), (-2.7605853, -2.7903936), (-1.6677655, -2.584163), (-1.1405936, -1.35449), (-2.1975727, -1.5216658), (-2.7580805, -2.74626), (-1.7371871, -2.603938), (-1.1361488, -1.3886355), (-2.1154373, -1.4997257), (-2.7568254, -2.7186322), (-1.8186834, -2.6262577), (-1.1269054, -1.4086676), (-2.0175877, -1.4735742), (-2.7606027, -2.6904025), (-1.9181669, -2.6392162), (-1.1090326, -1.4058619), (-1.8956133, -1.4327605), (-2.7733047, -2.685154), (-2.0447283, -2.6714), (-1.0795587, -1.3571911), (-1.7414724, -1.3459319), (-2.7982814, -2.7214513), (-2.2070863, -2.7536755), (-1.0406592, -1.2817129), (-1.550503, -1.2133393), (-2.8229866, -2.716624), (-2.4017508, -2.8154395), (-1.0128036, -1.2449881), (-1.3360085, -1.1052581), (-2.8310802, -2.6683455), (-2.6116335, -2.8442564), (-1.0443294, -1.2721739), (-1.1663992, -1.0717031), (-2.770763, -2.6059089), (-2.733669, -2.8394697), (-1.1209081, -1.3359565), (-1.0903302, -1.0965657), (-2.6886163, -2.5572653), (-2.7709637, -2.819533), (-1.1993809, -1.4013201), (-1.0806173, -1.1436704), (-2.620114, -2.526654), (-2.7644243, -2.7971606), (-1.2578169, -1.4534009), (-1.0977418, -1.1906253), (-2.574453, -2.5080924), (-2.7447448, -2.7773666), (-1.2952588, -1.4917936), (-1.1204904, -1.2295951), (-2.5478969, -2.495838), (-2.7251248, -2.7553241), (-1.3175482, -1.5133786), (-1.1418012, -1.2550544), (-2.535035, -2.4833412), (-2.7097282, -2.7498307), (-1.3290167, -1.5424113), (-1.1570754, -1.272864), (-2.5295806, -2.4673822), (-2.7006447, -2.7476866), (-1.3359798, -1.5724361), (-1.1667176, -1.2842779), (-2.526782, -2.4458811), (-2.696934, -2.7489946), (-1.3422217, -1.606737), (-1.1716871, -1.2902687), (-2.5234113, -2.4168422), (-2.69748, -2.7540088), (-1.3502867, -1.6483521), (-1.172914, -1.2911142), (-2.5172534, -2.3778386), (-2.701388, -2.7635312), (-1.3619664, -1.7007674), (-1.1709825, -1.286063), (-2.5066156, -2.3252404), (-2.708257, -2.779337), (-1.3788114, -1.7689581), (-1.1658603, -1.2729167), (-2.4897482, -2.2529697), (-2.7112203, -2.804726), (-1.3939232, -1.8619452), (-1.1630777, -1.2501608), (-2.475534, -2.1533868), (-2.7226977, -2.8418133), (-1.4174368, -1.9861146), (-1.1608611, -1.2394156), (-2.4597747, -2.0455003), (-2.7328076, -2.8658593), (-1.4426017, -2.1141565), (-1.159561, -1.2373425), (-2.4411912, -1.9328007), (-2.7409987, -2.8784368), (-1.4699838, -2.2406244), (-1.1590065, -1.2426364), (-2.4187903, -1.8210404), (-2.7469654, -2.878804), (-1.5000168, -2.357473), (-1.1492512, -1.2557181), (-2.3794093, -1.7154988), (-2.7575564, -2.864505), (-1.5474708, -2.4587836), (-1.1448765, -1.2782259), (-2.3312836, -1.6267637), (-2.7616708, -2.837439), (-1.6007642, -2.5355687), (-1.1427301, -1.313471), (-2.2739317, -1.5656883), (-2.761043, -2.795454), (-1.6604807, -2.579039), (-1.1409086, -1.3500408), (-2.2062285, -1.5264553), (-2.7585294, -2.751692), (-1.7286998, -2.6011047), (-1.1368912, -1.384233), (-2.1258526, -1.5028799), (-2.7570064, -2.7105014), (-1.809123, -2.6098306), (-1.1282841, -1.4138278), (-2.0285068, -1.4880352), (-2.75911, -2.6879785), (-1.9066727, -2.6277077), (-1.112322, -1.4131172), (-1.9098377, -1.4493383), (-2.7702599, -2.6811123), (-2.0294752, -2.6575553), (-1.0844458, -1.3657051), (-1.7603245, -1.3652643), (-2.79438, -2.7175014), (-2.187127, -2.7375588), (-1.0459006, -1.2889704), (-1.5741967, -1.234373), (-2.8207757, -2.722041), (-2.3785763, -2.8062494), (-1.0149245, -1.2445849), (-1.3610127, -1.1188855), (-2.8338943, -2.6789656), (-2.5907524, -2.8407586), (-1.038432, -1.2637761), (-1.1828462, -1.074591), (-2.7800994, -2.6170914), (-2.7235808, -2.8405173), (-1.1110886, -1.3248159), (-1.0962723, -1.0931712), (-2.6989963, -2.56618), (-2.7691815, -2.8222213), (-1.1903979, -1.3909333), (-1.0806859, -1.1383805), (-2.628456, -2.5335364), (-2.7661793, -2.8000808), (-1.2511568, -1.4446822), (-1.0957812, -1.1855656), (-2.5804002, -2.5137548), (-2.7473469, -2.7799585), (-1.2906615, -1.4844326), (-1.1182542, -1.2254345), (-2.5520132, -2.5009654), (-2.7275152, -2.7637753), (-1.3140169, -1.5140128), (-1.1390324, -1.2455312), (-2.5378842, -2.4794767), (-2.7130828, -2.756772), (-1.3273255, -1.5455132), (-1.1537174, -1.2649584), (-2.5304356, -2.4614997), (-2.704148, -2.753474), (-1.336171, -1.5778741), (-1.1633893, -1.2777944), (-2.525771, -2.4381986), (-2.700154, -2.7539642), (-1.3440828, -1.6144254), (-1.1686828, -1.2848469), (-2.5208023, -2.4073403), (-2.7002618, -2.758561), (-1.353553, -1.6583915), (-1.1702831, -1.2862296), (-2.51329, -2.3661761), (-2.7037907, -2.768195), (-1.3664564, -1.7136124), (-1.1685748, -1.2810041), (-2.5013983, -2.3106005), (-2.7105267, -2.7848525), (-1.3845403, -1.7856673), (-1.1633418, -1.2667117), (-2.4831152, -2.2337542), (-2.7138815, -2.812151), (-1.4013462, -1.8845856), (-1.1615722, -1.2466041), (-2.4686697, -2.132743), (-2.724453, -2.846913), (-1.4250067, -2.0098228), (-1.1600409, -1.2376175), (-2.4526904, -2.0237947), (-2.7339098, -2.869066), (-1.4502896, -2.1381397), (-1.1591243, -1.2368693), (-2.4337914, -1.9107949), (-2.74163, -2.8796768), (-1.4778597, -2.26362), (-1.1587213, -1.2435272), (-2.4108539, -1.8000079), (-2.7471979, -2.877694), (-1.5082417, -2.3778074), (-1.149148, -1.258182), (-2.370848, -1.697061), (-2.757296, -2.8608186), (-1.5560563, -2.4748034), (-1.1447277, -1.2822285), (-2.321726, -1.6122129), (-2.7610307, -2.8315299), (-1.6100396, -2.5465584), (-1.1423748, -1.3186834), (-2.2629664, -1.5554702), (-2.760178, -2.788115), (-1.6708862, -2.5854204), (-1.1401751, -1.3556726), (-2.1933916, -1.5197896), (-2.757732, -2.7440019), (-1.7408514, -2.6042824), (-1.1355182, -1.3896759), (-2.1104941, -1.4985291), (-2.7566528, -2.716627), (-1.8231655, -2.6261609), (-1.1259642, -1.4094595), (-2.0115855, -1.4726892), (-2.760718, -2.688744), (-1.9238255, -2.6390412), (-1.107601, -1.4039013), (-1.8881803, -1.4295404), (-2.7741313, -2.6863332), (-2.0521019, -2.6740644), (-1.0774775, -1.3531518), (-1.732218, -1.3400561), (-2.7996714, -2.7242029), (-2.216424, -2.759034), (-1.0384287, -1.2772384), (-1.5395029, -1.2053615), (-2.8237634, -2.7156894), (-2.4122107, -2.818661), (-1.0120314, -1.2438378), (-1.3249619, -1.0999414), (-2.8297179, -2.6652775), (-2.620686, -2.845682), (-1.0470792, -1.2747288), (-1.1595495, -1.0707415), (-2.7667482, -2.6024597), (-2.7378442, -2.8392153), (-1.1251628, -1.3398546), (-1.0880758, -1.098101), (-2.6843832, -2.5545888), (-2.771666, -2.818675), (-1.2031436, -1.4050316), (-1.080797, -1.1459285), (-2.6168764, -2.524678), (-2.7637298, -2.7962248), (-1.2605435, -1.4565184), (-1.0987316, -1.1927669), (-2.572291, -2.5065207), (-2.74376, -2.7765663), (-1.2971022, -1.4944254), (-1.1215751, -1.2313665), (-2.5465398, -2.494421), (-2.7242694, -2.7547593), (-1.3187963, -1.5157793), (-1.142726, -1.2564521), (-2.5341837, -2.4819076), (-2.7091253, -2.749466), (-1.3299334, -1.5447266), (-1.1577824, -1.27391), (-2.5289972, -2.4658024), (-2.700298, -2.7475364), (-1.3367572, -1.574841), (-1.1672137, -1.2850052), (-2.5262947, -2.4440358), (-2.6968174, -2.7490745), (-1.3429971, -1.6093936), (-1.171991, -1.2906858), (-2.5228896, -2.4145963), (-2.6975632, -2.7543545), (-1.3511666, -1.6514425), (-1.1730429, -1.2911924), (-2.5165925, -2.3750079), (-2.701653, -2.764218), (-1.3630427, -1.7045363), (-1.1709349, -1.2857133), (-2.5057156, -2.3215425), (-2.7087157, -2.7805076), (-1.3801863, -1.7737807), (-1.1655958, -1.2719558), (-2.4884763, -2.2479217), (-2.7119503, -2.8066268), (-1.3957789, -1.8684567), (-1.1630042, -1.2498184), (-2.4742153, -2.147964), (-2.7232485, -2.843114), (-1.4193252, -1.9928333), (-1.1609161, -1.2394547), (-2.4583902, -2.0398116), (-2.7332187, -2.8666797), (-1.4445227, -2.1208684), (-1.1596816, -1.237651), (-2.4397082, -1.9270538), (-2.741301, -2.8787556), (-1.4719551, -2.2469943), (-1.1591423, -1.2432321), (-2.4171498, -1.8155777), (-2.747164, -2.8785162), (-1.5020744, -2.363056), (-1.1494123, -1.256664), (-2.3775866, -1.7107534), (-2.7576215, -2.8635454), (-1.5496148, -2.46314), (-1.1450123, -1.2795198), (-2.3291907, -1.6230844), (-2.7616222, -2.8358996), (-1.6030669, -2.5385063), (-1.1428052, -1.3150269), (-2.271477, -1.5631973), (-2.7609158, -2.7935522), (-1.6630377, -2.5806715), (-1.1408836, -1.3516421), (-2.203314, -1.5249462), (-2.758395, -2.749727), (-1.7316455, -2.60181), (-1.1367024, -1.3857071), (-2.1223392, -1.5020223), (-2.7569597, -2.7086782), (-1.8126508, -2.610011), (-1.1278511, -1.415085), (-2.0241828, -1.4875045), (-2.7592442, -2.6864066), (-1.911046, -2.6277082), (-1.111503, -1.4122342), (-1.9044356, -1.4471048), (-2.7709146, -2.6817315), (-2.0350957, -2.6596107), (-1.0830913, -1.3631077), (-1.7535439, -1.361037), (-2.7954957, -2.7194648), (-2.1942222, -2.7416363), (-1.0443177, -1.2857839), (-1.5660101, -1.228448), (-2.821543, -2.7213912), (-2.3866527, -2.8087764), (-1.0142634, -1.2437218), (-1.352512, -1.1148081), (-2.8331337, -2.6766646), (-2.598035, -2.841898), (-1.0404304, -1.2657017), (-1.1773152, -1.0738297), (-2.7771783, -2.61445), (-2.7271543, -2.8403), (-1.1143786, -1.3277841), (-1.0943501, -1.0944068), (-2.6957593, -2.564187), (-2.7698996, -2.8215067), (-1.1933624, -1.3937168), (-1.0807809, -1.1402082), (-2.6259544, -2.5321589), (-2.7656848, -2.7992926), (-1.2532871, -1.4469228), (-1.0965635, -1.1872935), (-2.578749, -2.5127552), (-2.7465653, -2.7792666), (-1.2920517, -1.4862028), (-1.1191218, -1.2268504), (-2.5510128, -2.5001433), (-2.726806, -2.7632353), (-1.3148926, -1.5154853), (-1.1397741, -1.2466403), (-2.5373063, -2.4787216), (-2.7125497, -2.7563946), (-1.3278873, -1.5468326), (-1.1542798, -1.265762), (-2.530097, -2.4606845), (-2.703805, -2.7532609), (-1.3365725, -1.5791812), (-1.1637727, -1.2783315), (-2.5255342, -2.437232), (-2.6999838, -2.7539158), (-1.3444395, -1.6158468), (-1.1689061, -1.2851437), (-2.5205636, -2.4061291), (-2.7002387, -2.7586908), (-1.3539495, -1.6600567), (-1.170366, -1.2862843), (-2.5129724, -2.3646014), (-2.7038994, -2.7685428), (-1.3669633, -1.7156847), (-1.1685232, -1.2807776), (-2.5009272, -2.3084855), (-2.7107642, -2.785499), (-1.3852311, -1.7883853), (-1.163142, -1.2661016), (-2.4824045, -2.2308002), (-2.714288, -2.8132415), (-1.4023348, -1.888346), (-1.1614745, -1.2463549), (-2.4679124, -2.129525), (-2.7247562, -2.8476448), (-1.426031, -2.013738), (-1.1600149, -1.2376), (-2.4518735, -2.020393), (-2.7341301, -2.8694985), (-1.4513503, -2.142065), (-1.159135, -1.2370278), (-2.4328947, -1.9073519), (-2.7417808, -2.879788), (-1.4789675, -2.267335), (-1.1587439, -1.2438773), (-2.4098437, -1.7967461), (-2.7472801, -2.8774202), (-1.5094173, -2.3810315), (-1.1491889, -1.2587576), (-2.3697093, -1.6942481), (-2.7572925, -2.8601332), (-1.5572997, -2.4772797), (-1.1447583, -1.283017), (-2.3204114, -1.6100471), (-2.760956, -2.8305056), (-1.6113918, -2.5481927), (-1.1423715, -1.3196201), (-2.2614233, -1.5540025), (-2.7600586, -2.786891), (-1.6724062, -2.5863044), (-1.1401107, -1.35663), (-2.1915586, -1.5188807), (-2.7576137, -2.74275), (-1.742625, -2.6046488), (-1.1353531, -1.390558), (-2.1082833, -1.4979795), (-2.756596, -2.715471), (-1.8253157, -2.6262505), (-1.1256495, -1.4102049), (-2.0088637, -1.4723138), (-2.7607768, -2.687739), (-1.9265144, -2.6390362), (-1.1070524, -1.4033492), (-1.8847809, -1.4281113), (-2.774501, -2.6866374), (-2.05557, -2.6753128), (-1.0766249, -1.3516), (-1.727977, -1.3374306), (-2.8002818, -2.7252533), (-2.220766, -2.7614632), (-1.0375086, -1.2754279), (-1.5344841, -1.2018093), (-2.8240685, -2.7151096), (-2.4170046, -2.820071), (-1.0117862, -1.2434694), (-1.319988, -1.0976266), (-2.8290367, -2.6638181), (-2.624753, -2.8462756), (-1.0484128, -1.2759603), (-1.1565378, -1.0703876), (-2.7648954, -2.6008956), (-2.7396677, -2.8390586), (-1.1271199, -1.3416108), (-1.0871265, -1.098831), (-2.682468, -2.5534093), (-2.7719378, -2.8182526), (-1.2048371, -1.4066391), (-1.0809114, -1.1469499), (-2.615427, -2.5238402), (-2.7633867, -2.7957666), (-1.2617459, -1.4578086), (-1.0991899, -1.1937178), (-2.5713336, -2.5058942), (-2.743289, -2.7761645), (-1.2978885, -1.4954474), (-1.1220653, -1.2321396), (-2.5459516, -2.493899), (-2.7238529, -2.7544553), (-1.3192956, -1.5166439), (-1.1431382, -1.2570492), (-2.5338328, -2.481417), (-2.7088163, -2.749245), (-1.330262, -1.5455012), (-1.1580946, -1.2743478), (-2.5287826, -2.4652913), (-2.7000992, -2.7474017), (-1.3369989, -1.5755966), (-1.1674286, -1.2853025), (-2.5261395, -2.44346), (-2.696715, -2.7490287), (-1.3432142, -1.6101954), (-1.1721225, -1.2908576), (-2.5227404, -2.413911), (-2.6975424, -2.7544057), (-1.3514026, -1.6523557), (-1.1731023, -1.2912388), (-2.516409, -2.374155), (-2.7017024, -2.764383), (-1.3633313, -1.7056386), (-1.1709259, -1.2856157), (-2.5054603, -2.3204348), (-2.708833, -2.7808254), (-1.3805642, -1.7751904), (-1.1655142, -1.2716664), (-2.4881096, -2.2464168), (-2.7121534, -2.8071673), (-1.3962994, -1.870362), (-1.1629735, -1.2497083), (-2.4738326, -2.146353), (-2.7234008, -2.843477), (-1.4198568, -1.9947923), (-1.1609217, -1.2394556), (-2.457986, -2.0381281), (-2.733331, -2.8669007), (-1.4450663, -2.122819), (-1.1597059, -1.2377317), (-2.439272, -1.9253596), (-2.7413805, -2.8788276), (-1.4725156, -2.2488365), (-1.1591705, -1.2433981), (-2.4166636, -1.8139745), (-2.7472126, -2.8784099), (-1.5026635, -2.3646607), (-1.149449, -1.2569329), (-2.3770454, -1.7093672), (-2.7576306, -2.8632433), (-1.55023, -2.4643803), (-1.1450418, -1.279889), (-2.3285701, -1.6220148), (-2.761598, -2.8354323), (-1.6037289, -2.5393302), (-1.142817, -1.3154681), (-2.2707512, -1.5624754), (-2.7608695, -2.792985), (-1.6637738, -2.5811179), (-1.1408656, -1.3520936), (-2.2024536, -1.5245085), (-2.7583473, -2.7491467), (-1.7324966, -2.6019917), (-1.1366371, -1.38612), (-2.121304, -1.501772), (-2.7569392, -2.7081425), (-1.8136747, -2.6100435), (-1.1277152, -1.4154329), (-2.02291, -1.487348), (-2.7592788, -2.685948), (-1.9123195, -2.6276917), (-1.1112539, -1.4119538), (-1.9028466, -1.4464462), (-2.7711008, -2.6819162), (-2.0367348, -2.660201), (-1.0826888, -1.3623286), (-1.7515543, -1.3597956), (-2.7958138, -2.7200406), (-2.196289, -2.7428186), (-1.043855, -1.2848412), (-1.5636185, -1.2267159), (-2.8217545, -2.7212024), (-2.3889947, -2.8095057), (-1.014077, -1.2434626), (-1.3500473, -1.1136221), (-2.8328965, -2.6759996), (-2.6001282, -2.8422256), (-1.0410167, -1.2662522), (-1.1757278, -1.0736072), (-2.7763228, -2.613692), (-2.728168, -2.8402371), (-1.11533, -1.3286319), (-1.093806, -1.0947585), (-2.6948225, -2.563617), (-2.7700956, -2.8213005), (-1.1942128, -1.3945091), (-1.0808103, -1.1407292), (-2.6252332, -2.5317652), (-2.7655377, -2.7990656), (-1.2538975, -1.4475589), (-1.0967898, -1.1877859), (-2.5782743, -2.512472), (-2.7463374, -2.779068), (-1.2924488, -1.4867035), (-1.1193717, -1.2272537), (-2.5507267, -2.4999125), (-2.726601, -2.7566395), (-1.3154917, -1.5085953), (-1.1408775, -1.2532151), (-2.5372458, -2.4874039), (-2.7109008, -2.750844), (-1.3273722, -1.5377241), (-1.1564308, -1.2715267), (-2.53147, -2.4716735), (-2.7014866, -2.7483437), (-1.3344936, -1.5675383), (-1.166359, -1.2834357), (-2.5285792, -2.4506445), (-2.6974587, -2.7492356), (-1.3406974, -1.6013129), (-1.1716031, -1.2899511), (-2.5252976, -2.4223692), (-2.6976995, -2.7537298), (-1.3485593, -1.6420239), (-1.1731082, -1.2914269), (-2.5193977, -2.384535), (-2.7012892, -2.7625446), (-1.3598671, -1.6930041), (-1.1714965, -1.2872291), (-2.5092092, -2.3337407), (-2.7077744, -2.777315), (-1.3761221, -1.7589293), (-1.1668038, -1.2753485), (-2.4930663, -2.2643294), (-2.710151, -2.8011172), (-1.3902442, -1.8482637), (-1.1635921, -1.2512332), (-2.4788804, -2.1655352), (-2.721994, -2.83945), (-1.4137175, -1.9719115), (-1.1610801, -1.2396475), (-2.463193, -2.0582504), (-2.7323732, -2.8644633), (-1.4388297, -2.0998287), (-1.1596206, -1.236982), (-2.4447565, -1.9457576), (-2.7407625, -2.8780448), (-1.466111, -2.226843), (-1.1590136, -1.2416439), (-2.4226289, -1.8335084), (-2.746908, -2.8796656), (-1.4959565, -2.3451817), (-1.1588943, -1.2539296), (-2.3957734, -1.7293005), (-2.7505133, -2.8678646), (-1.5288506, -2.4456847), (-1.1498983, -1.2742964), (-2.3516889, -1.6388052), (-2.758217, -2.8416686), (-1.5789763, -2.5241544), (-1.1456128, -1.3083375), (-2.2976289, -1.5753909), (-2.7591715, -2.8003933), (-1.6356823, -2.569999), (-1.1433637, -1.3444829), (-2.233657, -1.5341076), (-2.7568247, -2.7568715), (-1.699993, -2.593974), (-1.1401647, -1.3787413), (-2.158392, -1.5092757), (-2.7543979, -2.7155664), (-1.7747253, -2.603653), (-1.1334002, -1.4085873), (-2.0684354, -1.4941877), (-2.7547524, -2.6926174), (-1.8639839, -2.6211436), (-1.1206951, -1.423558), (-1.9601104, -1.4708955), (-2.7613027, -2.6698668), (-1.9744726, -2.6332207), (-1.097712, -1.3913286), (-1.8244607, -1.4069134), (-2.7806523, -2.6943016), (-2.116636, -2.6929843), (-1.0621679, -1.3205554), (-1.6540123, -1.2942934), (-2.809012, -2.7257595), (-2.2951233, -2.776782), (-1.0241433, -1.2528224), (-1.449127, -1.1621281), (-2.835908, -2.7024298), (-2.5092447, -2.8264878)]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5K0lEQVR4nO3de5xWZb338c9vzswwDDMDKEdBAwEBRUcTsQTRsh1pKhbUNmjXQ1vbT2q1zbZPuevJXttD7U5Pe0dbN2GKEJWaiaFpKttKwRRwUAFB5XxmzjP3zPyeP9aa4Z7DPef7MPB99xrXva51rbV+65pDP651rWuZuyMiIiIi8ZeW7ABEREREThZKvEREREQSRImXiIiISIIo8RIRERFJECVeIiIiIgmixEtEREQkQZR4iUjcmNl/mtk3kh1HvJjZWDNzM8vo4f6fNrM1fR2XiKQu0zxeItJbZvYn4GzgVHevTXI4CWNmY4HtQKa71/dVXRE5canHS0R6JUwoPgA4cGU39utRL5GISH+mxEtEeuszwF+ApcDC6A1mttTMvhN+nmVmO83sa2a2F/hvMxtgZr8wsyNmttnMbjWznVH732Zm28ys3MxKzezqqG2LzOx/zOwnZnbMzN4wszmxgjSzEWb2azM7YGbbzexLUeXVZlYUVXe6mR00s0wzSzOz/2Nm75jZfjNbZmYFMc6xw8wui1r/VzP7Zbj6fLg8amYVZjYjvIa1UfUvMrOXw+t52cwuitr2JzP7v+E1l5vZGjMbEvO7IiIpSYmXiPTWZ4AHw68Pm9kpHdQ9FSgCTgMWA3cAY4HTgcuBv29VfxtBb1oB8C3gl2Y2PGr7+8M6Q8Jj/SY6gWpiZmnA74DXgJHAHOBmM/uwu+8G/gxcG7XLp4BV7h4BFoVfs8M4BwI/6eAaY/lguBzs7gPd/c+tYiwCfg/8CCgGvg/83syKW8X1WWAYkAV8tQdxiEgSKfESkR4zs4sJkqiV7r6eIAn6VAe7NAJ3uHutu1cDnwC+6+5H3H0nQdLRzN1/5e673b3R3VcAW4ALoqrsB37g7pFw+5vAR9s57/nAUHf/trvXufvbwM+B+eH2h4AF4TVZWP5QuO3TwPfd/W13rwC+DsyPw63SjwJb3P0Bd6939+XAG8DHour8t7u/FbbdSuCcPo5BROJMiZeI9MZCYI27HwzXH6LV7cZWDrh7TdT6COC9qPXoz5jZZ8zsVTM7amZHgSkEvVtNdnnLJ4TeCY/Z2mnAiKbjhMf6F6Cpd+7XwIywN+2DBAniC1ExvtPqHBlR+/aV1udpOtfIqPW9UZ+rCHrfRKQf0eBWEekRMxtA0GOVHo7ZAsgGBpvZ2e7+Wju7tX6Meg8wCigN10dHHf80gl6pOcCf3b3BzF4FLGr/kWZmUcnXGOCxds77HrDd3ce3dy3ufiSc1uGTwCTg4ahj7iZI3JqMAeqBfWHs0SqB3Kj1U6NP0965o7Q+T9O5nuxkPxHpR9TjJSI99XGgAZhMcMvrHIKk5QWCcV9dsRL4upkVmtlI4J+ituURJCsHAMzsswQ9XtGGAV8KB8FfF57/iXbO8xJQHg7sH2Bm6WY2xczOj6rzUBj3PI7fZgRYDtxiZuPMbCDwXWBFjCkhXiW4DZlpZiXhsZocIOhJOz1GWzwBTDCzT5lZhpl9kqBtH49RX0T6ISVeItJTCwnGHL3r7nubvggGnn+6i2Ogvg3sJJjf6mlgFVAL4O6lwPcIBr7vA6YC/9Nq/78C44GDwJ3APHc/1Pok7t4AzCVIDreH9f+LYNB+k8fCY+1t1Vt3P/AAwVOJ24Ea4H/HuJ5vAGcARwgeBmhO4Ny9Kozxf8LbnRe2ivFQGONXgEPArcDcqNu4InIC0ASqIpIyzOwGYL67X9KFuouAz7v7xXEPTESkj6jHS0SSxsyGm9nMcK6sMwl6e36b7LhEROJFg+tFJJmygJ8B44CjwMPAT5MZkIhIPOlWo4iIiEiC6FajiIiISIIo8RIRERFJkH4xxmvIkCE+duzYZIchIiIi0qn169cfdPeh7W3rF4nX2LFjWbduXbLDEBEREemUmbV+/Vcz3WoUERERSRAlXiIiIiIJosRLREREJEH6xRgvERER6TuRSISdO3dSU1OT7FD6tZycHEaNGkVmZmaX91HiJSIicpLZuXMn+fn5jB07FjNLdjj9krtz6NAhdu7cybhx47q8n241ioiInGRqamooLi5W0tULZkZxcXG3ew2VeImIiJyElHT1Xk/aUImXiIiIJMUjjzyCmfHGG280l+3evZt58+YlNI433niDGTNmkJ2dzb333hvXcynxEhERkaRYvnw5F198McuXL28uGzFiBKtWrWpTt76+Pm5xFBUV8aMf/YivfvWrcTtHEyVeIiIiknAVFRWsXbuW++67j4cffri5fMeOHUyZMgWApUuXcuWVV3LppZcyZ84cqqqq+MQnPsHkyZO5+uqref/739/8ZpsbbriBkpISzjrrLO64447m440dO5Zbb72VqVOncsEFF7B169Y2sQwbNozzzz+/W08n9pSeahQREZGEe/TRR7niiiuYMGECxcXFrF+/nvPOO69NvVdeeYUNGzZQVFTEvffeS2FhIaWlpWzatIlzzjmnud6dd95JUVERDQ0NzJkzhw0bNjBt2jQACgoK2LhxI8uWLePmm2/m8ccfT9RltqHES0RE5CT2rd+9Tunusj495uQRg7jjY2d1WGf58uXcdNNNAMyfP5/ly5e3m3hdfvnlFBUVAbB27drmfaZMmdKcWAGsXLmSJUuWUF9fz549eygtLW3evmDBgublLbfc0vsL7IW4JV5mNhpYBpwCOLDE3X9oZvcAHwPqgG3AZ939aLziEBERkdRy+PBhnnnmGTZu3IiZ0dDQgJlxzz33tKmbl5fX6fG2b9/Ovffey8svv0xhYSGLFi1qMc1D9NOHyX6aM549XvXAV9z9FTPLB9ab2VPAU8DX3b3ezO4Cvg58LY5xiIiISAyd9UzFw6pVq7j++uv52c9+1lx2ySWX8MILLzBmzJiY+82cOZOVK1cye/ZsSktL2bhxIwBlZWXk5eVRUFDAvn37WL16NbNmzWreb8WKFdx2222sWLGCGTNmxO26uiJuiZe77wH2hJ/LzWwzMNLd10RV+wuQ2GdGRUREJKmWL1/O177Wss/l2muvbbc82o033sjChQuZPHkyEydO5KyzzqKgoIDx48czffp0Jk6cyOjRo5k5c2aL/Y4cOcK0adPIzs5u8QRlk71791JSUkJZWRlpaWn84Ac/oLS0lEGDBvXNBUcxd+/zg7Y5idlY4HlgiruXRZX/Dljh7r/saP+SkhJvempBREREemfz5s1MmjQp2WF0W0NDA5FIhJycHLZt28Zll13Gm2++SVZWVsx9xo4dy7p16xgyZEhcYmqvLc1svbuXtFc/7oPrzWwg8Gvg5lZJ1+0EtyMfjLHfYmAx0GG3o4iIiJwcqqqqmD17NpFIBHfnpz/9aYdJVyqKa+JlZpkESdeD7v6bqPJFwFxgjsfocnP3JcASCHq84hmniIiIpL78/Hy6ewdsx44d8Qmmh+L5VKMB9wGb3f37UeVXALcCl7h7VbzOLyIiIpJq4tnjNRO4HthoZq+GZf8C/AjIBp4KH+n8i7v/YxzjEBEREUkJ8XyqcS3Q3mQZT8TrnCIiIiKpTO9qFBEREUkQJV4iIiKSFI888ghmxhtvvNFctnv3bubNS+wUnw8++CDTpk1j6tSpXHTRRbz22mtxO5cSLxEREUmK5cuXc/HFF7eY1HTEiBGsWrWqTd36+vq4xTFu3Diee+45Nm7cyDe+8Q0WL14ct3Mp8RIREZGEq6ioYO3atdx33308/PDDzeU7duxgypQpACxdupQrr7ySSy+9lDlz5lBVVcUnPvEJJk+ezNVXX8373//+5uklbrjhBkpKSjjrrLO44447mo83duxYbr31VqZOncoFF1zA1q1b28Ry0UUXUVhYCMCFF17Izp0743bdcZ9AVURERKS1Rx99lCuuuIIJEyZQXFzM+vXrOe+889rUe+WVV9iwYQNFRUXce++9FBYWUlpayqZNmzjnnHOa6915550UFRXR0NDAnDlz2LBhA9OmTQOgoKCAjRs3smzZMm6++WYef/zxmHHdd999fOQjH+nz622ixEtERORktvo22Luxb4956lT4yL91WGX58uXcdNNNAMyfP5/ly5e3m3hdfvnlFBUVAbB27drmfaZMmdKcWAGsXLmSJUuWUF9fz549eygtLW3evmDBgublLbfcEjOmZ599lvvuu4+1a9d242K7R4mXiIiIJNThw4d55pln2LhxI2ZGQ0MDZsY999zTpm5eXl6nx9u+fTv33nsvL7/8MoWFhSxatIiamprm7eG8oW0+R9uwYQOf//znWb16NcXFxT24qq5R4iUiInIy66RnKh5WrVrF9ddfz89+9rPmsksuuYQXXnihw/czz5w5k5UrVzJ79mxKS0vZuDHoqSsrKyMvL4+CggL27dvH6tWrmTVrVvN+K1as4LbbbmPFihXMmDGjzXHfffddrrnmGh544AEmTJjQdxfaDiVeIiIiklDLly/na1/7Wouya6+9tt3yaDfeeCMLFy5k8uTJTJw4kbPOOouCggLGjx/P9OnTmThxIqNHj2bmzJkt9jty5AjTpk0jOzu7xROUTb797W9z6NAhbrzxRgAyMjK6/U7IrrIY76hOKSUlJR6vBhARETnZbN68mUmTJiU7jG5raGggEomQk5PDtm3buOyyy3jzzTfJysqKuc/YsWNZt24dQ4YMiUtM7bWlma1395L26qvHS0RERPqFqqoqZs+eTSQSwd356U9/2mHSlYqUeImIiEi/kJ+f3+1bgDt27IhPMD2kCVRFREREEkSJl4iIiEiCKPESERERSRAlXiIiIiIJosRLREREEm7nzp1cddVVjB8/njPOOIObbrqJuro6li5dyj/90z+1u89FF13Uo3M98sgjlJaWNq9/85vf5Omnn+7RsXpLiZeIiIgklLtzzTXX8PGPf5wtW7bw1ltvUVFRwe23397hfi+++GKPztc68fr2t7/NZZdd1qNj9ZYSLxEREUmoZ555hpycHD772c8CkJ6ezr//+79z//33U1VVxXvvvcesWbMYP3483/rWt5r3GzhwYPPne+65h/PPP59p06Zxxx13NJcvW7aMadOmcfbZZ3P99dfz4osv8thjj/HP//zPnHPOOWzbto1FixaxatUqnnzySa677rrmff/0pz8xd+5cANasWcOMGTM499xzue6666ioqOiTa9c8XiIiIpJQr7/+Ouedd16LskGDBjFmzBjq6+t56aWX2LRpE7m5uZx//vl89KMfpaTk+ETwa9asYcuWLbz00ku4O1deeSXPP/88xcXFfOc73+HFF19kyJAhHD58mKKiIq688krmzp3LvHnzWpzzsssuY/HixVRWVpKXl8eKFSuYP38+Bw8e5Dvf+Q5PP/00eXl53HXXXXz/+9/nm9/8Zq+vXYmXiIjISeyul+7ijcNv9OkxJxZN5GsXxH7nYmcuv/xyiouLAbjmmmtYu3Ztm8RrzZo1TJ8+HYCKigq2bNnCa6+9xnXXXdf8eqCioqIOz5ORkcEVV1zB7373O+bNm8fvf/977r77bp577jlKS0ub3/lYV1fX7su1e0KJl4iIiCTU5MmTWbVqVYuysrIy3n33XTIyMjCzFttar7s7X//61/nCF77QovzHP/5xt2OZP38+P/nJTygqKqKkpIT8/Hzcncsvv7zdF2r3lhIvERGRk1hveqZ6as6cOdx2220sW7aMz3zmMzQ0NPCVr3yFRYsWkZuby1NPPcXhw4cZMGAAjzzyCPfff3+L/T/84Q/zjW98g09/+tMMHDiQXbt2kZmZyaWXXsrVV1/Nl7/8ZYqLi5tvNebn51NeXt5uLJdccgn/8A//wM9//nPmz58PwIUXXsgXv/hFtm7dyvve9z4qKyvZtWsXEyZM6PW1a3C9iIiIJJSZ8dvf/pZf/epXjB8/ngkTJpCTk8N3v/tdAC644AKuvfZapk2bxrXXXtt8m7Gp5+tDH/oQn/rUp5gxYwZTp05l3rx5lJeXc9ZZZ3H77bdzySWXcPbZZ/PlL38ZCHq17rnnHqZPn862bdtaxJKens7cuXNZvXp188D6oUOHsnTpUhYsWMC0adOYMWMGb7zRN7djzd375EDxVFJS4t19KaaIiIi0b/PmzUyaNCnZYXTLoUOHOPfcc3nnnXeSHUoL7bWlma1395L26qvHS0RERFLa7t27mTFjBl/96leTHUqvaYyXiIiIpLQRI0bw1ltvJTuMPqEeLxEREZEEiVviZWajzexZMys1s9fN7KawvMjMnjKzLeGyMF4xiIiISPv6wxjvVNeTNoxnj1c98BV3nwxcCHzRzCYDtwF/dPfxwB/DdREREUmQnJwcDh06pOSrF9ydQ4cOkZOT06394jbGy933AHvCz+VmthkYCVwFzAqr/QL4E5D4SUREREROUqNGjWLnzp0cOHAg2aH0azk5OYwaNapb+yRkcL2ZjQWmA38FTgmTMoC9wCkx9lkMLAYYM2ZMAqIUERE5OWRmZjJu3Lhkh3FSivvgejMbCPwauNndy6K3edDH2W4/p7svcfcSdy8ZOnRovMMUERERibu4Jl5mlkmQdD3o7r8Ji/eZ2fBw+3BgfzxjEBEREUkV8Xyq0YD7gM3u/v2oTY8BC8PPC4FH4xWDiIiISCqJ5xivmcD1wEYzezUs+xfg34CVZvY54B3gE3GMQURERCRlxPOpxrWAxdg8J17nFREREUlVmrleREREJEGUeImIiIgkiBIvERERkQRR4iUiIiKSIEq8RERERBJEiZeIiIhIgijxEhEREUkQJV4iIiIiCaLES0RERCRBlHiJiIiIJIgSLxEREZEEUeIlIiIikiBKvEREREQSRImXiIiISIIo8eqpxgaorUh2FCIiItKPKPHqqeXz4Xtnwv7NyY5ERERE+gklXj1x4E3YsgbqKuClnyc7GhEREeknlHj1xLt/CZZDJ8K2Z5Ibi4iIiPQbSrx6Ytc6yBkMZy+AI9uh8mCyIxIREZF+QIlXTxzcAsMmw6iSYH3335Ibj4iIiPQLSrx64ui7UHgaDJ0UrB98K7nxiIiISL+gxKu76uugbDcMPg3yimFAkRIvERER6RIlXt117D3AYfCYYH3IhODWo4iIiEgnlHh117H3gmVT4lX8Pji0LXnxiIiISL+hxKu7Kg4Ey4GnBMvBo6FiL9TXJi8mERER6ReUeHVXVTh1RN6QYFkwOlge25mceERERKTfUOLVXZUHwdKDebwg6PGC47cgRURERGKIW+JlZveb2X4z2xRVdo6Z/cXMXjWzdWZ2QbzOHzeVByC3GNLCpmvq8TqqxEtEREQ6Fs8er6XAFa3K7ga+5e7nAN8M1/uXqkPHbzMCDBoJmG41ioiISKfilni5+/PA4dbFwKDwcwGwO17nj5vKg0GPV5OMLMg/VbcaRUREpFMZCT7fzcAfzOxegqTvogSfv/eqDsKpU1uWFYxS4iUiIiKdSvTg+huAW9x9NHALcF+sima2OBwHtu7AgQMJC7BTlQchd0jLsvxToXxfcuIRERGRfiPRiddC4Dfh518BMQfXu/sSdy9x95KhQ4cmJLhONTZCzTEYUNiyPH94MJeXiIiISAcSnXjtBi4JP18K9K937dRVAA7Z+S3L808NErK6qqSEJSIiIv1D3MZ4mdlyYBYwxMx2AncA/wv4oZllADXA4nidPy5qy4JlzqCW5QNPDZYVe6Ho9MTGJCIiIv1G3BIvd18QY9N58Tpn3NWEiVd2q8QrP0y8yvcp8RIREZGYNHN9d8Tq8cofHizL9yQ2HhEREelXlHh1R3OPV0GLYg9fmP3Mug3U1jckOioRERHpJ5R4dUeMHq8/vF1HrWfw1tatLHnu7SQEJiIiIv2BEq/uqDkWLFuN8Xp43XscSiti6qAqHvjLOzQ0ehKCExERkVSnxKs72unxqok08Odth2jIHcaZeZXsL69l3Y7Wb0oSERERUeLVPTVlYOmQmdtc9Ld3j1Jb30h24UgKGw+TZvDitkNJDFJERERSlRKv7qgtD3q7zJqLSvcEvWADh44mvWIfZ40o4M9vK/ESERGRtpR4dUdtWZvxXZv3lDFkYDa5RSOh9hgzxwzg1feOEmloTFKQIiIikqqUeHVHTfuJ1+QRg5onUT1vSB119Y1s3V+RjAhFREQkhSnx6o5IJWTlNa+6O28fqGT8sIHNidekvOB9jZt2HUtKiCIiIpK6OnxlkJmNAuYDHwBGANXAJuD3wGp3P7nup0WqIWtg8+rBijqqIw2MKcptnr1+RPpR8rLyeH13GdclK04RERFJSTF7vMzsv4H7gTrgLmABcCPwNHAFsNbMPpiIIFNGpAYyBzSvvns46N0aXTSguccrrWIvk0cMYqN6vERERKSVjnq8vufum9op3wT8xsyygDHxCStFRapaJF7vhYnXmKJcyBkIGTlQvoezRsxi5br3aGx00tIs1tFERETkJBOzxytG0hW9vc7dt/Z9SCksUg0ZbROvUYW5wRQT+adC+V4mDc+nqq6huUdMREREBLowuN7M5prZ38zssJmVmVm5mZUlIriUU1/d5lbjsPxscjLTg4KBTYlX8OTj5j0nZzOJiIhI+7ryVOMPgIVAsbsPcvd8dx/UyT4npkjLxGtvWQ3DBx9fb+rxmnBKPml2fHJVEREREeha4vUesMndT+43Pzc2Qn3LwfX7y2oZlp99vE7+cCjfS05mOmcMHageLxEREWmhw+kkQrcCT5jZc0BtU6G7fz9uUaWi+ppgGZV47Suv4fxxhcfr5J8KdeVQW86k4YNY/86RBAcpIiIiqawrPV53AlVADpAf9XVyaUq8wsH1tfUNHK2KMCw/53idcC4vyvcxafggdh2t5lhVpOPjlu+FVx+Cd16Ek7xTUURE5ETXlR6vEe4+Je6RpLpI+IRi2ON1oDzo/DtlUPStxmAuLyr2Mmn4BCAY5zXjjOL2j7n9BVi+IOglA5j8cbhmCWRkt19fRERE+rWu9Hg9YWYfinskqS7S8lbjvrIg8WrZ4xUmXuXBJKrQwZONFfvhV4tg0AhY/Ce49BtQ+gj89gvq+RIRETlBdaXH6wbgq2ZWC0QAA/yke7KxTY9XkIgNa6/Hq3wPw/JzGDIwK3bi9cL3oOYoLHochk2CEdMhLR2e/lcYfSFc+I/xuQ4RERFJmk57vMLpI9LcfcBJPZ1Eq8H1+8vb6fHKHgSZucG4LWDS8EFs3ttO4lV5ENYvhWnzg6SrycybiZzxIfypb8Duv8XjKkRERCSJunKrETObZmZXmtk1TV/xDizlNPV4ZTTdaqwhPc0ozss6Xqd59vo9QJB4vbW3gkhDq3eJb/xVkMjN+GJzUWOjc+cTm7lw8zz21A9k333z2bT1nbhekoiIiCRWp7cazex+YBrwOtCUQTjwmzjGlVDbj21n86HNHVfa8yrk5cKhV6HxGK8efofCoeU8uaNVvfxBUP42vP0EkZzDNOa9w7INVQwviJpoddMyGDkRKnfA28EBnti4hydf38uF04pYkr6I83YsofTXC/jeuIXMPXsEA5pmx29HeW2Ene+9S/WuTWSVv0d2QzmZjXU0WAaRtGzqMwbSmJ0P2YNIG1BARu5gMgYUkJ5bAGmZ4VEcb/4U/MfdcW/EGurB67HGBjBodAvrGZjR6AQ3oDFwaDRoyumdsG6rYWvR08K1GdHW1SFu7bwG0wCzYINZu1V6xQmSZCe4ptjT27U9cxhWc1PFYtGbrcWi+RhB2fGVpu9eUzje/J9g0aK1uxyydbjZOtgYvWqttnl0CFGfPeq/7YVpLf8TfcQYnztdbfNzadb0c9P0v1ZtR+t4W8bcOtim7+Xx74G3Wy/WdTdtjo6lzXHCSmkcP1aLeq21vYB2a7b3Y5IGWNrxeFrWO37GNr8W7Xwzm34/W9Rvc/Ftj9GiSlgnzTo+TtP3tLltuvI3xmOutNs2TX97ov/uxDpNYwc/ss2/8z1on3Z/djo6TtR2utM+3fzZsabrsfZ+dlr9XnV0sD5oG8f5uwkXc86o8SSLdTYvqpmVuvvkBMXTrpKSEl+3bl3cjv9A6QPc/fLdcTu+iIiIpIYb8/+OG665K67nMLP17l7S3rauDK7/s5lNdvfSPo4rZVx5xpVcPPLijiuV/g6e+RZ85lEYNJIbH3yF/JxM7rp2ast6L/w7vP4b+MLz1DfC3B+/wJXnjOTGWWcE219bCS/cDdc/AgWjALhr9Rs8++Z+Hvjc+xnaNBO+ezAAf8PDbMw8h3sqr2CnDyWDeqbadi5Le4VLM/5GNhEO572PunFzKJ56OZlFp7XtYgCor6Wx8hAVR/ZTU3aASPkBvPIQmbVHyYiU42npuGUEy7RMsEw8PRNPy6AxPTMsC35cgn+9OOaOBV1jwTL8N8Xx/rDGsMyiytsXXaO98GPxqK6dWP+I9KaK7Ry4Oz1iTVcSfFnQs9ZOr1q7/5TxtlujY29btW1k7V5f1HU194q1c3qL6inqtNOog6iiT9sVXe1cO/6tierH6+Rf9C3PcnzURHd+ftqcp6lXxb1Fz1xH8bZ3zqbvbXQ7NX0Pouse/xmwmHG3Plbr40RvP/7bFvt8ZrGvq41WbRPdw9D22mLs2qrXtumSY/VoWqs60cdp78e2Ta9ljOO0/hPQ5vfAaXOClr26nf9gNfWCd9T50qa8dQ9ZjJ/zDtunVRt3dKwW1x3dExTVPjH/RrRuv47E6Wenvb893f3ZGTUtuaOlupJ4LSNIvvYSzFxvgLv7tI52Cm9RzgX2R88DZmb/G/gi0AD83t1v7WnwfaUgu4CC7IKOK2XkQqQeBo+H/FOorNjBmUWDGVcwrmW9wvdBdTnkDIGcQZw3Yj+vvV3LuKvCerteg7yRMOYDAGzZV86a1xr53MXnc8GoiS2PNfcnUDyJcX/8NlfaOiKZ+aQ3VJPm9TSmD8CmfhI7/3OMG97ht+K4YmBM16qKiIhI3+tK4nUfcD2wkeNjvLpiKfATgsQNADObDVwFnO3utWY2rBvHS65IdbAMn2o8UllHYfTA+ibNs9fvgZxBzD5zGHc+sZndR6sZkZ8JO9bCWR9vrn7vmjfJzcrghlnva3ssM7jon2DqPHj9ETIPb4PsfBgxnbTTZwWfRUREpN/oSuJ1wN0f6+6B3f15MxvbqvgG4N/cvTass7+7x02aqAlU6+obKa+tpyi3ncRr8OhgefQ9GHoms84cyp1PbOaZN/bz96MOQO0xOP0SAF559wh/eH0ft1w2gaL2krgm+adqXi8REZETQFemk/ibmT1kZgv6YDqJCcAHzOyvZvacmZ3fw+MkXqQK0jIgPZOjVXUA7fd4DT4tWB7dAcD7hg3kfcMG8utXdsLbfwq2jbuExkbnu7/fzJCBWXz+A+PaHkdEREROOF3p8RpAMLYr+rVBTs+mk8gAioALgfOBlWZ2urfzaKWZLQYWA4wZkwIDk+prgslRgcNh4tVuL9XAUyA9G468AwSPFy+4YAz/9/FSytOfJv+UqZA3hKVrt7PunSPcPW8aedld+TaIiIhIf9fp/+O7+2f78Hw7gd+EidZLZtYIDAEOtHPeJcASCKaT6MMYeiZSBRnBLPWHK8Mer/ZuNaalBbcbj77TXHRdySjuf+Z1cva8TN0Fi1n96i7ufGIzcyYO47rzRiUkfBEREUm+mLcazez/mFlRB9svNbO53TzfI8DscP8JQBZwsJvHSI5IddTA+ggQo8cLgtuNR44nXoNyMvnhxbVkUs8NL+Zz08Ovcs7owfxwwfTmyT5FRETkxNdRj9dG4HdmVgO8QtArlQOMB84Bnga+G2tnM1sOzAKGmNlO4A7gfuB+M9sE1AEL27vNmJKiEq/DlcF7GmMmXoWnwe5XWhSVNGyg0TI4/dzLmD3iFD55/mgy07v0xiYRERE5QcRMvNz9UeBRMxsPzASGA2XAL4HF7l7d0YHdfUGMTX/fw1iTq0XiFfR4Dc7NbL/u4NOg+gjUlEFO+D7xt/9E2ugLuP2aCxIRrYiIiKSgrozx2gJsSUAsqa2+pvkF2Ueq6hiUkxG7x6pwbLA8sh2Gnw1Vh2HPazDrtsTEKiIiIilJ97q6KlIV1eNV1/G8W8MmBct94VuWtj8POJw+K64hioiISGpT4tVVkZrjg+urYsxa36TojOAJyH2bgvWtT0FOAYxs932ZIiIicpJQ4tVVrXu82ptKokl6RtDrtXdj8EbPrX+E02cH5SIiInLS6jTxMrPTzex3ZnbQzPab2aNmdnoigksp9TWdv6cx2ojpsGs9vPM/wXsbJ3w4AUGKiIhIKutKj9dDwErgVGAE8CtgeTyDSkmRqubB9YerOhnjBTDhI1BXAQ9cE8x4P+ljCQhSREREUllXEq9cd3/A3evDr18SzOd1cgnHeFXV1VMTaWx/1vpop8+CIROgoRYu+hJk5yckTBEREUldXRl0tNrMbgMeJnhH4yeBJ5pmtXf3w3GMLzU0NgQJVOaA5tcFFeXFmMOrSUYWfPZJ2F8Kp81MQJAiIiKS6rqSeH0iXH6hVfl8gkTsxB/vFQnnis0c0Py6oE57vADyimHcB+IYmIiIiPQnXZlAdVwiAklp9TXBMjOXY9VNs9Z3IfESERERidKl+Q3MbAowmaixXe6+LF5BpZxIVbDMyOFodXCrsWBAJ7caRURERFrpNPEyszsIXnY9GXgC+AiwFjiJEq+mHq8BHKsKeryUeImIiEh3deWpxnnAHGCvu38WOBsoiGtUqaapxytzQNStRiVeIiIi0j1dSbyq3b0RqDezQcB+YHR8w0ox9S17vLIy0sjJTE9uTCIiItLvdGWM1zozGwz8HFgPVAB/jmdQKad5jFfQ46XbjCIiItITXXmq8cbw43+a2ZPAIHffEN+wUkz0GK/qCIOVeImIiEgPdGVw/bntlJ0BvOPu9XGJKtU0j/HK5WjVYfV4iYiISI905VbjT4FzgQ2AAVOA14ECM7vB3dfEMb7U0DyBag7HqiMMLzj53pgkIiIivdeVwfW7genuXuLu5wHTgbeBy4G74xlcymg1gWqBnmgUERGRHuhK4jXB3V9vWnH3UmCiu78dv7BSTNQEqhpcLyIiIj3VlVuNpWb2HwQvyYbgJdmlZpYNROIWWSoJB9fXp2VTUVvP4AF6XZCIiIh0X1d6vBYCW4Gbw6+3gUUESdfsOMWVWiJVkJ5FWZ0DUDCgS29aEhEREWmhwwzCzNKBJ9x9NvC9dqpUxCWqVFNfA5kDOFoVvqdRY7xERESkBzrs8XL3BqDRzE6uVwS1FqlqnjwV0K1GERER6ZGu3DOrADaa2VNAZVOhu38pblGlmkjY4xUmXoM0uF5ERER6oCuJ12/Cr5NXpAoycykLEy891SgiIiI90ZVXBv0iEYGktPqa5slTAQZrjJeIiIj0QMwxXma2MlxuNLMNrb86O7CZ3W9m+81sUzvbvmJmbmZDehd+gkSqw9cFqcdLREREeq6jHq+bwuXcHh57KfATYFl0oZmNBj4EvNvD4yZepApyh3CsOkJuVjqZ6V2ZhUNERESkpZgZhLvvCZfvRH8Bo4FbOzuwuz8PHG5n07+H+3vPQk6CcHD9seoIg9XbJSIiIj3UpZlAzWw68CngOmA7PRxsb2ZXAbvc/TUz68khkiNSFTzVWB7RE40iIiLSYzETLzObACwIvw4CKwALJ1PtNjPLBf6F4DZjV+ovBhYDjBkzpien7DvhBKpl1RENrBcREZEe62iw0hvApcBcd7/Y3X8MNPTiXGcA44DXzGwHMAp4xcxOba+yuy9x9xJ3Lxk6dGgvTtsHwsH1ekG2iIiI9EZHtxqvAeYDz5rZkwQvye7x/UF33wgMa1oPk68Sdz/Y02MmTKQaMnI4Wl1HwYCTexJ/ERER6bmOBtc/4u7zgYnAswQvyB5mZv9hZp3eLjSz5cCfgTPNbKeZfa6PYk6shnpojDT3eA3O1euCREREpGe6MoFqJfAQ8JCZFRIMsP8asKaT/RZ0sn1s18NMovpqACJpWdREGnWrUURERHqsWxNSufuRcOzVnHgFlHIiQeJVQzag9zSKiIhIz2km0M5EqgCobAwSLs3jJSIiIj2lxKszkRoAKsLES7caRUREpKeUeHUm7PGqaFDiJSIiIr2jxKsz9UGPV1l9eKtRE6iKiIhIDynx6kzY43WsPngAVD1eIiIi0lNKvDoTjvE6GknHDPJzlHiJiIhIz3TpJdkntXA6iaORDPKz00hP60cv9xYREZGUoh6vzoQTqB6qTaNA47tERESkF5R4dSbs8TpYm87gAXpdkIiIiPScEq/ONCdepoH1IiIi0itKvDoTJl4HqpV4iYiISO8o8epMpAoyBnCspkFjvERERKRXlHh1JlKFZ+VytCqiHi8RERHpFSVenYlU4xkDqG90JV4iIiLSK0q8OlNXSUP6AAAGK/ESERGRXlDi1ZlINfXpOYBeFyQiIiK9o8SrM5EqImlKvERERKT3lHh1JlJFrYWJl55qFBERkV5Q4tWZSDU1lg2ox0tERER6R4lXZ+oqqfHgVUGDc/XKIBEREek5JV6diVRTSTbpaUZeVnqyoxEREZF+TIlXZyLVVDRkUTAgEzNLdjQiIiLSjynx6og7RCqpaMzUHF4iIiLSa0q8OtJQB95IWUMmg5R4iYiISC8p8epIXSUAx+oz9USjiIiI9JoSr45EqgE4EslksObwEhERkV5S4tWRMPE6XJeuHi8RERHptbglXmZ2v5ntN7NNUWX3mNkbZrbBzH5rZoPjdf4+EQluNR6OpGtwvYiIiPRaPHu8lgJXtCp7Cpji7tOAt4Cvx/H8vRf2eFV5DgWaPFVERER6KW6Jl7s/DxxuVbbG3evD1b8Ao+J1/j4RDq6v9iz1eImIiEivJXOM1z8Aq2NtNLPFZrbOzNYdOHAggWFFCXu8asjWGC8RERHptaQkXmZ2O1APPBirjrsvcfcSdy8ZOnRo4oKLFqkCoIpsPdUoIiIivZaR6BOa2SJgLjDH3T3R5++WMPGq9iwlXiIiItJrCU28zOwK4FbgEnevSuS5eyS81VhNNgUDNLheREREeiee00ksB/4MnGlmO83sc8BPgHzgKTN71cz+M17n7xNNg+s1xktERET6QNx6vNx9QTvF98XrfHERqaaRNDKyssnK0FyzIiIi0jsJH+PVr0SqqEvLYXC2bjOKiIhI76kbpyORKmotW5OnioiISJ9Q4tWRukqqydHkqSIiItInlHh1pK6SSs/RwHoRERHpE0q8OlJXQbnriUYRERHpG0q8OuC1FZQ1ZDM4T4mXiIiI9J4Srw54bdDjVajB9SIiItIHlHh1oLG2girPoVCvCxIREZE+oMSrI5FKKslhsHq8REREpA8o8YrFnbS6IPHSrUYRERHpC0q8YmmoI83rqfQcijS4XkRERPqAEq9YwhdkV+lWo4iIiPQRJV6x1FUABGO8NI+XiIiI9AElXrHUBolXY2YeGelqJhEREek9ZRSxhLca07IHJjkQEREROVEo8YolvNWYkaPES0RERPqGEq9Ywh6vjAGDkhyIiIiInCiUeMUS9nhl5+YnORARERE5USjxiiVMvHLyCpIciIiIiJwolHjF0FATJF65A5V4iYiISN9Q4hVDTWUZAAPzdatRRERE+kZGsgNIVXVVZeDZDM7LSXYoIiIicoJQ4hVDpLqcenIozNWs9SIiItI3lHjF0FBTRpXnUqj3NIqIiEgf0RivWGqOUU4ug9XjJSIiIn1EiVcMabXHKFOPl4iIiPQhJV4xZNSVU2F55GalJzsUEREROUHELfEys/vNbL+ZbYoqKzKzp8xsS7gsjNf5eyurvpyajHzMLNmhiIiIyAkinj1eS4ErWpXdBvzR3ccDfwzXU1J2QwX1GXpPo4iIiPSduCVe7v48cLhV8VXAL8LPvwA+Hq/zd0dDfT1vvfLc8YJIDVleR0O2Jk8VERGRvpPoMV6nuPue8PNe4JQEn79dLy37F8Y9ejUHdr0dFNQGs9Z7tl4XJCIiIn0naYPr3d0Bj7XdzBab2TozW3fgwIG4xlJ83tVkWgO7X3s6KKg5BkDagMFxPa+IiIicXBKdeO0zs+EA4XJ/rIruvsTdS9y9ZOjQoXENauSE6UQ8nfq9m4NzVx8FID1XPV4iIiLSdxKdeD0GLAw/LwQeTfD525U3IId9FJFRsQuA6vJgaFrWwJR96FJERET6oXhOJ7Ec+DNwppntNLPPAf8GXG5mW4DLwvWUcDB9GHnVewGoPhp0xGUOjG9Pm4iIiJxc4vauRndfEGPTnHidszeOZg5jZF0pALXH9gGQMzglxv6LiIjICUIz14cqc4ZT1HAAGhuoL99PnaeTP7g42WGJiIjICUSJVyiSdwrpNELVIbziAIcooDBP72kUERGRvqPEK+QDg9uKXr4HqzrIIR9EcV52kqMSERGRE4kSr1Ba/qkAVB/eTUb1QY4wiMG5mUmOSkRERE4kSrxCGQXHE6+cusNUZhbqBdkiIiLSp5R4hXKLRgJQd2QngyIHqMrWE40iIiLSt5R4hQbl51PmuWTs30gGDdQMHJnskEREROQEo8QrVJSXRRXZ5BzZGhQUjE5uQCIiInLCUeIVKszNpM4zyK/cDsDAU05PckQiIiJyolHiFRqUk0kdwVOMdZ7OkNFnJjkiEREROdEo8QqlpRkNacGEqVt9FBNGatZ6ERER6VtKvKJkpTUCsC1rIkMGavJUERER6VtKvKJUZxQA0DDluiRHIiIiIieijGQHkEoGfPyH/HXve3xs1lXJDkVEREROQEq8ooybfB7jJp+X7DBERETkBKVbjSIiIiIJosRLREREJEGUeImIiIgkiBIvERERkQRR4iUiIiKSIEq8RERERBJEiZeIiIhIgijxEhEREUkQJV4iIiIiCaLES0RERCRBlHiJiIiIJIgSLxEREZEESUriZWa3mNnrZrbJzJabWU4y4hARERFJpIQnXmY2EvgSUOLuU4B0YH6i4xARERFJtGTdaswABphZBpAL7E5SHCIiIiIJk/DEy913AfcC7wJ7gGPuvibRcYiIiIgkWkaiT2hmhcBVwDjgKPArM/t7d/9lq3qLgcXhaoWZvRnn0IYAB+N8jpON2rRvqT37ntq076lN+5bas+8lok1Pi7XB3D3O5251QrPrgCvc/XPh+meAC939xoQG0jaude5ekswYTjRq076l9ux7atO+pzbtW2rPvpfsNk3GGK93gQvNLNfMDJgDbE5CHCIiIiIJlYwxXn8FVgGvABvDGJYkOg4RERGRREv4GC8Ad78DuCMZ5+6Akr++pzbtW2rPvqc27Xtq076l9ux7SW3ThI/xEhERETlZ6ZVBIiIiIgmixAswsyvM7E0z22pmtyU7nlRlZveb2X4z2xRVVmRmT5nZlnBZGJabmf0obNMNZnZu1D4Lw/pbzGxhMq4lVZjZaDN71sxKw9do3RSWq117wMxyzOwlM3stbM9vheXjzOyvYbutMLOssDw7XN8abh8bdayvh+VvmtmHk3RJKcHM0s3sb2b2eLiu9uwFM9thZhvN7FUzWxeW6Xe+F8xssJmtMrM3zGyzmc1I2TZ195P6i+CVRduA04Es4DVgcrLjSsUv4IPAucCmqLK7gdvCz7cBd4Wf/w5YDRhwIfDXsLwIeDtcFoafC5N9bUls0+HAueHnfOAtYLLatcftacDA8HMm8NewnVYC88Py/wRuCD/fCPxn+Hk+sCL8PDn8W5BNMOfgNiA92deXxHb9MvAQ8Hi4rvbsXXvuAIa0KtPvfO/a9BfA58PPWcDgVG1T9XjBBcBWd3/b3euAhwkmeJVW3P154HCr4qsIfuAJlx+PKl/mgb8Ag81sOPBh4Cl3P+zuR4CngCviHnyKcvc97v5K+LmcYGqVkahdeyRsl4pwNTP8cuBSgqepoW17NrXzKmCOmVlY/rC717r7dmArwd+Kk46ZjQI+CvxXuG6oPeNBv/M9ZGYFBB0D9wG4e527HyVF21SJV/B/cu9Fre8My6RrTnH3PeHnvcAp4edY7ar2jiG8LTOdoJdG7dpD4W2xV4H9BH84twFH3b0+rBLdNs3tFm4/BhSj9oz2A+BWoDFcL0bt2VsOrDGz9Ra8pQX0O98b44ADwH+Ht8T/y8zySNE2VeIlfcaDvlo9JtsDZjYQ+DVws7uXRW9Tu3aPuze4+znAKIJelYnJjaj/MrO5wH53X5/sWE4wF7v7ucBHgC+a2QejN+p3vtsyCIbB/Ie7TwcqCW4tNkulNlXiBbuA0VHro8Iy6Zp9YRct4XJ/WB6rXdXerZhZJkHS9aC7/yYsVrv2Unir4VlgBsGthKZ5C6Pbprndwu0FwCHUnk1mAlea2Q6CYRiXAj9E7dkr7r4rXO4HfkvwDwT9zvfcTmCnBxO0Q3Cb+1xStE2VeMHLwPjwKZ0sggGhjyU5pv7kMaDpyY+FwKNR5Z8Jnx65EDgWdvn+AfiQmRWGT5h8KCw7KYXjX+4DNrv796M2qV17wMyGmtng8PMA4HKCcXPPAvPCaq3bs6md5wHPhP8yfgyYHz6lNw4YD7yUkItIIe7+dXcf5e5jCf42PuPun0bt2WNmlmdm+U2fCX5XN6Hf+R5z973Ae2Z2Zlg0ByglVdu0r0fr98cvgicc3iIYC3J7suNJ1S9gObAHiBD8C+NzBOM3/ghsAZ4GisK6Bvy/sE03AiVRx/kHgsG1W4HPJvu6ktymFxN0f28AXg2//k7t2uP2nAb8LWzPTcA3w/LTCf6PfivwKyA7LM8J17eG20+POtbtYTu/CXwk2deW7C9gFsefalR79rwdTyd4wvM14PWm/8/R73yv2/UcYF34u/8IwVOJKdmmmrleREREJEF0q1FEREQkQZR4iYiIiCSIEi8RERGRBFHiJSIiIpIgSrxEREREEkSJl4iIiEiCKPESkX7DzIrN7NXwa6+Z7Qo/V5jZT+N0zpvN7DMdbJ9rZt+Ox7lF5MSjebxEpF8ys38FKtz93jieIwN4BTjXj78UunUdC+vMdPeqeMUiIicG9XiJSL9nZrPM7PHw87+a2S/M7AUze8fMrjGzu81so5k9Gb4bEzM7z8yeM7P1ZvaHpne6tXIp8EpT0mVmXzKzUjPbYGYPQ/PLd/8EzE3IxYpIv6bES0RORGcQJE1XAr8EnnX3qUA18NEw+foxMM/dzwPuB+5s5zgzgfVR67cB0919GvCPUeXrgA/0+VWIyAkno/MqIiL9zmp3j5jZRiAdeDIs3wiMBc4EpgBPBXcKSSd4D2lrwwlest1kA/CgmT1C8D64JvuBEX0XvoicqJR4iciJqBbA3RvNLOLHB7M2EvzdM+B1d5/RyXGqCV783OSjwAeBjwG3m9nU8DZkTlhXRKRDutUoIiejN4GhZjYDwMwyzeysduptBt4X1kkDRrv7s8DXgAJgYFhvArAp7lGLSL+nxEtETjruXgfMA+4ys9eAV4GL2qm6mqCHC4Lbkb8Mb1/+DfiRux8Nt80Gfh/PmEXkxKDpJEREOmBmvwVudfctMbafAjzk7nMSG5mI9EdKvEREOmBmZwKnuPvzMbafD0Tc/dWEBiYi/ZISLxEREZEE0RgvERERkQRR4iUiIiKSIEq8RERERBJEiZeIiIhIgijxEhEREUmQ/w9hXa4xqv9OqwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5.7 5.7 0.  0.  0.  0. ]\n",
            "Time predict 10.859762907028198\n",
            "Time model 66.00779438018799\n",
            "Episode reward -1158.638298745267\n",
            "[(0.0, 0.0), (2.9835613, 2.9888337), (5.9398537, 5.951804), (8.847622, 8.868879), (11.754317, 11.786979), (14.67781, 14.72349), (17.57825, 17.638233), (20.448956, 20.527943), (23.29652, 23.431814), (26.143925, 26.345177), (28.942831, 29.192045), (31.593737, 31.905125), (34.069874, 34.456566), (36.390366, 36.850197), (38.580166, 39.103264), (40.648792, 41.224358), (42.60255, 43.21743), (44.441635, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (45.0, 45.0), (39.566925, 45.0), (36.635715, 45.0), (37.792313, 45.0), (39.37043, 45.0), (38.720245, 45.0), (36.331745, 45.0), (35.624992, 45.0), (36.601288, 45.0), (36.310024, 45.0), (34.30453, 45.0), (33.27886, 45.0), (33.79067, 45.0), (34.217304, 45.0), (34.591103, 45.0), (35.286476, 45.0), (35.83698, 45.0), (35.44453, 45.0), (34.444008, 45.0), (34.175438, 45.0), (34.51214, 45.0), (34.14164, 45.0), (33.138954, 45.0), (32.57058, 45.0), (32.422184, 45.0), (31.985817, 45.0), (31.137474, 45.0), (30.383854, 45.0), (29.952887, 44.46195), (29.108448, 37.334827), (25.474854, 30.685587), (19.643894, 25.263067), (14.140451, 20.29396), (10.201599, 15.0727625), (9.39433, 10.928634), (10.187905, 10.713902), (9.440065, 13.242279), (7.537298, 12.355067), (6.6703625, 8.226863), (6.484017, 5.686182), (5.467365, 6.5160794), (4.126546, 6.788192), (3.6154416, 3.9967742), (3.2919965, 1.4328432), (2.1561265, 1.9369627), (0.94323075, 2.637848), (0.61162454, 0.44855493), (0.36251843, -2.214341), (-0.7510499, -2.040543), (-1.9770212, -0.99943584), (-2.3008075, -2.6540203), (-2.4052546, -5.351978), (-3.358391, -5.5925775), (-4.6037345, -4.4770784), (-5.03672, -5.7050147), (-5.0596485, -8.249904), (-5.7784376, -8.734304), (-6.902258, -7.5799026), (-7.3458652, -8.352196), (-7.234917, -10.688458), (-7.634214, -11.0884905), (-8.519165, -9.687551), (-8.907229, -10.145443), (-8.774165, -12.359087), (-8.955582, -12.267658), (-9.526533, -10.183006), (-9.865742, -10.444741), (-9.8619175, -12.74693), (-9.965355, -13.312074), (-10.298369, -11.483047), (-10.592217, -11.324878), (-10.800888, -13.526814), (-10.95541, -13.848327), (-11.121933, -12.004836), (-11.782319, -12.044808), (-12.540105, -14.232481), (-12.020286, -15.618385), (-10.600895, -14.353368), (-10.85991, -11.3608465), (-12.820378, -11.027281), (-13.888957, -13.257156), (-12.791479, -15.147301), (-11.31315, -14.680635), (-11.900846, -12.032421), (-13.692233, -11.627932), (-14.167884, -13.689037), (-12.561825, -14.336274), (-11.786413, -13.149407), (-13.193491, -12.914142), (-14.003883, -13.625046), (-12.640716, -12.646566), (-11.819157, -11.597497), (-13.073346, -12.35978), (-13.542897, -12.610696), (-12.0439005, -11.0717325), (-11.522125, -10.4072895), (-12.859819, -11.419105), (-12.991812, -11.400949), (-11.496392, -10.0069065), (-11.127711, -9.487257), (-12.286775, -10.251914), (-12.490824, -10.45035), (-11.121784, -9.322741), (-10.481368, -8.540143), (-11.425364, -9.147506), (-11.750403, -9.42463), (-10.451061, -8.247212), (-9.661505, -7.42201), (-10.516902, -8.235861), (-10.73848, -8.359488), (-9.313501, -6.761019), (-8.813025, -6.362784), (-9.8954735, -7.7009945), (-9.5478, -7.273242), (-7.657591, -5.0534263), (-7.6714115, -5.0778036), (-9.368905, -7.150163), (-8.853261, -6.781912), (-6.331308, -4.048833), (-6.0585675, -3.7096527), (-7.9934077, -5.816602), (-7.8496485, -5.487656), (-5.569918, -2.6794784), (-5.272034, -2.2549405), (-7.0532784, -4.2696247), (-7.0564756, -4.01184), (-5.0424128, -1.4048599), (-4.5969934, -0.8420697), (-6.040205, -2.3830473), (-5.9568734, -2.0410438), (-4.0714183, 0.22579335), (-3.6366305, 0.718173), (-4.903462, -0.62864584), (-4.912628, -0.5246568), (-3.3421986, 1.2658149), (-2.8125815, 1.5549103), (-3.7753127, 0.13527165), (-3.881743, 0.18848874), (-2.5875132, 1.9327345), (-2.037911, 2.196408), (-2.832905, 0.7837199), (-2.982832, 0.8208404), (-1.8544036, 2.510372), (-1.3244246, 2.7559032), (-2.043276, 1.3541932), (-2.216854, 1.3435849), (-1.184359, 2.9580543), (-0.6886922, 3.2020476), (-1.3778076, 1.8340049), (-1.555803, 1.7990946), (-0.5965773, 3.349737), (-0.14747499, 3.578091), (-0.82510597, 2.2245717), (-1.013271, 2.1303484), (-0.09023902, 3.5995135), (0.31716728, 3.8241568), (-0.38994694, 2.5024095), (-0.5797532, 2.4006002), (0.3237782, 3.797537), (0.68805414, 3.988766), (-0.1472711, 2.6913655), (-0.42557722, 2.589587), (0.6502193, 3.9760506), (1.0348673, 4.1587863), (-0.0364715, 2.8666139), (-0.28580675, 2.684832), (0.99181026, 3.955927), (1.236201, 4.159019), (-0.098035075, 2.930215), (-0.13482726, 2.7539098), (1.3405999, 3.8938165), (1.2960044, 4.0395947), (-0.25374958, 2.887734), (-0.081273064, 2.7616696), (1.5665878, 3.8021567), (1.3331372, 3.8570106), (-0.40899995, 2.7708142), (-0.16182162, 2.684307), (1.6653035, 3.6495998), (1.4153047, 3.6433542), (-0.52054554, 2.6229918), (-0.36824143, 2.5090628), (1.6098768, 3.394217), (1.4739041, 3.3987954), (-0.6002793, 2.4479346), (-0.5851559, 2.2366605), (1.4240623, 3.0054226), (1.3826375, 3.0602946), (-0.6963625, 2.1661155), (-0.7685831, 1.9310123), (1.2045614, 2.5304582), (1.1765246, 2.511046), (-0.9098012, 1.7168987), (-0.9587969, 1.3411998), (1.022156, 1.6136147), (0.93903416, 1.1933899), (-1.1547651, 0.181761), (-1.3323315, 0.7403045), (0.63309944, 1.9589549), (0.8719118, 1.3792783), (-1.090682, -0.43467104), (-1.4762512, -0.48799375), (0.13819952, 1.2345452), (0.40635, 0.96291965), (-1.1345893, -1.3016855), (-1.436527, -1.3868742), (-0.069436684, 0.73868775), (0.07537545, 0.61485296), (-1.3395861, -1.7635689), (-1.516566, -1.9303118), (-0.20775075, 0.16103183), (-0.15523745, 0.21985166), (-1.548993, -1.8962379), (-1.6563338, -2.218685), (-0.33971283, -0.45737362), (-0.33484384, -0.1858606), (-1.7455157, -1.9044157), (-1.8169173, -2.5274704), (-0.4859769, -1.2440562), (-0.51148874, -0.37822515), (-1.9282306, -1.4591801), (-2.0287888, -3.0192091), (-0.7361405, -2.5163448), (-0.72137827, -0.69599074), (-2.0613992, -0.86275476), (-2.2061882, -2.9828358), (-0.93462676, -3.2571228), (-0.84362066, -1.2517041), (-2.181878, -0.7654275), (-2.395956, -2.679347), (-1.1369976, -3.656513), (-0.94854, -2.10118), (-2.283438, -0.74399513), (-2.6084278, -1.9757353), (-1.4225326, -4.0299935), (-1.1007726, -3.4119987), (-2.307269, -1.0016105), (-2.7514684, -1.2247622), (-1.6862676, -3.789353), (-1.2482381, -3.921792), (-2.2916307, -1.4846727), (-2.8237596, -1.3312217), (-1.9392331, -3.7550294), (-1.4278202, -4.033772), (-2.2759156, -1.7356663), (-2.8724415, -1.4741155), (-2.1711671, -3.777569), (-1.587343, -4.1421723), (-2.2349536, -1.9363369), (-2.9023592, -1.5859281), (-2.399402, -3.7693045), (-1.7318423, -4.224399), (-2.1838264, -2.1453729), (-2.9316926, -1.6768427), (-2.612192, -3.7337062), (-1.8513122, -4.2782073), (-2.1179943, -2.2987828), (-2.9584596, -1.7259665), (-2.8171859, -3.6769364), (-1.9534893, -4.329196), (-2.035665, -2.452624), (-2.9703696, -1.7238195), (-3.0105252, -3.534309), (-2.0634432, -4.449931), (-1.9543293, -2.7880728), (-2.951639, -1.5908538), (-3.1881518, -3.0114257), (-2.237334, -4.7097955), (-1.9106393, -3.7332356), (-2.8529093, -1.4625617), (-3.3113484, -1.9171894), (-2.5169113, -4.4425535), (-1.9437854, -4.371987), (-2.6044757, -1.759681), (-3.289415, -1.595736), (-2.8470724, -4.0471234), (-2.0570052, -4.448683), (-2.3011818, -2.2131743), (-3.16498, -1.7057114), (-3.0539217, -3.8066595), (-2.116668, -4.4545856), (-2.1000743, -2.4888518), (-3.0724018, -1.6389909), (-3.1980102, -3.40895), (-2.2078013, -4.517066), (-1.9681, -2.9968948), (-2.9620514, -1.5392742), (-3.3035116, -2.7147062), (-2.353564, -4.5777965), (-1.8887882, -3.7935486), (-2.7632616, -1.3138669), (-3.3386118, -1.5378115), (-2.6536467, -4.0772076), (-1.927639, -4.2005067), (-2.3975914, -1.748709), (-3.2037082, -1.4439812), (-2.9077232, -3.7347543), (-1.9799569, -4.278098), (-2.0724397, -2.1899722), (-3.030718, -1.3684813), (-3.0693781, -3.1535747), (-2.0582433, -4.10639), (-1.8542968, -2.5112352), (-2.8436837, -1.2830893), (-3.1207552, -2.5799284), (-2.151873, -3.942674), (-1.7579923, -2.8591592), (-2.6701717, -1.3270681), (-3.120509, -2.15615), (-2.2431922, -3.6988978), (-1.6834867, -3.0501802), (-2.477811, -1.4048481), (-3.0810819, -1.828077), (-2.3327935, -3.403581), (-1.6245203, -3.159194), (-2.27111, -1.5203329), (-3.0165026, -1.5073657), (-2.4177804, -3.025994), (-1.5449873, -3.0294456), (-2.0390189, -1.586575), (-2.974157, -1.5145568), (-2.5140767, -2.7578464), (-1.4542418, -2.693639), (-1.8313413, -1.5875064), (-2.9518347, -1.7035888), (-2.6196244, -2.5519536), (-1.402225, -2.2349343), (-1.6214283, -1.5049583), (-2.870407, -1.9511929), (-2.6902106, -2.4308536), (-1.3753072, -1.7268044), (-1.4558668, -1.3877223), (-2.7876444, -2.3029182), (-2.7298126, -2.3989177), (-1.345726, -1.1781883), (-1.3671927, -1.3034183), (-2.7532651, -2.7781312), (-2.696404, -2.4720182), (-1.258491, -0.6848287), (-1.3224227, -0.9724933), (-2.7871873, -2.9712398), (-2.6810176, -2.7539124), (-1.1354964, -0.59625435), (-1.2249173, -0.6798495), (-2.815034, -2.8665924), (-2.7277927, -2.8457556), (-1.0699384, -0.64534694), (-1.1061658, -0.5989799), (-2.778124, -2.756742), (-2.7641058, -2.8104627), (-1.0662935, -0.664469), (-1.0332072, -0.6001066), (-2.71267, -2.7134767), (-2.7562008, -2.7682745), (-1.0755559, -0.6632213), (-1.0047017, -0.61294436), (-2.658812, -2.7040555), (-2.7256024, -2.7490826), (-1.0738683, -0.6656733), (-0.9941483, -0.6228617), (-2.624935, -2.6975055), (-2.6947758, -2.7426999), (-1.0633111, -0.68159133), (-0.98649746, -0.63640004), (-2.6038818, -2.6870997), (-2.6716695, -2.7397099), (-1.0519958, -0.7099074), (-0.9786619, -0.65829307), (-2.5883124, -2.675337), (-2.6558688, -2.7374694), (-1.0449547, -0.74779505), (-0.9721754, -0.6890049), (-2.5747557, -2.6644146), (-2.6449308, -2.7361712), (-1.0435487, -0.7935122), (-0.9687712, -0.7272059), (-2.5624554, -2.6545033), (-2.637073, -2.7360697), (-1.0473354, -0.84617144), (-0.9691383, -0.7713334), (-2.551578, -2.64459), (-2.6314912, -2.7369137), (-1.0555046, -0.9052584), (-0.97313154, -0.8200219), (-2.5422287, -2.633251), (-2.6279268, -2.7381792), (-1.067461, -0.97048515), (-0.98024696, -0.8720973), (-2.5342002, -2.6189506), (-2.6262844, -2.7393394), (-1.0828936, -1.0418187), (-0.98990476, -0.9264562), (-2.5270174, -2.6000366), (-2.6264694, -2.7400615), (-1.1017039, -1.1196157), (-1.0015314, -0.9819118), (-2.5200205, -2.574559), (-2.6283884, -2.7403734), (-1.123962, -1.2048578), (-1.0145297, -1.036991), (-2.5123901, -2.5399504), (-2.6320295, -2.7409122), (-1.1499326, -1.299551), (-1.0281764, -1.0895771), (-2.503087, -2.4924705), (-2.6376026, -2.7361705), (-1.1805855, -1.3990982), (-1.0424542, -1.1432031), (-2.4908164, -2.4365826), (-2.6355526, -2.7396886), (-1.2048314, -1.5185922), (-1.0606822, -1.1718457), (-2.4859824, -2.3401878), (-2.6458168, -2.7688076), (-1.2369589, -1.6832787), (-1.066979, -1.1773734), (-2.4697435, -2.1929417), (-2.6684966, -2.824006), (-1.2847099, -1.9094639), (-1.0778016, -1.1967223), (-2.453018, -2.0236928), (-2.688026, -2.8648748), (-1.3336778, -2.1504717), (-1.092254, -1.2220266), (-2.436042, -1.8366083), (-2.704506, -2.888371), (-1.3827906, -2.3914456), (-1.1076934, -1.2556721), (-2.416425, -1.6462206), (-2.71804, -2.8812268), (-1.4335036, -2.6052814), (-1.122811, -1.3205538), (-2.3892417, -1.4918469), (-2.7251153, -2.8350787), (-1.4870882, -2.7663503), (-1.1390067, -1.4073782), (-2.3524845, -1.3792009), (-2.7247477, -2.740574), (-1.5455638, -2.856555), (-1.147868, -1.5231057), (-2.2920563, -1.3047094), (-2.723763, -2.5976331), (-1.6260922, -2.9101293), (-1.1637019, -1.6811252), (-2.2169876, -1.2764108), (-2.71523, -2.4366238), (-1.7162422, -2.927621), (-1.1796641, -1.8497219), (-2.1346588, -1.2973014), (-2.711076, -2.2982051), (-1.8087878, -2.912724), (-1.1845399, -1.9920511), (-2.0408378, -1.3308715), (-2.7131343, -2.1776104), (-1.91096, -2.8813562), (-1.1781764, -2.1013427), (-1.929066, -1.3630646), (-2.721996, -2.0815256), (-2.0298061, -2.8433702), (-1.1591039, -2.1525435), (-1.7903421, -1.3661337), (-2.73887, -2.031537), (-2.1748316, -2.824128), (-1.1283209, -2.1214108), (-1.6169928, -1.3103213), (-2.7577615, -1.997986), (-2.3506474, -2.8114936), (-1.1003437, -2.0743377), (-1.4170403, -1.2460318), (-2.7714746, -1.9675771), (-2.5507038, -2.8014479), (-1.1161537, -2.0543995), (-1.2410332, -1.2108307), (-2.727101, -1.9297359), (-2.6907735, -2.7897809), (-1.1814859, -2.0779734), (-1.1388175, -1.2073314), (-2.6460457, -1.8824685), (-2.7550123, -2.7803032), (-1.2666259, -2.1321166), (-1.1019284, -1.2186818), (-2.559708, -1.8297137), (-2.7700758, -2.7768579), (-1.3474922, -2.200552), (-1.0986983, -1.2329843), (-2.483443, -1.7737286), (-2.7580628, -2.776741), (-1.4082557, -2.272434), (-1.1120611, -1.2465745), (-2.429202, -1.7179316), (-2.746117, -2.7755342), (-1.4587111, -2.336983), (-1.1233319, -1.2587699), (-2.3826556, -1.666955), (-2.736753, -2.7709506), (-1.5033059, -2.391331), (-1.122113, -1.2710721), (-2.327966, -1.6205864), (-2.73693, -2.7612789), (-1.5592862, -2.437306), (-1.1225268, -1.2853502), (-2.27046, -1.5825434), (-2.735879, -2.7489367), (-1.6179916, -2.4732075), (-1.1219022, -1.3002225), (-2.2078242, -1.5526966), (-2.735636, -2.7353446), (-1.6822199, -2.5001266), (-1.1184729, -1.3200356), (-2.1363418, -1.5350496), (-2.7370217, -2.7164006), (-1.7559044, -2.5139158), (-1.1111966, -1.3362767), (-2.051599, -1.5217721), (-2.7419932, -2.7013428), (-1.8443034, -2.525128), (-1.0979421, -1.3451939), (-1.9476445, -1.505701), (-2.7527673, -2.6940074), (-1.9543628, -2.541911), (-1.0753754, -1.306519), (-1.817051, -1.4420583), (-2.7763627, -2.7474916), (-2.095644, -2.6318755), (-1.0409652, -1.2251524), (-1.6535527, -1.3070837), (-2.8105986, -2.7974453), (-2.274615, -2.755521), (-1.0032059, -1.1606863), (-1.4526985, -1.1467059), (-2.8432586, -2.7680175), (-2.4935107, -2.8322306), (-0.98931456, -1.165359), (-1.2335035, -1.049029), (-2.8223128, -2.6896076), (-2.6840696, -2.855876), (-1.0558262, -1.2357829), (-1.1053101, -1.0475721), (-2.7413368, -2.6194887), (-2.7650144, -2.843764), (-1.1452568, -1.319674), (-1.0693461, -1.0942417), (-2.6608438, -2.5742579), (-2.775874, -2.8211007), (-1.2196687, -1.3900208), (-1.0806172, -1.150464), (-2.603534, -2.5481236), (-2.7598672, -2.7990813), (-1.2698635, -1.4424108), (-1.1056534, -1.2000214), (-2.5693245, -2.5323675), (-2.7393692, -2.7811673), (-1.30043, -1.4813823), (-1.1304554, -1.2392051), (-2.5516052, -2.5207884), (-2.7222588, -2.767975), (-1.3180753, -1.5125167), (-1.1506457, -1.2576908), (-2.544529, -2.4977455), (-2.7119079, -2.7641609), (-1.3287439, -1.548144), (-1.163742, -1.2758039), (-2.5409057, -2.476122), (-2.7071376, -2.7638752), (-1.3374876, -1.5869441), (-1.1716739, -1.2875637), (-2.5374403, -2.4470415), (-2.7068744, -2.7672708), (-1.347258, -1.6321441), (-1.1753838, -1.2934564), (-2.5315878, -2.4080994), (-2.7101521, -2.7750525), (-1.3600799, -1.6872357), (-1.1755428, -1.292917), (-2.5214865, -2.3558488), (-2.7164545, -2.7889066), (-1.3775929, -1.7570165), (-1.1722397, -1.2839057), (-2.5053651, -2.2845638), (-2.7260144, -2.812053), (-1.401713, -1.8492612), (-1.164654, -1.2623296), (-2.4807465, -2.1841252), (-2.7332468, -2.8498523), (-1.4270769, -1.9782642), (-1.1659629, -1.2514071), (-2.46427, -2.0719376), (-2.7416108, -2.8766544), (-1.4532355, -2.1130624), (-1.1666776, -1.2488838), (-2.445988, -1.9542651), (-2.7490883, -2.891132), (-1.4807477, -2.2459562), (-1.1671038, -1.2536728), (-2.4242141, -1.837025), (-2.7549102, -2.8924341), (-1.5105683, -2.36868), (-1.1577246, -1.2665508), (-2.3854153, -1.726106), (-2.7655325, -2.8778121), (-1.5577061, -2.4745862), (-1.1533005, -1.2894765), (-2.3374782, -1.6329334), (-2.769638, -2.849036), (-1.6108277, -2.5540113), (-1.1509118, -1.3259006), (-2.2797892, -1.5690302), (-2.7687697, -2.8042152), (-1.6705718, -2.5980365), (-1.1487834, -1.3639958), (-2.2112231, -1.52837), (-2.765772, -2.7571635), (-1.7390438, -2.6192658), (-1.1444114, -1.399743), (-2.129499, -1.5042213), (-2.7636082, -2.71262), (-1.8199891, -2.6264637), (-1.1353666, -1.4307666), (-2.0303218, -1.4891355), (-2.7650118, -2.6869812), (-1.918395, -2.642695), (-1.1188054, -1.4299574), (-1.9093481, -1.449195), (-2.7756267, -2.6783557), (-2.0424626, -2.671702), (-1.090148, -1.381956), (-1.7571201, -1.3640743), (-2.7991168, -2.713463), (-2.2014806, -2.7502193), (-1.0510645, -1.304077), (-1.5684911, -1.2331979), (-2.8240128, -2.714084), (-2.3932414, -2.812899), (-1.020775, -1.2602377), (-1.3546665, -1.1216273), (-2.8345435, -2.6706853), (-2.6031816, -2.8436275), (-1.0469596, -1.2797596), (-1.1803603, -1.0814884), (-2.7777212, -2.610167), (-2.7302155, -2.8409019), (-1.1201553, -1.3390534), (-1.0979048, -1.1012878), (-2.6961048, -2.5610776), (-2.7716856, -2.8217235), (-1.198026, -1.4025228), (-1.084036, -1.1457057), (-2.6262012, -2.529601), (-2.766827, -2.7992618), (-1.2569367, -1.4536978), (-1.0991619, -1.1914124), (-2.5787694, -2.5104403), (-2.7472692, -2.7789462), (-1.2948557, -1.4912332), (-1.1210247, -1.229775), (-2.5507212, -2.4980073), (-2.727094, -2.7561572), (-1.3173008, -1.5116432), (-1.1419368, -1.2548592), (-2.5367982, -2.4857485), (-2.7109034, -2.7498841), (-1.3285639, -1.5391935), (-1.1570129, -1.2724241), (-2.5307038, -2.4704382), (-2.700975, -2.7469134), (-1.3350066, -1.5673747), (-1.1665475, -1.2837055), (-2.5276406, -2.4500196), (-2.696444, -2.7473319), (-1.3404652, -1.5994319), (-1.1714609, -1.2897332), (-2.5243373, -2.422582), (-2.6962104, -2.7513154), (-1.3475217, -1.6382878), (-1.1726962, -1.29091), (-2.5185628, -2.385887), (-2.699346, -2.7595105), (-1.3579625, -1.6871694), (-1.1708975, -1.2867002), (-2.5086648, -2.3366928), (-2.7053685, -2.7734323), (-1.3732595, -1.7505275), (-1.166151, -1.2752441), (-2.4930322, -2.2696555), (-2.7071762, -2.7959757), (-1.386216, -1.8364108), (-1.1624712, -1.2502633), (-2.4790223, -2.1722937), (-2.7189846, -2.8345554), (-1.4090619, -1.9581922), (-1.1594565, -1.2376947), (-2.4634142, -2.0659585), (-2.729479, -2.860257), (-1.4337859, -2.085235), (-1.1576521, -1.2343287), (-2.4451244, -1.9540561), (-2.7379656, -2.8746195), (-1.46072, -2.2121148), (-1.1568426, -1.2383286), (-2.4232626, -1.8417597), (-2.7442253, -2.8773105), (-1.4902163, -2.331327), (-1.1566297, -1.2498513), (-2.3968391, -1.7366208), (-2.7480009, -2.8668356), (-1.5227242, -2.4337642), (-1.1475364, -1.269437), (-2.35327, -1.6443379), (-2.7559729, -2.8419347), (-1.5725012, -2.5149097), (-1.1432773, -1.3028413), (-2.2999, -1.578638), (-2.757163, -2.8016372), (-1.6287572, -2.5635526), (-1.1411716, -1.3388163), (-2.2368016, -1.5352782), (-2.7549477, -2.7584934), (-1.6924372, -2.589779), (-1.1382657, -1.3733435), (-2.1626508, -1.508932), (-2.7524908, -2.7170851), (-1.7662508, -2.600994), (-1.1319782, -1.4037583), (-2.0741692, -1.4929578), (-2.7526357, -2.6937206), (-1.8541665, -2.61936), (-1.1199377, -1.4194866), (-1.967828, -1.4693161), (-2.758844, -2.6703954), (-1.9627799, -2.6317947), (-1.0978833, -1.3923504), (-1.8347085, -1.4094331), (-2.777263, -2.6900127), (-2.1023674, -2.6870615), (-1.0633513, -1.3253553), (-1.66711, -1.3012757), (-2.8054655, -2.7237685), (-2.2784703, -2.7721643), (-1.0249162, -1.2555625), (-1.464301, -1.1678861), (-2.834165, -2.7028599), (-2.4922142, -2.8233964), (-1.0087901, -1.240317), (-1.2465559, -1.0815916), (-2.8133063, -2.6425617), (-2.6780212, -2.8396142), (-1.0699528, -1.289638), (-1.1173915, -1.0764388), (-2.7344146, -2.5846305), (-2.758323, -2.8279395), (-1.1542794, -1.3583069), (-1.0774008, -1.1148536), (-2.6546288, -2.5449245), (-2.770217, -2.8069108), (-1.2256483, -1.4189867), (-1.0845413, -1.1637369), (-2.5963461, -2.520922), (-2.7547789, -2.785859), (-1.274278, -1.4647912), (-1.1064591, -1.2077329), (-2.5604258, -2.5062168), (-2.734071, -2.7681663), (-1.3038715, -1.498398), (-1.1291001, -1.2427684), (-2.5410087, -2.4958136), (-2.7162015, -2.7483633), (-1.3209224, -1.5172172), (-1.1487269, -1.2646974), (-2.532749, -2.4838016), (-2.7032487, -2.744858), (-1.3293631, -1.5449886), (-1.162044, -1.2795869), (-2.529917, -2.4674475), (-2.696527, -2.7445116), (-1.3349335, -1.5749989), (-1.1699132, -1.2887288), (-2.5282977, -2.4449036), (-2.6948307, -2.7473898), (-1.3409364, -1.6102383), (-1.1734542, -1.2929792), (-2.5251005, -2.4142997), (-2.6969695, -2.7538083), (-1.3495363, -1.6535678), (-1.1736145, -1.292409), (-2.5184436, -2.3731992), (-2.7020874, -2.7647154), (-1.3622512, -1.7084622), (-1.1708997, -1.2859869), (-2.5068486, -2.317784), (-2.7099068, -2.7821217), (-1.3804884, -1.7801231), (-1.165118, -1.2711514), (-2.4886332, -2.2415006), (-2.7137914, -2.8096623), (-1.39741, -1.8781719), (-1.1629217, -1.2500725), (-2.47401, -2.1406357), (-2.7248757, -2.8452692), (-1.421458, -2.0032558), (-1.1611793, -1.2403674), (-2.4578478, -2.0318878), (-2.7346401, -2.8681314), (-1.4470658, -2.1314824), (-1.1601939, -1.2390069), (-2.438836, -1.9189837), (-2.742541, -2.8794653), (-1.4748447, -2.2571077), (-1.1598047, -1.2450148), (-2.4159114, -1.8080019), (-2.748229, -2.878321), (-1.505288, -2.371842), (-1.1501979, -1.2589265), (-2.3759701, -1.7044189), (-2.758474, -2.8623712), (-1.5531013, -2.469847), (-1.1458285, -1.2822024), (-2.327108, -1.6185616), (-2.7622898, -2.833948), (-1.6068687, -2.5428252), (-1.143575, -1.3179153), (-2.2688308, -1.5606518), (-2.7614567, -2.7912328), (-1.6672399, -2.5828085), (-1.1415199, -1.3543416), (-2.1999958, -1.5240349), (-2.7589285, -2.7475545), (-1.7363979, -2.6023579), (-1.1370891, -1.3878927), (-2.1181831, -1.5022326), (-2.7576354, -2.7070246), (-1.8181872, -2.6095788), (-1.1278526, -1.4165508), (-2.01892, -1.4883522), (-2.760202, -2.6854885), (-1.9176935, -2.6267917), (-1.1108927, -1.4099593), (-1.8977246, -1.4454614), (-2.77267, -2.6846287), (-2.0433857, -2.661735), (-1.0816412, -1.3577427), (-1.7450101, -1.3563693), (-2.797969, -2.724859), (-2.2044373, -2.7469397), (-1.0424718, -1.2791686), (-1.5556381, -1.2209748), (-2.8236074, -2.7237587), (-2.3981137, -2.8127537), (-1.0136116, -1.2400186), (-1.3417945, -1.1091824), (-2.833161, -2.6767952), (-2.6083415, -2.8444018), (-1.0432462, -1.2657297), (-1.1705537, -1.0723222), (-2.7743733, -2.614025), (-2.7325065, -2.8410456), (-1.1187818, -1.3291162), (-1.0923111, -1.0955123), (-2.6926017, -2.564704), (-2.7714531, -2.8214734), (-1.1971815, -1.3945193), (-1.0813761, -1.1421782), (-2.6238635, -2.5338132), (-2.7655578, -2.7990007), (-1.2557963, -1.4465249), (-1.0980877, -1.1892422), (-2.5779479, -2.5154297), (-2.7459452, -2.778936), (-1.2932959, -1.4845315), (-1.120796, -1.2284954), (-2.5513158, -2.5037186), (-2.7261608, -2.7629426), (-1.3150972, -1.5126417), (-1.141316, -1.2478793), (-2.5384755, -2.4830415), (-2.7120366, -2.756147), (-1.3272732, -1.5429381), (-1.1556038, -1.2667408), (-2.531924, -2.4657817), (-2.703444, -2.7529893), (-1.3353139, -1.5742812), (-1.1648912, -1.2792046), (-2.527872, -2.4431686), (-2.6997318, -2.753492), (-1.342648, -1.6098813), (-1.169882, -1.2861185), (-2.5233314, -2.4130862), (-2.7000127, -2.7579203), (-1.3516722, -1.6528081), (-1.171303, -1.28764), (-2.516166, -2.372946), (-2.7035754, -2.7671301), (-1.3641566, -1.706684), (-1.1695733, -1.2828993), (-2.504653, -2.3188896), (-2.7101667, -2.7829995), (-1.3817248, -1.7767557), (-1.1645228, -1.269558), (-2.4869251, -2.2444632), (-2.7130916, -2.8089771), (-1.3976868, -1.8725126), (-1.1622972, -1.248221), (-2.472451, -2.1441247), (-2.7239838, -2.8448226), (-1.4213082, -1.9972813), (-1.1604648, -1.2384343), (-2.4564872, -2.0356355), (-2.733656, -2.8678641), (-1.4465295, -2.1255543), (-1.1593829, -1.2370652), (-2.4376912, -1.9226688), (-2.741523, -2.8794355), (-1.4739745, -2.2516701), (-1.1589144, -1.243043), (-2.4149973, -1.8112191), (-2.7472258, -2.87863), (-1.504135, -2.3673804), (-1.1492445, -1.2568939), (-2.3752966, -1.7067497), (-2.7575243, -2.8630388), (-1.5517156, -2.466749), (-1.1448338, -1.2801657), (-2.326676, -1.6197218), (-2.7614121, -2.8348176), (-1.6052977, -2.541204), (-1.14257, -1.3160417), (-2.2686265, -1.5606127), (-2.760632, -2.7920268), (-1.6655203, -2.5824928), (-1.1405503, -1.3528804), (-2.1999984, -1.5230123), (-2.758107, -2.74796), (-1.7345481, -2.6030183), (-1.1362127, -1.3870615), (-2.1183815, -1.5004967), (-2.7567527, -2.7067966), (-1.8162057, -2.6109102), (-1.1271313, -1.4165174), (-2.0193262, -1.486131), (-2.7592022, -2.6844788), (-1.9155583, -2.6285949), (-1.1104182, -1.4119064), (-1.8983653, -1.4439439), (-2.7713547, -2.6815708), (-2.0410058, -2.6625667), (-1.0815132, -1.361453), (-1.745926, -1.3558912), (-2.7963214, -2.7202156), (-2.2017555, -2.746468), (-1.042604, -1.2838951), (-1.5568647, -1.2218292), (-2.8218505, -2.7188776), (-2.395181, -2.811373), (-1.0136722, -1.2445612), (-1.3431411, -1.1106777), (-2.8316827, -2.6723793), (-2.6055412, -2.8427136), (-1.0427712, -1.2695274), (-1.1713121, -1.0734819), (-2.7734158, -2.6099012), (-2.7305877, -2.8396137), (-1.1180778, -1.3327028), (-1.0922419, -1.0961767), (-2.6916995, -2.56028), (-2.7703295, -2.8203382), (-1.1967231, -1.3985435), (-1.080767, -1.1425183), (-2.6226468, -2.5287046), (-2.7649047, -2.7981224), (-1.2558473, -1.451416), (-1.0972228, -1.1894048), (-2.5762472, -2.5093915), (-2.745555, -2.778277), (-1.2939728, -1.4905412), (-1.119823, -1.2285461), (-2.5490549, -2.4965792), (-2.7259386, -2.756098), (-1.3168044, -1.5126854), (-1.1411834, -1.2542181), (-2.5357065, -2.4836767), (-2.7104332, -2.750471), (-1.3286701, -1.5421454), (-1.1565688, -1.2722317), (-2.5299, -2.467427), (-2.70121, -2.748187), (-1.3359118, -1.5724602), (-1.1663371, -1.2838178), (-2.5268583, -2.4457312), (-2.6973588, -2.7493663), (-1.3423338, -1.6069359), (-1.1714233, -1.289939), (-2.52333, -2.4165769), (-2.697779, -2.7542717), (-1.3505044, -1.6486307), (-1.1727493, -1.2908732), (-2.5170813, -2.3775222), (-2.701583, -2.7637074), (-1.3622323, -1.7010487), (-1.1708938, -1.2858768), (-2.5064015, -2.3249311), (-2.708372, -2.7794414), (-1.3790847, -1.7691762), (-1.1658257, -1.2727674), (-2.4895275, -2.2527223), (-2.7112777, -2.804761), (-1.3941786, -1.8620344), (-1.1630664, -1.2500092), (-2.4753132, -2.1532211), (-2.7227213, -2.8418107), (-1.4176682, -1.9860649), (-1.160856, -1.23925), (-2.4595606, -2.0454361), (-2.7328112, -2.865829), (-1.4428065, -2.1139524), (-1.159552, -1.2371587), (-2.4409852, -1.9328558), (-2.7409916, -2.878393), (-1.4701631, -2.2402651), (-1.1589885, -1.2424275), (-2.418591, -1.8212172), (-2.746952, -2.8787713), (-1.5001737, -2.3569818), (-1.1492265, -1.2554686), (-2.3792236, -1.7157781), (-2.7575414, -2.8645208), (-1.5476024, -2.4582074), (-1.1448385, -1.2779119), (-2.3311126, -1.6271025), (-2.7616658, -2.8375397), (-1.6008757, -2.53497), (-1.1426749, -1.313069), (-2.2737758, -1.5660341), (-2.7610564, -2.7956674), (-1.6605783, -2.5784721), (-1.1408328, -1.3495451), (-2.2060857, -1.526779), (-2.7585683, -2.752023), (-1.7287894, -2.60059), (-1.1367921, -1.3836433), (-2.125719, -1.503172), (-2.7570732, -2.7109501), (-1.8092098, -2.609373), (-1.1281596, -1.4131455), (-2.028376, -1.4882971), (-2.7592072, -2.6885383), (-1.9067664, -2.6273022), (-1.112168, -1.412234), (-1.9097015, -1.4494542), (-2.7704039, -2.6818998), (-2.029591, -2.6573544), (-1.0842586, -1.3646836), (-1.7601721, -1.3652025), (-2.7945673, -2.7184088), (-2.1872845, -2.7375925), (-1.0456952, -1.2879528), (-1.5740057, -1.2341039), (-2.8209743, -2.7228537), (-2.37879, -2.8064492), (-1.014743, -1.2437452), (-1.3607824, -1.1185195), (-2.8340504, -2.6795638), (-2.5909936, -2.8409953), (-1.0383277, -1.2631633), (-1.1826376, -1.0742704), (-2.7801766, -2.6175272), (-2.7237654, -2.8407152), (-1.1110547, -1.3243688), (-1.0961468, -1.0929525), (-2.699031, -2.5665472), (-2.769284, -2.8223698), (-1.1903859, -1.390565), (-1.0806369, -1.1382438), (-2.628496, -2.5338907), (-2.766227, -2.800194), (-1.2511301, -1.4443388), (-1.0957763, -1.1854807), (-2.5804682, -2.5141191), (-2.7473679, -2.7800493), (-1.290607, -1.48409), (-1.1182704, -1.225382), (-2.5521147, -2.5013463), (-2.7275274, -2.7638524), (-1.3139343, -1.5136638), (-1.1390576, -1.2454928), (-2.5380173, -2.4798653), (-2.713095, -2.756842), (-1.3272189, -1.5451635), (-1.1537426, -1.264937), (-2.5305903, -2.4618964), (-2.704163, -2.753536), (-1.3360478, -1.5775279), (-1.1634139, -1.2777929), (-2.5259435, -2.4385998), (-2.7001739, -2.7540135), (-1.3439478, -1.6140827), (-1.1687055, -1.2848674), (-2.520985, -2.4077458), (-2.7002833, -2.7585948), (-1.3534117, -1.6580527), (-1.1703067, -1.2862788), (-2.5134797, -2.36659), (-2.7038128, -2.768205), (-1.3663113, -1.7132678), (-1.1686022, -1.2810897), (-2.501593, -2.311036), (-2.7105424, -2.784829), (-1.3843896, -1.7852985), (-1.1633804, -1.2668422), (-2.4833202, -2.234234), (-2.7138808, -2.8120813), (-1.4011774, -1.8841592), (-1.1616001, -1.246702), (-2.4688668, -2.1332026), (-2.7244625, -2.8468835), (-1.4248475, -2.0094354), (-1.1600641, -1.237697), (-2.4528828, -2.0242271), (-2.733926, -2.8690667), (-1.4501374, -2.1378005), (-1.1591461, -1.2369345), (-2.433982, -1.9111854), (-2.7416499, -2.8797069), (-1.4777101, -2.2633474), (-1.1587452, -1.243577), (-2.4110494, -1.800337), (-2.7472203, -2.8777533), (-1.5080898, -2.3776174), (-1.14917, -1.2582152), (-2.371047, -1.6973108), (-2.7573235, -2.860901), (-1.5559034, -2.474707), (-1.1447527, -1.2822537), (-2.3219354, -1.6123778), (-2.7610583, -2.831619), (-1.6098787, -2.546546), (-1.1424065, -1.3187099), (-2.263192, -1.555561), (-2.7602012, -2.7881947), (-1.6707082, -2.585469), (-1.1402162, -1.3557129), (-2.1936414, -1.5198276), (-2.757744, -2.7440531), (-1.7406437, -2.604362), (-1.1355733, -1.3897387), (-2.1107807, -1.4985372), (-2.756649, -2.7166417), (-1.8229139, -2.6262498), (-1.1260378, -1.4095479), (-2.0119247, -1.4726855), (-2.7606916, -2.6887176), (-1.9235097, -2.639118), (-1.1077045, -1.4041667), (-1.8885936, -1.429687), (-2.7740576, -2.6861186), (-2.0516903, -2.67395), (-1.077619, -1.3535434), (-1.7327296, -1.340375), (-2.7995608, -2.7238898), (-2.2159, -2.7587318), (-1.0385776, -1.2776402), (-1.5401113, -1.2058256), (-2.8236845, -2.715597), (-2.4116175, -2.8184462), (-1.0120944, -1.244025), (-1.325575, -1.1002735), (-2.8297617, -2.6653357), (-2.6201668, -2.845559), (-1.0469414, -1.2746797), (-1.1599331, -1.0708326), (-2.7669432, -2.6025631), (-2.737592, -2.8391864), (-1.1249349, -1.3397064), (-1.0882031, -1.098043), (-2.6845932, -2.5546632), (-2.7716074, -2.818685), (-1.2029393, -1.4048781), (-1.0807855, -1.1458199), (-2.6170328, -2.5247204), (-2.7637515, -2.7962432), (-1.2603946, -1.4563882), (-1.0986708, -1.1926566), (-2.5723863, -2.5065439), (-2.7438004, -2.776581), (-1.2970039, -1.4943172), (-1.1215072, -1.2312702), (-2.5465887, -2.494437), (-2.7243025, -2.754763), (-1.3187313, -1.5156794), (-1.142665, -1.2563728), (-2.534202, -2.481927), (-2.7091448, -2.74946), (-1.3298875, -1.5446261), (-1.157733, -1.2738472), (-2.5290008, -2.465835), (-2.700304, -2.747519), (-1.3367193, -1.5747267), (-1.1671758, -1.2849572), (-2.5262928, -2.4440901), (-2.6968098, -2.7490454), (-1.3429593, -1.6092572), (-1.1719655, -1.2906536), (-2.5228908, -2.4146817), (-2.6975431, -2.754312), (-1.3511217, -1.6512722), (-1.1730291, -1.2911782), (-2.5166044, -2.3751373), (-2.7016222, -2.7641563), (-1.3629836, -1.7043146), (-1.1709313, -1.285723), (-2.505744, -2.3217335), (-2.7086737, -2.7804174), (-1.380107, -1.7734852), (-1.1656069, -1.2720029), (-2.4885325, -2.248208), (-2.711892, -2.8064914), (-1.395666, -1.8680427), (-1.1630021, -1.2498252), (-2.4742773, -2.1482847), (-2.7232025, -2.8430164), (-1.419208, -1.9923952), (-1.160906, -1.2394365), (-2.4584596, -2.0401585), (-2.733183, -2.8666134), (-1.444402, -2.1204221), (-1.1596674, -1.2376153), (-2.439786, -1.9274143), (-2.7412724, -2.8787239), (-1.4718299, -2.2465627), (-1.1591271, -1.2431759), (-2.4172394, -1.815927), (-2.7471435, -2.8785276), (-1.5019432, -2.3626707), (-1.1493953, -1.2565837), (-2.37769, -1.7110618), (-2.7576118, -2.8636034), (-1.5494776, -2.4628317), (-1.144997, -1.2794156), (-2.329314, -1.6233271), (-2.761622, -2.8359993), (-1.602919, -2.5382898), (-1.1427935, -1.3149033), (-2.2716253, -1.5633645), (-2.7609215, -2.7936807), (-1.6628731, -2.580542), (-1.1408781, -1.3515141), (-2.2034929, -1.525051), (-2.7584028, -2.7498648), (-1.7314571, -2.601743), (-1.1367079, -1.3855855), (-2.1225576, -1.5020852), (-2.756964, -2.7088118), (-1.8124262, -2.609981), (-1.1278716, -1.414975), (-2.024453, -1.4875461), (-2.7592402, -2.6865294), (-1.9107707, -2.6276934), (-1.1115469, -1.4122539), (-1.9047731, -1.4472488), (-2.7708797, -2.681724), (-2.0347438, -2.6594758), (-1.0831696, -1.3632333), (-1.7539684, -1.3613008), (-2.795433, -2.7193758), (-2.1937795, -2.741383), (-1.0444107, -1.2859492), (-1.5665225, -1.2288128), (-2.8215048, -2.7214642), (-2.386152, -2.8086271), (-1.0142978, -1.2437475), (-1.35304, -1.1150533), (-2.8331907, -2.676833), (-2.597589, -2.8418374), (-1.0403016, -1.2655615), (-1.1776553, -1.0738685), (-2.7773678, -2.6146336), (-2.726941, -2.8403232), (-1.1141729, -1.3275853), (-1.0944675, -1.0943252), (-2.6959672, -2.5643282), (-2.7698627, -2.8215597), (-1.1931784, -1.3935328), (-1.0807745, -1.1400925), (-2.6261158, -2.5322607), (-2.7657218, -2.7993495), (-1.2531562, -1.4467752), (-1.0965168, -1.1871867), (-2.5788577, -2.512833), (-2.7466187, -2.7793167), (-1.2919668, -1.4860855), (-1.1190712, -1.2267648), (-2.551082, -2.5002108), (-2.7268546, -2.7632756), (-1.3148395, -1.5153863), (-1.1397319, -1.246574), (-2.5373497, -2.478786), (-2.7125876, -2.7564251), (-1.327853, -1.5467429), (-1.1542484, -1.265715), (-2.5301259, -2.4607525), (-2.7038312, -2.7532809), (-1.3365484, -1.5790932), (-1.1637539, -1.2783029), (-2.5255587, -2.4373105), (-2.6999989, -2.7539248), (-1.344416, -1.6157507), (-1.168897, -1.28513), (-2.5205889, -2.4062228), (-2.700245, -2.7586877), (-1.3539239, -1.6599451), (-1.1703652, -1.2862877), (-2.513002, -2.3647196), (-2.7038972, -2.7685256), (-1.366931, -1.7155454), (-1.1685315, -1.2807992), (-2.5009665, -2.3086393), (-2.7107527, -2.785461), (-1.3851862, -1.788203), (-1.1631604, -1.2661486), (-2.4824607, -2.2310097), (-2.7142656, -2.8131742), (-1.40227, -1.8880941), (-1.1614852, -1.2463764), (-2.4679701, -2.1297507), (-2.7247403, -2.8476021), (-1.425964, -2.0134766), (-1.1600204, -1.2376055), (-2.4519346, -2.0206301), (-2.7341194, -2.8694766), (-1.4512813, -2.141805), (-1.1591388, -1.2370212), (-2.4329612, -1.90759), (-2.7417748, -2.879787), (-1.4788946, -2.2670891), (-1.1587467, -1.2438579), (-2.409918, -1.7969713), (-2.7472775, -2.877444), (-1.5093387, -2.380819), (-1.14919, -1.2587224), (-2.369793, -1.6944411), (-2.7572966, -2.8601844), (-1.5572159, -2.4771185), (-1.1447598, -1.282968), (-2.3205073, -1.6101954), (-2.7609642, -2.8305776), (-1.6112999, -2.548088), (-1.1423752, -1.319562), (-2.261534, -1.5541031), (-2.7600684, -2.7869756), (-1.6723026, -2.5862496), (-1.1401194, -1.3565701), (-2.1916895, -1.5189447), (-2.7576222, -2.7428362), (-1.7425025, -2.6046264), (-1.1353685, -1.3905022), (-2.1084404, -1.4980197), (-2.7566006, -2.71555), (-1.8251662, -2.6262455), (-1.125674, -1.4101576), (-2.0090559, -1.4723417), (-2.7607732, -2.6878078), (-1.9263273, -2.6390371), (-1.1070923, -1.4033911), (-1.8850193, -1.428213), (-2.7744758, -2.686616), (-2.0553281, -2.6752267), (-1.0766853, -1.3517113), (-1.7282745, -1.3376148), (-2.8002398, -2.7251785), (-2.2204628, -2.7612936), (-1.0375733, -1.2755568), (-1.5348355, -1.2020584), (-2.8240483, -2.7151496), (-2.4166708, -2.8199728), (-1.0118027, -1.2434956), (-1.3203336, -1.097788), (-2.8290844, -2.6639192), (-2.6244705, -2.8462343), (-1.0483191, -1.275875), (-1.156746, -1.0704112), (-2.7650247, -2.6010025), (-2.7395413, -2.8390687), (-1.1269833, -1.3414894), (-1.0871919, -1.09878), (-2.6826007, -2.5534894), (-2.771918, -2.8182805), (-1.2047186, -1.4065276), (-1.080903, -1.1468788), (-2.6155272, -2.5238967), (-2.7634096, -2.7957969), (-1.261661, -1.4577191), (-1.099157, -1.1936517), (-2.5713997, -2.5059354), (-2.7433207, -2.77619), (-1.2978321, -1.4953767), (-1.1220299, -1.2320857), (-2.5459914, -2.4939322), (-2.7238805, -2.7544744), (-1.3192593, -1.516585), (-1.1431082, -1.2570065), (-2.533856, -2.481447), (-2.7088363, -2.749259), (-1.3302374, -1.5454481), (-1.1580713, -1.2743145), (-2.5287962, -2.4653227), (-2.7001119, -2.7474113), (-1.3369803, -1.5755459), (-1.1674113, -1.2852782), (-2.5261495, -2.4434946), (-2.6967225, -2.749033), (-1.3431976, -1.6101427), (-1.1721098, -1.2908418), (-2.5227485, -2.4139519), (-2.697544, -2.7544024), (-1.3513852, -1.6522964), (-1.173094, -1.2912316), (-2.516418, -2.3742058), (-2.7016997, -2.7643728), (-1.3633126, -1.7055684), (-1.1709228, -1.2856182), (-2.5054722, -2.320501), (-2.7088244, -2.7808053), (-1.3805406, -1.7751026), (-1.1655161, -1.2716814), (-2.4881282, -2.2465062), (-2.7121398, -2.8071337), (-1.3962672, -1.8702452), (-1.1629726, -1.2497123), (-2.4738524, -2.1464484), (-2.7233906, -2.8434544), (-1.4198246, -1.9946735), (-1.1609187, -1.2394536), (-2.4580066, -2.0382268), (-2.733323, -2.8668854), (-1.4450339, -2.1227014), (-1.1597028, -1.2377261), (-2.4392946, -1.9254591), (-2.741373, -2.8788214), (-1.4724811, -2.2487252), (-1.1591681, -1.2433865), (-2.4166915, -1.8140689), (-2.747208, -2.8784146), (-1.5026262, -2.3645625), (-1.1494445, -1.2569146), (-2.3770757, -1.7094485), (-2.7576292, -2.8632603), (-1.5501925, -2.4643052), (-1.1450385, -1.2798655), (-2.3286052, -1.6220769), (-2.761598, -2.8354585), (-1.6036887, -2.5392802), (-1.1428158, -1.3154407), (-2.270793, -1.5625172), (-2.76087, -2.7930179), (-1.6637286, -2.5810905), (-1.1408662, -1.3520654), (-2.202504, -1.5245336), (-2.758349, -2.749181), (-1.7324448, -2.60198), (-1.1366398, -1.3860935), (-2.121364, -1.5017855), (-2.7569396, -2.7081752), (-1.8136134, -2.610042), (-1.1277224, -1.4154106), (-2.022985, -1.4873554), (-2.7592762, -2.6859753), (-1.912243, -2.6276937), (-1.1112676, -1.4119695), (-1.9029399, -1.446484), (-2.7710893, -2.6819048), (-2.0366373, -2.660166), (-1.082712, -1.3623741), (-1.7516717, -1.3598679), (-2.7957942, -2.720006), (-2.196166, -2.742749), (-1.0438821, -1.2848969), (-1.56376, -1.2268178), (-2.8217416, -2.7212133), (-2.3888555, -2.809462), (-1.0140873, -1.2434767), (-1.3501925, -1.1136917), (-2.8329103, -2.6760395), (-2.6000044, -2.8422058), (-1.0409815, -1.2662178), (-1.1758208, -1.0736194), (-2.776373, -2.6137373), (-2.7281086, -2.8402412), (-1.1152738, -1.328581), (-1.0938368, -1.0947365), (-2.694877, -2.5636501), (-2.7700844, -2.8213139), (-1.1941636, -1.3944626), (-1.0808089, -1.140697), (-2.6252751, -2.531788), (-2.765546, -2.7990806), (-1.2538617, -1.4475226), (-1.0967761, -1.1877556), (-2.5783014, -2.5124865), (-2.746351, -2.7790804), (-1.2924261, -1.486676), (-1.119357, -1.2272295), (-2.5507433, -2.499924), (-2.726613, -2.7566483), (-1.3154773, -1.5085725), (-1.1408644, -1.2531967), (-2.5372546, -2.4874144), (-2.7109106, -2.75085), (-1.3273641, -1.5377047), (-1.1564211, -1.2715145), (-2.5314746, -2.471685), (-2.701493, -2.748347), (-1.334489, -1.5675195), (-1.1663524, -1.2834274), (-2.5285816, -2.4506578), (-2.6974623, -2.7492366), (-1.3406938, -1.6012931), (-1.1715997, -1.2899466), (-2.5253003, -2.4223862), (-2.6977005, -2.753729), (-1.3485552, -1.6420008), (-1.1731068, -1.291425), (-2.519401, -2.3845565), (-2.7012877, -2.762541), (-1.3598602, -1.6929761), (-1.1714966, -1.2872303), (-2.509215, -2.333768), (-2.7077715, -2.7773075), (-1.3761134, -1.7588936), (-1.1668062, -1.2753547), (-2.4930754, -2.2643678), (-2.710146, -2.8011043), (-1.3902317, -1.848215), (-1.1635933, -1.2512348), (-2.47889, -2.165576), (-2.7219906, -2.8394413), (-1.4137045, -1.9718609), (-1.1610796, -1.2396461), (-2.4632032, -2.0582938), (-2.732371, -2.8644588), (-1.4388171, -2.0997784), (-1.1596204, -1.2369794), (-2.4447668, -1.9458017), (-2.740761, -2.8780441), (-1.4660985, -2.226795), (-1.1590128, -1.2416391), (-2.4226398, -1.8335516), (-2.7469075, -2.8796692), (-1.4959437, -2.3451385), (-1.158894, -1.2539223), (-2.3957868, -1.729339), (-2.7505136, -2.8678734), (-1.5288361, -2.4456499), (-1.1498978, -1.2742858), (-2.3517046, -1.6388355), (-2.758218, -2.841682), (-1.5789603, -2.5241313), (-1.145613, -1.3083258), (-2.2976465, -1.5754117), (-2.759173, -2.8004093), (-1.6356653, -2.5699866), (-1.1433645, -1.3444707), (-2.2336779, -1.5341208), (-2.7568266, -2.7568874), (-1.699973, -2.5939674), (-1.1401657, -1.3787298), (-2.1584165, -1.509284), (-2.7543993, -2.7155817), (-1.7747021, -2.6036515), (-1.1334032, -1.4085774), (-2.0684657, -1.4941931), (-2.7547538, -2.6926315), (-1.8639551, -2.6211436), (-1.1207005, -1.4235499), (-1.9601485, -1.470901), (-2.7613015, -2.6698797), (-1.9744356, -2.6332202), (-1.0977206, -1.391337), (-1.8245089, -1.406935), (-2.780647, -2.6942978), (-2.1165872, -2.6929657), (-1.0621793, -1.3205758), (-1.6540719, -1.2943308), (-2.8090065, -2.725767), (-2.2950637, -2.7767704), (-1.0241522, -1.2528329), (-1.4491954, -1.1621579), (-2.8359098, -2.702446), (-2.509183, -2.8264825), (-1.0206145, -1.2422141), (-1.2459258, -1.0834726), (-2.8066797, -2.6465795), (-2.6781495, -2.8419492), (-1.0762907, -1.2905785), (-1.1207503, -1.0789772), (-2.732261, -2.5887988), (-2.7566152, -2.8296378), (-1.1567384, -1.3579725), (-1.0807446, -1.116548), (-2.6551569, -2.54878), (-2.7690785, -2.8084502), (-1.2259957, -1.417945), (-1.0867682, -1.1645375), (-2.5980272, -2.5242028), (-2.7544818, -2.787333), (-1.2737118, -1.4636176), (-1.1076956, -1.2079406), (-2.562378, -2.508813), (-2.734338, -2.769565), (-1.3030229, -1.4974518), (-1.1296504, -1.2426295), (-2.5427952, -2.4976666), (-2.716765, -2.7496464), (-1.320103, -1.5166874), (-1.1488228, -1.264363), (-2.5341618, -2.484869), (-2.7039304, -2.7460375), (-1.3287266, -1.5450431), (-1.1618358, -1.279152), (-2.53086, -2.4676788), (-2.6972213, -2.7455778), (-1.3345578, -1.5757409), (-1.1694983, -1.2882272), (-2.528729, -2.4442332), (-2.6954734, -2.7483706), (-1.3408624, -1.6117815), (-1.1728919, -1.2923832), (-2.525006, -2.412613), (-2.6975386, -2.7547848), (-1.3497914, -1.6560826), (-1.1729243, -1.2916162), (-2.5178094, -2.3702867), (-2.7026036, -2.7658453), (-1.3628783, -1.7122383), (-1.1700511, -1.2847946), (-2.505627, -2.313262), (-2.710445, -2.7836776), (-1.3815831, -1.7856848), (-1.1640201, -1.2692142), (-2.4866982, -2.2346625), (-2.7145228, -2.812084), (-1.3992046, -1.8864683), (-1.1621171, -1.2490563), (-2.471864, -2.1328125), (-2.7252312, -2.8467512), (-1.4233255, -2.0122442), (-1.1605741, -1.239949), (-2.4555051, -2.0232723), (-2.734704, -2.8688266), (-1.4489933, -2.1408107), (-1.1596918, -1.2390394), (-2.4362729, -1.9099215), (-2.742376, -2.8793201), (-1.476856, -2.2662282), (-1.1593351, -1.2455451), (-2.4130614, -1.7990584), (-2.747858, -2.8771515), (-1.5074369, -2.380021), (-1.1497713, -1.2600721), (-2.3727996, -1.6963089), (-2.757866, -2.8600504), (-1.5554011, -2.4763567), (-1.1453828, -1.2839893), (-2.3234963, -1.6118814), (-2.7614753, -2.8305929), (-1.6094291, -2.5473118), (-1.1430663, -1.3202459), (-2.2646492, -1.555652), (-2.760491, -2.78714), (-1.6702107, -2.5853894), (-1.1408999, -1.3569211), (-2.195101, -1.5204525), (-2.7579198, -2.7431545), (-1.7399894, -2.60357), (-1.1362736, -1.390489), (-2.1123712, -1.4996052), (-2.7567303, -2.7160397), (-1.8219754, -2.6248374), (-1.1267496, -1.4097277), (-2.0137975, -1.474148), (-2.7606962, -2.6885955), (-1.9221361, -2.6372051), (-1.1084359, -1.4039557), (-1.8909262, -1.431761), (-2.7739704, -2.6863608), (-2.0497005, -2.671364), (-1.0783643, -1.3527739), (-1.7356968, -1.3429506), (-2.7994804, -2.7248008), (-2.2132072, -2.7558122), (-1.0391712, -1.2760625), (-1.5436884, -1.2084308), (-2.8240016, -2.7186573), (-2.4085498, -2.817158), (-1.0120479, -1.2407892), (-1.3291618, -1.1012024), (-2.8309085, -2.6693404), (-2.6175919, -2.8453794), (-1.0456586, -1.2700794), (-1.1621293, -1.0700105), (-2.7689939, -2.60678), (-2.7365618, -2.8396459), (-1.1230714, -1.3344345), (-1.0889449, -1.0964018), (-2.6868048, -2.5589705), (-2.7716014, -2.819263), (-1.201059, -1.3991774), (-1.0808227, -1.1440613), (-2.6191208, -2.5294292), (-2.7640727, -2.7967165), (-1.2585613, -1.4501373), (-1.0985427, -1.1910828), (-2.574442, -2.5119848), (-2.744091, -2.7768662), (-1.295061, -1.4872736), (-1.1214434, -1.2299739), (-2.548801, -2.5008764), (-2.7244356, -2.761183), (-1.3161538, -1.5148027), (-1.1418661, -1.2490058), (-2.5366611, -2.4806206), (-2.7105927, -2.7547176), (-1.3278834, -1.5447495), (-1.1559784, -1.267489), (-2.5305765, -2.4635487), (-2.7023141, -2.7518988), (-1.3356837, -1.5759773), (-1.165083, -1.2796223), (-2.5268073, -2.440958), (-2.698905, -2.7527454), (-1.342937, -1.6116726), (-1.1699055, -1.2862382), (-2.5224035, -2.41073), (-2.6994665, -2.757533), (-1.3520066, -1.6549083), (-1.1711754, -1.2874604), (-2.515254, -2.3702443), (-2.7032905, -2.7671537), (-1.3646463, -1.7093501), (-1.1693001, -1.282366), (-2.5036416, -2.315563), (-2.710143, -2.78354), (-1.3824832, -1.7803465), (-1.164082, -1.2685338), (-2.4856832, -2.2400723), (-2.7133853, -2.8102307), (-1.3988835, -1.8775989), (-1.1620318, -1.2477351), (-2.4712255, -2.1394365), (-2.7242138, -2.8456774), (-1.4225905, -2.0026674), (-1.1603347, -1.2383091), (-2.45525, -2.0307245), (-2.7338407, -2.8683972), (-1.4479043, -2.1310701), (-1.1593437, -1.2372222), (-2.4364045, -1.9176992), (-2.7416778, -2.8796077), (-1.4754617, -2.2570288), (-1.1589295, -1.2435093), (-2.413604, -1.8064804), (-2.7473443, -2.878329), (-1.5057647, -2.3721879), (-1.1493232, -1.2577287), (-2.373767, -1.7026142), (-2.7575736, -2.8621929), (-1.5534883, -2.470611), (-1.144936, -1.2813693), (-2.3249328, -1.6164963), (-2.7614002, -2.833491), (-1.6072693, -2.543934), (-1.1426615, -1.3175422), (-2.266585, -1.5584102), (-2.7605853, -2.7903888), (-1.6677716, -2.584166), (-1.1405934, -1.3544937), (-2.1975653, -1.5216624), (-2.75808, -2.7462554), (-1.7371954, -2.6039402), (-1.1361488, -1.3886395), (-2.115427, -1.4997232), (-2.7568245, -2.7186272), (-1.8186936, -2.6262593), (-1.126905, -1.4086719), (-2.0175753, -1.4735723), (-2.7606032, -2.6903982), (-1.9181802, -2.6392171), (-1.1090305, -1.4058592), (-1.8955982, -1.4327532), (-2.7733078, -2.6851556), (-2.044745, -2.671407), (-1.0795538, -1.3571844), (-1.7414519, -1.3459193), (-2.7982848, -2.7214558), (-2.207108, -2.7536876), (-1.0406553, -1.2817051), (-1.5504788, -1.2133225), (-2.822988, -2.7166207), (-2.4017746, -2.8154464), (-1.0128027, -1.2449874), (-1.335984, -1.1052473), (-2.8310778, -2.668338), (-2.611655, -2.8442602), (-1.0443358, -1.2721815), (-1.1663827, -1.0717012), (-2.7707536, -2.6058993), (-2.7336798, -2.839469), (-1.1209191, -1.335968), (-1.0903252, -1.0965706), (-2.6886067, -2.5572577), (-2.7709658, -2.8195302), (-1.19939, -1.4013304), (-1.0806185, -1.1436771), (-2.6201072, -2.5266488), (-2.7644227, -2.7971575), (-1.2578232, -1.4534084), (-1.0977448, -1.1906303), (-2.574448, -2.5080879), (-2.7447424, -2.777365), (-1.2952634, -1.4918016), (-1.1204932, -1.2295995), (-2.5478938, -2.495833), (-2.7251227, -2.755323), (-1.3175509, -1.5133865), (-1.141803, -1.2550573), (-2.5350325, -2.4833353), (-2.7097275, -2.7498305), (-1.3290194, -1.5424191), (-1.1570762, -1.2728662), (-2.5295775, -2.4673758), (-2.7006438, -2.7476857), (-1.3359836, -1.5724443), (-1.1667202, -1.2842807), (-2.5267797, -2.4458752), (-2.696932, -2.7489939), (-1.3422241, -1.6067444), (-1.1716889, -1.2902701), (-2.5234094, -2.4168363), (-2.697479, -2.7540095), (-1.3502891, -1.6483603), (-1.1729149, -1.291114), (-2.5172513, -2.3778298), (-2.701388, -2.7635322), (-1.3619696, -1.7007778), (-1.1709828, -1.2860627), (-2.5066125, -2.3252304), (-2.7082577, -2.779339), (-1.3788155, -1.7689706), (-1.1658604, -1.2729146), (-2.489745, -2.2529564), (-2.7112212, -2.8047297), (-1.3939279, -1.8619615), (-1.163079, -1.2501606), (-2.4755306, -2.1533732), (-2.7226973, -2.8418162), (-1.4174408, -1.9861314), (-1.1608623, -1.239416), (-2.4597723, -2.0454862), (-2.7328076, -2.8658607), (-1.442605, -2.114172), (-1.1595618, -1.2373428), (-2.441189, -1.9327867), (-2.7409995, -2.8784366), (-1.4699868, -2.240638), (-1.1590062, -1.2426373), (-2.4187875, -1.8210275), (-2.746966, -2.8788025), (-1.5000198, -2.357485), (-1.1492509, -1.2557204), (-2.3794065, -1.7154878), (-2.7575567, -2.8645017), (-1.5474739, -2.4587924), (-1.1448759, -1.2782291), (-2.3312793, -1.6267552), (-2.7616713, -2.837434), (-1.6007689, -2.5355744), (-1.1427288, -1.3134755), (-2.273925, -1.5656824), (-2.7610435, -2.795448), (-1.660488, -2.579043), (-1.1409079, -1.3500458), (-2.20622, -1.5264517), (-2.7585297, -2.751686), (-1.7287086, -2.6011055), (-1.1368898, -1.3842374), (-2.1258411, -1.5028788), (-2.7570064, -2.7104962), (-1.8091345, -2.6098304), (-1.1282817, -1.4138311), (-2.0284915, -1.4880334), (-2.759111, -2.687974), (-1.9066886, -2.6277082), (-1.112318, -1.4131135), (-1.9098176, -1.44933), (-2.7702627, -2.6811152), (-2.0294957, -2.657564), (-1.0844404, -1.3656961), (-1.7602999, -1.3652481), (-2.7943857, -2.7175083), (-2.1871536, -2.7375746), (-1.0458937, -1.2889591), (-1.5741656, -1.2343502), (-2.8207788, -2.722037), (-2.3786073, -2.8062592), (-1.0149221, -1.2445827), (-1.3609803, -1.1188693), (-2.8338912, -2.6789548), (-2.5907803, -2.8407636), (-1.0384396, -1.263786), (-1.1828246, -1.074588), (-2.7800884, -2.6170783), (-2.7235954, -2.840516), (-1.1111019, -1.3248305), (-1.0962642, -1.0931777), (-2.6989822, -2.5661702), (-2.769184, -2.8222172), (-1.1904107, -1.3909463), (-1.0806869, -1.1383895), (-2.628445, -2.5335298), (-2.7661762, -2.800076), (-1.2511661, -1.4446918), (-1.0957862, -1.1855744), (-2.5803947, -2.5137515), (-2.7473419, -2.7799544), (-1.290665, -1.4844387), (-1.1182584, -1.2254407), (-2.5520113, -2.5009627), (-2.7275126, -2.7637725), (-1.314019, -1.5140173), (-1.1390357, -1.2455366), (-2.5378842, -2.4794755), (-2.713081, -2.7567701), (-1.3273262, -1.545517), (-1.1537192, -1.2649616), (-2.5304344, -2.4614978), (-2.704146, -2.7534735), (-1.336172, -1.5778779), (-1.1633918, -1.2777967), (-2.5257719, -2.4381971), (-2.7001534, -2.753964), (-1.3440826, -1.6144277), (-1.1686841, -1.2848471), (-2.520803, -2.4073381), (-2.7002609, -2.758561), (-1.3535526, -1.6583943), (-1.1702839, -1.2862295), (-2.5132911, -2.3661733), (-2.7037919, -2.768196), (-1.3664566, -1.7136166), (-1.1685741, -1.2810038), (-2.5013983, -2.3105967), (-2.7105281, -2.7848537), (-1.3845414, -1.7856723), (-1.1633403, -1.26671), (-2.4831133, -2.2337484), (-2.7138827, -2.8121538), (-1.4013481, -1.8845936), (-1.161572, -1.2466035), (-2.468669, -2.132736), (-2.7244542, -2.846915), (-1.4250089, -2.0098317), (-1.1600409, -1.2376174), (-2.452689, -2.0237873), (-2.7339113, -2.8690674), (-1.4502928, -2.138149), (-1.1591239, -1.2368703), (-2.4337885, -1.9107871), (-2.741631, -2.8796768), (-1.4778633, -2.2636287), (-1.1587216, -1.2435286), (-2.4108512, -1.800001), (-2.7471983, -2.877693), (-1.5082456, -2.377814), (-1.1491481, -1.2581825), (-2.370844, -1.6970543), (-2.7572963, -2.8608174), (-1.556061, -2.4748092), (-1.1447278, -1.2822298), (-2.3217213, -1.6122078), (-2.7610307, -2.8315282), (-1.610045, -2.5465622), (-1.1423744, -1.318684), (-2.2629597, -1.555466), (-2.7601783, -2.788114), (-1.6708935, -2.585424), (-1.1401747, -1.3556733), (-2.1933835, -1.5197862), (-2.757732, -2.7440004), (-1.7408594, -2.6042843), (-1.1355169, -1.3896761), (-2.1104841, -1.4985268), (-2.7566545, -2.716627), (-1.8231766, -2.6261632), (-1.1259614, -1.4094586), (-2.0115716, -1.4726853), (-2.7607203, -2.688744), (-1.9238406, -2.639045), (-1.1075975, -1.4038955), (-1.8881621, -1.4295309), (-2.7741346, -2.6863391), (-2.0521214, -2.6740744), (-1.0774724, -1.3531407), (-1.7321955, -1.3400398), (-2.7996764, -2.7242117), (-2.216448, -2.7590497), (-1.0384234, -1.2772274), (-1.5394758, -1.2053415), (-2.8237658, -2.7156882), (-2.4122376, -2.81867), (-1.0120298, -1.2438349), (-1.3249346, -1.0999279), (-2.8297153, -2.6652706), (-2.6207094, -2.8456862), (-1.0470862, -1.2747355), (-1.1595324, -1.0707393), (-2.7667387, -2.6024513), (-2.737855, -2.8392148), (-1.1251731, -1.3398646), (-1.0880708, -1.0981047), (-2.684374, -2.5545828), (-2.7716682, -2.8186734), (-1.203152, -1.40504), (-1.0807972, -1.1459335), (-2.61687, -2.524673), (-2.7637289, -2.7962224), (-1.2605491, -1.4565263), (-1.098734, -1.1927735), (-2.5722873, -2.5065172), (-2.7437587, -2.7765632), (-1.2971061, -1.4944311), (-1.121578, -1.2313718), (-2.5465379, -2.4944184), (-2.7242677, -2.7547567), (-1.3187984, -1.5157839), (-1.1427279, -1.2564563), (-2.5341816, -2.4819047), (-2.7091236, -2.7494638), (-1.3299351, -1.544731), (-1.1577841, -1.2739133), (-2.5289967, -2.4657998), (-2.7002974, -2.7475348), (-1.3367579, -1.5748444), (-1.1672146, -1.2850071), (-2.5262945, -2.4440327), (-2.6968164, -2.7490728), (-1.3429979, -1.6093972), (-1.1719923, -1.2906878), (-2.5228891, -2.414594), (-2.697563, -2.7543545), (-1.3511682, -1.6514466), (-1.1730437, -1.2911934), (-2.516592, -2.375005), (-2.7016537, -2.7642179), (-1.3630443, -1.70454), (-1.1709344, -1.2857144), (-2.5057132, -2.321539), (-2.7087162, -2.7805076), (-1.3801892, -1.7737852), (-1.1655959, -1.271956), (-2.4884737, -2.2479174), (-2.7119508, -2.8066282), (-1.3957818, -1.8684626), (-1.1630038, -1.2498188), (-2.4742131, -2.1479595), (-2.7232492, -2.8431144), (-1.4193274, -1.9928383), (-1.160916, -1.239455), (-2.458389, -2.039807), (-2.7332196, -2.8666797), (-1.444525, -2.1208737), (-1.159682, -1.2376521), (-2.439706, -1.9270498), (-2.7413006, -2.878755), (-1.4719576, -2.2469988), (-1.1591427, -1.2432333), (-2.4171472, -1.8155744), (-2.7471638, -2.878516), (-1.5020776, -2.3630595), (-1.1494131, -1.2566649), (-2.3775835, -1.7107508), (-2.7576208, -2.8635445), (-1.5496175, -2.4631424), (-1.1450131, -1.2795211), (-2.3291893, -1.6230824), (-2.7616222, -2.8358972), (-1.6030681, -2.5385065), (-1.1428055, -1.3150274), (-2.2714763, -1.5631964), (-2.7609158, -2.7935517), (-1.6630383, -2.5806718), (-1.1408832, -1.3516419), (-2.2033134, -1.5249453), (-2.758395, -2.7497263), (-1.7316457, -2.6018105), (-1.1367025, -1.3857077), (-2.1223388, -1.5020218), (-2.756959, -2.7086768), (-1.8126512, -2.6100104), (-1.1278517, -1.4150854), (-2.0241823, -1.4875045), (-2.7592437, -2.686405), (-1.9110461, -2.6277068), (-1.1115028, -1.4122335), (-1.904435, -1.4471043), (-2.770914, -2.6817312), (-2.0350952, -2.6596107), (-1.0830913, -1.3631077), (-1.7535442, -1.3610364), (-2.7954943, -2.7194636), (-2.1942205, -2.741636), (-1.044319, -1.2857851), (-1.5660121, -1.2284491), (-2.821542, -2.7213914), (-2.3866498, -2.8087752), (-1.0142636, -1.2437216), (-1.352515, -1.1148096), (-2.833134, -2.6766658), (-2.5980325, -2.8418975), (-1.0404298, -1.2657001), (-1.1773173, -1.0738297), (-2.777179, -2.614452), (-2.7271523, -2.8403), (-1.1143773, -1.327781), (-1.094352, -1.0944054), (-2.6957614, -2.564189), (-2.7698982, -2.8215072), (-1.1933599, -1.393714), (-1.0807807, -1.1402066), (-2.6259558, -2.5321596), (-2.7656846, -2.7992935), (-1.2532854, -1.4469217), (-1.0965631, -1.1872916), (-2.5787497, -2.512755), (-2.7465646, -2.7792675), (-1.2920499, -1.4862024), (-1.1191214, -1.2268482), (-2.551014, -2.5001423), (-2.7268054, -2.763237), (-1.3148906, -1.5154858), (-1.1397728, -1.2466372), (-2.5373063, -2.478719), (-2.7125502, -2.7563965), (-1.3278868, -1.5468351), (-1.1542784, -1.2657596), (-2.5300963, -2.460681), (-2.703806, -2.7532623), (-1.336573, -1.579184), (-1.1637717, -1.2783296), (-2.5255337, -2.4372284), (-2.6999843, -2.753917), (-1.3444391, -1.6158502), (-1.1689053, -1.2851418), (-2.5205631, -2.4061244), (-2.7002392, -2.758692), (-1.35395, -1.6600618), (-1.1703646, -1.2862831), (-2.5129702, -2.3645957), (-2.7038996, -2.7685452), (-1.3669654, -1.7156912), (-1.1685232, -1.2807744), (-2.500925, -2.3084776), (-2.7107637, -2.785503), (-1.3852328, -1.7883946), (-1.1631417, -1.2660972), (-2.4824023, -2.230789), (-2.714288, -2.8132474), (-1.4023366, -1.8883603), (-1.1614742, -1.2463517), (-2.467911, -2.1295125), (-2.7247572, -2.847649), (-1.4260324, -2.013752), (-1.1600143, -1.2375976), (-2.4518723, -2.0203798), (-2.7341306, -2.8695014), (-1.4513525, -2.1420803), (-1.1591353, -1.2370279), (-2.4328928, -1.9073385), (-2.741781, -2.8797882), (-1.4789699, -2.2673485), (-1.1587435, -1.2438787), (-2.4098406, -1.7967347), (-2.7472806, -2.877419), (-1.509421, -2.381043), (-1.149189, -1.2587594), (-2.3697057, -1.694238), (-2.757293, -2.860131), (-1.5573039, -2.4772885), (-1.1447581, -1.2830198), (-2.3204074, -1.6100394), (-2.760956, -2.8305018), (-1.6113962, -2.5481987), (-1.1423713, -1.3196237), (-2.2614174, -1.5539967), (-2.7600574, -2.7868865), (-1.6724116, -2.5863082), (-1.140112, -1.3566338), (-2.1915534, -1.5188774), (-2.7576122, -2.742746), (-1.7426299, -2.6046507), (-1.135354, -1.3905619), (-2.1082778, -1.497978), (-2.7565951, -2.7154663), (-1.825321, -2.6262507), (-1.1256495, -1.4102091), (-2.008857, -1.4723132), (-2.760776, -2.6877344), (-1.9265203, -2.639035), (-1.1070509, -1.403349), (-1.8847723, -1.4281086), (-2.774502, -2.6866372), (-2.0555785, -2.6753154), (-1.0766222, -1.3515971), (-1.7279661, -1.3374242), (-2.8002837, -2.7252548), (-2.2207775, -2.761469), (-1.0375062, -1.2754244), (-1.5344712, -1.2018011), (-2.8240695, -2.7151074), (-2.4170167, -2.820074), (-1.011785, -1.2434686), (-1.319975, -1.0976206), (-2.829035, -2.6638136), (-2.6247633, -2.8462775), (-1.0484157, -1.2759649), (-1.1565301, -1.0703859), (-2.7648916, -2.6008894), (-2.7396724, -2.8390586), (-1.1271242, -1.3416176), (-1.0871238, -1.0988321), (-2.6824632, -2.5534031), (-2.7719386, -2.8182516), (-1.204842, -1.4066466), (-1.080912, -1.1469527), (-2.615423, -2.5238352), (-2.763386, -2.7957656), (-1.2617495, -1.4578153), (-1.0991907, -1.1937207), (-2.5713305, -2.5058894), (-2.7432878, -2.7761633), (-1.2978905, -1.495454), (-1.122066, -1.2321415), (-2.5459495, -2.493893), (-2.723852, -2.754454), (-1.3192973, -1.5166514), (-1.1431396, -1.2570513), (-2.533832, -2.4814105), (-2.708816, -2.7492442), (-1.330263, -1.5455081), (-1.1580948, -1.2743487), (-2.5287814, -2.4652848), (-2.7000995, -2.747402), (-1.3370008, -1.575604), (-1.167429, -1.2853029), (-2.5261383, -2.4434536), (-2.6967156, -2.74903), (-1.3432161, -1.6102034), (-1.172122, -1.2908573), (-2.522738, -2.4139035), (-2.6975427, -2.7544057), (-1.351405, -1.6523641), (-1.1731015, -1.2912388), (-2.5164058, -2.374146), (-2.7017033, -2.7643845), (-1.3633353, -1.7056496), (-1.1709247, -1.2856146), (-2.5054548, -2.3204231), (-2.708834, -2.7808278), (-1.3805698, -1.7752045), (-1.1655127, -1.2716638), (-2.4881032, -2.2464013), (-2.7121556, -2.807172), (-1.3963068, -1.8703814), (-1.1629727, -1.2497073), (-2.4738264, -2.1463366), (-2.7234018, -2.8434806), (-1.4198638, -1.994812), (-1.1609216, -1.2394549), (-2.4579797, -2.03811), (-2.7333314, -2.8669028), (-1.4450724, -2.1228392), (-1.159706, -1.2377323), (-2.4392667, -1.9253418), (-2.7413807, -2.878828), (-1.4725214, -2.2488554), (-1.1591711, -1.2434001), (-2.4166589, -1.8139583), (-2.7472124, -2.8784082), (-1.5026683, -2.3646762), (-1.1494493, -1.2569351), (-2.3770409, -1.7093533), (-2.7576303, -2.8632402), (-1.5502346, -2.4643922), (-1.1450422, -1.2798923), (-2.3285656, -1.6220042), (-2.7615979, -2.835427), (-1.6037335, -2.539337), (-1.1428175, -1.3154721), (-2.2707455, -1.5624694), (-2.7608678, -2.7929797), (-1.663779, -2.581121), (-1.1408665, -1.3520975), (-2.2024481, -1.5245057), (-2.758346, -2.749142), (-1.7325017, -2.6019921), (-1.1366373, -1.3861227), (-2.1212978, -1.5017704), (-2.7569382, -2.7081382), (-1.81368, -2.6100426), (-1.1277144, -1.415435), (-2.022903, -1.487347), (-2.7592785, -2.6859436), (-1.9123263, -2.6276903), (-1.1112525, -1.4119527), (-1.9028378, -1.4464436), (-2.7711017, -2.6819162), (-2.0367432, -2.6602025), (-1.0826861, -1.3623245), (-1.7515438, -1.3597895), (-2.795816, -2.7200427), (-2.1962996, -2.7428236), (-1.043852, -1.2848364), (-1.5636054, -1.2267082), (-2.8217552, -2.7212012), (-2.3890069, -2.8095074), (-1.014076, -1.2434593), (-1.3500344, -1.1136165), (-2.832895, -2.6759982), (-2.6001387, -2.8422272), (-1.0410193, -1.2662529), (-1.1757189, -1.0736051), (-2.7763176, -2.613689), (-2.7281733, -2.8402376), (-1.1153352, -1.3286353), (-1.0938029, -1.0947595), (-2.6948175, -2.5636153), (-2.7700965, -2.8213005), (-1.1942177, -1.3945117), (-1.0808111, -1.140731), (-2.6252296, -2.5317655), (-2.765536, -2.799066), (-1.2538995, -1.4475607), (-1.0967916, -1.1877873), (-2.578274, -2.5124717), (-2.7463362, -2.779067), (-1.2924485, -1.4867045), (-1.1193724, -1.2272551), (-2.5507271, -2.4999115), (-2.7266002, -2.756638), (-1.3154908, -1.5085967), (-1.1408777, -1.2532167), (-2.5372462, -2.4874027), (-2.7109013, -2.750843), (-1.3273722, -1.5377256), (-1.15643, -1.271528), (-2.5314696, -2.4716718), (-2.701487, -2.748343), (-1.3344945, -1.5675406), (-1.1663585, -1.2834367), (-2.5285778, -2.4506423), (-2.697459, -2.7492354), (-1.3406994, -1.6013165), (-1.1716033, -1.289952), (-2.5252962, -2.422366), (-2.6976998, -2.7537308), (-1.348561, -1.6420293), (-1.1731082, -1.2914264), (-2.5193963, -2.384529), (-2.7012894, -2.7625456), (-1.3598692, -1.693012), (-1.1714962, -1.2872288), (-2.509206, -2.3337324), (-2.7077744, -2.7773163), (-1.3761253, -1.7589394), (-1.1668034, -1.2753472), (-2.4930627, -2.2643192), (-2.7101517, -2.801121), (-1.3902475, -1.8482772), (-1.1635917, -1.2512326), (-2.4788778, -2.165524), (-2.721995, -2.839452), (-1.413721, -1.9719245), (-1.16108, -1.2396477), (-2.4631908, -2.0582395), (-2.7323737, -2.8644648), (-1.4388326, -2.0998416), (-1.1596214, -1.236983), (-2.4447541, -1.9457465), (-2.7407622, -2.8780448), (-1.4661136, -2.2268555), (-1.1590145, -1.2416462), (-2.422627, -1.8334985), (-2.7469072, -2.8796642), (-1.4959592, -2.3451915), (-1.1588956, -1.2539319), (-2.395771, -1.729292), (-2.750512, -2.8678622), (-1.528853, -2.4456918), (-1.1498995, -1.2742989), (-2.351687, -1.6387994), (-2.7582166, -2.841666), (-1.578978, -2.5241592), (-1.1456127, -1.3083402), (-2.2976258, -1.5753862), (-2.75917, -2.8003895), (-1.6356852, -2.570002), (-1.1433642, -1.3444856), (-2.2336528, -1.5341041), (-2.7568238, -2.7568676), (-1.6999966, -2.5939755), (-1.1401647, -1.3787439), (-2.1583874, -1.5092738), (-2.7543967, -2.7155633), (-1.7747288, -2.603653), (-1.1333998, -1.4085886), (-2.0684304, -1.494187), (-2.7547524, -2.6926155), (-1.8639891, -2.6211429), (-1.1206945, -1.4235584), (-1.9601046, -1.4708953), (-2.761303, -2.669866), (-1.9744782, -2.6332202), (-1.097711, -1.3913265), (-1.8244538, -1.406911), (-2.780653, -2.6943026), (-2.1166425, -2.692986), (-1.0621662, -1.3205522), (-1.6540047, -1.2942891), (-2.8090131, -2.7257593), (-2.295131, -2.776783), (-1.0241425, -1.2528204), (-1.4491193, -1.1621255), (-2.8359077, -2.7024295), (-2.5092516, -2.8264878), (-1.0206258, -1.2422224), (-1.2458681, -1.0834644), (-2.806657, -2.646559), (-2.6781883, -2.8419437), (-1.0763166, -1.2905971), (-1.1207258, -1.0789876), (-2.7322323, -2.5887854), (-2.7566254, -2.829629), (-1.1567643, -1.3579897), (-1.0807425, -1.1165642), (-2.6551347, -2.5487735), (-2.769075, -2.8084416), (-1.2260133, -1.4179574), (-1.0867735, -1.1645538), (-2.5980124, -2.5242012), (-2.7544746, -2.787325), (-1.2737232, -1.4636247), (-1.107704, -1.207954), (-2.5623698, -2.5088148), (-2.7343295, -2.7695584), (-1.3030283, -1.4974539), (-1.1296581, -1.24264), (-2.5427926, -2.4976702), (-2.7167587, -2.7496414), (-1.3201046, -1.5166875), (-1.1488286, -1.2643708), (-2.5341625, -2.4848728), (-2.7039266, -2.7460337), (-1.3287255, -1.5450412), (-1.1618395, -1.2791574), (-2.530862, -2.4676833), (-2.6972184, -2.7455757), (-1.3345556, -1.5757391), (-1.1695011, -1.2882305), (-2.5287325, -2.4442365), (-2.6954722, -2.74837), (-1.3408594, -1.6117803), (-1.1728935, -1.292385), (-2.52501, -2.4126155), (-2.6975386, -2.7547848), (-1.349789, -1.6560817), (-1.1729244, -1.2916172), (-2.5178115, -2.3702881), (-2.7026045, -2.7658453), (-1.3628775, -1.7122386), (-1.1700516, -1.284796), (-2.5056286, -2.3132632), (-2.7104456, -2.7836785), (-1.3815827, -1.785686), (-1.1640205, -1.2692145), (-2.4866998, -2.2346625), (-2.7145243, -2.8120852), (-1.3992044, -1.8864704), (-1.1621172, -1.2490572), (-2.4718652, -2.1328115), (-2.7252321, -2.8467515), (-1.4233253, -2.0122464), (-1.1605744, -1.2399503), (-2.4555063, -2.0232713), (-2.7347047, -2.8688266), (-1.4489932, -2.1408134), (-1.1596929, -1.2390413), (-2.4362745, -1.9099205), (-2.7423759, -2.8793197), (-1.4768556, -2.2662306), (-1.1593363, -1.2455472), (-2.4130633, -1.7990578), (-2.747858, -2.8771505), (-1.5074368, -2.3800223), (-1.1497722, -1.2600739), (-2.3728006, -1.6963093), (-2.7578661, -2.86005), (-1.5554013, -2.4763577), (-1.1453837, -1.2839911), (-2.323497, -1.6118823), (-2.7614753, -2.8305922), (-1.6094297, -2.5473106), (-1.1430675, -1.3202468), (-2.2646496, -1.5556544), (-2.7604904, -2.7871404), (-1.6702105, -2.5853882), (-1.140901, -1.3569207), (-2.195103, -1.520454), (-2.7579205, -2.7431552), (-1.739989, -2.6035688), (-1.1362743, -1.3904885), (-2.1123726, -1.4996065), (-2.7567308, -2.7160408), (-1.8219751, -2.6248362), (-1.1267507, -1.4097264), (-2.0137994, -1.47415), (-2.760696, -2.6885982), (-1.9221345, -2.6372035), (-1.108437, -1.4039531), (-1.8909298, -1.4317634), (-2.7739716, -2.6863642), (-2.0496979, -2.671362), (-1.0783639, -1.352772), (-1.7357, -1.3429534), (-2.7994812, -2.7248025), (-2.2132046, -2.7558093), (-1.0391716, -1.276061), (-1.5436921, -1.2084336), (-2.8240018, -2.7186606), (-2.4085472, -2.8171577), (-1.0120476, -1.2407866), (-1.329165, -1.101202), (-2.8309104, -2.6693435), (-2.6175902, -2.8453803), (-1.045657, -1.2700762), (-1.1621311, -1.0700088), (-2.7689962, -2.6067824), (-2.7365613, -2.8396473), (-1.1230689, -1.3344319), (-1.0889452, -1.0964), (-2.6868072, -2.558972), (-2.7716024, -2.8192637), (-1.2010573, -1.3991747), (-1.0808212, -1.1440587), (-2.619122, -2.5294302), (-2.7640743, -2.7967172), (-1.2585605, -1.4501343), (-1.0985413, -1.1910816), (-2.5744424, -2.5119874), (-2.7440922, -2.776867), (-1.2950611, -1.4872708), (-1.1214433, -1.2299739), (-2.548801, -2.5008802), (-2.7244358, -2.761183), (-1.3161533, -1.514799), (-1.1418653, -1.2490059), (-2.536661, -2.4806242), (-2.7105927, -2.7547174), (-1.327883, -1.5447453), (-1.155979, -1.267489), (-2.530578, -2.4635532), (-2.7023134, -2.7518992), (-1.3356819, -1.5759732), (-1.1650834, -1.279622), (-2.526809, -2.440962), (-2.6989048, -2.7527452), (-1.342935, -1.6116674), (-1.1699052, -1.2862378), (-2.5224054, -2.4107344), (-2.6994667, -2.7575324), (-1.3520048, -1.6549027), (-1.1711751, -1.2874607), (-2.5152557, -2.3702505), (-2.7032905, -2.7671528), (-1.364644, -1.7093432), (-1.1693001, -1.2823672), (-2.503644, -2.3155699), (-2.7101429, -2.783537), (-1.3824804, -1.7803376), (-1.1640817, -1.268537), (-2.4856844, -2.2400818), (-2.7133844, -2.8102267), (-1.3988818, -1.8775872), (-1.1620324, -1.2477367), (-2.4712272, -2.1394475), (-2.7242126, -2.8456752), (-1.422588, -2.002655), (-1.1603348, -1.238309), (-2.4552517, -2.0307355), (-2.7338402, -2.8683968), (-1.4479022, -2.1310585), (-1.1593441, -1.2372222), (-2.436407, -1.9177108), (-2.741677, -2.8796067), (-1.4754581, -2.2570162), (-1.1589296, -1.2435085), (-2.4136076, -1.8064914), (-2.7473438, -2.87833), (-1.5057606, -2.3721774), (-1.1493239, -1.2577269), (-2.3737721, -1.7026238), (-2.7575738, -2.8621953), (-1.5534836, -2.4706028), (-1.1449358, -1.2813661), (-2.3249376, -1.6165031), (-2.7614007, -2.8334951), (-1.6072645, -2.543929), (-1.1426622, -1.3175384), (-2.2665913, -1.5584149), (-2.7605848, -2.790394), (-1.6677648, -2.584164), (-1.1405942, -1.3544903), (-2.1975732, -1.5216646), (-2.7580798, -2.7462595), (-1.7371876, -2.6039398), (-1.1361504, -1.388637), (-2.1154382, -1.4997251), (-2.7568247, -2.718631), (-1.818682, -2.6262584), (-1.126906, -1.4086689), (-2.0175896, -1.473574), (-2.7606025, -2.6904016), (-1.9181652, -2.6392164), (-1.1090336, -1.4058636), (-1.8956158, -1.4327613), (-2.7733045, -2.6851518), (-2.0447264, -2.6713986), (-1.0795596, -1.3571931), (-1.7414752, -1.3459342), (-2.798281, -2.7214496), (-2.207083, -2.753673), (-1.0406599, -1.281715), (-1.5505068, -1.2133424), (-2.8229861, -2.7166235), (-2.401747, -2.8154378), (-1.0128037, -1.2449884), (-1.3360125, -1.1052599), (-2.8310812, -2.6683464), (-2.6116304, -2.8442564), (-1.0443283, -1.2721734), (-1.1664014, -1.0717025), (-2.770764, -2.6059086), (-2.7336671, -2.8394701), (-1.1209065, -1.3359568), (-1.090331, -1.0965652), (-2.688618, -2.5572643), (-2.7709632, -2.819533), (-1.199379, -1.4013205), (-1.0806173, -1.1436702), (-2.6201158, -2.5266533), (-2.7644243, -2.79716), (-1.2578152, -1.4533997), (-1.0977416, -1.1906246), (-2.5744543, -2.5080926), (-2.7447448, -2.7773664), (-1.2952578, -1.4917928), (-1.1204907, -1.2295947), (-2.5478978, -2.4958384), (-2.7251241, -2.755324), (-1.3175468, -1.5133777), (-1.1418006, -1.255054), (-2.5350351, -2.4833412), (-2.7097292, -2.749831), (-1.3290169, -1.5424116), (-1.1570742, -1.272864), (-2.5295794, -2.4673817), (-2.700645, -2.7476861), (-1.3359808, -1.5724362), (-1.1667175, -1.2842784), (-2.5267804, -2.4458811), (-2.696933, -2.7489936), (-1.3422227, -1.6067368), (-1.1716875, -1.2902695), (-2.5234094, -2.416843), (-2.6974788, -2.7540078), (-1.3502878, -1.6483511), (-1.1729151, -1.2911152), (-2.5172524, -2.3778398), (-2.7013867, -2.76353), (-1.3619677, -1.7007658), (-1.1709838, -1.286065), (-2.5066144, -2.3252435), (-2.708256, -2.7793353), (-1.3788123, -1.7689548), (-1.165862, -1.272919), (-2.4897482, -2.2529743), (-2.7112184, -2.8047228), (-1.3939226, -1.8619397), (-1.1630795, -1.2501636), (-2.475535, -2.1533926), (-2.7226963, -2.8418102), (-1.4174352, -1.9861068), (-1.1608622, -1.239417), (-2.4597766, -2.0455077), (-2.7328062, -2.8658574), (-1.4425985, -2.1141472), (-1.1595614, -1.2373425), (-2.4411945, -1.932808), (-2.7409985, -2.8784354), (-1.4699801, -2.2406156), (-1.1590065, -1.2426354), (-2.418794, -1.8210472), (-2.746966, -2.878804), (-1.500013, -2.3574655), (-1.1492506, -1.2557173), (-2.379413, -1.715505), (-2.7575562, -2.8645058), (-1.5474666, -2.4587777), (-1.1448762, -1.278224), (-2.3312879, -1.626768), (-2.7616718, -2.837441), (-1.60076, -2.5355651), (-1.1427293, -1.3134693), (-2.273936, -1.565691), (-2.7610445, -2.7954562), (-1.6604773, -2.5790377), (-1.1409085, -1.3500397), (-2.2062325, -1.5264572), (-2.7585301, -2.7516935), (-1.7286963, -2.6011035), (-1.1368914, -1.3842326), (-2.1258564, -1.5028816), (-2.7570064, -2.7105021), (-1.8091195, -2.6098292), (-1.128284, -1.4138267), (-2.0285099, -1.4880364), (-2.7591102, -2.6879804), (-1.90667, -2.6277075), (-1.112322, -1.4131173), (-1.9098407, -1.4493405), (-2.7702594, -2.6811128), (-2.0294712, -2.657554), (-1.0844465, -1.365707), (-1.7603294, -1.3652679), (-2.79438, -2.7175), (-2.187122, -2.737555), (-1.0459006, -1.288972), (-1.5742011, -1.2343774), (-2.820775, -2.7220411), (-2.3785715, -2.8062472), (-1.0149254, -1.2445854), (-1.3610178, -1.118888), (-2.833894, -2.6789663), (-2.5907474, -2.8407576), (-1.0384315, -1.2637755), (-1.1828505, -1.0745919), (-2.7801006, -2.6170928), (-2.723578, -2.8405173), (-1.1110872, -1.3248146), (-1.0962741, -1.0931712), (-2.698997, -2.5661814), (-2.7691796, -2.8222213), (-1.1903969, -1.3909315), (-1.0806874, -1.1383799), (-2.6284578, -2.5335376), (-2.7661786, -2.8000813), (-1.2511551, -1.4446809), (-1.0957814, -1.1855645), (-2.5804024, -2.513755), (-2.7473462, -2.7799587), (-1.2906586, -1.4844317), (-1.1182537, -1.225433), (-2.5520153, -2.5009642), (-2.727516, -2.7637756), (-1.3140156, -1.5140126), (-1.1390322, -1.2455302), (-2.5378864, -2.479477), (-2.7130842, -2.7567732), (-1.327325, -1.5455133), (-1.1537167, -1.2649574), (-2.5304353, -2.4614995), (-2.7041478, -2.7534754), (-1.3361708, -1.5778745), (-1.1633897, -1.2777929), (-2.525771, -2.4381979), (-2.7001529, -2.753965), (-1.3440822, -1.6144255), (-1.1686834, -1.2848449), (-2.520802, -2.4073386), (-2.7002606, -2.7585623), (-1.3535523, -1.6583934), (-1.1702828, -1.2862264), (-2.5132902, -2.366172), (-2.7037919, -2.7681983), (-1.3664566, -1.7136177), (-1.1685736, -1.2810009), (-2.5013978, -2.3105946), (-2.7105284, -2.7848556), (-1.3845415, -1.7856743), (-1.1633404, -1.2667083), (-2.4831135, -2.2337456), (-2.7138827, -2.8121545), (-1.4013475, -1.8845958), (-1.1615708, -1.2466023), (-2.4686682, -2.132733), (-2.7244546, -2.846915), (-1.425009, -2.0098338), (-1.1600403, -1.2376168), (-2.452689, -2.0237849), (-2.7339115, -2.869068), (-1.4502922, -2.138151), (-1.159123, -1.2368693), (-2.4337883, -1.9107848), (-2.7416313, -2.8796773), (-1.4778632, -2.2636309), (-1.1587206, -1.2435281), (-2.4108505, -1.7999983), (-2.7471983, -2.8776925), (-1.5082456, -2.3778157), (-1.1491477, -1.2581832), (-2.3708444, -1.6970534), (-2.7572968, -2.8608165), (-1.55606, -2.47481), (-1.1447272, -1.2822317), (-2.3217223, -1.6122074), (-2.761031, -2.8315258), (-1.6100439, -2.5465617), (-1.1423743, -1.3186857), (-2.2629611, -1.5554663), (-2.7601786, -2.7881122), (-1.6708922, -2.5854232), (-1.1401745, -1.3556752), (-2.1933846, -1.519787), (-2.757732, -2.7439983), (-1.7408583, -2.6042833), (-1.1355166, -1.3896772), (-2.1104848, -1.4985266), (-2.7566543, -2.7166252), (-1.8231752, -2.626163), (-1.1259613, -1.4094605), (-2.0115733, -1.4726862), (-2.7607207, -2.6887424), (-1.9238392, -2.6390443), (-1.1075969, -1.4038974), (-1.8881626, -1.4295315), (-2.7741344, -2.6863368), (-2.0521204, -2.6740744), (-1.0774724, -1.3531445), (-1.7321957, -1.3400406), (-2.7996752, -2.7242079), (-2.2164474, -2.759049), (-1.0384239, -1.2772311), (-1.5394768, -1.2053422), (-2.8237655, -2.7156851), (-2.4122362, -2.8186696), (-1.0120299, -1.243838), (-1.3249358, -1.0999293), (-2.8297153, -2.6652687), (-2.620708, -2.8456857), (-1.0470853, -1.2747378), (-1.1595327, -1.0707402), (-2.7667394, -2.602449), (-2.7378554, -2.8392143), (-1.125173, -1.3398669), (-1.0880696, -1.0981054), (-2.6843734, -2.5545797), (-2.7716694, -2.8186727), (-1.2031534, -1.4050436), (-1.0807961, -1.1459349), (-2.6168683, -2.5246701), (-2.7637303, -2.7962215), (-1.2605516, -1.4565297), (-1.0987333, -1.192774), (-2.5722845, -2.5065138), (-2.7437584, -2.7765634), (-1.2971083, -1.4944355), (-1.1215777, -1.2313718), (-2.5465355, -2.494414), (-2.724268, -2.7547572), (-1.3188012, -1.5157897), (-1.1427281, -1.256457), (-2.5341797, -2.4818997), (-2.7091234, -2.7494638), (-1.3299373, -1.544737), (-1.1577846, -1.2739141), (-2.528994, -2.465794), (-2.700297, -2.747535), (-1.3367609, -1.5748513), (-1.1672149, -1.2850074), (-2.526292, -2.4440267), (-2.6968172, -2.7490745), (-1.3430009, -1.6094041), (-1.1719912, -1.2906867), (-2.5228858, -2.414587), (-2.6975636, -2.754356), (-1.3511715, -1.651454), (-1.1730431, -1.2911919), (-2.5165882, -2.3749976), (-2.7016544, -2.7642205), (-1.3630484, -1.7045486), (-1.1709337, -1.2857116), (-2.505709, -2.32153), (-2.7087169, -2.7805111), (-1.3801937, -1.7737957), (-1.1655955, -1.2719535), (-2.488469, -2.2479072), (-2.7119517, -2.806631), (-1.3957872, -1.8684738), (-1.1630038, -1.2498175), (-2.4742079, -2.14795), (-2.723249, -2.843117), (-1.4193332, -1.9928495), (-1.160917, -1.2394543), (-2.4583836, -2.0397973), (-2.733219, -2.8666818), (-1.4445304, -2.120885), (-1.1596828, -1.2376509), (-2.4397018, -1.9270394), (-2.7413003, -2.8787575), (-1.471962, -2.24701), (-1.1591437, -1.2432319), (-2.4171433, -1.8155639), (-2.7471633, -2.8785167), (-1.5020813, -2.3630698), (-1.1494135, -1.256665), (-2.3775797, -1.710741), (-2.7576206, -2.8635435), (-1.5496219, -2.463151), (-1.1450132, -1.2795221), (-2.3291843, -1.6230745), (-2.761622, -2.835896), (-1.6030732, -2.5385132), (-1.1428059, -1.3150295), (-2.2714708, -1.5631905), (-2.7609143, -2.7935472), (-1.663043, -2.5806751), (-1.1408837, -1.3516452), (-2.203307, -1.5249416), (-2.7583935, -2.7497218), (-1.7316515, -2.6018121), (-1.1367024, -1.3857108), (-2.1223316, -1.502019), (-2.756959, -2.7086723), (-1.8126584, -2.6100113), (-1.1278502, -1.4150884), (-2.024173, -1.4875026), (-2.7592447, -2.6864011), (-1.9110565, -2.6277075), (-1.1115007, -1.4122322), (-1.9044222, -1.4470989), (-2.7709157, -2.6817317), (-2.035109, -2.6596153), (-1.0830885, -1.3631022), (-1.7535278, -1.3610271), (-2.7954967, -2.7194679), (-2.194238, -2.7416449), (-1.0443153, -1.2857772), (-1.5659922, -1.2284356), (-2.821544, -2.7213905), (-2.3866699, -2.8087811), (-1.0142621, -1.2437191), (-1.3524936, -1.1147999), (-2.8331318, -2.6766603), (-2.5980504, -2.8419), (-1.0404345, -1.2657053), (-1.1773032, -1.0738281), (-2.7771714, -2.614445), (-2.7271616, -2.8402994), (-1.1143862, -1.32779), (-1.0943469, -1.09441), (-2.6957529, -2.5641835), (-2.7699003, -2.8215044), (-1.1933676, -1.3937212), (-1.0807811, -1.1402122), (-2.6259494, -2.5321565), (-2.7656827, -2.7992907), (-1.2532905, -1.4469265), (-1.0965662, -1.1872966), (-2.5787468, -2.512754), (-2.7465622, -2.779266), (-1.2920524, -1.4862062), (-1.1191244, -1.226852), (-2.551013, -2.5001411), (-2.7268035, -2.7632353), (-1.314892, -1.5154883), (-1.1397759, -1.2466407), (-2.5373063, -2.478719), (-2.7125478, -2.7563953), (-1.3278863, -1.5468366), (-1.1542803, -1.2657613), (-2.5300975, -2.4606805), (-2.7038045, -2.7532625), (-1.3365722, -1.5791867), (-1.1637726, -1.2783304), (-2.5255342, -2.4372263), (-2.699984, -2.753918), (-1.3444395, -1.6158541), (-1.1689057, -1.2851415), (-2.5205636, -2.4061205), (-2.7002401, -2.7586937), (-1.3539505, -1.6600677), (-1.1703639, -1.2862822), (-2.5129693, -2.3645895), (-2.7039008, -2.7685475), (-1.3669672, -1.7157004), (-1.1685219, -1.2807733), (-2.5009232, -2.3084688), (-2.710766, -2.7855058), (-1.3852359, -1.788406), (-1.16314, -1.2660941), (-2.4823992, -2.2307758), (-2.7142909, -2.8132524), (-1.4023416, -1.8883767), (-1.1614729, -1.2463504), (-2.4679062, -2.1294978), (-2.7247586, -2.8476522), (-1.4260389, -2.0137706), (-1.1600143, -1.237598), (-2.4518666, -2.0203638), (-2.734132, -2.8695037), (-1.4513594, -2.142099), (-1.1591351, -1.237028), (-2.432886, -1.9073216), (-2.7417812, -2.87979), (-1.4789773, -2.2673678), (-1.1587442, -1.2438799), (-2.4098344, -1.7967182), (-2.7472801, -2.8774183), (-1.5094275, -2.3810594), (-1.1491896, -1.2587618), (-2.3696995, -1.6942235), (-2.7572923, -2.8601277), (-1.5573103, -2.477302), (-1.1447587, -1.2830253), (-2.3204007, -1.6100289), (-2.7609553, -2.830496), (-1.6114032, -2.548207), (-1.1423723, -1.3196291), (-2.2614102, -1.5539898), (-2.7600563, -2.7868805), (-1.6724181, -2.5863128), (-1.1401116, -1.3566383), (-2.1915445, -1.5188726), (-2.7576113, -2.7427397), (-1.7426381, -2.6046524), (-1.1353533, -1.390565), (-2.1082673, -1.497975), (-2.7565951, -2.7154622), (-1.8253313, -2.626251), (-1.1256477, -1.41021), (-2.0088441, -1.4723105), (-2.7607768, -2.6877317), (-1.9265335, -2.639036), (-1.1070485, -1.403345), (-1.8847564, -1.4281013), (-2.774504, -2.6866403), (-2.0555942, -2.6753218), (-1.0766181, -1.3515886), (-1.7279471, -1.337411), (-2.8002868, -2.7252603), (-2.2207966, -2.7614808), (-1.0375015, -1.2754164), (-1.5344493, -1.2017846), (-2.8240712, -2.7151046), (-2.4170372, -2.82008), (-1.0117842, -1.2434671), (-1.3199537, -1.097611), (-2.8290317, -2.6638079), (-2.624781, -2.8462799), (-1.048422, -1.2759693), (-1.1565176, -1.0703851), (-2.7648835, -2.6008842), (-2.7396798, -2.8390567), (-1.1271325, -1.3416231), (-1.0871199, -1.098836), (-2.6824548, -2.5534003), (-2.7719393, -2.8182492), (-1.2048492, -1.4066514), (-1.0809127, -1.1469575), (-2.6154168, -2.5238335), (-2.763384, -2.7957628), (-1.2617549, -1.4578183), (-1.0991939, -1.1937255), (-2.571327, -2.5058894), (-2.7432845, -2.7761605), (-1.2978936, -1.4954551), (-1.1220698, -1.2321459), (-2.5459483, -2.4938953), (-2.723849, -2.7544522), (-1.319298, -1.5166502), (-1.1431421, -1.2570543), (-2.533832, -2.4814136), (-2.708814, -2.7492423), (-1.3302629, -1.5455058), (-1.158097, -1.274351), (-2.5287821, -2.4652884), (-2.7000973, -2.7474003), (-1.336999, -1.5756005), (-1.1674306, -1.2853049), (-2.5261407, -2.4434578), (-2.6967144, -2.7490284), (-1.3432132, -1.6101993), (-1.1721228, -1.2908593), (-2.5227413, -2.4139078), (-2.6975427, -2.7544043), (-1.3514019, -1.65236), (-1.1731008, -1.29124), (-2.5164082, -2.3741503), (-2.7017043, -2.764383), (-1.3633337, -1.7056447), (-1.1709247, -1.2856162), (-2.5054574, -2.320429), (-2.7088344, -2.7808256), (-1.380568, -1.775197), (-1.1655128, -1.2716663), (-2.4881058, -2.2464101), (-2.7121558, -2.8071694), (-1.3963045, -1.870372), (-1.162972, -1.249709), (-2.4738276, -2.1463447), (-2.7234027, -2.8434777), (-1.4198629, -1.9948031), (-1.1609206, -1.2394571), (-2.4579802, -2.0381188), (-2.7333324, -2.8669004), (-1.445073, -2.1228304), (-1.1597056, -1.2377336), (-2.439266, -1.9253498), (-2.7413805, -2.8788273), (-1.4725214, -2.2488477), (-1.1591713, -1.2433995), (-2.4166589, -1.8139657), (-2.7472124, -2.8784094), (-1.5026685, -2.3646696), (-1.1494493, -1.2569342), (-2.3770406, -1.7093593), (-2.7576303, -2.8632414), (-1.5502346, -2.464387), (-1.1450422, -1.2798916), (-2.3285658, -1.6220094), (-2.7615979, -2.8354292), (-1.6037331, -2.5393345), (-1.142818, -1.3154714), (-2.2707472, -1.5624722), (-2.7608678, -2.7929811), (-1.6637777, -2.5811188), (-1.1408669, -1.352096), (-2.2024496, -1.524508), (-2.758346, -2.7491438), (-1.7325006, -2.60199), (-1.1366376, -1.3861212), (-2.1212995, -1.5017732), (-2.7569385, -2.708141), (-1.8136784, -2.6100414), (-1.1277151, -1.415433), (-2.0229056, -1.4873489), (-2.7592783, -2.685947), (-1.9123232, -2.6276898), (-1.1112531, -1.4119515), (-1.902842, -1.4464458), (-2.771102, -2.6819181), (-2.0367396, -2.6602004), (-1.0826862, -1.3623235), (-1.7515478, -1.359793), (-2.795816, -2.7200441), (-2.1962962, -2.7428205), (-1.043853, -1.2848355), (-1.5636109, -1.2267125), (-2.8217556, -2.7212055), (-2.389002, -2.8095062), (-1.0140759, -1.2434565), (-1.3500397, -1.1136185), (-2.8328965, -2.6760025), (-2.600135, -2.8422267), (-1.0410182, -1.2662488), (-1.1757224, -1.0736054), (-2.7763195, -2.613694), (-2.728171, -2.8402379), (-1.1153326, -1.3286303), (-1.0938039, -1.094758), (-2.6948204, -2.5636191), (-2.7700968, -2.8213017), (-1.1942152, -1.3945074), (-1.0808104, -1.1407287), (-2.6252315, -2.531768), (-2.7655377, -2.7990665), (-1.2538989, -1.4475571), (-1.0967901, -1.1877856), (-2.5782735, -2.512474), (-2.7463367, -2.7790682), (-1.2924484, -1.4867017), (-1.1193721, -1.2272538), (-2.5507271, -2.4999142), (-2.7266002, -2.7566392), (-1.315491, -1.5085943), (-1.1408781, -1.2532157), (-2.5372472, -2.4874048), (-2.7109013, -2.7508435), (-1.3273716, -1.5377234), (-1.1564308, -1.2715274), (-2.531471, -2.4716742), (-2.701487, -2.7483437), (-1.3344935, -1.5675383), (-1.1663586, -1.2834362), (-2.5285792, -2.4506443), (-2.6974597, -2.7492354), (-1.340698, -1.601314), (-1.171602, -1.2899519), (-2.5252972, -2.4223685), (-2.697701, -2.753731), (-1.3485602, -1.6420258), (-1.173107, -1.2914256), (-2.5193963, -2.3845327), (-2.7012904, -2.7625468), (-1.3598689, -1.6930081), (-1.1714946, -1.2872277), (-2.5092065, -2.3337367), (-2.7077758, -2.7773166), (-1.3761252, -1.7589344), (-1.1668028, -1.275347), (-2.493063, -2.2643247), (-2.710152, -2.8011203), (-1.3902475, -1.8482705), (-1.1635914, -1.2512323), (-2.4788775, -2.1655297), (-2.721995, -2.8394516), (-1.4137208, -1.9719182), (-1.1610799, -1.2396475), (-2.4631908, -2.0582452), (-2.7323737, -2.8644643), (-1.4388324, -2.099835), (-1.1596202, -1.2369817), (-2.4447534, -1.9457515), (-2.740763, -2.8780465), (-1.466114, -2.2268507), (-1.1590128, -1.2416435), (-2.4226255, -1.8335017), (-2.7469087, -2.8796666), (-1.49596, -2.3451896), (-1.1588948, -1.2539302), (-2.3957715, -1.7292942), (-2.7505133, -2.8678644), (-1.5288529, -2.44569), (-1.1498985, -1.2742965), (-2.3516867, -1.6388005), (-2.7582169, -2.841668), (-1.5789785, -2.524159), (-1.1456133, -1.308339), (-2.2976258, -1.5753869), (-2.7591698, -2.8003912), (-1.6356846, -2.5700018), (-1.1433647, -1.3444844), (-2.2336543, -1.5341054), (-2.756823, -2.7568693), (-1.6999946, -2.5939748), (-1.1401654, -1.3787427), (-2.1583903, -1.5092746), (-2.7543967, -2.7155638), (-1.7747262, -2.6036522), (-1.1334008, -1.4085878), (-2.0684345, -1.4941876), (-2.754752, -2.692616), (-1.8639842, -2.6211424), (-1.1206955, -1.4235586), (-1.9601105, -1.4708962), (-2.7613025, -2.6698656), (-1.9744719, -2.633219), (-1.0977114, -1.391329), (-1.824461, -1.4069145), (-2.780652, -2.6943007), (-2.116635, -2.6929827), (-1.0621678, -1.3205563), (-1.6540127, -1.2942945), (-2.8090117, -2.7257588), (-2.2951226, -2.7767808), (-1.0241431, -1.2528225), (-1.4491273, -1.1621294), (-2.835908, -2.7024302), (-2.509244, -2.826486), (-1.0206245, -1.2422217), (-1.2458748, -1.083466), (-2.8066597, -2.646561), (-2.6781836, -2.8419435), (-1.0763135, -1.2905952), (-1.1207287, -1.0789865), (-2.7322361, -2.5887864), (-2.7566245, -2.8296297), (-1.1567608, -1.3579882), (-1.080742, -1.1165621), (-2.6551366, -2.5487733), (-2.7690754, -2.8084428), (-1.2260114, -1.4179572), (-1.0867721, -1.1645513), (-2.5980132, -2.5241992), (-2.754475, -2.7873256), (-1.2737223, -1.4636252), (-1.1077033, -1.2079523), (-2.562371, -2.5088131), (-2.7343304, -2.7695594), (-1.3030272, -1.4974554), (-1.1296573, -1.2426387), (-2.5427938, -2.4976683), (-2.7167594, -2.7496426), (-1.3201038, -1.5166886), (-1.1488274, -1.2643687), (-2.5341623, -2.4848707), (-2.7039275, -2.7460358), (-1.3287264, -1.5450444), (-1.1618387, -1.2791556), (-2.5308602, -2.4676797), (-2.6972182, -2.7455761), (-1.3345572, -1.5757421), (-1.1695015, -1.2882303), (-2.5287316, -2.444234), (-2.6954722, -2.7483702), (-1.3408602, -1.6117829), (-1.1728935, -1.2923845), (-2.525009, -2.4126127), (-2.6975381, -2.7547853), (-1.3497896, -1.656085), (-1.172925, -1.2916164), (-2.5178123, -2.3702848), (-2.7026045, -2.7658467), (-1.3628768, -1.7122418), (-1.1700504, -1.2847937), (-2.5056283, -2.3132582), (-2.7104468, -2.7836804), (-1.3815836, -1.785692), (-1.1640196, -1.269213), (-2.486698, -2.2346554), (-2.7145243, -2.8120873), (-1.3992068, -1.8864788), (-1.1621177, -1.2490563), (-2.4718637, -2.1328044), (-2.7252324, -2.8467536), (-1.4233272, -2.0122552), (-1.1605747, -1.2399498), (-2.4555047, -2.0232637), (-2.7347047, -2.8688278), (-1.4489954, -2.1408217), (-1.1596935, -1.2390407), (-2.4362733, -1.9099122), (-2.7423759, -2.87932), (-1.4768573, -2.2662385), (-1.1593364, -1.2455473), (-2.4130614, -1.7990503), (-2.747858, -2.8771505), (-1.5074387, -2.3800302), (-1.1497724, -1.260075), (-2.3727992, -1.6963022), (-2.7578664, -2.860049), (-1.555403, -2.4763641), (-1.1453829, -1.2839924), (-2.323495, -1.6118762), (-2.7614758, -2.8305902), (-1.609432, -2.5473158), (-1.1430665, -1.3202487), (-2.2646465, -1.5556494), (-2.7604914, -2.7871373), (-1.670214, -2.5853918), (-1.1409003, -1.356924), (-2.1950982, -1.5204506), (-2.75792, -2.7431514), (-1.739993, -2.603572), (-1.1362734, -1.390492), (-2.1123674, -1.499603), (-2.7567313, -2.716036), (-1.8219804, -2.6248386), (-1.1267488, -1.4097314), (-2.0137918, -1.4741474), (-2.760697, -2.688592), (-1.922142, -2.637205), (-1.1084344, -1.4039555), (-1.8909192, -1.431758), (-2.7739723, -2.6863608), (-2.0497081, -2.6713667), (-1.0783617, -1.3527713), (-1.7356875, -1.3429447), (-2.7994828, -2.7248018), (-2.213218, -2.7558174), (-1.0391698, -1.2760596), (-1.5436773, -1.2084235), (-2.824002, -2.7186556), (-2.4085608, -2.8171608), (-1.0120479, -1.2407889), (-1.3291513, -1.1011975), (-2.8309073, -2.6693373), (-2.6176004, -2.84538), (-1.0456612, -1.2700814), (-1.1621234, -1.0700103), (-2.768991, -2.6067777), (-2.736566, -2.839645), (-1.1230747, -1.3344363), (-1.0889426, -1.0964037), (-2.686801, -2.55897), (-2.771602, -2.8192623), (-1.2010621, -1.3991785), (-1.0808219, -1.1440625), (-2.6191175, -2.5294292), (-2.764073, -2.7967155), (-1.2585639, -1.4501373), (-1.0985423, -1.1910846), (-2.5744398, -2.5119865), (-2.744091, -2.7768652), (-1.295063, -1.4872723), (-1.1214447, -1.2299762), (-2.5488, -2.5008798), (-2.7244349, -2.7611818), (-1.3161541, -1.5147997), (-1.1418657, -1.2490071), (-2.5366592, -2.480624), (-2.710591, -2.7547164), (-1.3278841, -1.5447463), (-1.1559811, -1.2674909), (-2.530578, -2.463554), (-2.7023122, -2.7518983), (-1.3356817, -1.5759723), (-1.165084, -1.2796227), (-2.5268095, -2.440963), (-2.6989048, -2.7527447), (-1.3429352, -1.6116667), (-1.1699059, -1.2862384), (-2.5224056, -2.4107358), (-2.6994665, -2.7575324), (-1.3520044, -1.6549021), (-1.1711762, -1.2874612), (-2.5152571, -2.3702512), (-2.7032897, -2.7671523), (-1.3646426, -1.7093424), (-1.1693008, -1.2823681), (-2.503646, -2.315571), (-2.7101429, -2.7835364), (-1.3824791, -1.780337), (-1.1640824, -1.2685376), (-2.4856868, -2.2400825), (-2.7133846, -2.8102257), (-1.3988796, -1.877586), (-1.1620321, -1.2477375), (-2.4712293, -2.1394482), (-2.7242136, -2.8456738), (-1.422587, -2.002654), (-1.1603343, -1.2383102), (-2.4552526, -2.0307362), (-2.7338405, -2.8683956), (-1.4479017, -2.1310577), (-1.1593444, -1.2372233), (-2.4364083, -1.9177115), (-2.7416778, -2.8796065), (-1.4754579, -2.2570157), (-1.1589295, -1.2435089), (-2.4136083, -1.8064927), (-2.7473438, -2.87833), (-1.5057601, -2.3721764), (-1.149324, -1.2577268), (-2.3737733, -1.702625), (-2.757574, -2.8621955), (-1.5534825, -2.4706013), (-1.1449357, -1.2813666), (-2.3249385, -1.6165048), (-2.7614007, -2.8334944), (-1.6072639, -2.5439281), (-1.1426622, -1.3175396), (-2.2665918, -1.5584158), (-2.760585, -2.7903926), (-1.6677648, -2.5841634), (-1.1405945, -1.3544918), (-2.1975741, -1.5216659), (-2.75808, -2.7462587), (-1.7371862, -2.6039386), (-1.1361495, -1.3886375), (-2.1154382, -1.4997256), (-2.7568252, -2.7186298), (-1.8186833, -2.6262577), (-1.1269065, -1.4086696), (-2.0175884, -1.4735746), (-2.760603, -2.690401), (-1.9181671, -2.639216), (-1.1090332, -1.4058639), (-1.895614, -1.4327612), (-2.7733045, -2.6851518), (-2.0447283, -2.6713989), (-1.0795597, -1.3571929), (-1.7414733, -1.3459339), (-2.7982807, -2.7214508), (-2.207085, -2.7536738), (-1.0406597, -1.2817134), (-1.5505047, -1.213341), (-2.8229861, -2.7166235), (-2.4017494, -2.8154383), (-1.0128039, -1.244989), (-1.3360102, -1.1052599), (-2.8310812, -2.6683457), (-2.611633, -2.844256), (-1.0443294, -1.2721741), (-1.1663998, -1.0717037), (-2.7707632, -2.6059086), (-2.733669, -2.8394697), (-1.1209084, -1.335957), (-1.0903307, -1.096566), (-2.6886165, -2.5572648), (-2.7709632, -2.819533), (-1.1993809, -1.401321), (-1.0806184, -1.1436712), (-2.6201148, -2.5266535), (-2.7644234, -2.7971601), (-1.2578163, -1.4534009), (-1.0977427, -1.1906253), (-2.5744534, -2.5080924), (-2.7447443, -2.777366), (-1.2952589, -1.4917929), (-1.1204916, -1.2295959), (-2.5478973, -2.4958394), (-2.7251234, -2.755323), (-1.3175473, -1.513377), (-1.1418011, -1.255055), (-2.5350342, -2.483343), (-2.7097278, -2.74983), (-1.3290168, -1.542409), (-1.1570762, -1.2728647), (-2.5295808, -2.4673848), (-2.7006433, -2.7476854), (-1.3359787, -1.5724325), (-1.1667187, -1.2842789), (-2.5267835, -2.4458847), (-2.6969333, -2.7489927), (-1.3422202, -1.6067324), (-1.1716877, -1.2902704), (-2.5234127, -2.4168472), (-2.697479, -2.7540064), (-1.3502847, -1.6483465), (-1.1729141, -1.2911161), (-2.517255, -2.3778436), (-2.7013876, -2.7635283), (-1.3619651, -1.7007606), (-1.1709828, -1.2860659), (-2.5066168, -2.325248), (-2.708256, -2.779333), (-1.3788095, -1.7689488), (-1.1658614, -1.27292), (-2.4897501, -2.2529793), (-2.7112179, -2.8047209), (-1.3939204, -1.861933), (-1.1630796, -1.250163), (-2.4755366, -2.1533978), (-2.7226956, -2.8418107), (-1.4174329, -1.986102), (-1.1608629, -1.2394161), (-2.45978, -2.0455117), (-2.7328062, -2.8658578), (-1.4425955, -2.114143), (-1.1595615, -1.2373418), (-2.4411976, -1.9328123), (-2.7409987, -2.8784356), (-1.4699771, -2.2406104), (-1.1590059, -1.2426347), (-2.4187965, -1.821052), (-2.7469661, -2.8788052), (-1.5000103, -2.3574617), (-1.1492499, -1.255716), (-2.3794155, -1.7155082), (-2.7575572, -2.8645072), (-1.5474639, -2.4587748), (-1.1448755, -1.278223), (-2.331291, -1.6267712), (-2.761672, -2.8374424), (-1.6007566, -2.5355623), (-1.1427289, -1.3134677), (-2.273939, -1.5656936), (-2.7610443, -2.795458), (-1.6604738, -2.5790358), (-1.140908, -1.3500378), (-2.2062361, -1.5264587), (-2.7585306, -2.7516963), (-1.7286928, -2.6011035), (-1.1368914, -1.3842301), (-2.1258614, -1.5028815), (-2.7570071, -2.7105052), (-1.8091148, -2.6098306), (-1.1282841, -1.4138252), (-2.0285163, -1.4880356), (-2.759111, -2.6879816), (-1.9066641, -2.6277082), (-1.1123229, -1.4131192), (-1.9098487, -1.4493425), (-2.7702591, -2.6811109), (-2.0294633, -2.657552), (-1.0844477, -1.365711), (-1.7603389, -1.3652725), (-2.7943788, -2.7174969), (-2.1871123, -2.7375505), (-1.0459026, -1.2889773), (-1.5742124, -1.2343853), (-2.8207743, -2.7220411), (-2.3785608, -2.8062437), (-1.014926, -1.244587), (-1.3610294, -1.1188939), (-2.8338957, -2.678969), (-2.590738, -2.840756), (-1.0384287, -1.2637739), (-1.182857, -1.0745927), (-2.7801042, -2.6170948), (-2.7235737, -2.8405175), (-1.1110829, -1.3248124), (-1.0962758, -1.0931699), (-2.6990013, -2.5661821), (-2.7691798, -2.8222218), (-1.1903933, -1.3909296), (-1.0806859, -1.138378), (-2.6284592, -2.5335374), (-2.7661796, -2.8000808), (-1.2511543, -1.4446794), (-1.0957808, -1.1855643), (-2.5804024, -2.5137565), (-2.7473464, -2.7799582), (-1.2906586, -1.4844289), (-1.118254, -1.225433), (-2.5520158, -2.5009673), (-2.7275155, -2.763775), (-1.3140146, -1.5140086), (-1.1390324, -1.2455304), (-2.5378873, -2.4794803), (-2.713084, -2.756772), (-1.3273231, -1.545508), (-1.153716, -1.2649573), (-2.5304368, -2.4615045), (-2.7041485, -2.7534742), (-1.3361697, -1.5778679), (-1.1633888, -1.2777934), (-2.5257723, -2.4382038), (-2.700154, -2.753964), (-1.3440812, -1.6144189), (-1.1686833, -1.2848462), (-2.5208042, -2.4073462), (-2.7002602, -2.75856), (-1.35355, -1.6583836), (-1.170284, -1.2862289), (-2.5132933, -2.3661835), (-2.7037904, -2.768194), (-1.3664525, -1.7136025), (-1.168575, -1.2810042), (-2.5014026, -2.3106105), (-2.7105265, -2.7848504), (-1.3845358, -1.7856544), (-1.1633421, -1.2667131), (-2.4831195, -2.233767), (-2.7138796, -2.812147), (-1.4013399, -1.88457), (-1.1615728, -1.2466048), (-2.4686756, -2.1327558), (-2.7244525, -2.8469107), (-1.4250004, -2.0098076), (-1.1600405, -1.2376169), (-2.4526958, -2.0238075), (-2.73391, -2.8690648), (-1.450284, -2.1381254), (-1.1591233, -1.2368691), (-2.4337964, -1.9108073), (-2.7416308, -2.8796756), (-1.4778545, -2.2636068), (-1.158721, -1.2435263), (-2.4108596, -1.80002), (-2.7471979, -2.8776944), (-1.5082363, -2.3777947), (-1.1491473, -1.2581799), (-2.3708525, -1.6970713), (-2.7572968, -2.8608208), (-1.5560517, -2.4747946), (-1.1447271, -1.2822262), (-2.321731, -1.6122209), (-2.7610314, -2.8315337), (-1.610035, -2.546553), (-1.1423744, -1.3186797), (-2.2629714, -1.5554749), (-2.7601793, -2.78812), (-1.6708815, -2.5854185), (-1.1401749, -1.3556694), (-2.193397, -1.5197924), (-2.7577326, -2.7440069), (-1.7408466, -2.6042821), (-1.1355183, -1.3896728), (-2.1104999, -1.4985296), (-2.7566538, -2.7166312), (-1.8231609, -2.6261625), (-1.1259646, -1.4094566), (-2.0115914, -1.4726884), (-2.7607188, -2.6887486), (-1.9238209, -2.6390448), (-1.1076019, -1.4039019), (-1.8881865, -1.4295409), (-2.7741308, -2.6863337), (-2.052096, -2.6740656), (-1.0774797, -1.3531557), (-1.7322265, -1.3400594), (-2.7996705, -2.7242), (-2.2164156, -2.7590306), (-1.0384312, -1.2772431), (-1.5395138, -1.2053682), (-2.8237627, -2.71569), (-2.4122007, -2.8186588), (-1.0120326, -1.2438397), (-1.3249732, -1.0999464), (-2.8297193, -2.6652803), (-2.6206765, -2.8456805), (-1.0470763, -1.2747264), (-1.1595566, -1.0707428), (-2.7667525, -2.6024635), (-2.7378397, -2.8392158), (-1.1251583, -1.3398514), (-1.0880779, -1.0980994), (-2.6843872, -2.5545907), (-2.7716656, -2.8186762), (-1.2031398, -1.4050287), (-1.0807959, -1.1459264), (-2.6168795, -2.524679), (-2.763732, -2.7962253), (-1.2605413, -1.4565161), (-1.09873, -1.1927656), (-2.5722926, -2.5065217), (-2.7437615, -2.776566), (-1.2971015, -1.4944233), (-1.1215748, -1.2313666), (-2.5465407, -2.4944232), (-2.7242694, -2.7547584), (-1.3187956, -1.5157766), (-1.1427252, -1.2564522), (-2.534183, -2.481909), (-2.7091248, -2.7494643), (-1.3299332, -1.544724), (-1.1577826, -1.2739111), (-2.5289974, -2.4658048), (-2.7002976, -2.747534), (-1.3367565, -1.574836), (-1.1672136, -1.2850062), (-2.5262952, -2.4440403), (-2.6968167, -2.7490716), (-1.3429962, -1.6093868), (-1.1719913, -1.2906882), (-2.5228899, -2.4146032), (-2.697562, -2.7543511), (-1.3511659, -1.6514338), (-1.1730442, -1.2911954), (-2.5165942, -2.375018), (-2.7016523, -2.7642136), (-1.3630413, -1.7045233), (-1.1709356, -1.2857176), (-2.5057166, -2.3215568), (-2.708714, -2.7805016), (-1.3801838, -1.7737623), (-1.1655983, -1.2719622), (-2.4884806, -2.2479436), (-2.7119477, -2.806618), (-1.3957732, -1.8684288), (-1.1630045, -1.2498211), (-2.4742198, -2.1479883), (-2.723247, -2.8431087), (-1.4193189, -1.9928039), (-1.1609162, -1.2394552), (-2.4583957, -2.0398374), (-2.7332177, -2.8666763), (-1.4445165, -2.120839), (-1.1596823, -1.2376504), (-2.439714, -1.9270802), (-2.7412994, -2.8787544), (-1.4719477, -2.2469661), (-1.1591425, -1.2432301), (-2.4171567, -1.8156031), (-2.7471638, -2.8785179), (-1.5020667, -2.3630307), (-1.149412, -1.2566603), (-2.3775942, -1.7107761), (-2.7576218, -2.8635492), (-1.5496064, -2.4631193), (-1.1450126, -1.2795147), (-2.329201, -1.6231023), (-2.761623, -2.835906), (-1.6030562, -2.5384912), (-1.1428057, -1.3150196), (-2.2714891, -1.5632101), (-2.7609155, -2.7935617), (-1.6630249, -2.580664), (-1.1408848, -1.3516347), (-2.2033288, -1.5249537), (-2.758395, -2.7497365), (-1.7316304, -2.6018062), (-1.1367043, -1.3857), (-2.1223578, -1.5020274), (-2.7569594, -2.7086878), (-1.8126326, -2.6100097), (-1.1278538, -1.4150777), (-2.0242054, -1.487507), (-2.759244, -2.6864147), (-1.9110236, -2.627708), (-1.1115066, -1.4122375), (-1.9044629, -1.4471147), (-2.770911, -2.6817281), (-2.0350666, -2.6596012), (-1.0830992, -1.3631214), (-1.7535801, -1.361058), (-2.795489, -2.7194536), (-2.194184, -2.7416158), (-1.0443273, -1.2858016), (-1.566055, -1.2284791), (-2.821539, -2.7213953), (-2.3866086, -2.8087633), (-1.0142671, -1.2437261), (-1.352559, -1.1148298), (-2.8331378, -2.6766777), (-2.5979946, -2.8418927), (-1.040419, -1.2656908), (-1.1773456, -1.0738324), (-2.7771947, -2.6144645), (-2.727135, -2.840302), (-1.1143605, -1.3277677), (-1.094361, -1.0944), (-2.6957774, -2.5641983), (-2.7698956, -2.8215103), (-1.193346, -1.393701), (-1.0807809, -1.1401986), (-2.625968, -2.532167), (-2.7656865, -2.7992966), (-1.2532753, -1.4469092), (-1.0965598, -1.1872838), (-2.578757, -2.5127614), (-2.7465677, -2.77927), (-1.2920434, -1.4861922), (-1.1191181, -1.2268424), (-2.5510194, -2.5001483), (-2.7268088, -2.7632384), (-1.3148861, -1.5154757), (-1.1397693, -1.2466327), (-2.5373092, -2.4787257), (-2.7125528, -2.7563975), (-1.3278838, -1.5468252), (-1.1542759, -1.2657561), (-2.5300984, -2.4606886), (-2.703807, -2.7532632), (-1.3365705, -1.5791743), (-1.1637702, -1.2783269), (-2.5255356, -2.4372365), (-2.699985, -2.7539177), (-1.344437, -1.6158396), (-1.1689048, -1.28514), (-2.5205655, -2.406134), (-2.7002397, -2.7586913), (-1.3539473, -1.6600494), (-1.170365, -1.2862833), (-2.5129733, -2.3646088), (-2.7038987, -2.7685425), (-1.366961, -1.7156757), (-1.1685234, -1.2807775), (-2.5009294, -2.3084939), (-2.7107632, -2.785497), (-1.3852276, -1.7883744), (-1.1631427, -1.2661028), (-2.4824083, -2.230811), (-2.7142859, -2.8132384), (-1.4023296, -1.8883321), (-1.161475, -1.2463549), (-2.467916, -2.1295369), (-2.7247543, -2.8476427), (-1.426026, -2.0137231), (-1.1600151, -1.2375995), (-2.4518776, -2.0204055), (-2.7341287, -2.8694975), (-1.4513451, -2.142051), (-1.1591351, -1.237027), (-2.4328983, -1.9073641), (-2.7417793, -2.879788), (-1.4789621, -2.2673228), (-1.1587442, -1.2438767), (-2.4098487, -1.7967577), (-2.7472792, -2.8774207), (-1.509411, -2.38102), (-1.1491889, -1.2587558), (-2.3697152, -1.6942582), (-2.7572925, -2.860135), (-1.5572937, -2.4772704), (-1.1447589, -1.283015), (-2.3204184, -1.6100559), (-2.7609556, -2.8305085), (-1.6113847, -2.5481858), (-1.1423721, -1.3196174), (-2.261431, -1.5540086), (-2.7600582, -2.786895), (-1.6723983, -2.5863008), (-1.1401123, -1.3566271), (-2.1915681, -1.5188847), (-2.7576127, -2.7427542), (-1.7426151, -2.6046462), (-1.135355, -1.3905541), (-2.108295, -1.497982), (-2.7565954, -2.7154756), (-1.825304, -2.6262498), (-1.1256512, -1.4102024), (-2.0088778, -1.4723155), (-2.7607758, -2.6877425), (-1.9265001, -2.6390355), (-1.1070554, -1.4033529), (-1.884798, -1.4281191), (-2.7744985, -2.686635), (-2.0555515, -2.6753054), (-1.0766294, -1.3516084), (-1.7279994, -1.3374447), (-2.8002782, -2.7252471), (-2.2207422, -2.7614496), (-1.0375137, -1.2754388), (-1.5345114, -1.2018291), (-2.8240662, -2.7151117), (-2.4169776, -2.8200626), (-1.0117874, -1.2434713), (-1.3200154, -1.0976388), (-2.82904, -2.6638255), (-2.6247299, -2.8462725), (-1.0484045, -1.2759542), (-1.156554, -1.0703895), (-2.764906, -2.6009028), (-2.7396576, -2.8390586), (-1.1271092, -1.3416021), (-1.0871315, -1.0988277), (-2.6824775, -2.5534153), (-2.7719352, -2.8182538), (-1.2048286, -1.4066309), (-1.0809121, -1.1469463), (-2.6154344, -2.5238457), (-2.763387, -2.7957673), (-1.2617391, -1.4577997), (-1.0991884, -1.1937138), (-2.5713394, -2.5058994), (-2.7432902, -2.7761655), (-1.2978823, -1.4954395), (-1.1220628, -1.232135), (-2.5459566, -2.4939036), (-2.7238553, -2.7544563), (-1.3192908, -1.5166361), (-1.1431358, -1.2570453), (-2.5338368, -2.4814217), (-2.7088187, -2.7492464), (-1.3302579, -1.5454936), (-1.1580917, -1.2743438), (-2.5287855, -2.4652965), (-2.7001016, -2.7474039), (-1.3369957, -1.5755895), (-1.167426, -1.2852987), (-2.526142, -2.4434655), (-2.6967168, -2.749031), (-1.3432114, -1.6101886), (-1.1721207, -1.2908542), (-2.5227425, -2.413916), (-2.6975431, -2.7544067), (-1.3513992, -1.652349), (-1.1731004, -1.2912363), (-2.5164108, -2.37416), (-2.7017033, -2.7643843), (-1.3633292, -1.7056326), (-1.1709247, -1.2856133), (-2.5054615, -2.3204398), (-2.708833, -2.7808263), (-1.3805627, -1.7751845), (-1.165514, -1.2716657), (-2.4881105, -2.2464225), (-2.7121527, -2.8071673), (-1.3962976, -1.8703562), (-1.162973, -1.2497071), (-2.4738343, -2.1463578), (-2.7234013, -2.8434775), (-1.4198552, -1.9947871), (-1.1609205, -1.2394546), (-2.4579873, -2.0381317), (-2.733332, -2.8668997), (-1.4450648, -2.1228147), (-1.1597046, -1.2377319), (-2.4392724, -1.9253631), (-2.7413807, -2.878827), (-1.4725144, -2.2488334), (-1.1591709, -1.2433978), (-2.4166656, -1.8139777), (-2.7472122, -2.8784099), (-1.5026611, -2.364657), (-1.1494489, -1.2569327), (-2.377047, -1.709371), (-2.7576306, -2.8632438), (-1.5502284, -2.4643776), (-1.1450422, -1.279889), (-2.328572, -1.6220173), (-2.7615976, -2.8354318), (-1.6037267, -2.5393276), (-1.1428181, -1.3154683), (-2.2707536, -1.5624784), (-2.7608683, -2.7929854), (-1.6637714, -2.5811157), (-1.1408665, -1.3520932), (-2.2024565, -1.5245109), (-2.7583468, -2.749148), (-1.7324938, -2.6019905), (-1.1366382, -1.3861192), (-2.1213078, -1.5017736), (-2.7569382, -2.7081442), (-1.81367, -2.6100428), (-1.1277167, -1.415432), (-2.0229158, -1.4873495), (-2.7592776, -2.6859498), (-1.9123135, -2.627691), (-1.1112558, -1.4119555), (-1.9028537, -1.4464504), (-2.7710989, -2.6819148), (-2.0367272, -2.6601965), (-1.0826916, -1.3623319), (-1.7515641, -1.359803), (-2.7958121, -2.7200387), (-2.1962788, -2.7428114), (-1.0438571, -1.2848443), (-1.5636303, -1.2267258), (-2.8217542, -2.7212055), (-2.388984, -2.8095007), (-1.0140775, -1.2434605), (-1.3500595, -1.1136281), (-2.8328986, -2.6760063), (-2.600118, -2.8422236), (-1.0410129, -1.266246), (-1.1757352, -1.0736082), (-2.776327, -2.6136985), (-2.7281632, -2.8402376), (-1.1153251, -1.3286247), (-1.0938083, -1.0947559), (-2.694828, -2.5636227), (-2.7700956, -2.8213031), (-1.194208, -1.3945024), (-1.0808095, -1.1407249), (-2.6252375, -2.5317698), (-2.76554, -2.7990685), (-1.2538939, -1.4475536), (-1.0967879, -1.1877824), (-2.578278, -2.5124755), (-2.7463396, -2.7790697), (-1.2924454, -1.4866985), (-1.11937, -1.2272507), (-2.5507302, -2.4999163), (-2.7266026, -2.7566404), (-1.3154888, -1.5085901), (-1.1408762, -1.2532134), (-2.5372481, -2.4874082), (-2.7109022, -2.7508445), (-1.32737, -1.5377188), (-1.1564298, -1.2715254), (-2.5314722, -2.4716775), (-2.7014878, -2.7483444), (-1.3344918, -1.5675335), (-1.1663582, -1.2834347), (-2.528581, -2.4506485), (-2.6974592, -2.7492354), (-1.3406961, -1.601309), (-1.1716027, -1.2899508), (-2.5252988, -2.4223726), (-2.6976998, -2.7537303), (-1.3485582, -1.6420206), (-1.173108, -1.2914265), (-2.5193987, -2.384538), (-2.7012894, -2.7625449), (-1.3598665, -1.6930014), (-1.1714959, -1.2872293), (-2.5092092, -2.3337438), (-2.7077744, -2.7773147), (-1.3761216, -1.7589262), (-1.1668031, -1.2753487), (-2.4930665, -2.2643328), (-2.7101514, -2.8011172), (-1.3902436, -1.8482604), (-1.1635915, -1.251233), (-2.4788804, -2.1655376), (-2.7219937, -2.839449), (-1.4137168, -1.9719082), (-1.1610793, -1.2396479), (-2.4631927, -2.0582528), (-2.7323732, -2.864463), (-1.4388298, -2.0998268), (-1.1596206, -1.2369823), (-2.444756, -1.9457594), (-2.7407622, -2.8780444), (-1.4661111, -2.2268407), (-1.1590134, -1.2416435), (-2.4226282, -1.8335103), (-2.7469077, -2.8796663), (-1.4959567, -2.3451805), (-1.1588944, -1.2539295), (-2.3957736, -1.7293022), (-2.7505133, -2.8678648), (-1.5288506, -2.4456832), (-1.1498989, -1.2742962), (-2.3516898, -1.6388065), (-2.7582169, -2.8416693), (-1.578975, -2.5241535), (-1.1456128, -1.308337), (-2.2976298, -1.5753918), (-2.7591715, -2.8003938), (-1.6356816, -2.569998), (-1.1433632, -1.3444822), (-2.2336571, -1.5341084), (-2.7568252, -2.7568717), (-1.6999927, -2.5939734), (-1.1401646, -1.3787411), (-2.1583924, -1.5092763), (-2.7543976, -2.7155664), (-1.774725, -2.6036522), (-1.1334006, -1.4085871), (-2.0684361, -1.4941885), (-2.7547526, -2.6926181), (-1.8639839, -2.6211436), (-1.1206954, -1.4235581), (-1.9601104, -1.4708956), (-2.7613022, -2.669867), (-1.9744728, -2.6332207), (-1.0977126, -1.391329), (-1.8244607, -1.406914), (-2.7806516, -2.6943016), (-2.1166358, -2.692984), (-1.062169, -1.320556), (-1.6540133, -1.2942942), (-2.8090117, -2.7257597), (-2.2951226, -2.7767816), (-1.024144, -1.2528226), (-1.4491283, -1.1621294), (-2.8359082, -2.702431), (-2.509244, -2.8264873), (-1.0206249, -1.242222), (-1.2458754, -1.0834653), (-2.8066602, -2.6465614), (-2.678184, -2.8419452), (-1.0763136, -1.2905958), (-1.1207289, -1.0789864), (-2.7322366, -2.5887864), (-2.7566252, -2.8296301), (-1.156761, -1.3579886), (-1.0807418, -1.1165614), (-2.6551366, -2.5487726), (-2.769076, -2.8084438), (-1.2260125, -1.417958), (-1.0867727, -1.1645509), (-2.598013, -2.5241992), (-2.754475, -2.787327), (-1.2737234, -1.4636263), (-1.1077038, -1.2079518), (-2.56237, -2.5088124), (-2.7343304, -2.7695603), (-1.3030288, -1.4974569), (-1.1296573, -1.2426388), (-2.542792, -2.4976668), (-2.71676, -2.749642), (-1.3201057, -1.51669), (-1.1488281, -1.2643701), (-2.534161, -2.4848704), (-2.7039268, -2.746035), (-1.328727, -1.5450444), (-1.1618394, -1.2791572), (-2.5308602, -2.4676807), (-2.6972182, -2.7455754), (-1.3345578, -1.5757414), (-1.169502, -1.2882317), (-2.528731, -2.4442358), (-2.6954715, -2.7483695), (-1.3408611, -1.6117808), (-1.1728948, -1.2923856), (-2.5250092, -2.4126163), (-2.6975374, -2.7547843), (-1.3497894, -1.6560802), (-1.1729258, -1.291618), (-2.5178125, -2.3702908), (-2.7026033, -2.765844), (-1.3628763, -1.7122352), (-1.1700524, -1.2847971), (-2.50563, -2.3132668), (-2.7104445, -2.7836769), (-1.3815806, -1.7856812), (-1.1640216, -1.2692162), (-2.486702, -2.2346675), (-2.7145228, -2.8120825), (-1.3992021, -1.8864636), (-1.1621182, -1.2490581), (-2.471868, -2.1328175), (-2.725232, -2.84675), (-1.4233227, -2.012239), (-1.1605744, -1.2399502), (-2.4555085, -2.0232778), (-2.7347038, -2.868825), (-1.4489906, -2.1408043), (-1.159693, -1.239041), (-2.4362767, -1.9099286), (-2.7423759, -2.8793194), (-1.4768529, -2.2662213), (-1.1593354, -1.2455453), (-2.4130647, -1.799065), (-2.7478585, -2.877152), (-1.5074345, -2.3800151), (-1.1497713, -1.2600722), (-2.3728027, -1.6963158), (-2.757867, -2.860052), (-1.5553991, -2.4763515), (-1.1453829, -1.2839884), (-2.3234997, -1.6118872), (-2.7614758, -2.830595), (-1.6094265, -2.5473065), (-1.1430666, -1.3202431), (-2.2646527, -1.5556568), (-2.7604911, -2.7871437), (-1.670208, -2.585387), (-1.1409004, -1.3569183), (-2.1951048, -1.5204549), (-2.7579212, -2.743158), (-1.7399869, -2.6035688), (-1.1362736, -1.3904858), (-2.1123753, -1.4996063), (-2.7567315, -2.7160432), (-1.8219721, -2.6248376), (-1.1267499, -1.4097259), (-2.0138025, -1.4741495), (-2.760697, -2.6885982), (-1.9221317, -2.6372037), (-1.1084366, -1.4039539), (-1.8909328, -1.4317639), (-2.7739704, -2.6863635), (-2.0496936, -2.6713622), (-1.0783654, -1.3527741), (-1.7357051, -1.3429546), (-2.79948, -2.7248008), (-2.2131994, -2.7558086), (-1.0391729, -1.276064), (-1.543698, -1.208436), (-2.8240016, -2.7186604), (-2.4085414, -2.8171573), (-1.0120479, -1.2407881), (-1.3291707, -1.1012042), (-2.83091, -2.669344), (-2.617584, -2.8453796), (-1.0456554, -1.2700757), (-1.1621351, -1.0700095), (-2.7689981, -2.6067836), (-2.7365592, -2.8396475), (-1.123067, -1.3344303), (-1.0889459, -1.0963992), (-2.6868086, -2.5589726), (-2.7716012, -2.8192647), (-1.2010558, -1.3991743), (-1.0808225, -1.1440582), (-2.6191237, -2.5294304), (-2.7640736, -2.7967176), (-1.2585592, -1.450134), (-1.0985416, -1.19108), (-2.5744438, -2.5119865), (-2.7440922, -2.7768674), (-1.2950594, -1.4872704), (-1.1214426, -1.2299716), (-2.5488024, -2.5008788), (-2.7244368, -2.7611847), (-1.3161522, -1.5147997), (-1.1418647, -1.2490039), (-2.5366619, -2.4806225), (-2.7105932, -2.7547183), (-1.3278825, -1.5447465), (-1.155978, -1.2674873), (-2.5305777, -2.463551), (-2.7023149, -2.7518995), (-1.3356823, -1.5759739), (-1.1650815, -1.2796209), (-2.5268078, -2.4409606), (-2.698906, -2.7527454), (-1.3429364, -1.6116687), (-1.1699047, -1.2862375), (-2.522404, -2.410733), (-2.6994672, -2.7575326), (-1.3520061, -1.6549038), (-1.1711754, -1.2874604), (-2.5152547, -2.370249), (-2.70329, -2.767153), (-1.3646448, -1.7093447), (-1.1693001, -1.2823675), (-2.503643, -2.3155687), (-2.710143, -2.7835374), (-1.3824816, -1.7803397), (-1.1640816, -1.2685372), (-2.4856832, -2.2400796), (-2.713384, -2.810226), (-1.3988823, -1.8775896), (-1.1620325, -1.2477374), (-2.4712265, -2.1394444), (-2.7242124, -2.8456743), (-1.4225888, -2.0026584), (-1.1603347, -1.2383106), (-2.4552505, -2.0307324), (-2.7338402, -2.868395), (-1.4479036, -2.1310616), (-1.1593446, -1.2372237), (-2.4364057, -1.9177085), (-2.7416768, -2.879606), (-1.4754599, -2.257018), (-1.1589296, -1.2435086), (-2.4136052, -1.8064891), (-2.7473433, -2.87833), (-1.5057625, -2.3721795), (-1.1493237, -1.2577273), (-2.373769, -1.7026217), (-2.757573, -2.8621945), (-1.553486, -2.4706051), (-1.1449363, -1.2813684), (-2.324935, -1.6165019), (-2.7614002, -2.8334925), (-1.6072673, -2.5439298), (-1.1426622, -1.3175411), (-2.2665884, -1.558414), (-2.7605848, -2.790391), (-1.6677678, -2.5841649), (-1.1405939, -1.3544935), (-2.1975698, -1.5216643), (-2.75808, -2.7462556), (-1.7371906, -2.6039383), (-1.1361489, -1.3886392), (-2.1154323, -1.4997257), (-2.7568245, -2.718629), (-1.8186886, -2.6262577), (-1.1269059, -1.4086708), (-2.017582, -1.4735746), (-2.7606025, -2.6904), (-1.918173, -2.639216), (-1.1090323, -1.4058615), (-1.8956068, -1.4327585), (-2.773306, -2.6851544), (-2.0447357, -2.671402), (-1.0795575, -1.3571885), (-1.741464, -1.345928), (-2.7982821, -2.721454), (-2.2070947, -2.7536802), (-1.040658, -1.2817094), (-1.5504935, -1.2133327), (-2.8229873, -2.716623), (-2.4017608, -2.8154423), (-1.0128027, -1.2449874), (-1.3359977, -1.1052533), (-2.8310797, -2.6683426), (-2.6116436, -2.844258), (-1.0443324, -1.272177), (-1.1663916, -1.0717024), (-2.770759, -2.605905), (-2.7336748, -2.8394697), (-1.1209129, -1.3359612), (-1.0903275, -1.0965674), (-2.6886127, -2.557262), (-2.7709656, -2.8195322), (-1.199385, -1.4013252), (-1.0806173, -1.1436731), (-2.620111, -2.5266507), (-2.7644238, -2.7971587), (-1.2578199, -1.4534045), (-1.0977429, -1.1906279), (-2.5744503, -2.5080895), (-2.744744, -2.777365), (-1.2952617, -1.4917977), (-1.1204921, -1.2295979), (-2.5478952, -2.495836), (-2.7251234, -2.755323), (-1.3175497, -1.513382), (-1.1418015, -1.2550564), (-2.5350323, -2.4833386), (-2.709728, -2.7498302), (-1.3290194, -1.542415), (-1.1570761, -1.2728659), (-2.5295777, -2.46738), (-2.7006435, -2.7476854), (-1.3359822, -1.5724386), (-1.1667193, -1.28428), (-2.5267806, -2.4458802), (-2.6969323, -2.748993), (-1.3422228, -1.6067381), (-1.1716889, -1.2902707), (-2.52341, -2.4168427), (-2.697478, -2.754008), (-1.3502876, -1.6483526), (-1.1729158, -1.2911162), (-2.517253, -2.3778389), (-2.7013862, -2.7635298), (-1.3619672, -1.7007666), (-1.1709846, -1.286065), (-2.5066154, -2.3252425), (-2.7082546, -2.7793348), (-1.3788108, -1.7689553), (-1.1658628, -1.2729187), (-2.4897501, -2.2529733), (-2.711218, -2.8047237), (-1.3939204, -1.8619403), (-1.1630794, -1.2501613), (-2.4755366, -2.1533906), (-2.7226958, -2.8418121), (-1.4174339, -1.9861097), (-1.1608629, -1.2394166), (-2.4597785, -2.0455058), (-2.7328067, -2.8658578), (-1.4425976, -2.1141493), (-1.1595616, -1.2373426), (-2.441196, -1.9328072), (-2.7409987, -2.8784347), (-1.4699793, -2.2406151), (-1.1590064, -1.242636), (-2.4187946, -1.8210479), (-2.7469656, -2.8788035), (-1.5000119, -2.3574648), (-1.1492503, -1.2557176), (-2.3794134, -1.7155061), (-2.7575567, -2.8645055), (-1.547466, -2.458776), (-1.1448756, -1.2782236), (-2.3312883, -1.6267698), (-2.7616715, -2.837441), (-1.6007596, -2.5355635), (-1.1427296, -1.3134687), (-2.2739367, -1.5656918), (-2.7610445, -2.7954564), (-1.6604768, -2.5790367), (-1.1409084, -1.3500394), (-2.206233, -1.5264579), (-2.7585306, -2.7516942), (-1.7286962, -2.601103), (-1.136891, -1.3842312), (-2.1258569, -1.5028815), (-2.7570066, -2.7105038), (-1.809119, -2.60983), (-1.1282845, -1.4138263), (-2.0285115, -1.4880366), (-2.7591105, -2.687981), (-1.9066689, -2.6277075), (-1.1123224, -1.4131172), (-1.909843, -1.4493409), (-2.7702596, -2.6811135), (-2.0294695, -2.6575537), (-1.0844467, -1.3657063), (-1.760331, -1.3652688), (-2.7943797, -2.7175016), (-2.1871207, -2.7375553), (-1.0459015, -1.2889717), (-1.5742037, -1.2343781), (-2.8207755, -2.7220426), (-2.3785696, -2.806247), (-1.0149251, -1.2445843), (-1.3610201, -1.1188887), (-2.8338947, -2.678968), (-2.590746, -2.8407574), (-1.0384305, -1.263774), (-1.1828508, -1.0745916), (-2.7801018, -2.6170938), (-2.7235777, -2.8405175), (-1.1110857, -1.3248136), (-1.0962735, -1.0931705), (-2.6989982, -2.5661814), (-2.7691805, -2.822222), (-1.1903964, -1.3909316), (-1.0806867, -1.1383796), (-2.628458, -2.5335376), (-2.7661793, -2.8000808), (-1.2511555, -1.4446802), (-1.0957814, -1.1855651), (-2.5804017, -2.5137558), (-2.747346, -2.7799582), (-1.2906593, -1.4844308), (-1.1182544, -1.2254338), (-2.552015, -2.5009663), (-2.7275152, -2.763775), (-1.3140153, -1.5140104), (-1.1390326, -1.2455308), (-2.5378861, -2.4794788), (-2.7130828, -2.756772), (-1.3273239, -1.5455106), (-1.1537172, -1.2649579), (-2.530436, -2.461502), (-2.7041476, -2.7534745), (-1.3361704, -1.5778712), (-1.1633902, -1.277794), (-2.5257719, -2.4382014), (-2.7001526, -2.753964), (-1.3440808, -1.6144216), (-1.1686835, -1.2848458), (-2.520804, -2.4073424), (-2.7002614, -2.7585602), (-1.3535513, -1.6583879), (-1.1702832, -1.286229), (-2.5132916, -2.3661785), (-2.7037907, -2.7681942), (-1.3664547, -1.7136089), (-1.1685747, -1.281005), (-2.5014005, -2.310605), (-2.7105274, -2.784851), (-1.3845387, -1.7856615), (-1.1633415, -1.2667125), (-2.4831169, -2.2337599), (-2.7138813, -2.8121488), (-1.4013437, -1.884579), (-1.1615722, -1.2466052), (-2.468672, -2.1327486), (-2.7244527, -2.846911), (-1.4250041, -2.0098157), (-1.1600407, -1.2376187), (-2.4526927, -2.0238016), (-2.73391, -2.8690643), (-1.4502873, -2.1381319), (-1.1591234, -1.2368705), (-2.4337928, -1.910802), (-2.7416303, -2.8796747), (-1.4778581, -2.2636123), (-1.1587212, -1.243528), (-2.4108553, -1.8000146), (-2.7471976, -2.8776927), (-1.5082396, -2.3778), (-1.1491474, -1.2581816), (-2.3708494, -1.6970674), (-2.7572966, -2.8608198), (-1.5560545, -2.474798), (-1.1447271, -1.282227), (-2.321728, -1.6122174), (-2.7610314, -2.8315322), (-1.610038, -2.546555), (-1.1423744, -1.3186806), (-2.262968, -1.5554731), (-2.7601793, -2.7881188), (-1.6708851, -2.5854187), (-1.1401743, -1.3556693), (-2.1933932, -1.5197906), (-2.757733, -2.7440052), (-1.7408506, -2.6042824), (-1.1355177, -1.3896732), (-2.1104956, -1.4985287), (-2.7566543, -2.7166302), (-1.823165, -2.6261632), (-1.1259636, -1.4094571), (-2.0115862, -1.4726876), (-2.7607193, -2.6887472), (-1.9238256, -2.6390443), (-1.1076007, -1.4039001), (-1.8881804, -1.4295384), (-2.7741318, -2.686335), (-2.0521019, -2.6740677), (-1.0774777, -1.3531522), (-1.732219, -1.3400543), (-2.7996714, -2.7242026), (-2.2164233, -2.7590356), (-1.0384296, -1.2772397), (-1.5395048, -1.2053614), (-2.8237636, -2.7156894), (-2.4122095, -2.8186622), (-1.0120316, -1.2438387), (-1.3249639, -1.0999411), (-2.8297186, -2.6652772), (-2.6206846, -2.8456826), (-1.0470783, -1.2747294), (-1.1595502, -1.070742), (-2.7667491, -2.6024597), (-2.7378438, -2.8392148), (-1.1251621, -1.3398547), (-1.088076, -1.0981011), (-2.6843834, -2.5545883), (-2.7716663, -2.8186748), (-1.2031431, -1.4050318), (-1.0807961, -1.1459285), (-2.6168768, -2.5246773), (-2.7637305, -2.7962239), (-1.2605431, -1.4565182), (-1.0987309, -1.1927675), (-2.572291, -2.5065207), (-2.7437608, -2.7765653), (-1.2971026, -1.4944245), (-1.1215754, -1.2313668), (-2.5465403, -2.494422), (-2.7242696, -2.754759), (-1.3187958, -1.5157784), (-1.1427246, -1.2564516), (-2.5341825, -2.4819074), (-2.7091258, -2.749466), (-1.3299339, -1.5447265), (-1.1577822, -1.2739103), (-2.5289974, -2.4658027), (-2.7002985, -2.7475352), (-1.3367566, -1.5748396), (-1.1672126, -1.2850052), (-2.5262947, -2.4440362), (-2.6968176, -2.7490733), (-1.3429977, -1.6093919), (-1.1719912, -1.290687), (-2.5228891, -2.4145987), (-2.697563, -2.7543535), (-1.3511673, -1.6514394), (-1.1730429, -1.2911941), (-2.5165915, -2.3750126), (-2.7016528, -2.7642162), (-1.3630435, -1.7045306), (-1.1709356, -1.2857156), (-2.505715, -2.3215492), (-2.7087145, -2.7805047), (-1.3801863, -1.7737724), (-1.1655978, -1.2719599), (-2.4884775, -2.2479327), (-2.7119484, -2.8066213), (-1.3957764, -1.868442), (-1.1630048, -1.249821), (-2.4742177, -2.147978), (-2.7232475, -2.8431103), (-1.419322, -1.9928156), (-1.1609164, -1.2394553), (-2.458393, -2.0398273), (-2.733218, -2.8666768), (-1.4445192, -2.1208503), (-1.1596824, -1.2376511), (-2.4397118, -1.9270699), (-2.7412996, -2.8787546), (-1.4719507, -2.246977), (-1.1591429, -1.243231), (-2.4171538, -1.8155934), (-2.7471633, -2.8785167), (-1.5020694, -2.3630397), (-1.1494125, -1.256662), (-2.3775916, -1.7107679), (-2.757621, -2.863547), (-1.5496088, -2.4631262), (-1.1450127, -1.2795168), (-2.329198, -1.6230959), (-2.7616224, -2.8359032), (-1.6030594, -2.5384967), (-1.1428057, -1.3150225), (-2.2714856, -1.5632051), (-2.7609155, -2.7935574), (-1.6630288, -2.580666), (-1.1408844, -1.3516375), (-2.203324, -1.5249511), (-2.7583947, -2.7497325), (-1.7316349, -2.6018076), (-1.1367037, -1.3857032), (-2.122352, -1.5020251), (-2.756959, -2.7086828), (-1.8126379, -2.6100097), (-1.1278534, -1.4150817), (-2.0241983, -1.487507), (-2.759244, -2.6864104), (-1.9110314, -2.627707), (-1.1115053, -1.4122373), (-1.9044534, -1.4471112), (-2.7709126, -2.6817276), (-2.0350776, -2.659604), (-1.0830965, -1.3631172), (-1.7535661, -1.3610508), (-2.7954903, -2.7194571), (-2.1941981, -2.7416224), (-1.0443248, -1.2857959), (-1.5660387, -1.2284691), (-2.8215396, -2.7213936), (-2.3866234, -2.8087668), (-1.0142658, -1.2437248), (-1.3525428, -1.1148229), (-2.8331358, -2.676673), (-2.5980082, -2.8418941), (-1.0404235, -1.2656947), (-1.1773355, -1.0738319), (-2.7771878, -2.6144593), (-2.727141, -2.8403008), (-1.1143675, -1.3277736), (-1.0943582, -1.0944033), (-2.6957712, -2.564195), (-2.769896, -2.8215084), (-1.1933508, -1.3937055), (-1.080781, -1.1402017), (-2.6259637, -2.532164), (-2.7656858, -2.7992957), (-1.253279, -1.4469147), (-1.0965617, -1.1872874), (-2.5787556, -2.512759), (-2.746567, -2.779269), (-1.2920452, -1.486196), (-1.1191189, -1.2268447), (-2.551018, -2.5001462), (-2.7268083, -2.763238), (-1.3148879, -1.5154796), (-1.1397706, -1.2466345), (-2.5373087, -2.4787238), (-2.7125525, -2.7563975), (-1.327885, -1.5468283), (-1.1542767, -1.2657571), (-2.530098, -2.4606864), (-2.7038064, -2.753263), (-1.3365706, -1.5791777), (-1.1637708, -1.2783285), (-2.5255356, -2.4372342), (-2.6999848, -2.7539172), (-1.3444372, -1.6158423), (-1.1689047, -1.2851404), (-2.5205648, -2.406132), (-2.70024, -2.7586923), (-1.3539482, -1.6600531), (-1.1703647, -1.2862831), (-2.5129728, -2.364605), (-2.7038994, -2.7685432), (-1.3669617, -1.7156796), (-1.1685231, -1.2807764), (-2.5009286, -2.30849), (-2.710764, -2.7854996), (-1.3852289, -1.7883799), (-1.1631423, -1.266101), (-2.482407, -2.2308054), (-2.7142873, -2.8132412), (-1.4023321, -1.8883402), (-1.161475, -1.2463545), (-2.467915, -2.1295302), (-2.724756, -2.847645), (-1.426028, -2.0137317), (-1.1600144, -1.2375987), (-2.4518757, -2.0203981), (-2.7341294, -2.869499), (-1.4513476, -2.142059), (-1.1591352, -1.2370269), (-2.4328973, -1.9073571), (-2.7417808, -2.8797884), (-1.4789641, -2.2673295), (-1.1587441, -1.243877), (-2.4098477, -1.7967519), (-2.7472804, -2.8774207), (-1.5094135, -2.381026), (-1.1491889, -1.2587564), (-2.3697133, -1.6942526), (-2.757293, -2.860134), (-1.5572959, -2.4772756), (-1.1447581, -1.2830167), (-2.3204155, -1.6100512), (-2.7609558, -2.8305066), (-1.6113877, -2.54819), (-1.1423724, -1.3196199), (-2.261428, -1.5540051), (-2.760058, -2.7868922), (-1.6724013, -2.5863028), (-1.1401123, -1.3566288), (-2.191565, -1.5188825), (-2.757613, -2.7427523), (-1.7426186, -2.6046474), (-1.1353549, -1.390556), (-2.1082919, -1.4979813), (-2.7565954, -2.7154734), (-1.8253071, -2.6262493), (-1.1256514, -1.4102035), (-2.008874, -1.4723155), (-2.760775, -2.6877413), (-1.926503, -2.6390352), (-1.107055, -1.4033523), (-1.8847945, -1.4281176), (-2.7744987, -2.6866355), (-2.0555549, -2.6753066), (-1.0766279, -1.3516061), (-1.7279944, -1.3374418), (-2.8002794, -2.7252488), (-2.2207472, -2.7614522), (-1.0375125, -1.2754359), (-1.5345058, -1.2018253), (-2.8240666, -2.7151117), (-2.4169827, -2.820064), (-1.0117872, -1.2434702), (-1.3200102, -1.0976366), (-2.829039, -2.6638253), (-2.6247337, -2.846273), (-1.0484068, -1.2759541), (-1.1565523, -1.0703892), (-2.7649038, -2.600903), (-2.7396574, -2.8390586), (-1.1271102, -1.341602), (-1.0871321, -1.0988276), (-2.6824765, -2.5534148), (-2.7719343, -2.8182538), (-1.2048286, -1.4066316), (-1.080912, -1.1469458), (-2.615434, -2.5238445), (-2.763387, -2.795768), (-1.2617396, -1.4578011), (-1.0991886, -1.1937134), (-2.5713391, -2.5058985), (-2.74329, -2.7761657), (-1.2978826, -1.4954404), (-1.1220633, -1.2321345), (-2.5459561, -2.4939022), (-2.7238545, -2.7544568), (-1.3192909, -1.5166384), (-1.143136, -1.2570454), (-2.5338361, -2.4814198), (-2.7088175, -2.7492461), (-1.330258, -1.5454954), (-1.1580927, -1.274344), (-2.5287855, -2.4652948), (-2.7001007, -2.7474036), (-1.3369956, -1.5755908), (-1.1674268, -1.2852986), (-2.5261424, -2.443464), (-2.6967168, -2.7490313), (-1.3432109, -1.61019), (-1.1721202, -1.2908539), (-2.522743, -2.4139144), (-2.6975439, -2.7544072), (-1.3513993, -1.6523507), (-1.1730998, -1.291236), (-2.5164106, -2.3741584), (-2.701704, -2.7643843), (-1.3633294, -1.7056341), (-1.1709237, -1.2856143), (-2.5054607, -2.3204384), (-2.7088337, -2.7808254), (-1.3805629, -1.7751864), (-1.1655123, -1.2716659), (-2.48811, -2.2464194), (-2.712155, -2.807167), (-1.3962989, -1.8703593), (-1.1629713, -1.2497072), (-2.473832, -2.1463542), (-2.7234023, -2.8434775), (-1.4198574, -1.9947916), (-1.1609199, -1.239455), (-2.4579842, -2.0381274), (-2.7333317, -2.8669007), (-1.4450676, -2.1228206), (-1.1597048, -1.237732), (-2.4392698, -1.9253577), (-2.7413802, -2.8788276), (-1.4725168, -2.2488396), (-1.1591713, -1.243399), (-2.4166632, -1.8139724), (-2.7472115, -2.8784091), (-1.5026629, -2.364663), (-1.1494497, -1.2569338), (-2.3770459, -1.7093654), (-2.7576296, -2.863242), (-1.5502294, -2.464382), (-1.1450427, -1.2798908), (-2.328571, -1.6220137), (-2.7615972, -2.8354301), (-1.6037277, -2.5393307), (-1.1428186, -1.31547), (-2.2707527, -1.5624754), (-2.7608678, -2.7929833), (-1.6637723, -2.5811176), (-1.1408677, -1.3520954), (-2.202456, -1.5245104), (-2.7583454, -2.749146), (-1.7324934, -2.6019897), (-1.136639, -1.3861201), (-2.1213078, -1.5017734), (-2.7569375, -2.7081425), (-1.8136697, -2.6100423), (-1.1277169, -1.4154332), (-2.022916, -1.4873495), (-2.7592773, -2.685948), (-1.9123129, -2.62769), (-1.1112554, -1.411956), (-1.9028542, -1.4464505), (-2.7710993, -2.681914), (-2.036726, -2.6601965), (-1.0826902, -1.3623325), (-1.751564, -1.3598021), (-2.7958117, -2.7200367), (-2.196278, -2.7428107), (-1.0438575, -1.2848452), (-1.5636308, -1.226726), (-2.821753, -2.721204), (-2.3889823, -2.8095), (-1.0140778, -1.2434614), (-1.3500599, -1.1136289), (-2.8328974, -2.6760054), (-2.6001172, -2.8422227), (-1.041014, -1.2662457), (-1.175736, -1.0736078), (-2.776326, -2.6136982), (-2.7281618, -2.8402376), (-1.1153252, -1.3286245), (-1.0938096, -1.0947559), (-2.6948273, -2.5636232), (-2.7700934, -2.8213024), (-1.1942077, -1.3945014), (-1.0808111, -1.1407249), (-2.625238, -2.53177), (-2.7655377, -2.7990677), (-1.2538923, -1.4475528), (-1.0967884, -1.1877819), (-2.5782783, -2.5124748), (-2.7463388, -2.779069), (-1.2924448, -1.4866989), (-1.1193707, -1.2272515), (-2.5507305, -2.4999154), (-2.7266018, -2.756639), (-1.3154885, -1.5085903), (-1.1408759, -1.2532144), (-2.5372477, -2.487408), (-2.710902, -2.7508435), (-1.3273706, -1.5377182), (-1.1564301, -1.2715262), (-2.5314717, -2.4716785), (-2.7014866, -2.7483432), (-1.334492, -1.567532), (-1.1663594, -1.2834357), (-2.5285816, -2.4506507), (-2.6974583, -2.7492347), (-1.340695, -1.601306), (-1.1716038, -1.2899518), (-2.5253007, -2.4223764), (-2.697699, -2.753729), (-1.3485565, -1.6420158), (-1.1731085, -1.2914275), (-2.5194008, -2.384544), (-2.7012894, -2.762544), (-1.359864, -1.6929951), (-1.1714963, -1.2872311), (-2.5092118, -2.3337507), (-2.707774, -2.7773118), (-1.3761189, -1.7589172), (-1.1668043, -1.2753521), (-2.4930696, -2.2643435), (-2.71015, -2.8011122), (-1.3902403, -1.8482466), (-1.1635922, -1.2512355), (-2.4788837, -2.1655512), (-2.7219934, -2.839446), (-1.4137136, -1.9718925), (-1.1610798, -1.2396485), (-2.4631963, -2.0582674), (-2.7323725, -2.8644612), (-1.4388256, -2.0998106), (-1.1596208, -1.236982), (-2.4447603, -1.9457741), (-2.7407625, -2.878045), (-1.4661068, -2.2268267), (-1.1590135, -1.2416427), (-2.4226332, -1.8335233), (-2.7469084, -2.879667), (-1.4959524, -2.3451676), (-1.1588944, -1.2539281), (-2.395778, -1.7293135), (-2.7505128, -2.8678668), (-1.5288459, -2.4456732), (-1.1498986, -1.2742945), (-2.3516943, -1.6388164), (-2.7582178, -2.8416722), (-1.5789716, -2.5241463), (-1.1456131, -1.3083346), (-2.2976344, -1.5753981), (-2.7591717, -2.8003976), (-1.6356772, -2.5699952), (-1.1433641, -1.3444799), (-2.2336636, -1.5341111), (-2.7568254, -2.7568758), (-1.699987, -2.5939734), (-1.1401656, -1.3787394), (-2.1583998, -1.5092771), (-2.754398, -2.7155695), (-1.774718, -2.6036537), (-1.1334016, -1.408586), (-2.068445, -1.4941887), (-2.7547524, -2.6926205), (-1.8639752, -2.6211445), (-1.1206976, -1.4235573), (-1.960123, -1.4708967), (-2.7613025, -2.6698697), (-1.9744612, -2.633222), (-1.0977149, -1.3913331), (-1.8244756, -1.4069188), (-2.7806506, -2.6942985), (-2.1166213, -2.69298), (-1.0621717, -1.3205646), (-1.6540304, -1.2943046), (-2.80901, -2.7257593), (-2.2951055, -2.776779), (-1.0241468, -1.2528288), (-1.4491473, -1.1621376), (-2.8359075, -2.702432), (-2.5092263, -2.826485), (-1.020623, -1.2422225), (-1.24589, -1.0834692), (-2.8066645, -2.6465647), (-2.6781735, -2.8419447), (-1.0763074, -1.2905922), (-1.1207347, -1.0789847), (-2.7322419, -2.5887885), (-2.756622, -2.829631), (-1.1567558, -1.3579842), (-1.0807436, -1.1165587), (-2.6551423, -2.5487754), (-2.769076, -2.808445), (-1.2260071, -1.4179534), (-1.0867717, -1.1645474), (-2.5980177, -2.5242012), (-2.754477, -2.7873282), (-1.2737191, -1.4636225), (-1.1077014, -1.2079487), (-2.5623734, -2.5088136), (-2.734332, -2.7695606), (-1.3030257, -1.4974533), (-1.1296555, -1.2426364), (-2.5427938, -2.4976685), (-2.7167606, -2.7496424), (-1.3201039, -1.5166872), (-1.148827, -1.2643687), (-2.5341618, -2.484872), (-2.7039275, -2.7460346), (-1.3287265, -1.5450412), (-1.1618388, -1.2791563), (-2.5308604, -2.4676828), (-2.6972182, -2.7455752), (-1.3345565, -1.575738), (-1.1695013, -1.2882305), (-2.5287318, -2.444238), (-2.6954718, -2.7483692), (-1.3408599, -1.6117775), (-1.1728947, -1.2923851), (-2.5250106, -2.4126186), (-2.697537, -2.7547836), (-1.3497871, -1.6560762), (-1.172926, -1.291617), (-2.5178149, -2.3702939), (-2.7026036, -2.7658443), (-1.3628734, -1.7122307), (-1.1700516, -1.2847956), (-2.5056324, -2.3132699), (-2.710445, -2.7836766), (-1.3815784, -1.7856771), (-1.164021, -1.2692158), (-2.4867039, -2.2346716), (-2.714523, -2.8120818), (-1.3991998, -1.8864578), (-1.1621168, -1.2490568), (-2.4718688, -2.1328218), (-2.725232, -2.84675), (-1.4233214, -2.012234), (-1.1605736, -1.2399498), (-2.4555087, -2.0232825), (-2.7347043, -2.8688257), (-1.4489903, -2.1407998), (-1.1596925, -1.2390397), (-2.4362767, -1.909932), (-2.7423759, -2.8793201), (-1.4768533, -2.2662175), (-1.1593355, -1.2455448), (-2.4130647, -1.7990689), (-2.747858, -2.8771524), (-1.5074339, -2.3800118), (-1.1497713, -1.2600716), (-2.372803, -1.6963184), (-2.7578669, -2.8600526), (-1.5553986, -2.4763496), (-1.1453824, -1.283988), (-2.3234992, -1.6118888), (-2.7614765, -2.8305962), (-1.6094277, -2.5473063), (-1.1430665, -1.320243), (-2.2646518, -1.5556582), (-2.7604916, -2.787145), (-1.670209, -2.5853863), (-1.1409009, -1.356917), (-2.1951056, -1.5204558), (-2.7579212, -2.74316), (-1.7399864, -2.6035688), (-1.1362741, -1.3904848), (-2.112377, -1.499607), (-2.7567317, -2.716045), (-1.8219706, -2.6248372), (-1.1267499, -1.4097235), (-2.013804, -1.4741493), (-2.7606964, -2.688601), (-1.92213, -2.6372054), (-1.1084377, -1.403954), (-1.8909354, -1.4317642), (-2.7739701, -2.6863637), (-2.0496917, -2.6713622), (-1.0783659, -1.3527755), (-1.7357081, -1.3429567), (-2.79948, -2.7248), (-2.2131963, -2.7558064), (-1.0391732, -1.2760658), (-1.5437016, -1.2084395), (-2.8240018, -2.7186604), (-2.4085383, -2.8171556), (-1.0120475, -1.2407886), (-1.3291738, -1.1012064), (-2.8309112, -2.6693447), (-2.6175818, -2.8453786), (-1.0456543, -1.2700751), (-1.1621366, -1.0700098), (-2.7689996, -2.6067839), (-2.736558, -2.8396468), (-1.1230655, -1.3344294), (-1.0889467, -1.0963988), (-2.68681, -2.558973), (-2.771601, -2.819265), (-1.2010543, -1.3991737), (-1.0808215, -1.1440573), (-2.6191244, -2.52943), (-2.7640748, -2.7967174), (-1.258559, -1.450134), (-1.0985408, -1.19108), (-2.5744433, -2.5119863), (-2.7440925, -2.7768676), (-1.2950602, -1.4872705), (-1.1214429, -1.2299718), (-2.5488017, -2.5008793), (-2.724436, -2.761184), (-1.3161528, -1.5147986), (-1.141865, -1.2490041), (-2.5366611, -2.4806237), (-2.710593, -2.7547183), (-1.3278824, -1.5447454), (-1.155978, -1.2674875), (-2.5305777, -2.463552), (-2.7023146, -2.7518995), (-1.3356824, -1.5759734), (-1.1650822, -1.2796209), (-2.5268087, -2.4409611), (-2.698906, -2.752746), (-1.3429352, -1.6116686), (-1.1699042, -1.2862357), (-2.5224047, -2.410732), (-2.6994667, -2.7575343), (-1.3520046, -1.6549054), (-1.1711749, -1.2874587), (-2.5152557, -2.3702466), (-2.703291, -2.767154), (-1.3646443, -1.7093469), (-1.1692994, -1.2823659), (-2.503643, -2.3155653), (-2.7101436, -2.783538), (-1.382482, -1.7803428), (-1.1640812, -1.2685355), (-2.485683, -2.2400758), (-2.7133853, -2.8102286), (-1.3988835, -1.8775942), (-1.1620312, -1.2477357), (-2.4712248, -2.13944), (-2.7242134, -2.8456767), (-1.4225906, -2.0026634), (-1.1603341, -1.2383084), (-2.4552486, -2.0307274), (-2.73384, -2.868398), (-1.4479053, -2.131068), (-1.1593441, -1.2372222), (-2.4364035, -1.9177018), (-2.7416778, -2.879608), (-1.4754622, -2.2570267), (-1.1589288, -1.2435081), (-2.413603, -1.8064814), (-2.7473443, -2.8783307), (-1.5057656, -2.372188), (-1.1493232, -1.2577277), (-2.3737664, -1.7026143), (-2.7575743, -2.862194), (-1.5534891, -2.4706113), (-1.1449345, -1.2813685), (-2.3249311, -1.6164956), (-2.7614014, -2.8334918), (-1.6072711, -2.5439355), (-1.1426606, -1.3175426), (-2.2665825, -1.558409), (-2.7605853, -2.7903886), (-1.6677738, -2.584168), (-1.140593, -1.354495), (-2.1975632, -1.521661), (-2.7580807, -2.7462542), (-1.7371976, -2.6039417), (-1.1361477, -1.3886408), (-2.1154242, -1.4997219), (-2.7568257, -2.7186263), (-1.8186972, -2.6262608), (-1.1269038, -1.408673), (-2.0175712, -1.473571), (-2.7606041, -2.690397), (-1.9181848, -2.6392186), (-1.1090297, -1.4058603), (-1.8955927, -1.4327509), (-2.7733083, -2.6851547), (-2.0447514, -2.6714096), (-1.0795535, -1.3571842), (-1.7414453, -1.3459154), (-2.7982862, -2.7214568), (-2.207116, -2.7536922), (-1.0406537, -1.2817038), (-1.55047, -1.2133166), (-2.822989, -2.7166183), (-2.401784, -2.8154492), (-1.0128027, -1.2449887), (-1.3359745, -1.105244), (-2.831076, -2.6683345), (-2.6116629, -2.844261), (-1.0443398, -1.2721854), (-1.166378, -1.071702), (-2.7707493, -2.6058962), (-2.7336824, -2.8394685), (-1.1209236, -1.3359716), (-1.0903248, -1.096572), (-2.6886034, -2.557256), (-2.7709656, -2.8195302), (-1.1993932, -1.4013338), (-1.0806195, -1.1436793), (-2.620106, -2.5266476), (-2.764423, -2.7971566), (-1.2578251, -1.4534099), (-1.0977448, -1.1906323), (-2.5744462, -2.5080884), (-2.7447424, -2.7773643), (-1.2952656, -1.491802), (-1.1204945, -1.2296016), (-2.5478933, -2.495834), (-2.7251222, -2.755322), (-1.3175523, -1.513386), (-1.1418042, -1.2550591), (-2.5350318, -2.483337), (-2.709727, -2.7498293), (-1.3290206, -1.5424174), (-1.1570773, -1.272868), (-2.529577, -2.467379), (-2.700643, -2.7476845), (-1.3359841, -1.5724409), (-1.1667212, -1.2842822), (-2.5267804, -2.4458797), (-2.6969311, -2.7489924), (-1.3422232, -1.60674), (-1.1716906, -1.2902722), (-2.5234113, -2.4168417), (-2.6974778, -2.754007), (-1.3502868, -1.6483539), (-1.1729164, -1.2911165), (-2.5172544, -2.3778372), (-2.7013862, -2.7635303), (-1.3619658, -1.7007694), (-1.1709837, -1.2860643), (-2.5066166, -2.3252392), (-2.7082572, -2.7793365), (-1.3788111, -1.7689598), (-1.1658611, -1.2729168), (-2.48975, -2.2529676), (-2.7112198, -2.8047261), (-1.3939222, -1.8619479), (-1.1630791, -1.2501609), (-2.4755359, -2.1533844), (-2.7226973, -2.8418145), (-1.4174356, -1.986118), (-1.160862, -1.2394156), (-2.4597769, -2.045498), (-2.7328079, -2.8658593), (-1.4426004, -2.1141582), (-1.1595618, -1.237343), (-2.4411938, -1.9327993), (-2.7409992, -2.8784356), (-1.4699819, -2.2406247), (-1.1590065, -1.2426368), (-2.4187925, -1.8210396), (-2.7469656, -2.8788025), (-1.5000147, -2.357473), (-1.1492516, -1.2557186), (-2.3794122, -1.7154986), (-2.757556, -2.8645046), (-1.5474672, -2.458783), (-1.1448762, -1.2782258), (-2.3312871, -1.6267638), (-2.7616713, -2.837438), (-1.6007606, -2.5355678), (-1.1427292, -1.3134717), (-2.2739341, -1.5656881), (-2.7610443, -2.7954528), (-1.6604793, -2.5790384), (-1.1409082, -1.3500415), (-2.20623, -1.526456), (-2.7585294, -2.751691), (-1.7286986, -2.601103), (-1.1368916, -1.3842328), (-2.1258545, -1.5028815), (-2.7570064, -2.7105024), (-1.8091213, -2.6098297), (-1.1282843, -1.4138266), (-2.028509, -1.4880354), (-2.75911, -2.6879792), (-1.9066709, -2.627707), (-1.1123216, -1.4131165), (-1.9098399, -1.449339), (-2.77026, -2.6811128), (-2.0294724, -2.657555), (-1.0844456, -1.3657056), (-1.7603273, -1.3652651), (-2.79438, -2.7175004), (-2.1871235, -2.7375567), (-1.0459, -1.2889711), (-1.5741993, -1.234375), (-2.8207757, -2.7220407), (-2.3785732, -2.8062482), (-1.014924, -1.2445847), (-1.361015, -1.1188855), (-2.8338945, -2.678965), (-2.5907505, -2.8407586), (-1.0384318, -1.2637761), (-1.1828471, -1.0745907), (-2.780099, -2.617091), (-2.7235794, -2.8405178), (-1.1110882, -1.3248167), (-1.0962728, -1.0931711), (-2.6989963, -2.5661795), (-2.7691803, -2.8222206), (-1.1903973, -1.3909326), (-1.0806861, -1.1383802), (-2.6284564, -2.5335362), (-2.766179, -2.8000805), (-1.2511563, -1.4446815), (-1.0957805, -1.1855652), (-2.5804, -2.5137548), (-2.7473466, -2.7799578), (-1.2906612, -1.4844314), (-1.118254, -1.2254344), (-2.552013, -2.500966), (-2.727515, -2.7637746), (-1.3140169, -1.5140107), (-1.139033, -1.2455313), (-2.537885, -2.4794793), (-2.7130826, -2.756772), (-1.3273251, -1.5455104), (-1.1537175, -1.2649585), (-2.5304348, -2.461503), (-2.7041469, -2.7534735), (-1.3361709, -1.5778698), (-1.1633908, -1.2777953), (-2.5257716, -2.438204), (-2.700152, -2.753963), (-1.3440808, -1.614419), (-1.1686848, -1.284847), (-2.520805, -2.4073462), (-2.7002594, -2.7585595), (-1.3535491, -1.6583833), (-1.1702842, -1.2862293), (-2.5132933, -2.3661833), (-2.70379, -2.768194), (-1.3664528, -1.7136033), (-1.1685754, -1.2810047), (-2.5014021, -2.3106098), (-2.710526, -2.7848496), (-1.3845358, -1.7856554), (-1.1633421, -1.2667133), (-2.4831195, -2.2337654), (-2.7138798, -2.8121471), (-1.4013404, -1.8845717), (-1.1615727, -1.2466046), (-2.4686756, -2.1327543), (-2.7244525, -2.8469102), (-1.4250007, -2.009808), (-1.1600403, -1.2376171), (-2.4526956, -2.023807), (-2.7339106, -2.8690648), (-1.4502842, -2.1381264), (-1.1591235, -1.2368693), (-2.4337966, -1.9108068), (-2.7416306, -2.8796754), (-1.4778539, -2.263607), (-1.1587212, -1.2435273), (-2.4108603, -1.8000201), (-2.7471986, -2.877694), (-1.5082357, -2.3777957), (-1.1491472, -1.2581805), (-2.3708534, -1.6970707), (-2.757297, -2.8608203), (-1.5560511, -2.474795), (-1.1447269, -1.2822268), (-2.321731, -1.6122205), (-2.761031, -2.8315322), (-1.610035, -2.5465524), (-1.1423751, -1.318681), (-2.2629716, -1.5554754), (-2.7601783, -2.7881196), (-1.6708813, -2.585418), (-1.140176, -1.355669), (-2.193398, -1.5197926), (-2.757732, -2.744007), (-1.7408456, -2.604282), (-1.1355188, -1.3896722), (-2.1105013, -1.49853), (-2.756654, -2.7166321), (-1.8231591, -2.6261625), (-1.1259646, -1.4094563), (-2.0115933, -1.4726886), (-2.760718, -2.6887484), (-1.9238181, -2.639044), (-1.1076026, -1.4039028), (-1.8881898, -1.429543), (-2.7741296, -2.6863332), (-2.0520911, -2.6740627), (-1.0774809, -1.3531566), (-1.7322314, -1.3400631), (-2.7996683, -2.7241995), (-2.2164094, -2.759027), (-1.0384322, -1.2772444), (-1.5395197, -1.2053728), (-2.8237615, -2.7156918), (-2.4121943, -2.818657), (-1.0120327, -1.2438391), (-1.3249793, -1.099949), (-2.8297205, -2.6652827), (-2.6206722, -2.8456802), (-1.0470746, -1.2747241), (-1.1595596, -1.0707425), (-2.7667544, -2.602465), (-2.7378378, -2.8392155), (-1.1251556, -1.3398483), (-1.0880786, -1.0980971), (-2.6843898, -2.5545921), (-2.7716658, -2.8186767), (-1.2031374, -1.4050257), (-1.0807949, -1.1459244), (-2.6168804, -2.5246804), (-2.763732, -2.7962265), (-1.2605401, -1.4565146), (-1.0987294, -1.1927636), (-2.5722933, -2.506522), (-2.7437623, -2.7765672), (-1.2971009, -1.4944222), (-1.1215731, -1.2313638), (-2.5465403, -2.4944222), (-2.7242706, -2.7547605), (-1.3187957, -1.5157769), (-1.142724, -1.2564495), (-2.534183, -2.4819083), (-2.7091265, -2.7494671), (-1.3299335, -1.5447246), (-1.1577811, -1.273908), (-2.5289972, -2.4658036), (-2.7002993, -2.7475364), (-1.3367572, -1.5748379), (-1.1672121, -1.2850034), (-2.526294, -2.4440377), (-2.6968179, -2.7490745), (-1.3429977, -1.6093898), (-1.1719908, -1.2906852), (-2.522889, -2.4146001), (-2.6975632, -2.754354), (-1.3511676, -1.6514378), (-1.173043, -1.2911934), (-2.5165918, -2.3750143), (-2.701653, -2.7642167), (-1.3630428, -1.704528), (-1.1709348, -1.2857147), (-2.5057144, -2.3215508), (-2.7087145, -2.7805042), (-1.3801858, -1.7737697), (-1.1655973, -1.2719598), (-2.488478, -2.2479353), (-2.7119484, -2.8066208), (-1.3957754, -1.8684387), (-1.1630046, -1.2498198), (-2.474218, -2.1479793), (-2.7232468, -2.8431103), (-1.4193208, -1.9928138), (-1.1609159, -1.239454), (-2.4583933, -2.039828), (-2.733218, -2.8666775), (-1.4445186, -2.1208491), (-1.159682, -1.2376506), (-2.439712, -1.9270712), (-2.7412999, -2.8787549), (-1.47195, -2.2469752), (-1.1591426, -1.2432306), (-2.4171543, -1.8155947), (-2.7471633, -2.8785174), (-1.5020694, -2.3630395), (-1.1494128, -1.2566621), (-2.3775918, -1.7107682), (-2.7576208, -2.8635473), (-1.5496083, -2.4631267), (-1.1450126, -1.2795172), (-2.3291981, -1.6230956), (-2.7616227, -2.8359025), (-1.6030599, -2.538497), (-1.1428058, -1.3150237), (-2.2714853, -1.5632055), (-2.7609155, -2.7935562), (-1.6630288, -2.580666), (-1.1408843, -1.3516388), (-2.203324, -1.5249512), (-2.7583947, -2.7497315), (-1.7316356, -2.601807), (-1.1367029, -1.3857034), (-2.1223495, -1.5020252), (-2.7569592, -2.7086828), (-1.8126407, -2.6100101), (-1.1278534, -1.4150819), (-2.0241954, -1.4875067), (-2.7592435, -2.6864107), (-1.9110342, -2.6277075), (-1.1115056, -1.4122365), (-1.90445, -1.4471109), (-2.7709122, -2.6817293), (-2.0350804, -2.6596053), (-1.0830964, -1.3631157), (-1.7535635, -1.3610493), (-2.7954915, -2.719459), (-2.1942017, -2.7416248), (-1.0443239, -1.2857939), (-1.5660349, -1.2284659), (-2.82154, -2.7213938), (-2.3866284, -2.8087695), (-1.0142655, -1.2437245), (-1.3525376, -1.1148196), (-2.8331358, -2.6766713), (-2.5980132, -2.8418953), (-1.0404247, -1.2656965), (-1.1773317, -1.0738312), (-2.7771866, -2.6144574), (-2.727144, -2.8403015), (-1.1143696, -1.3277764), (-1.0943567, -1.0944033), (-2.695769, -2.5641923), (-2.7698972, -2.821509), (-1.1933537, -1.3937088), (-1.0807812, -1.1402025), (-2.6259623, -2.5321627), (-2.7656858, -2.7992954), (-1.2532804, -1.4469168), (-1.0965619, -1.1872878), (-2.5787542, -2.5127573), (-2.7465668, -2.7792695), (-1.2920467, -1.4861983), (-1.1191195, -1.2268453), (-2.551017, -2.5001452), (-2.7268076, -2.7632384), (-1.3148884, -1.5154815), (-1.1397713, -1.2466352), (-2.5373092, -2.4787226), (-2.7125525, -2.7563972), (-1.3278846, -1.5468299), (-1.1542761, -1.2657579), (-2.5300982, -2.4606848), (-2.7038076, -2.7532625), (-1.3365716, -1.5791795), (-1.1637706, -1.2783289), (-2.525535, -2.437233), (-2.699985, -2.7539172), (-1.3444374, -1.6158442), (-1.1689045, -1.285141), (-2.520565, -2.40613), (-2.7002404, -2.7586923), (-1.3539485, -1.6600552), (-1.1703638, -1.2862831), (-2.5129716, -2.3646026), (-2.7039, -2.768544), (-1.3669635, -1.715683), (-1.1685225, -1.280776), (-2.5009265, -2.3084867), (-2.7107646, -2.7855), (-1.3852311, -1.7883836), (-1.1631418, -1.2661004), (-2.4824052, -2.2308018), (-2.7142882, -2.8132424), (-1.4023346, -1.8883439), (-1.1614745, -1.2463542), (-2.4679124, -2.1295273), (-2.7247558, -2.8476453), (-1.4260303, -2.013735), (-1.1600142, -1.2375988), (-2.451873, -2.0203953), (-2.73413, -2.8694997), (-1.4513501, -2.1420631), (-1.1591352, -1.2370268), (-2.432895, -1.9073535), (-2.7417803, -2.8797886), (-1.4789668, -2.2673335), (-1.1587441, -1.2438776), (-2.4098446, -1.7967488), (-2.74728, -2.87742), (-1.5094163, -2.3810284), (-1.1491888, -1.2587566), (-2.3697095, -1.69425), (-2.757293, -2.8601341), (-1.5572997, -2.4772785), (-1.1447583, -1.2830166), (-2.3204124, -1.610049), (-2.7609563, -2.8305066), (-1.6113908, -2.5481913), (-1.1423715, -1.319619), (-2.2614243, -1.5540036), (-2.7600584, -2.7868927), (-1.672405, -2.5863037), (-1.1401113, -1.3566284), (-2.1915596, -1.5188817), (-2.757613, -2.742752), (-1.7426239, -2.6046476), (-1.1353542, -1.390556), (-2.108285, -1.4979811), (-2.7565958, -2.7154737), (-1.8253139, -2.6262498), (-1.125649, -1.410203), (-2.008865, -1.4723141), (-2.7607775, -2.687741), (-1.9265133, -2.639036), (-1.1070518, -1.4033477), (-1.8847821, -1.4281125), (-2.7745018, -2.68664), (-2.0555685, -2.6753123), (-1.0766239, -1.3515983), (-1.7279781, -1.3374308), (-2.8002822, -2.7252545), (-2.2207649, -2.7614632), (-1.0375088, -1.2754282), (-1.5344859, -1.2018105), (-2.8240683, -2.7151103), (-2.4170027, -2.8200707), (-1.0117869, -1.2434694), (-1.3199903, -1.0976275), (-2.829037, -2.6638196), (-2.624752, -2.8462763), (-1.0484122, -1.2759598), (-1.1565386, -1.0703875), (-2.7648964, -2.6008961), (-2.739667, -2.8390582), (-1.1271194, -1.3416104), (-1.0871272, -1.0988319), (-2.6824691, -2.5534098), (-2.7719376, -2.8182516), (-1.2048366, -1.4066384), (-1.080912, -1.1469512), (-2.6154275, -2.5238416), (-2.7633865, -2.7957656), (-1.2617456, -1.457807), (-1.0991905, -1.1937188), (-2.5713348, -2.5058963), (-2.7432888, -2.776163), (-1.2978873, -1.4954444), (-1.122065, -1.2321395), (-2.5459526, -2.4939008), (-2.7238529, -2.7544546), (-1.3192942, -1.5166422), (-1.1431382, -1.2570497), (-2.5338342, -2.4814188), (-2.7088165, -2.7492445), (-1.3302605, -1.5454985), (-1.1580942, -1.2743472), (-2.5287836, -2.4652936), (-2.7000992, -2.7474022), (-1.3369979, -1.5755938), (-1.1674291, -1.285302), (-2.5261414, -2.4434633), (-2.696715, -2.749029), (-1.3432126, -1.6101915), (-1.1721227, -1.2908578), (-2.522742, -2.4139156), (-2.6975417, -2.7544048), (-1.3514003, -1.6523502), (-1.1731024, -1.2912397), (-2.516411, -2.3741612), (-2.7017026, -2.764382), (-1.3633299, -1.7056315), (-1.170926, -1.2856168), (-2.505461, -2.3204422), (-2.7088315, -2.780823), (-1.3805629, -1.7751817), (-1.1655152, -1.2716693), (-2.4881105, -2.2464263), (-2.7121518, -2.8071642), (-1.396297, -1.8703518), (-1.162974, -1.2497098), (-2.473835, -2.146362), (-2.7234004, -2.8434741), (-1.4198545, -1.9947811), (-1.1609213, -1.2394562), (-2.4579875, -2.038137), (-2.7333305, -2.8668983), (-1.445064, -2.1228082), (-1.1597055, -1.2377317), (-2.4392738, -1.9253693), (-2.74138, -2.8788276), (-1.4725121, -2.2488267), (-1.1591712, -1.2433971), (-2.4166677, -1.8139832), (-2.7472115, -2.87841), (-1.5026582, -2.3646512), (-1.149449, -1.2569313), (-2.3770506, -1.7093751), (-2.757631, -2.8632448), (-1.5502245, -2.4643729), (-1.1450412, -1.2798871), (-2.3285763, -1.6220211), (-2.7615995, -2.8354344), (-1.6037228, -2.5393252), (-1.1428171, -1.3154663), (-2.2707586, -1.5624802), (-2.7608693, -2.792987), (-1.663766, -2.5811143), (-1.1408669, -1.3520917), (-2.2024634, -1.5245128), (-2.7583473, -2.7491496), (-1.732487, -2.6019878), (-1.1366386, -1.3861166), (-2.1213164, -1.5017757), (-2.7569385, -2.7081466), (-1.8136613, -2.6100402), (-1.1277173, -1.4154292), (-2.0229263, -1.4873518), (-2.7592776, -2.685952), (-1.9123024, -2.6276882), (-1.111257, -1.4119577), (-1.902867, -1.4464576), (-2.7710984, -2.6819127), (-2.0367131, -2.6601892), (-1.0826927, -1.3623371), (-1.7515796, -1.3598136), (-2.7958107, -2.7200341), (-2.1962626, -2.7428005), (-1.0438598, -1.2848504), (-1.5636493, -1.2267398), (-2.8217528, -2.7212079), (-2.388965, -2.809494), (-1.0140783, -1.2434605), (-1.3500781, -1.1136372), (-2.8328996, -2.676012), (-2.6001012, -2.842221), (-1.0410082, -1.2662406), (-1.1757474, -1.0736089), (-2.7763338, -2.6137047), (-2.7281554, -2.840238), (-1.115317, -1.3286173), (-1.0938116, -1.0947529), (-2.6948342, -2.563627), (-2.7700937, -2.8213038), (-1.1942024, -1.3944954), (-1.0808098, -1.1407212), (-2.625242, -2.5317738), (-2.76554, -2.7990694), (-1.2538894, -1.4475478), (-1.0967863, -1.1877793), (-2.57828, -2.5124784), (-2.7463408, -2.7790704), (-1.2924436, -1.4866941), (-1.1193684, -1.2272485), (-2.550731, -2.4999182), (-2.7266033, -2.7566407), (-1.3154875, -1.5085869), (-1.140875, -1.2532119), (-2.5372488, -2.48741), (-2.7109027, -2.7508447), (-1.3273687, -1.5377154), (-1.1564292, -1.2715241), (-2.5314734, -2.4716802), (-2.7014878, -2.7483435), (-1.3344902, -1.5675286), (-1.1663575, -1.2834338), (-2.528582, -2.4506521), (-2.6974595, -2.7492354), (-1.3406938, -1.6013037), (-1.1716017, -1.2899504), (-2.5253003, -2.4223769), (-2.6976998, -2.7537296), (-1.3485563, -1.642015), (-1.1731074, -1.291426), (-2.5194004, -2.3845432), (-2.7012897, -2.7625444), (-1.3598641, -1.6929951), (-1.1714956, -1.2872297), (-2.5092113, -2.33375), (-2.7077744, -2.777313), (-1.3761189, -1.7589183), (-1.1668034, -1.2753505), (-2.493069, -2.264341), (-2.7101502, -2.8011138), (-1.3902407, -1.8482498), (-1.1635917, -1.2512338), (-2.4788823, -2.1655474), (-2.7219934, -2.8394477), (-1.4137145, -1.9718971), (-1.1610793, -1.2396476), (-2.4631948, -2.058263), (-2.732373, -2.8644621), (-1.4388273, -2.0998147), (-1.1596202, -1.2369815), (-2.4447582, -1.9457699), (-2.7407625, -2.8780448), (-1.4661083, -2.2268307), (-1.1590133, -1.2416435), (-2.422631, -1.8335198), (-2.7469075, -2.879666), (-1.4959537, -2.345171), (-1.1588945, -1.2539284), (-2.3957763, -1.7293103), (-2.7505124, -2.8678668), (-1.5288466, -2.4456763), (-1.1498991, -1.2742939), (-2.351694, -1.6388128), (-2.758217, -2.8416724), (-1.5789713, -2.524149), (-1.1456127, -1.3083341), (-2.297634, -1.5753957), (-2.7591717, -2.8003979), (-1.6356771, -2.569996), (-1.1433632, -1.344479), (-2.2336628, -1.5341102), (-2.756826, -2.7568762), (-1.6999875, -2.5939732), (-1.140164, -1.3787382), (-2.1583982, -1.5092766), (-2.754399, -2.71557), (-1.7747197, -2.6036534), (-1.1334, -1.4085847), (-2.0684423, -1.4941878), (-2.7547538, -2.6926212), (-1.8639778, -2.621146), (-1.1206962, -1.4235563), (-1.9601196, -1.4708949), (-2.7613032, -2.66987), (-1.9744642, -2.6332226), (-1.0977138, -1.3913312), (-1.824472, -1.406917), (-2.780651, -2.6942997), (-2.1166246, -2.692981), (-1.0621709, -1.3205618), (-1.6540266, -1.2943019), (-2.8090105, -2.7257597), (-2.2951095, -2.7767797), (-1.0241461, -1.2528266), (-1.4491429, -1.1621351), (-2.8359084, -2.7024324), (-2.5092306, -2.826486), (-1.0206227, -1.2422218), (-1.2458858, -1.0834666), (-2.8066638, -2.6465633), (-2.6781769, -2.8419461), (-1.0763093, -1.2905941), (-1.1207325, -1.0789841), (-2.73224, -2.5887866), (-2.7566235, -2.8296316), (-1.1567582, -1.357987), (-1.0807425, -1.1165594), (-2.655139, -2.5487728), (-2.7690756, -2.8084443), (-1.22601, -1.4179566), (-1.0867722, -1.1645492), (-2.5980153, -2.524199), (-2.7544768, -2.787327), (-1.2737217, -1.4636252), (-1.1077021, -1.2079508), (-2.562371, -2.5088127), (-2.7343311, -2.7695594), (-1.3030275, -1.4974545), (-1.129657, -1.2426382), (-2.5427935, -2.497669), (-2.7167594, -2.749642), (-1.320104, -1.5166869), (-1.1488281, -1.264369), (-2.5341625, -2.4848726), (-2.7039266, -2.7460346), (-1.3287255, -1.5450408), (-1.1618395, -1.2791562), (-2.5308616, -2.4676836), (-2.6972175, -2.7455752), (-1.3345556, -1.5757372), (-1.1695017, -1.2882304), (-2.528733, -2.4442384), (-2.695472, -2.748369), (-1.3408593, -1.6117767), (-1.1728942, -1.2923856), (-2.5250103, -2.4126196), (-2.697537, -2.754783), (-1.3497872, -1.6560754), (-1.1729254, -1.2916179), (-2.517814, -2.3702943), (-2.7026033, -2.765843), (-1.3628745, -1.7122302), (-1.1700523, -1.2847975), (-2.5056312, -2.3132713), (-2.7104445, -2.7836754), (-1.3815794, -1.7856756), (-1.1640217, -1.2692173), (-2.4867032, -2.2346737), (-2.714522, -2.8120806), (-1.3992001, -1.8864561), (-1.1621184, -1.2490582), (-2.4718688, -2.1328237), (-2.7252302, -2.8467486), (-1.4233208, -2.0122316), (-1.160575, -1.2399504), (-2.45551, -2.023284), (-2.7347033, -2.8688247), (-1.4489887, -2.140798), (-1.1596935, -1.2390407), (-2.4362786, -1.9099339), (-2.742375, -2.879319), (-1.4768507, -2.2662156), (-1.1593367, -1.2455455), (-2.413068, -1.7990707), (-2.7478578, -2.877152), (-1.5074313, -2.3800094), (-1.1497724, -1.2600714), (-2.372807, -1.6963203), (-2.7578661, -2.8600528), (-1.5553945, -2.4763477), (-1.1453838, -1.2839874), (-2.3235047, -1.6118903), (-2.7614756, -2.8305964), (-1.6094216, -2.5473049), (-1.143067, -1.3202425), (-2.264658, -1.5556582), (-2.7604914, -2.7871442), (-1.6702029, -2.5853858), (-1.1409012, -1.3569182), (-2.1951113, -1.5204563), (-2.7579205, -2.7431583), (-1.7399802, -2.6035678), (-1.1362755, -1.390486), (-2.1123838, -1.4996082), (-2.75673, -2.716044), (-1.8219633, -2.6248364), (-1.1267524, -1.4097248), (-2.0138133, -1.4741508), (-2.7606955, -2.6885996), (-1.9221206, -2.6372032), (-1.1084399, -1.4039589), (-1.8909471, -1.4317703), (-2.7739682, -2.686359), (-2.0496786, -2.6713545), (-1.0783691, -1.3527822), (-1.7357231, -1.3429676), (-2.7994766, -2.7247946), (-2.21318, -2.7557957), (-1.0391771, -1.2760719), (-1.5437198, -1.2084534), (-2.8239994, -2.7186627), (-2.4085197, -2.817149), (-1.0120494, -1.2407893), (-1.3291929, -1.1012162), (-2.8309126, -2.6693513), (-2.6175659, -2.8453755), (-1.0456498, -1.2700685), (-1.1621486, -1.0700114), (-2.7690063, -2.606792), (-2.73655, -2.8396475), (-1.1230577, -1.3344208), (-1.0889511, -1.0963963), (-2.6868181, -2.5589797), (-2.7715995, -2.819266), (-1.201047, -1.3991646), (-1.080821, -1.1440532), (-2.6191304, -2.5294359), (-2.764075, -2.7967196), (-1.258553, -1.4501262), (-1.0985388, -1.1910759), (-2.5744479, -2.511991), (-2.7440944, -2.7768686), (-1.2950559, -1.4872638), (-1.1214396, -1.2299685), (-2.5488043, -2.500883), (-2.724439, -2.761185), (-1.3161504, -1.5147929), (-1.1418623, -1.2490014), (-2.5366619, -2.4806273), (-2.710594, -2.7547183), (-1.3278815, -1.5447401), (-1.155977, -1.2674866), (-2.5305784, -2.463557), (-2.7023149, -2.7518992), (-1.335681, -1.5759673), (-1.1650815, -1.2796205), (-2.526809, -2.4409666), (-2.6989057, -2.752745), (-1.3429345, -1.6116617), (-1.1699045, -1.286237), (-2.5224056, -2.4107392), (-2.6994667, -2.7575316), (-1.3520033, -1.6548969), (-1.171175, -1.2874616), (-2.5152564, -2.3702564), (-2.7032895, -2.7671504)]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-6.3 -6.3  0.   0.   0.   0. ]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#print(action)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#if env.airgap > 10.1: action = max(action, [-50])\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#print(action)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m t3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 38\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m t4 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     40\u001b[0m distancia1, distancia2, vel, velangular, current1, current2 \u001b[38;5;241m=\u001b[39m state\n",
            "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mBobinaEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;66;03m# Apply action\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrash, angulo, error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msistema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistancia1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistancia2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelangular, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;66;03m# Reduce the time of the experiment\u001b[39;00m\n",
            "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTwoGDL.step\u001b[1;34m(self, target_voltage1, target_voltage2, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     68\u001b[0m vertical_force1, current1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\u001b[38;5;241m.\u001b[39mvertical_force(airgap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mairgap1, target_voltage \u001b[38;5;241m=\u001b[39m target_voltage1, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m vertical_force2, current2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertical_force\u001b[49m\u001b[43m(\u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mairgap2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_voltage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_voltage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFuerza vertical 1: \u001b[39m\u001b[38;5;124m\"\u001b[39m, vertical_force1)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFuerza vertical 2: \u001b[39m\u001b[38;5;124m\"\u001b[39m, vertical_force2)\n",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mBobina.vertical_force\u001b[1;34m(self, airgap, target_voltage, temperature)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvertical_force\u001b[39m(\u001b[38;5;28mself\u001b[39m, airgap, target_voltage, temperature):\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;66;03m# Get current from RL circuit\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m   I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_voltage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_voltage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Clip result\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m I \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m45\u001b[39m:\n",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mBobina.RL\u001b[1;34m(self, airgap, target_voltage, temperature)\u001b[0m\n\u001b[0;32m     34\u001b[0m V \u001b[38;5;241m=\u001b[39m target_voltage\n\u001b[0;32m     35\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent\n\u001b[1;32m---> 36\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m dIdt \u001b[38;5;241m=\u001b[39m ((V\u001b[38;5;241m/\u001b[39mR) \u001b[38;5;241m-\u001b[39m I)\u001b[38;5;241m/\u001b[39m(L\u001b[38;5;241m/\u001b[39mR)                                      \n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_RL\u001b[38;5;241m.\u001b[39mappend(dIdt)                                   \n",
            "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mBobina.L\u001b[1;34m(self, airgap, current)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mL\u001b[39m(\u001b[38;5;28mself\u001b[39m, airgap, current):\n\u001b[0;32m     46\u001b[0m   \u001b[38;5;66;03m# Return predicted inductance\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minductance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mairgap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:1004\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m-> 1004\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m y_hat \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:664\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    658\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m    666\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\tree\\_classes.py:506\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    504\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    505\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m--> 506\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "import time\n",
        "\n",
        "MODEL_PATH = f\"models/2gdl8x8nocrash/model.zip\"\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = BobinaEnv(duration = 6)\n",
        "\n",
        "# Load the trained agent\n",
        "model = SAC.load(MODEL_PATH, env=env, learning_rate = 0.00001)\n",
        "\n",
        "# Evaluate the agent\n",
        "for i in range(5):\n",
        "    obs, seed = env.reset()\n",
        "    print(obs)\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    velocidad = 0\n",
        "    current = 0\n",
        "    observations = []\n",
        "    noises = []\n",
        "    timepredict = 0\n",
        "    timemodel = 0\n",
        "    distancia1, distancia2, vel, velangular, current1, current2 = obs\n",
        "    while not done:\n",
        "        #noise = random.uniform(-1, 1)\n",
        "        #obs = obs + noise\n",
        "        observations.append(obs)\n",
        "        #noises.append(noise)\n",
        "        t1 = time.time()\n",
        "        action, _states = model.predict(np.array([distancia1, distancia2, vel, velangular, current1, current2]), deterministic=True)\n",
        "        t2 = time.time()\n",
        "        #print(action)\n",
        "        #if env.airgap > 10.1: action = max(action, [-50])\n",
        "        #print(action)\n",
        "        t3 = time.time()\n",
        "        state, reward, terminated, truncated, info = env.step(np.array(action))\n",
        "        t4 = time.time()\n",
        "        distancia1, distancia2, vel, velangular, current1, current2 = state\n",
        "        done = truncated or terminated\n",
        "        episode_reward += reward\n",
        "        timepredict += t2 - t1\n",
        "        timemodel += t4 - t3\n",
        "        noises.append((current1, current2))\n",
        "    print(\"Time predict\", timepredict)\n",
        "    print(\"Time model\", timemodel)\n",
        "    print(\"Episode reward\", episode_reward)\n",
        "    print(noises)\n",
        "    env.render(\"yes\", normalize = True, seed = i+5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.savefig(\"images/floor-2gdl8x8nocrash.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "\n",
        "#import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class OnnxablePolicy(th.nn.Module):\n",
        "    def __init__(self, actor: th.nn.Module):\n",
        "        super().__init__()\n",
        "        # Removing the flatten layer because it can't be onnxed\n",
        "        self.actor = th.nn.Sequential(\n",
        "            actor.latent_pi,\n",
        "            actor.mu,\n",
        "            # For gSDE\n",
        "            # th.nn.Hardtanh(min_val=-actor.clip_mean, max_val=actor.clip_mean),\n",
        "            # Squash the output\n",
        "            th.nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, observation: th.Tensor) -> th.Tensor:\n",
        "        # NOTE: You may have to process (normalize) observation in the correct\n",
        "        #       way before using this. See `common.preprocessing.preprocess_obs`\n",
        "        return self.actor(observation)\n",
        "\n",
        "\n",
        "# Example: model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
        "model = SAC.load(f\"models/sac8x8h9.3/model.zip\", device=\"cpu\")\n",
        "onnxable_model = OnnxablePolicy(model.policy.actor)\n",
        "\n",
        "observation_size = model.observation_space.shape\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "th.onnx.export(\n",
        "    onnxable_model,\n",
        "    dummy_input,\n",
        "    \"./ONNX/1gdlComplexH98x8.onnx\",\n",
        "    opset_version=9,\n",
        "    input_names=[\"input\"],\n",
        ")\n",
        "\n",
        "##### Load and test with onnx\n",
        "\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "onnx_path = \"./ONNX/1gdlComplexH98x8.onnx\"\n",
        "\n",
        "observation = np.zeros((1, *observation_size)).astype(np.float32)\n",
        "ort_sess = ort.InferenceSession(onnx_path)\n",
        "action = ort_sess.run(None, {\"input\": observation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SACPolicy(\n",
              "  (actor): Actor(\n",
              "    (features_extractor): FlattenExtractor(\n",
              "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (latent_pi): Sequential(\n",
              "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=8, out_features=8, bias=True)\n",
              "      (3): ReLU()\n",
              "    )\n",
              "    (mu): Linear(in_features=8, out_features=1, bias=True)\n",
              "    (log_std): Linear(in_features=8, out_features=1, bias=True)\n",
              "  )\n",
              "  (critic): ContinuousCritic(\n",
              "    (features_extractor): FlattenExtractor(\n",
              "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (qf0): Sequential(\n",
              "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "    (qf1): Sequential(\n",
              "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (critic_target): ContinuousCritic(\n",
              "    (features_extractor): FlattenExtractor(\n",
              "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (qf0): Sequential(\n",
              "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "    (qf1): Sequential(\n",
              "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See \"ONNX export\" for imports and OnnxablePolicy\n",
        "jit_path = \"1gdlComplex.pt\"\n",
        "\n",
        "# Trace and optimize the module\n",
        "traced_module = th.jit.trace(onnxable_model.eval(), dummy_input)\n",
        "frozen_module = th.jit.freeze(traced_module)\n",
        "frozen_module = th.jit.optimize_for_inference(frozen_module)\n",
        "th.jit.save(frozen_module, jit_path)\n",
        "\n",
        "##### Load and test with torch\n",
        "\n",
        "import torch as th\n",
        "\n",
        "dummy_input = th.randn(1, *observation_size)\n",
        "loaded_module = th.jit.load(jit_path)\n",
        "action_jit = loaded_module(dummy_input)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
