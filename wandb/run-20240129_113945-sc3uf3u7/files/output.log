
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 2gdl/SAC_51
Eval num_timesteps=10000, episode_reward=-49521511.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+07 |
| time/              |           |
|    total_timesteps | 10000     |
| train/             |           |
|    actor_loss      | 3.8e+05   |
|    critic_loss     | 3.91e+05  |
|    ent_coef        | 11.6      |
|    ent_coef_loss   | -0.177    |
|    learning_rate   | 0.000999  |
|    n_updates       | 11257     |
----------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-934157.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.34e+05 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | 3.15e+05  |
|    critic_loss     | 5.9e+06   |
|    ent_coef        | 256       |
|    ent_coef_loss   | -0.947    |
|    learning_rate   | 0.000998  |
|    n_updates       | 21257     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.73e+07 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 17        |
|    time_elapsed    | 1170      |
|    total_timesteps | 20000     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-39674.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.97e+04 |
| time/              |           |
|    total_timesteps | 30000     |
| train/             |           |
|    actor_loss      | 1.89e+05  |
|    critic_loss     | 1.26e+07  |
|    ent_coef        | 211       |
|    ent_coef_loss   | -1.39     |
|    learning_rate   | 0.000997  |
|    n_updates       | 31257     |
----------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=-96437.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.64e+04 |
| time/              |           |
|    total_timesteps | 40000     |
| train/             |           |
|    actor_loss      | 1.16e+05  |
|    critic_loss     | 6.39e+06  |
|    ent_coef        | 147       |
|    ent_coef_loss   | -0.0605   |
|    learning_rate   | 0.000996  |
|    n_updates       | 41257     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+07 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 17        |
|    time_elapsed    | 2334      |
|    total_timesteps | 40000     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-851019.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.51e+05 |
| time/              |           |
|    total_timesteps | 50000     |
| train/             |           |
|    actor_loss      | 8.82e+04  |
|    critic_loss     | 5.35e+06  |
|    ent_coef        | 132       |
|    ent_coef_loss   | -0.118    |
|    learning_rate   | 0.000995  |
|    n_updates       | 51257     |
----------------------------------
Eval num_timesteps=60000, episode_reward=-48613.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+04 |
| time/              |           |
|    total_timesteps | 60000     |
| train/             |           |
|    actor_loss      | 7.88e+04  |
|    critic_loss     | 4.52e+06  |
|    ent_coef        | 118       |
|    ent_coef_loss   | -1.26     |
|    learning_rate   | 0.000994  |
|    n_updates       | 61257     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.25e+07 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 17        |
|    time_elapsed    | 3487      |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-33522.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.35e+04 |
| time/              |           |
|    total_timesteps | 70000     |
| train/             |           |
|    actor_loss      | 4.56e+04  |
|    critic_loss     | 2.77e+06  |
|    ent_coef        | 81        |
|    ent_coef_loss   | 0.787     |
|    learning_rate   | 0.000993  |
|    n_updates       | 71257     |
----------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=-1763.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.76e+03 |
| time/              |           |
|    total_timesteps | 80000     |
| train/             |           |
|    actor_loss      | 4.54e+04  |
|    critic_loss     | 2.33e+06  |
|    ent_coef        | 55.9      |
|    ent_coef_loss   | -0.87     |
|    learning_rate   | 0.000992  |
|    n_updates       | 81257     |
----------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -9.4e+06 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 17       |
|    time_elapsed    | 4642     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-48470.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.85e+04 |
| time/              |           |
|    total_timesteps | 90000     |
| train/             |           |
|    actor_loss      | 3.9e+04   |
|    critic_loss     | 1.53e+06  |
|    ent_coef        | 39.3      |
|    ent_coef_loss   | 0.155     |
|    learning_rate   | 0.000991  |
|    n_updates       | 91257     |
----------------------------------
Eval num_timesteps=100000, episode_reward=-60267.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.03e+04 |
| time/              |           |
|    total_timesteps | 100000    |
| train/             |           |
|    actor_loss      | 3.59e+04  |
|    critic_loss     | 7.08e+05  |
|    ent_coef        | 33.6      |
|    ent_coef_loss   | 0.706     |
|    learning_rate   | 0.00099   |
|    n_updates       | 101257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -7.54e+06 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 17        |
|    time_elapsed    | 5797      |
|    total_timesteps | 100000    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-172597.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.73e+05 |
| time/              |           |
|    total_timesteps | 110000    |
| train/             |           |
|    actor_loss      | 4.75e+04  |
|    critic_loss     | 1.88e+06  |
|    ent_coef        | 30.7      |
|    ent_coef_loss   | -0.4      |
|    learning_rate   | 0.000989  |
|    n_updates       | 111257    |
----------------------------------
Eval num_timesteps=120000, episode_reward=-53304.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.33e+04 |
| time/              |           |
|    total_timesteps | 120000    |
| train/             |           |
|    actor_loss      | 3.73e+04  |
|    critic_loss     | 1e+06     |
|    ent_coef        | 25.7      |
|    ent_coef_loss   | 0.669     |
|    learning_rate   | 0.000988  |
|    n_updates       | 121257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -6.3e+06 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 17       |
|    time_elapsed    | 6991     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-51111.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.11e+04 |
| time/              |           |
|    total_timesteps | 130000    |
| train/             |           |
|    actor_loss      | 5.24e+04  |
|    critic_loss     | 1.74e+06  |
|    ent_coef        | 24.2      |
|    ent_coef_loss   | -1.03     |
|    learning_rate   | 0.000987  |
|    n_updates       | 131257    |
----------------------------------
Eval num_timesteps=140000, episode_reward=-42702.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.27e+04 |
| time/              |           |
|    total_timesteps | 140000    |
| train/             |           |
|    actor_loss      | 3.97e+04  |
|    critic_loss     | 1.3e+06   |
|    ent_coef        | 19.6      |
|    ent_coef_loss   | -0.44     |
|    learning_rate   | 0.000986  |
|    n_updates       | 141257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -5.4e+06 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 17       |
|    time_elapsed    | 8218     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-66270.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.63e+04 |
| time/              |           |
|    total_timesteps | 150000    |
| train/             |           |
|    actor_loss      | 3.4e+04   |
|    critic_loss     | 2.64e+06  |
|    ent_coef        | 18.3      |
|    ent_coef_loss   | 0.725     |
|    learning_rate   | 0.000985  |
|    n_updates       | 151257    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-30825.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.08e+04 |
| time/              |           |
|    total_timesteps | 160000    |
| train/             |           |
|    actor_loss      | 2.54e+04  |
|    critic_loss     | 9.71e+05  |
|    ent_coef        | 18.9      |
|    ent_coef_loss   | 0.833     |
|    learning_rate   | 0.000984  |
|    n_updates       | 161257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.74e+06 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 16        |
|    time_elapsed    | 9423      |
|    total_timesteps | 160000    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-75364.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.54e+04 |
| time/              |           |
|    total_timesteps | 170000    |
| train/             |           |
|    actor_loss      | 3.62e+04  |
|    critic_loss     | 1.12e+06  |
|    ent_coef        | 18.8      |
|    ent_coef_loss   | -0.397    |
|    learning_rate   | 0.000983  |
|    n_updates       | 171257    |
----------------------------------
Eval num_timesteps=180000, episode_reward=-100867.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 180000    |
| train/             |           |
|    actor_loss      | 2.74e+04  |
|    critic_loss     | 8.49e+05  |
|    ent_coef        | 16.3      |
|    ent_coef_loss   | 0.189     |
|    learning_rate   | 0.000982  |
|    n_updates       | 181257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.22e+06 |
| time/              |           |
|    episodes        | 36        |
|    fps             | 16        |
|    time_elapsed    | 10588     |
|    total_timesteps | 180000    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-72690.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.27e+04 |
| time/              |           |
|    total_timesteps | 190000    |
| train/             |           |
|    actor_loss      | 2.78e+04  |
|    critic_loss     | 7.34e+05  |
|    ent_coef        | 16.4      |
|    ent_coef_loss   | 0.791     |
|    learning_rate   | 0.000981  |
|    n_updates       | 191257    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-42052.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.21e+04 |
| time/              |           |
|    total_timesteps | 200000    |
| train/             |           |
|    actor_loss      | 2.31e+04  |
|    critic_loss     | 7.6e+05   |
|    ent_coef        | 16.7      |
|    ent_coef_loss   | 0.912     |
|    learning_rate   | 0.00098   |
|    n_updates       | 201257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.8e+06 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 17       |
|    time_elapsed    | 11754    |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-56708.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.67e+04 |
| time/              |           |
|    total_timesteps | 210000    |
| train/             |           |
|    actor_loss      | 3.34e+04  |
|    critic_loss     | 8.82e+05  |
|    ent_coef        | 14.8      |
|    ent_coef_loss   | 0.0855    |
|    learning_rate   | 0.000979  |
|    n_updates       | 211257    |
----------------------------------
Eval num_timesteps=220000, episode_reward=-66802.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.68e+04 |
| time/              |           |
|    total_timesteps | 220000    |
| train/             |           |
|    actor_loss      | 2.63e+04  |
|    critic_loss     | 3.32e+05  |
|    ent_coef        | 13.2      |
|    ent_coef_loss   | 0.125     |
|    learning_rate   | 0.000978  |
|    n_updates       | 221257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.46e+06 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 17        |
|    time_elapsed    | 12922     |
|    total_timesteps | 220000    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-43263.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.33e+04 |
| time/              |           |
|    total_timesteps | 230000    |
| train/             |           |
|    actor_loss      | 2.97e+04  |
|    critic_loss     | 7.01e+05  |
|    ent_coef        | 12.4      |
|    ent_coef_loss   | 0.634     |
|    learning_rate   | 0.000977  |
|    n_updates       | 231257    |
----------------------------------
Eval num_timesteps=240000, episode_reward=-69010.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -6.9e+04 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 3.14e+04 |
|    critic_loss     | 3.32e+06 |
|    ent_coef        | 13       |
|    ent_coef_loss   | -0.852   |
|    learning_rate   | 0.000976 |
|    n_updates       | 241257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.18e+06 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 17        |
|    time_elapsed    | 14113     |
|    total_timesteps | 240000    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-41456.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.15e+04 |
| time/              |           |
|    total_timesteps | 250000    |
| train/             |           |
|    actor_loss      | 3.23e+04  |
|    critic_loss     | 7.61e+05  |
|    ent_coef        | 12.8      |
|    ent_coef_loss   | -1.09     |
|    learning_rate   | 0.000975  |
|    n_updates       | 251257    |
----------------------------------
Eval num_timesteps=260000, episode_reward=-16584.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.66e+04 |
| time/              |           |
|    total_timesteps | 260000    |
| train/             |           |
|    actor_loss      | 2.57e+04  |
|    critic_loss     | 6.41e+05  |
|    ent_coef        | 13.2      |
|    ent_coef_loss   | -0.0138   |
|    learning_rate   | 0.000974  |
|    n_updates       | 261257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.94e+06 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 17        |
|    time_elapsed    | 15290     |
|    total_timesteps | 260000    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-17130.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.71e+04 |
| time/              |           |
|    total_timesteps | 270000    |
| train/             |           |
|    actor_loss      | 2.29e+04  |
|    critic_loss     | 5.99e+05  |
|    ent_coef        | 11.2      |
|    ent_coef_loss   | 0.651     |
|    learning_rate   | 0.000973  |
|    n_updates       | 271257    |
----------------------------------
Eval num_timesteps=280000, episode_reward=-16232.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+04 |
| time/              |           |
|    total_timesteps | 280000    |
| train/             |           |
|    actor_loss      | 2.94e+04  |
|    critic_loss     | 1.39e+06  |
|    ent_coef        | 11.9      |
|    ent_coef_loss   | -0.198    |
|    learning_rate   | 0.000972  |
|    n_updates       | 281257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.73e+06 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 17        |
|    time_elapsed    | 16467     |
|    total_timesteps | 280000    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-38954.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.9e+04 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 2.86e+04 |
|    critic_loss     | 8.28e+05 |
|    ent_coef        | 11.5     |
|    ent_coef_loss   | -0.607   |
|    learning_rate   | 0.000971 |
|    n_updates       | 291257   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-86296.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.63e+04 |
| time/              |           |
|    total_timesteps | 300000    |
| train/             |           |
|    actor_loss      | 2.28e+04  |
|    critic_loss     | 5.18e+05  |
|    ent_coef        | 11.6      |
|    ent_coef_loss   | 0.98      |
|    learning_rate   | 0.00097   |
|    n_updates       | 301257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.55e+06 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 16        |
|    time_elapsed    | 17653     |
|    total_timesteps | 300000    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-40709.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.07e+04 |
| time/              |           |
|    total_timesteps | 310000    |
| train/             |           |
|    actor_loss      | 2.06e+04  |
|    critic_loss     | 6.01e+05  |
|    ent_coef        | 11.2      |
|    ent_coef_loss   | -0.432    |
|    learning_rate   | 0.000969  |
|    n_updates       | 311257    |
----------------------------------
Eval num_timesteps=320000, episode_reward=-90552.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.06e+04 |
| time/              |           |
|    total_timesteps | 320000    |
| train/             |           |
|    actor_loss      | 2.35e+04  |
|    critic_loss     | 3.49e+05  |
|    ent_coef        | 11.2      |
|    ent_coef_loss   | 0.318     |
|    learning_rate   | 0.000968  |
|    n_updates       | 321257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.4e+06 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 17       |
|    time_elapsed    | 18812    |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-69006.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -6.9e+04 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 1.42e+04 |
|    critic_loss     | 3.31e+05 |
|    ent_coef        | 11.7     |
|    ent_coef_loss   | -0.00988 |
|    learning_rate   | 0.000967 |
|    n_updates       | 331257   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-123067.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 340000    |
| train/             |           |
|    actor_loss      | 2.48e+04  |
|    critic_loss     | 1.37e+07  |
|    ent_coef        | 10.8      |
|    ent_coef_loss   | -0.802    |
|    learning_rate   | 0.000966  |
|    n_updates       | 341257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.26e+06 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 17        |
|    time_elapsed    | 19974     |
|    total_timesteps | 340000    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-31685.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+04 |
| time/              |           |
|    total_timesteps | 350000    |
| train/             |           |
|    actor_loss      | 2.73e+04  |
|    critic_loss     | 4.33e+06  |
|    ent_coef        | 12.1      |
|    ent_coef_loss   | -0.1      |
|    learning_rate   | 0.000965  |
|    n_updates       | 351257    |
----------------------------------
Eval num_timesteps=360000, episode_reward=-24163.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.42e+04 |
| time/              |           |
|    total_timesteps | 360000    |
| train/             |           |
|    actor_loss      | 2.21e+04  |
|    critic_loss     | 1.33e+06  |
|    ent_coef        | 11.3      |
|    ent_coef_loss   | 0.15      |
|    learning_rate   | 0.000964  |
|    n_updates       | 361257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.13e+06 |
| time/              |           |
|    episodes        | 72        |
|    fps             | 17        |
|    time_elapsed    | 21127     |
|    total_timesteps | 360000    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-8753.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.75e+03 |
| time/              |           |
|    total_timesteps | 370000    |
| train/             |           |
|    actor_loss      | 1.87e+04  |
|    critic_loss     | 4.09e+05  |
|    ent_coef        | 10.7      |
|    ent_coef_loss   | 0.704     |
|    learning_rate   | 0.000963  |
|    n_updates       | 371257    |
----------------------------------
Eval num_timesteps=380000, episode_reward=-11015.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.1e+04 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | 2.22e+04 |
|    critic_loss     | 3.3e+05  |
|    ent_coef        | 11.1     |
|    ent_coef_loss   | 0.259    |
|    learning_rate   | 0.000962 |
|    n_updates       | 381257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.02e+06 |
| time/              |           |
|    episodes        | 76        |
|    fps             | 17        |
|    time_elapsed    | 22315     |
|    total_timesteps | 380000    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-11002.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.1e+04 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | 2.89e+04 |
|    critic_loss     | 1.43e+06 |
|    ent_coef        | 9.83     |
|    ent_coef_loss   | -0.0952  |
|    learning_rate   | 0.000961 |
|    n_updates       | 391257   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-17278.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.73e+04 |
| time/              |           |
|    total_timesteps | 400000    |
| train/             |           |
|    actor_loss      | 1.65e+04  |
|    critic_loss     | 3.9e+05   |
|    ent_coef        | 9.47      |
|    ent_coef_loss   | 0.704     |
|    learning_rate   | 0.00096   |
|    n_updates       | 401257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.92e+06 |
| time/              |           |
|    episodes        | 80        |
|    fps             | 17        |
|    time_elapsed    | 23474     |
|    total_timesteps | 400000    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-53827.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.38e+04 |
| time/              |           |
|    total_timesteps | 410000    |
| train/             |           |
|    actor_loss      | 2.1e+04   |
|    critic_loss     | 2.11e+06  |
|    ent_coef        | 8.19      |
|    ent_coef_loss   | -0.883    |
|    learning_rate   | 0.000959  |
|    n_updates       | 411257    |
----------------------------------
Eval num_timesteps=420000, episode_reward=-41184.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.12e+04 |
| time/              |           |
|    total_timesteps | 420000    |
| train/             |           |
|    actor_loss      | 2.45e+04  |
|    critic_loss     | 7.11e+05  |
|    ent_coef        | 9.24      |
|    ent_coef_loss   | -0.594    |
|    learning_rate   | 0.000958  |
|    n_updates       | 421257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+06 |
| time/              |           |
|    episodes        | 84        |
|    fps             | 17        |
|    time_elapsed    | 24630     |
|    total_timesteps | 420000    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-29329.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.93e+04 |
| time/              |           |
|    total_timesteps | 430000    |
| train/             |           |
|    actor_loss      | 1.77e+04  |
|    critic_loss     | 5.45e+05  |
|    ent_coef        | 7.48      |
|    ent_coef_loss   | 0.567     |
|    learning_rate   | 0.000957  |
|    n_updates       | 431257    |
----------------------------------
Eval num_timesteps=440000, episode_reward=-19610.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.96e+04 |
| time/              |           |
|    total_timesteps | 440000    |
| train/             |           |
|    actor_loss      | 2.05e+04  |
|    critic_loss     | 1.08e+06  |
|    ent_coef        | 8.88      |
|    ent_coef_loss   | -0.184    |
|    learning_rate   | 0.000956  |
|    n_updates       | 441257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.75e+06 |
| time/              |           |
|    episodes        | 88        |
|    fps             | 17        |
|    time_elapsed    | 25812     |
|    total_timesteps | 440000    |
----------------------------------
Eval num_timesteps=450000, episode_reward=-182292.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.82e+05 |
| time/              |           |
|    total_timesteps | 450000    |
| train/             |           |
|    actor_loss      | 1.1e+04   |
|    critic_loss     | 3.98e+05  |
|    ent_coef        | 10.2      |
|    ent_coef_loss   | 0.599     |
|    learning_rate   | 0.000955  |
|    n_updates       | 451257    |
----------------------------------
Eval num_timesteps=460000, episode_reward=-184592.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.85e+05 |
| time/              |           |
|    total_timesteps | 460000    |
| train/             |           |
|    actor_loss      | 1.4e+04   |
|    critic_loss     | 4.05e+05  |
|    ent_coef        | 9.7       |
|    ent_coef_loss   | 0.0765    |
|    learning_rate   | 0.000954  |
|    n_updates       | 461257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.68e+06 |
| time/              |           |
|    episodes        | 92        |
|    fps             | 16        |
|    time_elapsed    | 27121     |
|    total_timesteps | 460000    |
----------------------------------
Eval num_timesteps=470000, episode_reward=-173735.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.74e+05 |
| time/              |           |
|    total_timesteps | 470000    |
| train/             |           |
|    actor_loss      | 1.25e+04  |
|    critic_loss     | 2.55e+05  |
|    ent_coef        | 9.93      |
|    ent_coef_loss   | 0.482     |
|    learning_rate   | 0.000953  |
|    n_updates       | 471257    |
----------------------------------
Eval num_timesteps=480000, episode_reward=-9997.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1e+04   |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 1.6e+04  |
|    critic_loss     | 1.93e+05 |
|    ent_coef        | 8.1      |
|    ent_coef_loss   | 0.495    |
|    learning_rate   | 0.000952 |
|    n_updates       | 481257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.61e+06 |
| time/              |           |
|    episodes        | 96        |
|    fps             | 16        |
|    time_elapsed    | 28430     |
|    total_timesteps | 480000    |
----------------------------------
Eval num_timesteps=490000, episode_reward=-7859.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.86e+03 |
| time/              |           |
|    total_timesteps | 490000    |
| train/             |           |
|    actor_loss      | 1.5e+04   |
|    critic_loss     | 6.34e+05  |
|    ent_coef        | 6.33      |
|    ent_coef_loss   | 0.502     |
|    learning_rate   | 0.000951  |
|    n_updates       | 491257    |
----------------------------------
Eval num_timesteps=500000, episode_reward=-14783.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+04 |
| time/              |           |
|    total_timesteps | 500000    |
| train/             |           |
|    actor_loss      | 1.77e+04  |
|    critic_loss     | 2.45e+05  |
|    ent_coef        | 6.49      |
|    ent_coef_loss   | -0.701    |
|    learning_rate   | 0.00095   |
|    n_updates       | 501257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.55e+06 |
| time/              |           |
|    episodes        | 100       |
|    fps             | 16        |
|    time_elapsed    | 29755     |
|    total_timesteps | 500000    |
----------------------------------
Eval num_timesteps=510000, episode_reward=-22822.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.28e+04 |
| time/              |           |
|    total_timesteps | 510000    |
| train/             |           |
|    actor_loss      | 1.99e+04  |
|    critic_loss     | 4.18e+05  |
|    ent_coef        | 8.96      |
|    ent_coef_loss   | -0.285    |
|    learning_rate   | 0.000949  |
|    n_updates       | 511257    |
----------------------------------
Eval num_timesteps=520000, episode_reward=-9428.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.43e+03 |
| time/              |           |
|    total_timesteps | 520000    |
| train/             |           |
|    actor_loss      | 9.32e+03  |
|    critic_loss     | 4.01e+05  |
|    ent_coef        | 10.4      |
|    ent_coef_loss   | 0.601     |
|    learning_rate   | 0.000948  |
|    n_updates       | 521257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.85e+04 |
| time/              |           |
|    episodes        | 104       |
|    fps             | 16        |
|    time_elapsed    | 31035     |
|    total_timesteps | 520000    |
----------------------------------
Eval num_timesteps=530000, episode_reward=-112369.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.12e+05 |
| time/              |           |
|    total_timesteps | 530000    |
| train/             |           |
|    actor_loss      | 1.09e+04  |
|    critic_loss     | 5.29e+05  |
|    ent_coef        | 11.7      |
|    ent_coef_loss   | 0.337     |
|    learning_rate   | 0.000947  |
|    n_updates       | 531257    |
----------------------------------
Eval num_timesteps=540000, episode_reward=-3573.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.57e+03 |
| time/              |           |
|    total_timesteps | 540000    |
| train/             |           |
|    actor_loss      | 1.29e+04  |
|    critic_loss     | 5.9e+05   |
|    ent_coef        | 7.47      |
|    ent_coef_loss   | 0.131     |
|    learning_rate   | 0.000946  |
|    n_updates       | 541257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.69e+04 |
| time/              |           |
|    episodes        | 108       |
|    fps             | 16        |
|    time_elapsed    | 32299     |
|    total_timesteps | 540000    |
----------------------------------
Eval num_timesteps=550000, episode_reward=-75341.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.53e+04 |
| time/              |           |
|    total_timesteps | 550000    |
| train/             |           |
|    actor_loss      | 1.08e+04  |
|    critic_loss     | 5.63e+05  |
|    ent_coef        | 8.77      |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.000945  |
|    n_updates       | 551257    |
----------------------------------
Eval num_timesteps=560000, episode_reward=-11277.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.13e+04 |
| time/              |           |
|    total_timesteps | 560000    |
| train/             |           |
|    actor_loss      | 1.56e+04  |
|    critic_loss     | 8.31e+05  |
|    ent_coef        | 7.32      |
|    ent_coef_loss   | -0.276    |
|    learning_rate   | 0.000944  |
|    n_updates       | 561257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.33e+04 |
| time/              |           |
|    episodes        | 112       |
|    fps             | 16        |
|    time_elapsed    | 33565     |
|    total_timesteps | 560000    |
----------------------------------
Eval num_timesteps=570000, episode_reward=-9125.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.13e+03 |
| time/              |           |
|    total_timesteps | 570000    |
| train/             |           |
|    actor_loss      | 1.55e+04  |
|    critic_loss     | 6.05e+05  |
|    ent_coef        | 7.46      |
|    ent_coef_loss   | -0.191    |
|    learning_rate   | 0.000943  |
|    n_updates       | 571257    |
----------------------------------
Eval num_timesteps=580000, episode_reward=-74963.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -7.5e+04 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 1.87e+04 |
|    critic_loss     | 6.06e+05 |
|    ent_coef        | 7.44     |
|    ent_coef_loss   | -0.0418  |
|    learning_rate   | 0.000942 |
|    n_updates       | 581257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.32e+04 |
| time/              |           |
|    episodes        | 116       |
|    fps             | 16        |
|    time_elapsed    | 34827     |
|    total_timesteps | 580000    |
----------------------------------
Eval num_timesteps=590000, episode_reward=-6306.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.31e+03 |
| time/              |           |
|    total_timesteps | 590000    |
| train/             |           |
|    actor_loss      | 1.55e+04  |
|    critic_loss     | 2.71e+05  |
|    ent_coef        | 6.87      |
|    ent_coef_loss   | 0.577     |
|    learning_rate   | 0.000941  |
|    n_updates       | 591257    |
----------------------------------
Eval num_timesteps=600000, episode_reward=-12396.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.24e+04 |
| time/              |           |
|    total_timesteps | 600000    |
| train/             |           |
|    actor_loss      | 1.5e+04   |
|    critic_loss     | 2.35e+05  |
|    ent_coef        | 6.61      |
|    ent_coef_loss   | -0.169    |
|    learning_rate   | 0.00094   |
|    n_updates       | 601257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+04 |
| time/              |           |
|    episodes        | 120       |
|    fps             | 16        |
|    time_elapsed    | 36087     |
|    total_timesteps | 600000    |
----------------------------------
Eval num_timesteps=610000, episode_reward=-5579.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.58e+03 |
| time/              |           |
|    total_timesteps | 610000    |
| train/             |           |
|    actor_loss      | 1.42e+04  |
|    critic_loss     | 6.07e+05  |
|    ent_coef        | 7.53      |
|    ent_coef_loss   | 0.764     |
|    learning_rate   | 0.000939  |
|    n_updates       | 611257    |
----------------------------------
Eval num_timesteps=620000, episode_reward=-419535.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.2e+05 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | 1.35e+04 |
|    critic_loss     | 2.94e+05 |
|    ent_coef        | 10.6     |
|    ent_coef_loss   | -0.611   |
|    learning_rate   | 0.000938 |
|    n_updates       | 621257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.03e+04 |
| time/              |           |
|    episodes        | 124       |
|    fps             | 16        |
|    time_elapsed    | 37354     |
|    total_timesteps | 620000    |
----------------------------------
Eval num_timesteps=630000, episode_reward=-172940.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.73e+05 |
| time/              |           |
|    total_timesteps | 630000    |
| train/             |           |
|    actor_loss      | 1.63e+04  |
|    critic_loss     | 5.08e+05  |
|    ent_coef        | 8.99      |
|    ent_coef_loss   | -0.0235   |
|    learning_rate   | 0.000937  |
|    n_updates       | 631257    |
----------------------------------
Eval num_timesteps=640000, episode_reward=-5921.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.92e+03 |
| time/              |           |
|    total_timesteps | 640000    |
| train/             |           |
|    actor_loss      | 8.81e+03  |
|    critic_loss     | 5.32e+05  |
|    ent_coef        | 7.84      |
|    ent_coef_loss   | 0.581     |
|    learning_rate   | 0.000936  |
|    n_updates       | 641257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.61e+04 |
| time/              |           |
|    episodes        | 128       |
|    fps             | 16        |
|    time_elapsed    | 38627     |
|    total_timesteps | 640000    |
----------------------------------
Eval num_timesteps=650000, episode_reward=-9312.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.31e+03 |
| time/              |           |
|    total_timesteps | 650000    |
| train/             |           |
|    actor_loss      | 1.3e+04   |
|    critic_loss     | 5.84e+05  |
|    ent_coef        | 7.44      |
|    ent_coef_loss   | -0.676    |
|    learning_rate   | 0.000935  |
|    n_updates       | 651257    |
----------------------------------
Eval num_timesteps=660000, episode_reward=-21312.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.13e+04 |
| time/              |           |
|    total_timesteps | 660000    |
| train/             |           |
|    actor_loss      | 1.57e+04  |
|    critic_loss     | 2.03e+05  |
|    ent_coef        | 7.29      |
|    ent_coef_loss   | 0.199     |
|    learning_rate   | 0.000934  |
|    n_updates       | 661257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.38e+04 |
| time/              |           |
|    episodes        | 132       |
|    fps             | 16        |
|    time_elapsed    | 39929     |
|    total_timesteps | 660000    |
----------------------------------
Eval num_timesteps=670000, episode_reward=-65116.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.51e+04 |
| time/              |           |
|    total_timesteps | 670000    |
| train/             |           |
|    actor_loss      | 9.92e+03  |
|    critic_loss     | 2.61e+05  |
|    ent_coef        | 8.43      |
|    ent_coef_loss   | -0.275    |
|    learning_rate   | 0.000933  |
|    n_updates       | 671257    |
----------------------------------
Eval num_timesteps=680000, episode_reward=-124993.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.25e+05 |
| time/              |           |
|    total_timesteps | 680000    |
| train/             |           |
|    actor_loss      | 9.1e+03   |
|    critic_loss     | 3.03e+05  |
|    ent_coef        | 9.05      |
|    ent_coef_loss   | 0.909     |
|    learning_rate   | 0.000932  |
|    n_updates       | 681257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.25e+04 |
| time/              |           |
|    episodes        | 136       |
|    fps             | 16        |
|    time_elapsed    | 41262     |
|    total_timesteps | 680000    |
----------------------------------
Eval num_timesteps=690000, episode_reward=-13213.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.32e+04 |
| time/              |           |
|    total_timesteps | 690000    |
| train/             |           |
|    actor_loss      | 7.67e+03  |
|    critic_loss     | 1.38e+05  |
|    ent_coef        | 7.32      |
|    ent_coef_loss   | -0.392    |
|    learning_rate   | 0.000931  |
|    n_updates       | 691257    |
----------------------------------
Eval num_timesteps=700000, episode_reward=-295426.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.95e+05 |
| time/              |           |
|    total_timesteps | 700000    |
| train/             |           |
|    actor_loss      | 1.49e+04  |
|    critic_loss     | 2.58e+05  |
|    ent_coef        | 9.01      |
|    ent_coef_loss   | -0.572    |
|    learning_rate   | 0.00093   |
|    n_updates       | 701257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -5.3e+04 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 16       |
|    time_elapsed    | 42544    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-65928.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.59e+04 |
| time/              |           |
|    total_timesteps | 710000    |
| train/             |           |
|    actor_loss      | 1.05e+04  |
|    critic_loss     | 3.76e+05  |
|    ent_coef        | 8.43      |
|    ent_coef_loss   | 0.275     |
|    learning_rate   | 0.000929  |
|    n_updates       | 711257    |
----------------------------------
Eval num_timesteps=720000, episode_reward=-20276.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.03e+04 |
| time/              |           |
|    total_timesteps | 720000    |
| train/             |           |
|    actor_loss      | 1.08e+04  |
|    critic_loss     | 5.33e+05  |
|    ent_coef        | 7.89      |
|    ent_coef_loss   | 0.108     |
|    learning_rate   | 0.000928  |
|    n_updates       | 721257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.29e+04 |
| time/              |           |
|    episodes        | 144       |
|    fps             | 16        |
|    time_elapsed    | 43829     |
|    total_timesteps | 720000    |
----------------------------------
Eval num_timesteps=730000, episode_reward=-21055.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+04 |
| time/              |           |
|    total_timesteps | 730000    |
| train/             |           |
|    actor_loss      | 1.41e+04  |
|    critic_loss     | 1.81e+05  |
|    ent_coef        | 8.18      |
|    ent_coef_loss   | 0.126     |
|    learning_rate   | 0.000927  |
|    n_updates       | 731257    |
----------------------------------
Eval num_timesteps=740000, episode_reward=-25382.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.54e+04 |
| time/              |           |
|    total_timesteps | 740000    |
| train/             |           |
|    actor_loss      | 6.25e+03  |
|    critic_loss     | 2.08e+05  |
|    ent_coef        | 8.32      |
|    ent_coef_loss   | -0.437    |
|    learning_rate   | 0.000926  |
|    n_updates       | 741257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.14e+04 |
| time/              |           |
|    episodes        | 148       |
|    fps             | 16        |
|    time_elapsed    | 45109     |
|    total_timesteps | 740000    |
----------------------------------
Eval num_timesteps=750000, episode_reward=-26479.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.65e+04 |
| time/              |           |
|    total_timesteps | 750000    |
| train/             |           |
|    actor_loss      | 1.25e+04  |
|    critic_loss     | 2.88e+05  |
|    ent_coef        | 7.08      |
|    ent_coef_loss   | -0.284    |
|    learning_rate   | 0.000925  |
|    n_updates       | 751257    |
----------------------------------
Eval num_timesteps=760000, episode_reward=-42741.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.27e+04 |
| time/              |           |
|    total_timesteps | 760000    |
| train/             |           |
|    actor_loss      | 1.15e+04  |
|    critic_loss     | 8.69e+05  |
|    ent_coef        | 7.22      |
|    ent_coef_loss   | 0.249     |
|    learning_rate   | 0.000924  |
|    n_updates       | 761257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.15e+04 |
| time/              |           |
|    episodes        | 152       |
|    fps             | 16        |
|    time_elapsed    | 46376     |
|    total_timesteps | 760000    |
----------------------------------
Eval num_timesteps=770000, episode_reward=-14944.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+04 |
| time/              |           |
|    total_timesteps | 770000    |
| train/             |           |
|    actor_loss      | 1.17e+04  |
|    critic_loss     | 4.36e+05  |
|    ent_coef        | 7.67      |
|    ent_coef_loss   | -0.569    |
|    learning_rate   | 0.000923  |
|    n_updates       | 771257    |
----------------------------------
Eval num_timesteps=780000, episode_reward=-29925.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 780000    |
| train/             |           |
|    actor_loss      | 1.29e+04  |
|    critic_loss     | 4.25e+05  |
|    ent_coef        | 6.36      |
|    ent_coef_loss   | -0.136    |
|    learning_rate   | 0.000922  |
|    n_updates       | 781257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.09e+04 |
| time/              |           |
|    episodes        | 156       |
|    fps             | 16        |
|    time_elapsed    | 47635     |
|    total_timesteps | 780000    |
----------------------------------
Eval num_timesteps=790000, episode_reward=-73853.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.39e+04 |
| time/              |           |
|    total_timesteps | 790000    |
| train/             |           |
|    actor_loss      | 1.49e+04  |
|    critic_loss     | 5.37e+05  |
|    ent_coef        | 5.85      |
|    ent_coef_loss   | 0.161     |
|    learning_rate   | 0.000921  |
|    n_updates       | 791257    |
----------------------------------
Eval num_timesteps=800000, episode_reward=-325330.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.25e+05 |
| time/              |           |
|    total_timesteps | 800000    |
| train/             |           |
|    actor_loss      | 1.08e+04  |
|    critic_loss     | 5.17e+05  |
|    ent_coef        | 7.35      |
|    ent_coef_loss   | 1.05      |
|    learning_rate   | 0.00092   |
|    n_updates       | 801257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.42e+04 |
| time/              |           |
|    episodes        | 160       |
|    fps             | 16        |
|    time_elapsed    | 48896     |
|    total_timesteps | 800000    |
----------------------------------
Eval num_timesteps=810000, episode_reward=-374945.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.75e+05 |
| time/              |           |
|    total_timesteps | 810000    |
| train/             |           |
|    actor_loss      | 7.8e+03   |
|    critic_loss     | 2.07e+05  |
|    ent_coef        | 9.83      |
|    ent_coef_loss   | -0.389    |
|    learning_rate   | 0.000919  |
|    n_updates       | 811257    |
----------------------------------
Eval num_timesteps=820000, episode_reward=-8956.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.96e+03 |
| time/              |           |
|    total_timesteps | 820000    |
| train/             |           |
|    actor_loss      | 1.34e+04  |
|    critic_loss     | 6.91e+05  |
|    ent_coef        | 9.24      |
|    ent_coef_loss   | -0.821    |
|    learning_rate   | 0.000918  |
|    n_updates       | 821257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.76e+04 |
| time/              |           |
|    episodes        | 164       |
|    fps             | 16        |
|    time_elapsed    | 50155     |
|    total_timesteps | 820000    |
----------------------------------
Eval num_timesteps=830000, episode_reward=-7198.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -7.2e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | 1.05e+04 |
|    critic_loss     | 6.86e+05 |
|    ent_coef        | 6.39     |
|    ent_coef_loss   | -0.0129  |
|    learning_rate   | 0.000917 |
|    n_updates       | 831257   |
---------------------------------
Eval num_timesteps=840000, episode_reward=-10422.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.04e+04 |
| time/              |           |
|    total_timesteps | 840000    |
| train/             |           |
|    actor_loss      | 8.26e+03  |
|    critic_loss     | 2.55e+05  |
|    ent_coef        | 13        |
|    ent_coef_loss   | 0.873     |
|    learning_rate   | 0.000916  |
|    n_updates       | 841257    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -5.7e+04 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 16       |
|    time_elapsed    | 51422    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-7214.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.21e+03 |
| time/              |           |
|    total_timesteps | 850000    |
| train/             |           |
|    actor_loss      | 1.16e+04  |
|    critic_loss     | 1.69e+05  |
|    ent_coef        | 7.24      |
|    ent_coef_loss   | -0.683    |
|    learning_rate   | 0.000915  |
|    n_updates       | 851257    |
----------------------------------
Eval num_timesteps=860000, episode_reward=-4188.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 860000    |
| train/             |           |
|    actor_loss      | 1.37e+04  |
|    critic_loss     | 2.03e+05  |
|    ent_coef        | 6.11      |
|    ent_coef_loss   | 0.0205    |
|    learning_rate   | 0.000914  |
|    n_updates       | 861257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.56e+04 |
| time/              |           |
|    episodes        | 172       |
|    fps             | 16        |
|    time_elapsed    | 52698     |
|    total_timesteps | 860000    |
----------------------------------
Eval num_timesteps=870000, episode_reward=-8550.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.55e+03 |
| time/              |           |
|    total_timesteps | 870000    |
| train/             |           |
|    actor_loss      | 1.45e+04  |
|    critic_loss     | 3.47e+05  |
|    ent_coef        | 5.02      |
|    ent_coef_loss   | -0.297    |
|    learning_rate   | 0.000913  |
|    n_updates       | 871257    |
----------------------------------
Eval num_timesteps=880000, episode_reward=-2567.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.57e+03 |
| time/              |           |
|    total_timesteps | 880000    |
| train/             |           |
|    actor_loss      | 1.1e+04   |
|    critic_loss     | 4.31e+05  |
|    ent_coef        | 6.12      |
|    ent_coef_loss   | 0.116     |
|    learning_rate   | 0.000912  |
|    n_updates       | 881257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.55e+04 |
| time/              |           |
|    episodes        | 176       |
|    fps             | 16        |
|    time_elapsed    | 53976     |
|    total_timesteps | 880000    |
----------------------------------
Eval num_timesteps=890000, episode_reward=-15573.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+04 |
| time/              |           |
|    total_timesteps | 890000    |
| train/             |           |
|    actor_loss      | 9.28e+03  |
|    critic_loss     | 1.68e+05  |
|    ent_coef        | 5.36      |
|    ent_coef_loss   | -0.213    |
|    learning_rate   | 0.000911  |
|    n_updates       | 891257    |
----------------------------------
Eval num_timesteps=900000, episode_reward=-14311.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 900000    |
| train/             |           |
|    actor_loss      | 1.36e+04  |
|    critic_loss     | 4.76e+05  |
|    ent_coef        | 4.88      |
|    ent_coef_loss   | 0.0775    |
|    learning_rate   | 0.00091   |
|    n_updates       | 901257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.73e+04 |
| time/              |           |
|    episodes        | 180       |
|    fps             | 16        |
|    time_elapsed    | 55265     |
|    total_timesteps | 900000    |
----------------------------------
Eval num_timesteps=910000, episode_reward=-86239.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.62e+04 |
| time/              |           |
|    total_timesteps | 910000    |
| train/             |           |
|    actor_loss      | 9.8e+03   |
|    critic_loss     | 1.96e+05  |
|    ent_coef        | 4.55      |
|    ent_coef_loss   | 0.213     |
|    learning_rate   | 0.000909  |
|    n_updates       | 911257    |
----------------------------------
Eval num_timesteps=920000, episode_reward=-27368.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.74e+04 |
| time/              |           |
|    total_timesteps | 920000    |
| train/             |           |
|    actor_loss      | 1.23e+04  |
|    critic_loss     | 1.57e+05  |
|    ent_coef        | 5.4       |
|    ent_coef_loss   | 0.0462    |
|    learning_rate   | 0.000908  |
|    n_updates       | 921257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.74e+04 |
| time/              |           |
|    episodes        | 184       |
|    fps             | 16        |
|    time_elapsed    | 56585     |
|    total_timesteps | 920000    |
----------------------------------
Eval num_timesteps=930000, episode_reward=-16090.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 930000    |
| train/             |           |
|    actor_loss      | 1.74e+04  |
|    critic_loss     | 1.86e+05  |
|    ent_coef        | 4.72      |
|    ent_coef_loss   | -0.756    |
|    learning_rate   | 0.000907  |
|    n_updates       | 931257    |
----------------------------------
Eval num_timesteps=940000, episode_reward=-11823.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.18e+04 |
| time/              |           |
|    total_timesteps | 940000    |
| train/             |           |
|    actor_loss      | 7.66e+03  |
|    critic_loss     | 2.78e+05  |
|    ent_coef        | 4.7       |
|    ent_coef_loss   | 0.0261    |
|    learning_rate   | 0.000906  |
|    n_updates       | 941257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.99e+04 |
| time/              |           |
|    episodes        | 188       |
|    fps             | 16        |
|    time_elapsed    | 57863     |
|    total_timesteps | 940000    |
----------------------------------
Eval num_timesteps=950000, episode_reward=-25219.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.52e+04 |
| time/              |           |
|    total_timesteps | 950000    |
| train/             |           |
|    actor_loss      | 9.9e+03   |
|    critic_loss     | 3.7e+05   |
|    ent_coef        | 4.8       |
|    ent_coef_loss   | 0.326     |
|    learning_rate   | 0.000905  |
|    n_updates       | 951257    |
----------------------------------
Eval num_timesteps=960000, episode_reward=-26838.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.68e+04 |
| time/              |           |
|    total_timesteps | 960000    |
| train/             |           |
|    actor_loss      | 1.55e+04  |
|    critic_loss     | 2.25e+05  |
|    ent_coef        | 4.68      |
|    ent_coef_loss   | -0.176    |
|    learning_rate   | 0.000904  |
|    n_updates       | 961257    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -6.03e+04 |
| time/              |           |
|    episodes        | 192       |
|    fps             | 16        |
|    time_elapsed    | 59132     |
|    total_timesteps | 960000    |
----------------------------------
Eval num_timesteps=970000, episode_reward=-30425.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.04e+04 |
| time/              |           |
|    total_timesteps | 970000    |
| train/             |           |
|    actor_loss      | 1.46e+04  |
|    critic_loss     | 2.32e+05  |
|    ent_coef        | 5.17      |
|    ent_coef_loss   | -0.241    |
|    learning_rate   | 0.000903  |
|    n_updates       | 971257    |
----------------------------------
Eval num_timesteps=980000, episode_reward=-25003.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.5e+04 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | 1.08e+04 |
|    critic_loss     | 3.33e+05 |
|    ent_coef        | 4.87     |
|    ent_coef_loss   | 0.0372   |
|    learning_rate   | 0.000902 |
|    n_updates       | 981257   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.94e+04 |
| time/              |           |
|    episodes        | 196       |
|    fps             | 16        |
|    time_elapsed    | 60414     |
|    total_timesteps | 980000    |
----------------------------------
Eval num_timesteps=990000, episode_reward=-11105.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+04 |
| time/              |           |
|    total_timesteps | 990000    |
| train/             |           |
|    actor_loss      | 1.34e+04  |
|    critic_loss     | 2.95e+05  |
|    ent_coef        | 4.75      |
|    ent_coef_loss   | -0.0837   |
|    learning_rate   | 0.000901  |
|    n_updates       | 991257    |
----------------------------------
Eval num_timesteps=1000000, episode_reward=-12789.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.28e+04 |
| time/              |           |
|    total_timesteps | 1000000   |
| train/             |           |
|    actor_loss      | 1.05e+04  |
|    critic_loss     | 1.67e+05  |
|    ent_coef        | 4.99      |
|    ent_coef_loss   | -0.188    |
|    learning_rate   | 0.0009    |
|    n_updates       | 1001257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.88e+04 |
| time/              |           |
|    episodes        | 200       |
|    fps             | 16        |
|    time_elapsed    | 61675     |
|    total_timesteps | 1000000   |
----------------------------------
Eval num_timesteps=1010000, episode_reward=-21772.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.18e+04 |
| time/              |           |
|    total_timesteps | 1010000   |
| train/             |           |
|    actor_loss      | 4.76e+03  |
|    critic_loss     | 5.63e+04  |
|    ent_coef        | 4.66      |
|    ent_coef_loss   | 0.514     |
|    learning_rate   | 0.000899  |
|    n_updates       | 1011257   |
----------------------------------
Eval num_timesteps=1020000, episode_reward=-63905.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.39e+04 |
| time/              |           |
|    total_timesteps | 1020000   |
| train/             |           |
|    actor_loss      | 4.71e+03  |
|    critic_loss     | 5.24e+04  |
|    ent_coef        | 4.34      |
|    ent_coef_loss   | -0.434    |
|    learning_rate   | 0.000898  |
|    n_updates       | 1021257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.89e+04 |
| time/              |           |
|    episodes        | 204       |
|    fps             | 16        |
|    time_elapsed    | 62940     |
|    total_timesteps | 1020000   |
----------------------------------
Eval num_timesteps=1030000, episode_reward=-17716.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+04 |
| time/              |           |
|    total_timesteps | 1030000   |
| train/             |           |
|    actor_loss      | 4.31e+03  |
|    critic_loss     | 5.47e+04  |
|    ent_coef        | 3.87      |
|    ent_coef_loss   | -0.618    |
|    learning_rate   | 0.000897  |
|    n_updates       | 1031257   |
----------------------------------
Eval num_timesteps=1040000, episode_reward=-20030.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2e+04   |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | 4.13e+03 |
|    critic_loss     | 4.55e+04 |
|    ent_coef        | 3.71     |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.000896 |
|    n_updates       | 1041257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.51e+04 |
| time/              |           |
|    episodes        | 208       |
|    fps             | 16        |
|    time_elapsed    | 64207     |
|    total_timesteps | 1040000   |
----------------------------------
Eval num_timesteps=1050000, episode_reward=-13415.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.34e+04 |
| time/              |           |
|    total_timesteps | 1050000   |
| train/             |           |
|    actor_loss      | 3.98e+03  |
|    critic_loss     | 4.3e+04   |
|    ent_coef        | 3.27      |
|    ent_coef_loss   | 0.65      |
|    learning_rate   | 0.000895  |
|    n_updates       | 1051257   |
----------------------------------
Eval num_timesteps=1060000, episode_reward=-9757.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.76e+03 |
| time/              |           |
|    total_timesteps | 1060000   |
| train/             |           |
|    actor_loss      | 3.89e+03  |
|    critic_loss     | 6.95e+04  |
|    ent_coef        | 3.13      |
|    ent_coef_loss   | -0.0342   |
|    learning_rate   | 0.000894  |
|    n_updates       | 1061257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.42e+04 |
| time/              |           |
|    episodes        | 212       |
|    fps             | 16        |
|    time_elapsed    | 65479     |
|    total_timesteps | 1060000   |
----------------------------------
Eval num_timesteps=1070000, episode_reward=-10618.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.06e+04 |
| time/              |           |
|    total_timesteps | 1070000   |
| train/             |           |
|    actor_loss      | 3.72e+03  |
|    critic_loss     | 3.55e+04  |
|    ent_coef        | 3.43      |
|    ent_coef_loss   | -0.101    |
|    learning_rate   | 0.000893  |
|    n_updates       | 1071257   |
----------------------------------
Eval num_timesteps=1080000, episode_reward=-12033.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+04 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | 3.68e+03 |
|    critic_loss     | 8.31e+04 |
|    ent_coef        | 2.91     |
|    ent_coef_loss   | -0.0026  |
|    learning_rate   | 0.000892 |
|    n_updates       | 1081257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.29e+04 |
| time/              |           |
|    episodes        | 216       |
|    fps             | 16        |
|    time_elapsed    | 66763     |
|    total_timesteps | 1080000   |
----------------------------------
Eval num_timesteps=1090000, episode_reward=-15092.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.51e+04 |
| time/              |           |
|    total_timesteps | 1090000   |
| train/             |           |
|    actor_loss      | 3.46e+03  |
|    critic_loss     | 4.15e+04  |
|    ent_coef        | 3.21      |
|    ent_coef_loss   | -0.00688  |
|    learning_rate   | 0.000891  |
|    n_updates       | 1091257   |
----------------------------------
Eval num_timesteps=1100000, episode_reward=-16888.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.69e+04 |
| time/              |           |
|    total_timesteps | 1100000   |
| train/             |           |
|    actor_loss      | 3.35e+03  |
|    critic_loss     | 5.3e+04   |
|    ent_coef        | 3.02      |
|    ent_coef_loss   | 0.438     |
|    learning_rate   | 0.00089   |
|    n_updates       | 1101257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.31e+04 |
| time/              |           |
|    episodes        | 220       |
|    fps             | 16        |
|    time_elapsed    | 68052     |
|    total_timesteps | 1100000   |
----------------------------------
Eval num_timesteps=1110000, episode_reward=-14688.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+04 |
| time/              |           |
|    total_timesteps | 1110000   |
| train/             |           |
|    actor_loss      | 3.22e+03  |
|    critic_loss     | 8.28e+04  |
|    ent_coef        | 2.77      |
|    ent_coef_loss   | -0.541    |
|    learning_rate   | 0.000889  |
|    n_updates       | 1111257   |
----------------------------------
Eval num_timesteps=1120000, episode_reward=-9830.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.83e+03 |
| time/              |           |
|    total_timesteps | 1120000   |
| train/             |           |
|    actor_loss      | 3.06e+03  |
|    critic_loss     | 4.54e+05  |
|    ent_coef        | 3.11      |
|    ent_coef_loss   | 0.0143    |
|    learning_rate   | 0.000888  |
|    n_updates       | 1121257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.02e+04 |
| time/              |           |
|    episodes        | 224       |
|    fps             | 16        |
|    time_elapsed    | 69347     |
|    total_timesteps | 1120000   |
----------------------------------
Eval num_timesteps=1130000, episode_reward=-56021.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -5.6e+04 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | 3.29e+03 |
|    critic_loss     | 1.77e+05 |
|    ent_coef        | 2.58     |
|    ent_coef_loss   | -0.293   |
|    learning_rate   | 0.000887 |
|    n_updates       | 1131257  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=-164958.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.65e+05 |
| time/              |           |
|    total_timesteps | 1140000   |
| train/             |           |
|    actor_loss      | 3.26e+03  |
|    critic_loss     | 8.6e+04   |
|    ent_coef        | 2.7       |
|    ent_coef_loss   | -0.366    |
|    learning_rate   | 0.000886  |
|    n_updates       | 1141257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.3e+04 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 16       |
|    time_elapsed    | 70639    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=-14308.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 1150000   |
| train/             |           |
|    actor_loss      | 3.33e+03  |
|    critic_loss     | 3.71e+05  |
|    ent_coef        | 2.74      |
|    ent_coef_loss   | 0.0269    |
|    learning_rate   | 0.000885  |
|    n_updates       | 1151257   |
----------------------------------
Eval num_timesteps=1160000, episode_reward=-11566.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.16e+04 |
| time/              |           |
|    total_timesteps | 1160000   |
| train/             |           |
|    actor_loss      | 3.38e+03  |
|    critic_loss     | 1.55e+05  |
|    ent_coef        | 2.42      |
|    ent_coef_loss   | 0.124     |
|    learning_rate   | 0.000884  |
|    n_updates       | 1161257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.36e+04 |
| time/              |           |
|    episodes        | 232       |
|    fps             | 16        |
|    time_elapsed    | 71925     |
|    total_timesteps | 1160000   |
----------------------------------
Eval num_timesteps=1170000, episode_reward=-20174.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.02e+04 |
| time/              |           |
|    total_timesteps | 1170000   |
| train/             |           |
|    actor_loss      | 3.16e+03  |
|    critic_loss     | 2.69e+04  |
|    ent_coef        | 2.31      |
|    ent_coef_loss   | 0.115     |
|    learning_rate   | 0.000883  |
|    n_updates       | 1171257   |
----------------------------------
Eval num_timesteps=1180000, episode_reward=-15020.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | 3.43e+03 |
|    critic_loss     | 4.17e+04 |
|    ent_coef        | 2.36     |
|    ent_coef_loss   | -0.268   |
|    learning_rate   | 0.000882 |
|    n_updates       | 1181257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.35e+04 |
| time/              |           |
|    episodes        | 236       |
|    fps             | 16        |
|    time_elapsed    | 73205     |
|    total_timesteps | 1180000   |
----------------------------------
Eval num_timesteps=1190000, episode_reward=-14777.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+04 |
| time/              |           |
|    total_timesteps | 1190000   |
| train/             |           |
|    actor_loss      | 3.57e+03  |
|    critic_loss     | 7.74e+04  |
|    ent_coef        | 2.24      |
|    ent_coef_loss   | 0.0162    |
|    learning_rate   | 0.000881  |
|    n_updates       | 1191257   |
----------------------------------
Eval num_timesteps=1200000, episode_reward=-4910.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1200000   |
| train/             |           |
|    actor_loss      | 3.22e+03  |
|    critic_loss     | 4.17e+04  |
|    ent_coef        | 2.29      |
|    ent_coef_loss   | -0.109    |
|    learning_rate   | 0.00088   |
|    n_updates       | 1201257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.09e+04 |
| time/              |           |
|    episodes        | 240       |
|    fps             | 16        |
|    time_elapsed    | 74482     |
|    total_timesteps | 1200000   |
----------------------------------
Eval num_timesteps=1210000, episode_reward=-15562.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+04 |
| time/              |           |
|    total_timesteps | 1210000   |
| train/             |           |
|    actor_loss      | 3.09e+03  |
|    critic_loss     | 1.42e+05  |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | -0.391    |
|    learning_rate   | 0.000879  |
|    n_updates       | 1211257   |
----------------------------------
Eval num_timesteps=1220000, episode_reward=-15864.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+04 |
| time/              |           |
|    total_timesteps | 1220000   |
| train/             |           |
|    actor_loss      | 3.13e+03  |
|    critic_loss     | 3.53e+04  |
|    ent_coef        | 2.56      |
|    ent_coef_loss   | -0.102    |
|    learning_rate   | 0.000878  |
|    n_updates       | 1221257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.96e+04 |
| time/              |           |
|    episodes        | 244       |
|    fps             | 16        |
|    time_elapsed    | 75761     |
|    total_timesteps | 1220000   |
----------------------------------
Eval num_timesteps=1230000, episode_reward=-13763.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.38e+04 |
| time/              |           |
|    total_timesteps | 1230000   |
| train/             |           |
|    actor_loss      | 2.72e+03  |
|    critic_loss     | 3.03e+04  |
|    ent_coef        | 2.04      |
|    ent_coef_loss   | 0.0431    |
|    learning_rate   | 0.000877  |
|    n_updates       | 1231257   |
----------------------------------
Eval num_timesteps=1240000, episode_reward=-109418.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.09e+05 |
| time/              |           |
|    total_timesteps | 1240000   |
| train/             |           |
|    actor_loss      | 2.79e+03  |
|    critic_loss     | 1.94e+05  |
|    ent_coef        | 2.48      |
|    ent_coef_loss   | -0.136    |
|    learning_rate   | 0.000876  |
|    n_updates       | 1241257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.78e+04 |
| time/              |           |
|    episodes        | 248       |
|    fps             | 16        |
|    time_elapsed    | 76991     |
|    total_timesteps | 1240000   |
----------------------------------
Eval num_timesteps=1250000, episode_reward=-23614.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.36e+04 |
| time/              |           |
|    total_timesteps | 1250000   |
| train/             |           |
|    actor_loss      | 2.76e+03  |
|    critic_loss     | 2.96e+04  |
|    ent_coef        | 2.28      |
|    ent_coef_loss   | 0.0033    |
|    learning_rate   | 0.000875  |
|    n_updates       | 1251257   |
----------------------------------
Eval num_timesteps=1260000, episode_reward=-14901.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+04 |
| time/              |           |
|    total_timesteps | 1260000   |
| train/             |           |
|    actor_loss      | 2.61e+03  |
|    critic_loss     | 1.09e+05  |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | 0.00523   |
|    learning_rate   | 0.000874  |
|    n_updates       | 1261257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.64e+04 |
| time/              |           |
|    episodes        | 252       |
|    fps             | 16        |
|    time_elapsed    | 78207     |
|    total_timesteps | 1260000   |
----------------------------------
Eval num_timesteps=1270000, episode_reward=-19198.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.92e+04 |
| time/              |           |
|    total_timesteps | 1270000   |
| train/             |           |
|    actor_loss      | 2.71e+03  |
|    critic_loss     | 4.82e+04  |
|    ent_coef        | 2.43      |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 0.000873  |
|    n_updates       | 1271257   |
----------------------------------
Eval num_timesteps=1280000, episode_reward=-21354.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.14e+04 |
| time/              |           |
|    total_timesteps | 1280000   |
| train/             |           |
|    actor_loss      | 2.76e+03  |
|    critic_loss     | 8.4e+04   |
|    ent_coef        | 2.54      |
|    ent_coef_loss   | -0.00128  |
|    learning_rate   | 0.000872  |
|    n_updates       | 1281257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.61e+04 |
| time/              |           |
|    episodes        | 256       |
|    fps             | 16        |
|    time_elapsed    | 79437     |
|    total_timesteps | 1280000   |
----------------------------------
Eval num_timesteps=1290000, episode_reward=-15387.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+04 |
| time/              |           |
|    total_timesteps | 1290000   |
| train/             |           |
|    actor_loss      | 2.88e+03  |
|    critic_loss     | 3.05e+04  |
|    ent_coef        | 2.79      |
|    ent_coef_loss   | 0.0274    |
|    learning_rate   | 0.000871  |
|    n_updates       | 1291257   |
----------------------------------
Eval num_timesteps=1300000, episode_reward=-23752.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.38e+04 |
| time/              |           |
|    total_timesteps | 1300000   |
| train/             |           |
|    actor_loss      | 2.94e+03  |
|    critic_loss     | 2.42e+05  |
|    ent_coef        | 2.48      |
|    ent_coef_loss   | 0.183     |
|    learning_rate   | 0.00087   |
|    n_updates       | 1301257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.24e+04 |
| time/              |           |
|    episodes        | 260       |
|    fps             | 16        |
|    time_elapsed    | 80674     |
|    total_timesteps | 1300000   |
----------------------------------
Eval num_timesteps=1310000, episode_reward=-19057.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.91e+04 |
| time/              |           |
|    total_timesteps | 1310000   |
| train/             |           |
|    actor_loss      | 2.77e+03  |
|    critic_loss     | 2.23e+04  |
|    ent_coef        | 2.16      |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 0.000869  |
|    n_updates       | 1311257   |
----------------------------------
Eval num_timesteps=1320000, episode_reward=-13235.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.32e+04 |
| time/              |           |
|    total_timesteps | 1320000   |
| train/             |           |
|    actor_loss      | 2.77e+03  |
|    critic_loss     | 2.04e+04  |
|    ent_coef        | 2.07      |
|    ent_coef_loss   | 0.351     |
|    learning_rate   | 0.000868  |
|    n_updates       | 1321257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.79e+04 |
| time/              |           |
|    episodes        | 264       |
|    fps             | 16        |
|    time_elapsed    | 81914     |
|    total_timesteps | 1320000   |
----------------------------------
Eval num_timesteps=1330000, episode_reward=-14201.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+04 |
| time/              |           |
|    total_timesteps | 1330000   |
| train/             |           |
|    actor_loss      | 2.73e+03  |
|    critic_loss     | 1.29e+05  |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | -0.00293  |
|    learning_rate   | 0.000867  |
|    n_updates       | 1331257   |
----------------------------------
Eval num_timesteps=1340000, episode_reward=-14493.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.45e+04 |
| time/              |           |
|    total_timesteps | 1340000   |
| train/             |           |
|    actor_loss      | 2.82e+03  |
|    critic_loss     | 1.29e+05  |
|    ent_coef        | 1.96      |
|    ent_coef_loss   | 0.185     |
|    learning_rate   | 0.000866  |
|    n_updates       | 1341257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.73e+04 |
| time/              |           |
|    episodes        | 268       |
|    fps             | 16        |
|    time_elapsed    | 83156     |
|    total_timesteps | 1340000   |
----------------------------------
Eval num_timesteps=1350000, episode_reward=-16441.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.64e+04 |
| time/              |           |
|    total_timesteps | 1350000   |
| train/             |           |
|    actor_loss      | 2.7e+03   |
|    critic_loss     | 4.2e+04   |
|    ent_coef        | 1.83      |
|    ent_coef_loss   | 0.0304    |
|    learning_rate   | 0.000865  |
|    n_updates       | 1351257   |
----------------------------------
Eval num_timesteps=1360000, episode_reward=-14043.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.4e+04 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | 2.62e+03 |
|    critic_loss     | 1.52e+04 |
|    ent_coef        | 1.79     |
|    ent_coef_loss   | -0.00156 |
|    learning_rate   | 0.000864 |
|    n_updates       | 1361257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.75e+04 |
| time/              |           |
|    episodes        | 272       |
|    fps             | 16        |
|    time_elapsed    | 84399     |
|    total_timesteps | 1360000   |
----------------------------------
Eval num_timesteps=1370000, episode_reward=-15882.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+04 |
| time/              |           |
|    total_timesteps | 1370000   |
| train/             |           |
|    actor_loss      | 2.66e+03  |
|    critic_loss     | 2.54e+04  |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.152     |
|    learning_rate   | 0.000863  |
|    n_updates       | 1371257   |
----------------------------------
Eval num_timesteps=1380000, episode_reward=-10137.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+04 |
| time/              |           |
|    total_timesteps | 1380000   |
| train/             |           |
|    actor_loss      | 2.69e+03  |
|    critic_loss     | 1.41e+05  |
|    ent_coef        | 2.07      |
|    ent_coef_loss   | -0.724    |
|    learning_rate   | 0.000862  |
|    n_updates       | 1381257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.75e+04 |
| time/              |           |
|    episodes        | 276       |
|    fps             | 16        |
|    time_elapsed    | 85635     |
|    total_timesteps | 1380000   |
----------------------------------
Eval num_timesteps=1390000, episode_reward=-19439.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.94e+04 |
| time/              |           |
|    total_timesteps | 1390000   |
| train/             |           |
|    actor_loss      | 2.62e+03  |
|    critic_loss     | 2.13e+04  |
|    ent_coef        | 1.93      |
|    ent_coef_loss   | 0.0594    |
|    learning_rate   | 0.000861  |
|    n_updates       | 1391257   |
----------------------------------
Eval num_timesteps=1400000, episode_reward=-989080.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.89e+05 |
| time/              |           |
|    total_timesteps | 1400000   |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 8.5e+04   |
|    ent_coef        | 2.1       |
|    ent_coef_loss   | -0.617    |
|    learning_rate   | 0.00086   |
|    n_updates       | 1401257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.57e+04 |
| time/              |           |
|    episodes        | 280       |
|    fps             | 16        |
|    time_elapsed    | 86860     |
|    total_timesteps | 1400000   |
----------------------------------
Eval num_timesteps=1410000, episode_reward=-18950.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.9e+04 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | 2.41e+03 |
|    critic_loss     | 3.06e+04 |
|    ent_coef        | 2.29     |
|    ent_coef_loss   | -0.0915  |
|    learning_rate   | 0.000859 |
|    n_updates       | 1411257  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=-15762.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.58e+04 |
| time/              |           |
|    total_timesteps | 1420000   |
| train/             |           |
|    actor_loss      | 2.26e+03  |
|    critic_loss     | 3.14e+04  |
|    ent_coef        | 2.17      |
|    ent_coef_loss   | 0.0249    |
|    learning_rate   | 0.000858  |
|    n_updates       | 1421257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.51e+04 |
| time/              |           |
|    episodes        | 284       |
|    fps             | 16        |
|    time_elapsed    | 88083     |
|    total_timesteps | 1420000   |
----------------------------------
Eval num_timesteps=1430000, episode_reward=-29414.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.94e+04 |
| time/              |           |
|    total_timesteps | 1430000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 6.65e+04  |
|    ent_coef        | 2.38      |
|    ent_coef_loss   | 0.00145   |
|    learning_rate   | 0.000857  |
|    n_updates       | 1431257   |
----------------------------------
Eval num_timesteps=1440000, episode_reward=-19052.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.91e+04 |
| time/              |           |
|    total_timesteps | 1440000   |
| train/             |           |
|    actor_loss      | 2.24e+03  |
|    critic_loss     | 2.26e+04  |
|    ent_coef        | 2.04      |
|    ent_coef_loss   | 0.196     |
|    learning_rate   | 0.000856  |
|    n_updates       | 1441257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.08e+04 |
| time/              |           |
|    episodes        | 288       |
|    fps             | 16        |
|    time_elapsed    | 89296     |
|    total_timesteps | 1440000   |
----------------------------------
Eval num_timesteps=1450000, episode_reward=-20287.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.03e+04 |
| time/              |           |
|    total_timesteps | 1450000   |
| train/             |           |
|    actor_loss      | 2.29e+03  |
|    critic_loss     | 1.61e+04  |
|    ent_coef        | 2         |
|    ent_coef_loss   | 0.239     |
|    learning_rate   | 0.000855  |
|    n_updates       | 1451257   |
----------------------------------
Eval num_timesteps=1460000, episode_reward=-16320.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+04 |
| time/              |           |
|    total_timesteps | 1460000   |
| train/             |           |
|    actor_loss      | 2.26e+03  |
|    critic_loss     | 3.44e+04  |
|    ent_coef        | 2.05      |
|    ent_coef_loss   | -2.13     |
|    learning_rate   | 0.000854  |
|    n_updates       | 1461257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+04 |
| time/              |           |
|    episodes        | 292       |
|    fps             | 16        |
|    time_elapsed    | 90505     |
|    total_timesteps | 1460000   |
----------------------------------
Eval num_timesteps=1470000, episode_reward=-18063.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.81e+04 |
| time/              |           |
|    total_timesteps | 1470000   |
| train/             |           |
|    actor_loss      | 2.06e+03  |
|    critic_loss     | 1.86e+04  |
|    ent_coef        | 1.83      |
|    ent_coef_loss   | 0.239     |
|    learning_rate   | 0.000853  |
|    n_updates       | 1471257   |
----------------------------------
Eval num_timesteps=1480000, episode_reward=-21868.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.19e+04 |
| time/              |           |
|    total_timesteps | 1480000   |
| train/             |           |
|    actor_loss      | 2.05e+03  |
|    critic_loss     | 1.14e+04  |
|    ent_coef        | 2.15      |
|    ent_coef_loss   | 0.175     |
|    learning_rate   | 0.000852  |
|    n_updates       | 1481257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.78e+04 |
| time/              |           |
|    episodes        | 296       |
|    fps             | 16        |
|    time_elapsed    | 91747     |
|    total_timesteps | 1480000   |
----------------------------------
Eval num_timesteps=1490000, episode_reward=-22930.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.29e+04 |
| time/              |           |
|    total_timesteps | 1490000   |
| train/             |           |
|    actor_loss      | 2.16e+03  |
|    critic_loss     | 2.65e+04  |
|    ent_coef        | 1.88      |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.000851  |
|    n_updates       | 1491257   |
----------------------------------
Eval num_timesteps=1500000, episode_reward=-6095.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -6.1e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | 2.06e+03 |
|    critic_loss     | 2.82e+04 |
|    ent_coef        | 2.17     |
|    ent_coef_loss   | 0.142    |
|    learning_rate   | 0.00085  |
|    n_updates       | 1501257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.76e+04 |
| time/              |           |
|    episodes        | 300       |
|    fps             | 16        |
|    time_elapsed    | 92983     |
|    total_timesteps | 1500000   |
----------------------------------
Eval num_timesteps=1510000, episode_reward=-14401.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+04 |
| time/              |           |
|    total_timesteps | 1510000   |
| train/             |           |
|    actor_loss      | 2.36e+03  |
|    critic_loss     | 1.37e+04  |
|    ent_coef        | 2.14      |
|    ent_coef_loss   | -0.056    |
|    learning_rate   | 0.000849  |
|    n_updates       | 1511257   |
----------------------------------
Eval num_timesteps=1520000, episode_reward=-16229.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+04 |
| time/              |           |
|    total_timesteps | 1520000   |
| train/             |           |
|    actor_loss      | 2.2e+03   |
|    critic_loss     | 1.29e+04  |
|    ent_coef        | 2.1       |
|    ent_coef_loss   | 0.101     |
|    learning_rate   | 0.000848  |
|    n_updates       | 1521257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.72e+04 |
| time/              |           |
|    episodes        | 304       |
|    fps             | 16        |
|    time_elapsed    | 94215     |
|    total_timesteps | 1520000   |
----------------------------------
Eval num_timesteps=1530000, episode_reward=-14478.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.45e+04 |
| time/              |           |
|    total_timesteps | 1530000   |
| train/             |           |
|    actor_loss      | 2.09e+03  |
|    critic_loss     | 1.08e+04  |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | 0.00163   |
|    learning_rate   | 0.000847  |
|    n_updates       | 1531257   |
----------------------------------
Eval num_timesteps=1540000, episode_reward=-12103.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.21e+04 |
| time/              |           |
|    total_timesteps | 1540000   |
| train/             |           |
|    actor_loss      | 2.07e+03  |
|    critic_loss     | 9.03e+03  |
|    ent_coef        | 1.8       |
|    ent_coef_loss   | 0.247     |
|    learning_rate   | 0.000846  |
|    n_updates       | 1541257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.71e+04 |
| time/              |           |
|    episodes        | 308       |
|    fps             | 16        |
|    time_elapsed    | 95452     |
|    total_timesteps | 1540000   |
----------------------------------
Eval num_timesteps=1550000, episode_reward=-16132.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 1550000   |
| train/             |           |
|    actor_loss      | 1.99e+03  |
|    critic_loss     | 9.46e+03  |
|    ent_coef        | 1.79      |
|    ent_coef_loss   | -0.0456   |
|    learning_rate   | 0.000845  |
|    n_updates       | 1551257   |
----------------------------------
Eval num_timesteps=1560000, episode_reward=-10732.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+04 |
| time/              |           |
|    total_timesteps | 1560000   |
| train/             |           |
|    actor_loss      | 2.13e+03  |
|    critic_loss     | 1.5e+04   |
|    ent_coef        | 1.51      |
|    ent_coef_loss   | 0.00242   |
|    learning_rate   | 0.000844  |
|    n_updates       | 1561257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.69e+04 |
| time/              |           |
|    episodes        | 312       |
|    fps             | 16        |
|    time_elapsed    | 96694     |
|    total_timesteps | 1560000   |
----------------------------------
Eval num_timesteps=1570000, episode_reward=-19715.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.97e+04 |
| time/              |           |
|    total_timesteps | 1570000   |
| train/             |           |
|    actor_loss      | 2.15e+03  |
|    critic_loss     | 1.24e+04  |
|    ent_coef        | 1.86      |
|    ent_coef_loss   | 0.354     |
|    learning_rate   | 0.000843  |
|    n_updates       | 1571257   |
----------------------------------
Eval num_timesteps=1580000, episode_reward=-19154.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.92e+04 |
| time/              |           |
|    total_timesteps | 1580000   |
| train/             |           |
|    actor_loss      | 2.05e+03  |
|    critic_loss     | 1.09e+04  |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | 0.207     |
|    learning_rate   | 0.000842  |
|    n_updates       | 1581257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.71e+04 |
| time/              |           |
|    episodes        | 316       |
|    fps             | 16        |
|    time_elapsed    | 97932     |
|    total_timesteps | 1580000   |
----------------------------------
Eval num_timesteps=1590000, episode_reward=-12415.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.24e+04 |
| time/              |           |
|    total_timesteps | 1590000   |
| train/             |           |
|    actor_loss      | 2.07e+03  |
|    critic_loss     | 1.01e+04  |
|    ent_coef        | 1.39      |
|    ent_coef_loss   | 0.0712    |
|    learning_rate   | 0.000841  |
|    n_updates       | 1591257   |
----------------------------------
Eval num_timesteps=1600000, episode_reward=-18114.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.81e+04 |
| time/              |           |
|    total_timesteps | 1600000   |
| train/             |           |
|    actor_loss      | 2.15e+03  |
|    critic_loss     | 5.09e+04  |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.0616    |
|    learning_rate   | 0.00084   |
|    n_updates       | 1601257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.71e+04 |
| time/              |           |
|    episodes        | 320       |
|    fps             | 16        |
|    time_elapsed    | 99162     |
|    total_timesteps | 1600000   |
----------------------------------
Eval num_timesteps=1610000, episode_reward=-23078.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.31e+04 |
| time/              |           |
|    total_timesteps | 1610000   |
| train/             |           |
|    actor_loss      | 2.11e+03  |
|    critic_loss     | 3.63e+04  |
|    ent_coef        | 2.26      |
|    ent_coef_loss   | 0.464     |
|    learning_rate   | 0.000839  |
|    n_updates       | 1611257   |
----------------------------------
Eval num_timesteps=1620000, episode_reward=-15658.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+04 |
| time/              |           |
|    total_timesteps | 1620000   |
| train/             |           |
|    actor_loss      | 2.29e+03  |
|    critic_loss     | 2.88e+05  |
|    ent_coef        | 1.9       |
|    ent_coef_loss   | -0.19     |
|    learning_rate   | 0.000838  |
|    n_updates       | 1621257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.12e+04 |
| time/              |           |
|    episodes        | 324       |
|    fps             | 16        |
|    time_elapsed    | 100396    |
|    total_timesteps | 1620000   |
----------------------------------
Eval num_timesteps=1630000, episode_reward=-14537.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.45e+04 |
| time/              |           |
|    total_timesteps | 1630000   |
| train/             |           |
|    actor_loss      | 2.01e+03  |
|    critic_loss     | 7.22e+03  |
|    ent_coef        | 1.55      |
|    ent_coef_loss   | 0.0308    |
|    learning_rate   | 0.000837  |
|    n_updates       | 1631257   |
----------------------------------
Eval num_timesteps=1640000, episode_reward=-16970.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.7e+04 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | 2.26e+03 |
|    critic_loss     | 1.15e+04 |
|    ent_coef        | 1.65     |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.000836 |
|    n_updates       | 1641257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.11e+04 |
| time/              |           |
|    episodes        | 328       |
|    fps             | 16        |
|    time_elapsed    | 101610    |
|    total_timesteps | 1640000   |
----------------------------------
Eval num_timesteps=1650000, episode_reward=-14270.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 1650000   |
| train/             |           |
|    actor_loss      | 1.99e+03  |
|    critic_loss     | 8.62e+03  |
|    ent_coef        | 1.6       |
|    ent_coef_loss   | -0.423    |
|    learning_rate   | 0.000835  |
|    n_updates       | 1651257   |
----------------------------------
Eval num_timesteps=1660000, episode_reward=-16697.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.67e+04 |
| time/              |           |
|    total_timesteps | 1660000   |
| train/             |           |
|    actor_loss      | 1.96e+03  |
|    critic_loss     | 1.38e+04  |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.0774    |
|    learning_rate   | 0.000834  |
|    n_updates       | 1661257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.08e+04 |
| time/              |           |
|    episodes        | 332       |
|    fps             | 16        |
|    time_elapsed    | 102821    |
|    total_timesteps | 1660000   |
----------------------------------
Eval num_timesteps=1670000, episode_reward=-13831.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.38e+04 |
| time/              |           |
|    total_timesteps | 1670000   |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 1.2e+04   |
|    ent_coef        | 1.3       |
|    ent_coef_loss   | 0.0889    |
|    learning_rate   | 0.000833  |
|    n_updates       | 1671257   |
----------------------------------
Eval num_timesteps=1680000, episode_reward=-16178.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+04 |
| time/              |           |
|    total_timesteps | 1680000   |
| train/             |           |
|    actor_loss      | 1.89e+03  |
|    critic_loss     | 1.15e+04  |
|    ent_coef        | 1.13      |
|    ent_coef_loss   | -0.0237   |
|    learning_rate   | 0.000832  |
|    n_updates       | 1681257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.08e+04 |
| time/              |           |
|    episodes        | 336       |
|    fps             | 16        |
|    time_elapsed    | 104041    |
|    total_timesteps | 1680000   |
----------------------------------
Eval num_timesteps=1690000, episode_reward=-20439.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.04e+04 |
| time/              |           |
|    total_timesteps | 1690000   |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 8.96e+03  |
|    ent_coef        | 1.4       |
|    ent_coef_loss   | -0.0382   |
|    learning_rate   | 0.000831  |
|    n_updates       | 1691257   |
----------------------------------
Eval num_timesteps=1700000, episode_reward=-14777.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+04 |
| time/              |           |
|    total_timesteps | 1700000   |
| train/             |           |
|    actor_loss      | 1.86e+03  |
|    critic_loss     | 8.67e+03  |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | 0.0159    |
|    learning_rate   | 0.00083   |
|    n_updates       | 1701257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.1e+04 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 16       |
|    time_elapsed    | 105255   |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=-13511.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.35e+04 |
| time/              |           |
|    total_timesteps | 1710000   |
| train/             |           |
|    actor_loss      | 2.28e+03  |
|    critic_loss     | 3.47e+04  |
|    ent_coef        | 1.11      |
|    ent_coef_loss   | -0.00727  |
|    learning_rate   | 0.000829  |
|    n_updates       | 1711257   |
----------------------------------
Eval num_timesteps=1720000, episode_reward=-20398.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.04e+04 |
| time/              |           |
|    total_timesteps | 1720000   |
| train/             |           |
|    actor_loss      | 1.65e+03  |
|    critic_loss     | 1.73e+04  |
|    ent_coef        | 1.89      |
|    ent_coef_loss   | 0.0934    |
|    learning_rate   | 0.000828  |
|    n_updates       | 1721257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.1e+04 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 16       |
|    time_elapsed    | 106467   |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=-13557.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.36e+04 |
| time/              |           |
|    total_timesteps | 1730000   |
| train/             |           |
|    actor_loss      | 1.57e+03  |
|    critic_loss     | 9.82e+03  |
|    ent_coef        | 1.54      |
|    ent_coef_loss   | -0.0353   |
|    learning_rate   | 0.000827  |
|    n_updates       | 1731257   |
----------------------------------
Eval num_timesteps=1740000, episode_reward=-25463.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.55e+04 |
| time/              |           |
|    total_timesteps | 1740000   |
| train/             |           |
|    actor_loss      | 1.47e+03  |
|    critic_loss     | 3.57e+04  |
|    ent_coef        | 1.56      |
|    ent_coef_loss   | 0.104     |
|    learning_rate   | 0.000826  |
|    n_updates       | 1741257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.15e+04 |
| time/              |           |
|    episodes        | 348       |
|    fps             | 16        |
|    time_elapsed    | 107691    |
|    total_timesteps | 1740000   |
----------------------------------
Eval num_timesteps=1750000, episode_reward=-18622.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.86e+04 |
| time/              |           |
|    total_timesteps | 1750000   |
| train/             |           |
|    actor_loss      | 1.59e+03  |
|    critic_loss     | 1.84e+04  |
|    ent_coef        | 1.3       |
|    ent_coef_loss   | 0.0387    |
|    learning_rate   | 0.000825  |
|    n_updates       | 1751257   |
----------------------------------
Eval num_timesteps=1760000, episode_reward=-15878.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+04 |
| time/              |           |
|    total_timesteps | 1760000   |
| train/             |           |
|    actor_loss      | 1.71e+03  |
|    critic_loss     | 1.97e+04  |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.00507   |
|    learning_rate   | 0.000824  |
|    n_updates       | 1761257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.15e+04 |
| time/              |           |
|    episodes        | 352       |
|    fps             | 16        |
|    time_elapsed    | 108942    |
|    total_timesteps | 1760000   |
----------------------------------
Eval num_timesteps=1770000, episode_reward=-32887.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.29e+04 |
| time/              |           |
|    total_timesteps | 1770000   |
| train/             |           |
|    actor_loss      | 1.79e+03  |
|    critic_loss     | 5.16e+04  |
|    ent_coef        | 1.39      |
|    ent_coef_loss   | -0.23     |
|    learning_rate   | 0.000823  |
|    n_updates       | 1771257   |
----------------------------------
Eval num_timesteps=1780000, episode_reward=-20045.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2e+04   |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | 1.59e+03 |
|    critic_loss     | 9.34e+03 |
|    ent_coef        | 1.42     |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.000822 |
|    n_updates       | 1781257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.15e+04 |
| time/              |           |
|    episodes        | 356       |
|    fps             | 16        |
|    time_elapsed    | 110174    |
|    total_timesteps | 1780000   |
----------------------------------
Eval num_timesteps=1790000, episode_reward=-14318.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 1790000   |
| train/             |           |
|    actor_loss      | 1.68e+03  |
|    critic_loss     | 8.05e+03  |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.00518   |
|    learning_rate   | 0.000821  |
|    n_updates       | 1791257   |
----------------------------------
Eval num_timesteps=1800000, episode_reward=-16629.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.66e+04 |
| time/              |           |
|    total_timesteps | 1800000   |
| train/             |           |
|    actor_loss      | 1.68e+03  |
|    critic_loss     | 8.33e+03  |
|    ent_coef        | 0.941     |
|    ent_coef_loss   | 0.0166    |
|    learning_rate   | 0.00082   |
|    n_updates       | 1801257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.12e+04 |
| time/              |           |
|    episodes        | 360       |
|    fps             | 16        |
|    time_elapsed    | 111437    |
|    total_timesteps | 1800000   |
----------------------------------
Eval num_timesteps=1810000, episode_reward=-17982.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.8e+04 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | 1.62e+03 |
|    critic_loss     | 5.27e+03 |
|    ent_coef        | 0.851    |
|    ent_coef_loss   | 0.00888  |
|    learning_rate   | 0.000819 |
|    n_updates       | 1811257  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=-17689.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+04 |
| time/              |           |
|    total_timesteps | 1820000   |
| train/             |           |
|    actor_loss      | 1.61e+03  |
|    critic_loss     | 6.79e+03  |
|    ent_coef        | 0.915     |
|    ent_coef_loss   | -0.00804  |
|    learning_rate   | 0.000818  |
|    n_updates       | 1821257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.11e+04 |
| time/              |           |
|    episodes        | 364       |
|    fps             | 16        |
|    time_elapsed    | 112748    |
|    total_timesteps | 1820000   |
----------------------------------
Eval num_timesteps=1830000, episode_reward=-14986.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | 1.52e+03 |
|    critic_loss     | 9.05e+03 |
|    ent_coef        | 1.39     |
|    ent_coef_loss   | 0.0187   |
|    learning_rate   | 0.000817 |
|    n_updates       | 1831257  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=-11727.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.17e+04 |
| time/              |           |
|    total_timesteps | 1840000   |
| train/             |           |
|    actor_loss      | 1.51e+03  |
|    critic_loss     | 1.66e+04  |
|    ent_coef        | 1.62      |
|    ent_coef_loss   | 0.173     |
|    learning_rate   | 0.000816  |
|    n_updates       | 1841257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.24e+04 |
| time/              |           |
|    episodes        | 368       |
|    fps             | 16        |
|    time_elapsed    | 114023    |
|    total_timesteps | 1840000   |
----------------------------------
Eval num_timesteps=1850000, episode_reward=-19431.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.94e+04 |
| time/              |           |
|    total_timesteps | 1850000   |
| train/             |           |
|    actor_loss      | 1.62e+03  |
|    critic_loss     | 1.78e+04  |
|    ent_coef        | 1.05      |
|    ent_coef_loss   | -0.00341  |
|    learning_rate   | 0.000815  |
|    n_updates       | 1851257   |
----------------------------------
Eval num_timesteps=1860000, episode_reward=-18442.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.84e+04 |
| time/              |           |
|    total_timesteps | 1860000   |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 8.27e+03  |
|    ent_coef        | 0.874     |
|    ent_coef_loss   | -0.0236   |
|    learning_rate   | 0.000814  |
|    n_updates       | 1861257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.35e+04 |
| time/              |           |
|    episodes        | 372       |
|    fps             | 16        |
|    time_elapsed    | 115304    |
|    total_timesteps | 1860000   |
----------------------------------
Eval num_timesteps=1870000, episode_reward=-14074.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.41e+04 |
| time/              |           |
|    total_timesteps | 1870000   |
| train/             |           |
|    actor_loss      | 1.68e+03  |
|    critic_loss     | 6.21e+03  |
|    ent_coef        | 0.95      |
|    ent_coef_loss   | -0.0087   |
|    learning_rate   | 0.000813  |
|    n_updates       | 1871257   |
----------------------------------
Eval num_timesteps=1880000, episode_reward=-11577.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.16e+04 |
| time/              |           |
|    total_timesteps | 1880000   |
| train/             |           |
|    actor_loss      | 1.95e+03  |
|    critic_loss     | 4.45e+04  |
|    ent_coef        | 1.33      |
|    ent_coef_loss   | -0.0075   |
|    learning_rate   | 0.000812  |
|    n_updates       | 1881257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.36e+04 |
| time/              |           |
|    episodes        | 376       |
|    fps             | 16        |
|    time_elapsed    | 116590    |
|    total_timesteps | 1880000   |
----------------------------------
Eval num_timesteps=1890000, episode_reward=-17593.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.76e+04 |
| time/              |           |
|    total_timesteps | 1890000   |
| train/             |           |
|    actor_loss      | 1.97e+03  |
|    critic_loss     | 8.25e+03  |
|    ent_coef        | 0.923     |
|    ent_coef_loss   | -0.004    |
|    learning_rate   | 0.000811  |
|    n_updates       | 1891257   |
----------------------------------
Eval num_timesteps=1900000, episode_reward=-10681.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+04 |
| time/              |           |
|    total_timesteps | 1900000   |
| train/             |           |
|    actor_loss      | 1.63e+03  |
|    critic_loss     | 7.59e+03  |
|    ent_coef        | 0.958     |
|    ent_coef_loss   | -0.0196   |
|    learning_rate   | 0.00081   |
|    n_updates       | 1901257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.36e+04 |
| time/              |           |
|    episodes        | 380       |
|    fps             | 16        |
|    time_elapsed    | 117863    |
|    total_timesteps | 1900000   |
----------------------------------
Eval num_timesteps=1910000, episode_reward=-14394.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+04 |
| time/              |           |
|    total_timesteps | 1910000   |
| train/             |           |
|    actor_loss      | 1.79e+03  |
|    critic_loss     | 5.42e+04  |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | -0.0089   |
|    learning_rate   | 0.000809  |
|    n_updates       | 1911257   |
----------------------------------
Eval num_timesteps=1920000, episode_reward=-18663.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.87e+04 |
| time/              |           |
|    total_timesteps | 1920000   |
| train/             |           |
|    actor_loss      | 1.75e+03  |
|    critic_loss     | 9.83e+03  |
|    ent_coef        | 0.803     |
|    ent_coef_loss   | -0.0844   |
|    learning_rate   | 0.000808  |
|    n_updates       | 1921257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.42e+04 |
| time/              |           |
|    episodes        | 384       |
|    fps             | 16        |
|    time_elapsed    | 119139    |
|    total_timesteps | 1920000   |
----------------------------------
Eval num_timesteps=1930000, episode_reward=-13926.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.39e+04 |
| time/              |           |
|    total_timesteps | 1930000   |
| train/             |           |
|    actor_loss      | 1.82e+03  |
|    critic_loss     | 6.13e+03  |
|    ent_coef        | 1.21      |
|    ent_coef_loss   | 0.0292    |
|    learning_rate   | 0.000807  |
|    n_updates       | 1931257   |
----------------------------------
Eval num_timesteps=1940000, episode_reward=-9149.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.15e+03 |
| time/              |           |
|    total_timesteps | 1940000   |
| train/             |           |
|    actor_loss      | 1.72e+03  |
|    critic_loss     | 5.84e+03  |
|    ent_coef        | 0.828     |
|    ent_coef_loss   | 0.094     |
|    learning_rate   | 0.000806  |
|    n_updates       | 1941257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.41e+04 |
| time/              |           |
|    episodes        | 388       |
|    fps             | 16        |
|    time_elapsed    | 120419    |
|    total_timesteps | 1940000   |
----------------------------------
Eval num_timesteps=1950000, episode_reward=-15462.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+04 |
| time/              |           |
|    total_timesteps | 1950000   |
| train/             |           |
|    actor_loss      | 1.78e+03  |
|    critic_loss     | 3.94e+03  |
|    ent_coef        | 0.809     |
|    ent_coef_loss   | 0.0212    |
|    learning_rate   | 0.000805  |
|    n_updates       | 1951257   |
----------------------------------
Eval num_timesteps=1960000, episode_reward=-17620.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.76e+04 |
| time/              |           |
|    total_timesteps | 1960000   |
| train/             |           |
|    actor_loss      | 1.74e+03  |
|    critic_loss     | 4.13e+03  |
|    ent_coef        | 0.808     |
|    ent_coef_loss   | -0.0379   |
|    learning_rate   | 0.000804  |
|    n_updates       | 1961257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.4e+04 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 16       |
|    time_elapsed    | 121699   |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=-15358.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+04 |
| time/              |           |
|    total_timesteps | 1970000   |
| train/             |           |
|    actor_loss      | 1.87e+03  |
|    critic_loss     | 1.05e+04  |
|    ent_coef        | 0.925     |
|    ent_coef_loss   | 0.0191    |
|    learning_rate   | 0.000803  |
|    n_updates       | 1971257   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=-13615.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.36e+04 |
| time/              |           |
|    total_timesteps | 1980000   |
| train/             |           |
|    actor_loss      | 1.77e+03  |
|    critic_loss     | 4.96e+04  |
|    ent_coef        | 0.826     |
|    ent_coef_loss   | -0.0238   |
|    learning_rate   | 0.000802  |
|    n_updates       | 1981257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.39e+04 |
| time/              |           |
|    episodes        | 396       |
|    fps             | 16        |
|    time_elapsed    | 122981    |
|    total_timesteps | 1980000   |
----------------------------------
Eval num_timesteps=1990000, episode_reward=-18815.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.88e+04 |
| time/              |           |
|    total_timesteps | 1990000   |
| train/             |           |
|    actor_loss      | 1.73e+03  |
|    critic_loss     | 8.88e+03  |
|    ent_coef        | 0.646     |
|    ent_coef_loss   | -0.0151   |
|    learning_rate   | 0.000801  |
|    n_updates       | 1991257   |
----------------------------------
Eval num_timesteps=2000000, episode_reward=-17833.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.78e+04 |
| time/              |           |
|    total_timesteps | 2000000   |
| train/             |           |
|    actor_loss      | 1.66e+03  |
|    critic_loss     | 4.34e+03  |
|    ent_coef        | 0.643     |
|    ent_coef_loss   | -0.0106   |
|    learning_rate   | 0.0008    |
|    n_updates       | 2001257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.39e+04 |
| time/              |           |
|    episodes        | 400       |
|    fps             | 16        |
|    time_elapsed    | 124262    |
|    total_timesteps | 2000000   |
----------------------------------
Eval num_timesteps=2010000, episode_reward=-13698.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.37e+04 |
| time/              |           |
|    total_timesteps | 2010000   |
| train/             |           |
|    actor_loss      | 1.6e+03   |
|    critic_loss     | 4.23e+03  |
|    ent_coef        | 1.25      |
|    ent_coef_loss   | 0.0799    |
|    learning_rate   | 0.000799  |
|    n_updates       | 2011257   |
----------------------------------
Eval num_timesteps=2020000, episode_reward=-20730.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+04 |
| time/              |           |
|    total_timesteps | 2020000   |
| train/             |           |
|    actor_loss      | 1.61e+03  |
|    critic_loss     | 1.44e+04  |
|    ent_coef        | 0.716     |
|    ent_coef_loss   | 0.224     |
|    learning_rate   | 0.000798  |
|    n_updates       | 2021257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.58e+04 |
| time/              |           |
|    episodes        | 404       |
|    fps             | 16        |
|    time_elapsed    | 125545    |
|    total_timesteps | 2020000   |
----------------------------------
Eval num_timesteps=2030000, episode_reward=-19352.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.94e+04 |
| time/              |           |
|    total_timesteps | 2030000   |
| train/             |           |
|    actor_loss      | 1.63e+03  |
|    critic_loss     | 4.67e+04  |
|    ent_coef        | 0.758     |
|    ent_coef_loss   | -0.094    |
|    learning_rate   | 0.000797  |
|    n_updates       | 2031257   |
----------------------------------
Eval num_timesteps=2040000, episode_reward=-12399.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.24e+04 |
| time/              |           |
|    total_timesteps | 2040000   |
| train/             |           |
|    actor_loss      | 1.48e+03  |
|    critic_loss     | 7.65e+03  |
|    ent_coef        | 0.625     |
|    ent_coef_loss   | 0.0429    |
|    learning_rate   | 0.000796  |
|    n_updates       | 2041257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.58e+04 |
| time/              |           |
|    episodes        | 408       |
|    fps             | 16        |
|    time_elapsed    | 126874    |
|    total_timesteps | 2040000   |
----------------------------------
Eval num_timesteps=2050000, episode_reward=-13729.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.37e+04 |
| time/              |           |
|    total_timesteps | 2050000   |
| train/             |           |
|    actor_loss      | 1.49e+03  |
|    critic_loss     | 3.86e+03  |
|    ent_coef        | 0.769     |
|    ent_coef_loss   | -0.0899   |
|    learning_rate   | 0.000795  |
|    n_updates       | 2051257   |
----------------------------------
Eval num_timesteps=2060000, episode_reward=-15990.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.6e+04 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | 1.57e+03 |
|    critic_loss     | 5.67e+04 |
|    ent_coef        | 0.812    |
|    ent_coef_loss   | -0.0406  |
|    learning_rate   | 0.000794 |
|    n_updates       | 2061257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.58e+04 |
| time/              |           |
|    episodes        | 412       |
|    fps             | 16        |
|    time_elapsed    | 128147    |
|    total_timesteps | 2060000   |
----------------------------------
Eval num_timesteps=2070000, episode_reward=-14091.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.41e+04 |
| time/              |           |
|    total_timesteps | 2070000   |
| train/             |           |
|    actor_loss      | 1.38e+03  |
|    critic_loss     | 4.52e+03  |
|    ent_coef        | 1.22      |
|    ent_coef_loss   | 0.0226    |
|    learning_rate   | 0.000793  |
|    n_updates       | 2071257   |
----------------------------------
Eval num_timesteps=2080000, episode_reward=-14033.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.4e+04 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | 1.33e+03 |
|    critic_loss     | 3.41e+03 |
|    ent_coef        | 0.868    |
|    ent_coef_loss   | -0.00494 |
|    learning_rate   | 0.000792 |
|    n_updates       | 2081257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.57e+04 |
| time/              |           |
|    episodes        | 416       |
|    fps             | 16        |
|    time_elapsed    | 129416    |
|    total_timesteps | 2080000   |
----------------------------------
Eval num_timesteps=2090000, episode_reward=-15504.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+04 |
| time/              |           |
|    total_timesteps | 2090000   |
| train/             |           |
|    actor_loss      | 1.32e+03  |
|    critic_loss     | 4.58e+03  |
|    ent_coef        | 0.75      |
|    ent_coef_loss   | 0.106     |
|    learning_rate   | 0.000791  |
|    n_updates       | 2091257   |
----------------------------------
Eval num_timesteps=2100000, episode_reward=-13466.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.35e+04 |
| time/              |           |
|    total_timesteps | 2100000   |
| train/             |           |
|    actor_loss      | 1.34e+03  |
|    critic_loss     | 3.96e+03  |
|    ent_coef        | 1.06      |
|    ent_coef_loss   | 0.0265    |
|    learning_rate   | 0.00079   |
|    n_updates       | 2101257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.56e+04 |
| time/              |           |
|    episodes        | 420       |
|    fps             | 16        |
|    time_elapsed    | 130691    |
|    total_timesteps | 2100000   |
----------------------------------
Eval num_timesteps=2110000, episode_reward=-15378.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+04 |
| time/              |           |
|    total_timesteps | 2110000   |
| train/             |           |
|    actor_loss      | 1.57e+03  |
|    critic_loss     | 7.22e+03  |
|    ent_coef        | 0.805     |
|    ent_coef_loss   | -0.119    |
|    learning_rate   | 0.000789  |
|    n_updates       | 2111257   |
----------------------------------
Eval num_timesteps=2120000, episode_reward=-14254.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 2120000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 3.61e+03  |
|    ent_coef        | 0.629     |
|    ent_coef_loss   | -0.0904   |
|    learning_rate   | 0.000788  |
|    n_updates       | 2121257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.11e+04 |
| time/              |           |
|    episodes        | 424       |
|    fps             | 16        |
|    time_elapsed    | 131991    |
|    total_timesteps | 2120000   |
----------------------------------
Eval num_timesteps=2130000, episode_reward=-27332.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.73e+04 |
| time/              |           |
|    total_timesteps | 2130000   |
| train/             |           |
|    actor_loss      | 1.38e+03  |
|    critic_loss     | 1.07e+04  |
|    ent_coef        | 0.742     |
|    ent_coef_loss   | -0.0585   |
|    learning_rate   | 0.000787  |
|    n_updates       | 2131257   |
----------------------------------
Eval num_timesteps=2140000, episode_reward=-16903.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.69e+04 |
| time/              |           |
|    total_timesteps | 2140000   |
| train/             |           |
|    actor_loss      | 1.33e+03  |
|    critic_loss     | 1.26e+04  |
|    ent_coef        | 0.771     |
|    ent_coef_loss   | 0.0805    |
|    learning_rate   | 0.000786  |
|    n_updates       | 2141257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.12e+04 |
| time/              |           |
|    episodes        | 428       |
|    fps             | 16        |
|    time_elapsed    | 133303    |
|    total_timesteps | 2140000   |
----------------------------------
Eval num_timesteps=2150000, episode_reward=-18057.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.81e+04 |
| time/              |           |
|    total_timesteps | 2150000   |
| train/             |           |
|    actor_loss      | 1.35e+03  |
|    critic_loss     | 3.19e+03  |
|    ent_coef        | 0.896     |
|    ent_coef_loss   | 0.218     |
|    learning_rate   | 0.000785  |
|    n_updates       | 2151257   |
----------------------------------
Eval num_timesteps=2160000, episode_reward=-19429.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.94e+04 |
| time/              |           |
|    total_timesteps | 2160000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 3.29e+03  |
|    ent_coef        | 0.819     |
|    ent_coef_loss   | 0.0391    |
|    learning_rate   | 0.000784  |
|    n_updates       | 2161257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.2e+04 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 16       |
|    time_elapsed    | 134552   |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=-16736.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.67e+04 |
| time/              |           |
|    total_timesteps | 2170000   |
| train/             |           |
|    actor_loss      | 1.42e+03  |
|    critic_loss     | 1.06e+04  |
|    ent_coef        | 0.695     |
|    ent_coef_loss   | -0.0324   |
|    learning_rate   | 0.000783  |
|    n_updates       | 2171257   |
----------------------------------
Eval num_timesteps=2180000, episode_reward=-15046.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | 1.27e+03 |
|    critic_loss     | 4.56e+03 |
|    ent_coef        | 0.815    |
|    ent_coef_loss   | -0.0114  |
|    learning_rate   | 0.000782 |
|    n_updates       | 2181257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.21e+04 |
| time/              |           |
|    episodes        | 436       |
|    fps             | 16        |
|    time_elapsed    | 135804    |
|    total_timesteps | 2180000   |
----------------------------------
Eval num_timesteps=2190000, episode_reward=-15620.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+04 |
| time/              |           |
|    total_timesteps | 2190000   |
| train/             |           |
|    actor_loss      | 1.29e+03  |
|    critic_loss     | 4.12e+03  |
|    ent_coef        | 0.769     |
|    ent_coef_loss   | 0.0996    |
|    learning_rate   | 0.000781  |
|    n_updates       | 2191257   |
----------------------------------
Eval num_timesteps=2200000, episode_reward=-11980.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+04 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | 1.2e+03  |
|    critic_loss     | 1.3e+04  |
|    ent_coef        | 0.756    |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.00078  |
|    n_updates       | 2201257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.31e+04 |
| time/              |           |
|    episodes        | 440       |
|    fps             | 16        |
|    time_elapsed    | 137053    |
|    total_timesteps | 2200000   |
----------------------------------
Eval num_timesteps=2210000, episode_reward=-22676.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.27e+04 |
| time/              |           |
|    total_timesteps | 2210000   |
| train/             |           |
|    actor_loss      | 1.22e+03  |
|    critic_loss     | 1.1e+04   |
|    ent_coef        | 0.785     |
|    ent_coef_loss   | 0.0113    |
|    learning_rate   | 0.000779  |
|    n_updates       | 2211257   |
----------------------------------
Eval num_timesteps=2220000, episode_reward=-12748.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.27e+04 |
| time/              |           |
|    total_timesteps | 2220000   |
| train/             |           |
|    actor_loss      | 1.23e+03  |
|    critic_loss     | 4.2e+03   |
|    ent_coef        | 0.708     |
|    ent_coef_loss   | -0.00294  |
|    learning_rate   | 0.000778  |
|    n_updates       | 2221257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.55e+04 |
| time/              |           |
|    episodes        | 444       |
|    fps             | 16        |
|    time_elapsed    | 138300    |
|    total_timesteps | 2220000   |
----------------------------------
Eval num_timesteps=2230000, episode_reward=-14498.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.45e+04 |
| time/              |           |
|    total_timesteps | 2230000   |
| train/             |           |
|    actor_loss      | 1.51e+03  |
|    critic_loss     | 1.28e+04  |
|    ent_coef        | 0.861     |
|    ent_coef_loss   | 0.0655    |
|    learning_rate   | 0.000777  |
|    n_updates       | 2231257   |
----------------------------------
Eval num_timesteps=2240000, episode_reward=-10146.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+04 |
| time/              |           |
|    total_timesteps | 2240000   |
| train/             |           |
|    actor_loss      | 1.41e+03  |
|    critic_loss     | 1.67e+04  |
|    ent_coef        | 1.16      |
|    ent_coef_loss   | 0.0411    |
|    learning_rate   | 0.000776  |
|    n_updates       | 2241257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.5e+04 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 16       |
|    time_elapsed    | 139554   |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=-16134.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 2250000   |
| train/             |           |
|    actor_loss      | 1.41e+03  |
|    critic_loss     | 7.3e+03   |
|    ent_coef        | 0.699     |
|    ent_coef_loss   | 0.171     |
|    learning_rate   | 0.000775  |
|    n_updates       | 2251257   |
----------------------------------
Eval num_timesteps=2260000, episode_reward=-38832.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.88e+04 |
| time/              |           |
|    total_timesteps | 2260000   |
| train/             |           |
|    actor_loss      | 1.38e+03  |
|    critic_loss     | 2.77e+04  |
|    ent_coef        | 0.822     |
|    ent_coef_loss   | 0.0862    |
|    learning_rate   | 0.000774  |
|    n_updates       | 2261257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.47e+04 |
| time/              |           |
|    episodes        | 452       |
|    fps             | 16        |
|    time_elapsed    | 140806    |
|    total_timesteps | 2260000   |
----------------------------------
Eval num_timesteps=2270000, episode_reward=-13329.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.33e+04 |
| time/              |           |
|    total_timesteps | 2270000   |
| train/             |           |
|    actor_loss      | 1.66e+03  |
|    critic_loss     | 4.88e+04  |
|    ent_coef        | 0.706     |
|    ent_coef_loss   | 0.0695    |
|    learning_rate   | 0.000773  |
|    n_updates       | 2271257   |
----------------------------------
Eval num_timesteps=2280000, episode_reward=-16112.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 2280000   |
| train/             |           |
|    actor_loss      | 1.52e+03  |
|    critic_loss     | 6.05e+03  |
|    ent_coef        | 0.702     |
|    ent_coef_loss   | 0.0304    |
|    learning_rate   | 0.000772  |
|    n_updates       | 2281257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.46e+04 |
| time/              |           |
|    episodes        | 456       |
|    fps             | 16        |
|    time_elapsed    | 142064    |
|    total_timesteps | 2280000   |
----------------------------------
Eval num_timesteps=2290000, episode_reward=-10830.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.08e+04 |
| time/              |           |
|    total_timesteps | 2290000   |
| train/             |           |
|    actor_loss      | 1.5e+03   |
|    critic_loss     | 5.75e+03  |
|    ent_coef        | 0.798     |
|    ent_coef_loss   | 0.0435    |
|    learning_rate   | 0.000771  |
|    n_updates       | 2291257   |
----------------------------------
Eval num_timesteps=2300000, episode_reward=-4636.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.64e+03 |
| time/              |           |
|    total_timesteps | 2300000   |
| train/             |           |
|    actor_loss      | 1.51e+03  |
|    critic_loss     | 2.6e+04   |
|    ent_coef        | 0.6       |
|    ent_coef_loss   | 0.0309    |
|    learning_rate   | 0.00077   |
|    n_updates       | 2301257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.45e+04 |
| time/              |           |
|    episodes        | 460       |
|    fps             | 16        |
|    time_elapsed    | 143318    |
|    total_timesteps | 2300000   |
----------------------------------
Eval num_timesteps=2310000, episode_reward=-17625.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.76e+04 |
| time/              |           |
|    total_timesteps | 2310000   |
| train/             |           |
|    actor_loss      | 1.96e+03  |
|    critic_loss     | 4.89e+04  |
|    ent_coef        | 0.627     |
|    ent_coef_loss   | 0.0783    |
|    learning_rate   | 0.000769  |
|    n_updates       | 2311257   |
----------------------------------
Eval num_timesteps=2320000, episode_reward=-2507.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.51e+03 |
| time/              |           |
|    total_timesteps | 2320000   |
| train/             |           |
|    actor_loss      | 1.53e+03  |
|    critic_loss     | 1.18e+04  |
|    ent_coef        | 0.769     |
|    ent_coef_loss   | -0.0759   |
|    learning_rate   | 0.000768  |
|    n_updates       | 2321257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.43e+04 |
| time/              |           |
|    episodes        | 464       |
|    fps             | 16        |
|    time_elapsed    | 144597    |
|    total_timesteps | 2320000   |
----------------------------------
Eval num_timesteps=2330000, episode_reward=-15089.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.51e+04 |
| time/              |           |
|    total_timesteps | 2330000   |
| train/             |           |
|    actor_loss      | 1.43e+03  |
|    critic_loss     | 1.47e+04  |
|    ent_coef        | 0.694     |
|    ent_coef_loss   | 0.137     |
|    learning_rate   | 0.000767  |
|    n_updates       | 2331257   |
----------------------------------
Eval num_timesteps=2340000, episode_reward=-22555.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.26e+04 |
| time/              |           |
|    total_timesteps | 2340000   |
| train/             |           |
|    actor_loss      | 1.34e+03  |
|    critic_loss     | 9.25e+03  |
|    ent_coef        | 0.776     |
|    ent_coef_loss   | 0.0294    |
|    learning_rate   | 0.000766  |
|    n_updates       | 2341257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.29e+04 |
| time/              |           |
|    episodes        | 468       |
|    fps             | 16        |
|    time_elapsed    | 145844    |
|    total_timesteps | 2340000   |
----------------------------------
Eval num_timesteps=2350000, episode_reward=-14962.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | 1.28e+03 |
|    critic_loss     | 1.11e+04 |
|    ent_coef        | 0.948    |
|    ent_coef_loss   | 0.00895  |
|    learning_rate   | 0.000765 |
|    n_updates       | 2351257  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=-12494.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.25e+04 |
| time/              |           |
|    total_timesteps | 2360000   |
| train/             |           |
|    actor_loss      | 1.26e+03  |
|    critic_loss     | 6.38e+03  |
|    ent_coef        | 0.833     |
|    ent_coef_loss   | -0.0479   |
|    learning_rate   | 0.000764  |
|    n_updates       | 2361257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.18e+04 |
| time/              |           |
|    episodes        | 472       |
|    fps             | 16        |
|    time_elapsed    | 147090    |
|    total_timesteps | 2360000   |
----------------------------------
Eval num_timesteps=2370000, episode_reward=-15218.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.52e+04 |
| time/              |           |
|    total_timesteps | 2370000   |
| train/             |           |
|    actor_loss      | 1.21e+03  |
|    critic_loss     | 1.16e+04  |
|    ent_coef        | 0.544     |
|    ent_coef_loss   | 0.0299    |
|    learning_rate   | 0.000763  |
|    n_updates       | 2371257   |
----------------------------------
Eval num_timesteps=2380000, episode_reward=-14633.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.46e+04 |
| time/              |           |
|    total_timesteps | 2380000   |
| train/             |           |
|    actor_loss      | 1.48e+03  |
|    critic_loss     | 8.18e+04  |
|    ent_coef        | 0.691     |
|    ent_coef_loss   | 0.0413    |
|    learning_rate   | 0.000762  |
|    n_updates       | 2381257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.16e+04 |
| time/              |           |
|    episodes        | 476       |
|    fps             | 16        |
|    time_elapsed    | 148341    |
|    total_timesteps | 2380000   |
----------------------------------
Eval num_timesteps=2390000, episode_reward=-14985.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | 1.23e+03 |
|    critic_loss     | 1.25e+04 |
|    ent_coef        | 0.917    |
|    ent_coef_loss   | 0.00268  |
|    learning_rate   | 0.000761 |
|    n_updates       | 2391257  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=-15155.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.52e+04 |
| time/              |           |
|    total_timesteps | 2400000   |
| train/             |           |
|    actor_loss      | 1.26e+03  |
|    critic_loss     | 7.32e+03  |
|    ent_coef        | 0.799     |
|    ent_coef_loss   | -0.0552   |
|    learning_rate   | 0.00076   |
|    n_updates       | 2401257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.16e+04 |
| time/              |           |
|    episodes        | 480       |
|    fps             | 16        |
|    time_elapsed    | 149586    |
|    total_timesteps | 2400000   |
----------------------------------
Eval num_timesteps=2410000, episode_reward=-12647.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.26e+04 |
| time/              |           |
|    total_timesteps | 2410000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 6.44e+03  |
|    ent_coef        | 0.75      |
|    ent_coef_loss   | 0.0292    |
|    learning_rate   | 0.000759  |
|    n_updates       | 2411257   |
----------------------------------
Eval num_timesteps=2420000, episode_reward=-13094.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.31e+04 |
| time/              |           |
|    total_timesteps | 2420000   |
| train/             |           |
|    actor_loss      | 1.39e+03  |
|    critic_loss     | 8.26e+03  |
|    ent_coef        | 0.69      |
|    ent_coef_loss   | 0.0708    |
|    learning_rate   | 0.000758  |
|    n_updates       | 2421257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.07e+04 |
| time/              |           |
|    episodes        | 484       |
|    fps             | 16        |
|    time_elapsed    | 150835    |
|    total_timesteps | 2420000   |
----------------------------------
Eval num_timesteps=2430000, episode_reward=-4023.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.02e+03 |
| time/              |           |
|    total_timesteps | 2430000   |
| train/             |           |
|    actor_loss      | 1.6e+03   |
|    critic_loss     | 1.7e+05   |
|    ent_coef        | 0.637     |
|    ent_coef_loss   | 0.136     |
|    learning_rate   | 0.000757  |
|    n_updates       | 2431257   |
----------------------------------
Eval num_timesteps=2440000, episode_reward=-11526.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.15e+04 |
| time/              |           |
|    total_timesteps | 2440000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 6.36e+03  |
|    ent_coef        | 0.895     |
|    ent_coef_loss   | -0.0075   |
|    learning_rate   | 0.000756  |
|    n_updates       | 2441257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.1e+04 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 16       |
|    time_elapsed    | 152097   |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=-13737.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.37e+04 |
| time/              |           |
|    total_timesteps | 2450000   |
| train/             |           |
|    actor_loss      | 1.11e+03  |
|    critic_loss     | 3.12e+03  |
|    ent_coef        | 0.563     |
|    ent_coef_loss   | 0.217     |
|    learning_rate   | 0.000755  |
|    n_updates       | 2451257   |
----------------------------------
Eval num_timesteps=2460000, episode_reward=-4535.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 2460000   |
| train/             |           |
|    actor_loss      | 1.1e+03   |
|    critic_loss     | 1.13e+05  |
|    ent_coef        | 0.62      |
|    ent_coef_loss   | -0.0492   |
|    learning_rate   | 0.000754  |
|    n_updates       | 2461257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.16e+04 |
| time/              |           |
|    episodes        | 492       |
|    fps             | 16        |
|    time_elapsed    | 153362    |
|    total_timesteps | 2460000   |
----------------------------------
Eval num_timesteps=2470000, episode_reward=-13138.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.31e+04 |
| time/              |           |
|    total_timesteps | 2470000   |
| train/             |           |
|    actor_loss      | 1.45e+03  |
|    critic_loss     | 5.97e+04  |
|    ent_coef        | 0.666     |
|    ent_coef_loss   | 0.0308    |
|    learning_rate   | 0.000753  |
|    n_updates       | 2471257   |
----------------------------------
Eval num_timesteps=2480000, episode_reward=-9558.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.56e+03 |
| time/              |           |
|    total_timesteps | 2480000   |
| train/             |           |
|    actor_loss      | 1.15e+03  |
|    critic_loss     | 6.76e+04  |
|    ent_coef        | 1.17      |
|    ent_coef_loss   | -0.0792   |
|    learning_rate   | 0.000752  |
|    n_updates       | 2481257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.15e+04 |
| time/              |           |
|    episodes        | 496       |
|    fps             | 16        |
|    time_elapsed    | 154625    |
|    total_timesteps | 2480000   |
----------------------------------
Eval num_timesteps=2490000, episode_reward=-3693.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.69e+03 |
| time/              |           |
|    total_timesteps | 2490000   |
| train/             |           |
|    actor_loss      | 1.21e+03  |
|    critic_loss     | 7.87e+03  |
|    ent_coef        | 1.06      |
|    ent_coef_loss   | 0.0034    |
|    learning_rate   | 0.000751  |
|    n_updates       | 2491257   |
----------------------------------
Eval num_timesteps=2500000, episode_reward=-9730.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.73e+03 |
| time/              |           |
|    total_timesteps | 2500000   |
| train/             |           |
|    actor_loss      | 1.58e+03  |
|    critic_loss     | 2.01e+04  |
|    ent_coef        | 0.803     |
|    ent_coef_loss   | 0.0332    |
|    learning_rate   | 0.00075   |
|    n_updates       | 2501257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.13e+04 |
| time/              |           |
|    episodes        | 500       |
|    fps             | 16        |
|    time_elapsed    | 155888    |
|    total_timesteps | 2500000   |
----------------------------------
Eval num_timesteps=2510000, episode_reward=-3134.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+03 |
| time/              |           |
|    total_timesteps | 2510000   |
| train/             |           |
|    actor_loss      | 1.4e+03   |
|    critic_loss     | 1.66e+04  |
|    ent_coef        | 0.813     |
|    ent_coef_loss   | -0.0169   |
|    learning_rate   | 0.000749  |
|    n_updates       | 2511257   |
----------------------------------
Eval num_timesteps=2520000, episode_reward=-3426.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.43e+03 |
| time/              |           |
|    total_timesteps | 2520000   |
| train/             |           |
|    actor_loss      | 1.39e+03  |
|    critic_loss     | 4.05e+03  |
|    ent_coef        | 0.706     |
|    ent_coef_loss   | 0.05      |
|    learning_rate   | 0.000748  |
|    n_updates       | 2521257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.91e+04 |
| time/              |           |
|    episodes        | 504       |
|    fps             | 16        |
|    time_elapsed    | 157150    |
|    total_timesteps | 2520000   |
----------------------------------
Eval num_timesteps=2530000, episode_reward=-2118.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.12e+03 |
| time/              |           |
|    total_timesteps | 2530000   |
| train/             |           |
|    actor_loss      | 1.26e+03  |
|    critic_loss     | 5.74e+03  |
|    ent_coef        | 0.876     |
|    ent_coef_loss   | -0.0333   |
|    learning_rate   | 0.000747  |
|    n_updates       | 2531257   |
----------------------------------
Eval num_timesteps=2540000, episode_reward=-12845.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.28e+04 |
| time/              |           |
|    total_timesteps | 2540000   |
| train/             |           |
|    actor_loss      | 1.35e+03  |
|    critic_loss     | 7.23e+03  |
|    ent_coef        | 0.837     |
|    ent_coef_loss   | 0.00841   |
|    learning_rate   | 0.000746  |
|    n_updates       | 2541257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+04 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 16       |
|    time_elapsed    | 158402   |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=-21949.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.19e+04 |
| time/              |           |
|    total_timesteps | 2550000   |
| train/             |           |
|    actor_loss      | 1.3e+03   |
|    critic_loss     | 1.75e+04  |
|    ent_coef        | 1.1       |
|    ent_coef_loss   | -0.00708  |
|    learning_rate   | 0.000745  |
|    n_updates       | 2551257   |
----------------------------------
Eval num_timesteps=2560000, episode_reward=-13352.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.34e+04 |
| time/              |           |
|    total_timesteps | 2560000   |
| train/             |           |
|    actor_loss      | 1.18e+03  |
|    critic_loss     | 6.61e+03  |
|    ent_coef        | 0.695     |
|    ent_coef_loss   | -0.0242   |
|    learning_rate   | 0.000744  |
|    n_updates       | 2561257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.95e+04 |
| time/              |           |
|    episodes        | 512       |
|    fps             | 16        |
|    time_elapsed    | 159659    |
|    total_timesteps | 2560000   |
----------------------------------
Eval num_timesteps=2570000, episode_reward=-16486.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.65e+04 |
| time/              |           |
|    total_timesteps | 2570000   |
| train/             |           |
|    actor_loss      | 1.36e+03  |
|    critic_loss     | 6.73e+04  |
|    ent_coef        | 0.708     |
|    ent_coef_loss   | 0.138     |
|    learning_rate   | 0.000743  |
|    n_updates       | 2571257   |
----------------------------------
Eval num_timesteps=2580000, episode_reward=-59949.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.99e+04 |
| time/              |           |
|    total_timesteps | 2580000   |
| train/             |           |
|    actor_loss      | 1.63e+03  |
|    critic_loss     | 9.51e+03  |
|    ent_coef        | 0.587     |
|    ent_coef_loss   | 0.577     |
|    learning_rate   | 0.000742  |
|    n_updates       | 2581257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+04 |
| time/              |           |
|    episodes        | 516       |
|    fps             | 16        |
|    time_elapsed    | 160910    |
|    total_timesteps | 2580000   |
----------------------------------
Eval num_timesteps=2590000, episode_reward=-18768.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.88e+04 |
| time/              |           |
|    total_timesteps | 2590000   |
| train/             |           |
|    actor_loss      | 1.48e+03  |
|    critic_loss     | 8.34e+03  |
|    ent_coef        | 0.695     |
|    ent_coef_loss   | -0.0779   |
|    learning_rate   | 0.000741  |
|    n_updates       | 2591257   |
----------------------------------
Eval num_timesteps=2600000, episode_reward=-71556.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.16e+04 |
| time/              |           |
|    total_timesteps | 2600000   |
| train/             |           |
|    actor_loss      | 1.31e+03  |
|    critic_loss     | 2.62e+04  |
|    ent_coef        | 0.963     |
|    ent_coef_loss   | 0.00606   |
|    learning_rate   | 0.00074   |
|    n_updates       | 2601257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.99e+04 |
| time/              |           |
|    episodes        | 520       |
|    fps             | 16        |
|    time_elapsed    | 162178    |
|    total_timesteps | 2600000   |
----------------------------------
Eval num_timesteps=2610000, episode_reward=-16333.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+04 |
| time/              |           |
|    total_timesteps | 2610000   |
| train/             |           |
|    actor_loss      | 1.7e+03   |
|    critic_loss     | 3.27e+04  |
|    ent_coef        | 0.749     |
|    ent_coef_loss   | -0.0408   |
|    learning_rate   | 0.000739  |
|    n_updates       | 2611257   |
----------------------------------
Eval num_timesteps=2620000, episode_reward=-7862.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.86e+03 |
| time/              |           |
|    total_timesteps | 2620000   |
| train/             |           |
|    actor_loss      | 1.78e+03  |
|    critic_loss     | 2.77e+04  |
|    ent_coef        | 0.493     |
|    ent_coef_loss   | 0.116     |
|    learning_rate   | 0.000738  |
|    n_updates       | 2621257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.05e+04 |
| time/              |           |
|    episodes        | 524       |
|    fps             | 16        |
|    time_elapsed    | 163349    |
|    total_timesteps | 2620000   |
----------------------------------
Eval num_timesteps=2630000, episode_reward=-12259.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+04 |
| time/              |           |
|    total_timesteps | 2630000   |
| train/             |           |
|    actor_loss      | 1.52e+03  |
|    critic_loss     | 3.4e+05   |
|    ent_coef        | 0.586     |
|    ent_coef_loss   | -0.0178   |
|    learning_rate   | 0.000737  |
|    n_updates       | 2631257   |
----------------------------------
Eval num_timesteps=2640000, episode_reward=-13599.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.36e+04 |
| time/              |           |
|    total_timesteps | 2640000   |
| train/             |           |
|    actor_loss      | 1.52e+03  |
|    critic_loss     | 6.1e+03   |
|    ent_coef        | 0.718     |
|    ent_coef_loss   | -0.0695   |
|    learning_rate   | 0.000736  |
|    n_updates       | 2641257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.03e+04 |
| time/              |           |
|    episodes        | 528       |
|    fps             | 16        |
|    time_elapsed    | 164528    |
|    total_timesteps | 2640000   |
----------------------------------
Eval num_timesteps=2650000, episode_reward=-16343.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+04 |
| time/              |           |
|    total_timesteps | 2650000   |
| train/             |           |
|    actor_loss      | 1.57e+03  |
|    critic_loss     | 1.05e+04  |
|    ent_coef        | 0.722     |
|    ent_coef_loss   | -0.085    |
|    learning_rate   | 0.000735  |
|    n_updates       | 2651257   |
----------------------------------
Eval num_timesteps=2660000, episode_reward=-1844.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.84e+03 |
| time/              |           |
|    total_timesteps | 2660000   |
| train/             |           |
|    actor_loss      | 1.75e+03  |
|    critic_loss     | 1.35e+04  |
|    ent_coef        | 1.12      |
|    ent_coef_loss   | 0.038     |
|    learning_rate   | 0.000734  |
|    n_updates       | 2661257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.97e+04 |
| time/              |           |
|    episodes        | 532       |
|    fps             | 16        |
|    time_elapsed    | 165702    |
|    total_timesteps | 2660000   |
----------------------------------
Eval num_timesteps=2670000, episode_reward=-1271.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.27e+03 |
| time/              |           |
|    total_timesteps | 2670000   |
| train/             |           |
|    actor_loss      | 1.49e+03  |
|    critic_loss     | 6.5e+03   |
|    ent_coef        | 1.08      |
|    ent_coef_loss   | -0.0163   |
|    learning_rate   | 0.000733  |
|    n_updates       | 2671257   |
----------------------------------
New best mean reward!
Eval num_timesteps=2680000, episode_reward=-1084.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.08e+03 |
| time/              |           |
|    total_timesteps | 2680000   |
| train/             |           |
|    actor_loss      | 1.71e+03  |
|    critic_loss     | 2.03e+04  |
|    ent_coef        | 1.27      |
|    ent_coef_loss   | 0.00194   |
|    learning_rate   | 0.000732  |
|    n_updates       | 2681257   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.93e+04 |
| time/              |           |
|    episodes        | 536       |
|    fps             | 16        |
|    time_elapsed    | 166873    |
|    total_timesteps | 2680000   |
----------------------------------
Eval num_timesteps=2690000, episode_reward=-1423.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+03 |
| time/              |           |
|    total_timesteps | 2690000   |
| train/             |           |
|    actor_loss      | 1.33e+03  |
|    critic_loss     | 6.97e+03  |
|    ent_coef        | 1.07      |
|    ent_coef_loss   | 0.0211    |
|    learning_rate   | 0.000731  |
|    n_updates       | 2691257   |
----------------------------------
Eval num_timesteps=2700000, episode_reward=-14460.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.45e+04 |
| time/              |           |
|    total_timesteps | 2700000   |
| train/             |           |
|    actor_loss      | 1.33e+03  |
|    critic_loss     | 8.23e+03  |
|    ent_coef        | 0.881     |
|    ent_coef_loss   | -0.0274   |
|    learning_rate   | 0.00073   |
|    n_updates       | 2701257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.79e+04 |
| time/              |           |
|    episodes        | 540       |
|    fps             | 16        |
|    time_elapsed    | 168042    |
|    total_timesteps | 2700000   |
----------------------------------
Eval num_timesteps=2710000, episode_reward=-17659.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+04 |
| time/              |           |
|    total_timesteps | 2710000   |
| train/             |           |
|    actor_loss      | 1.07e+03  |
|    critic_loss     | 1.36e+05  |
|    ent_coef        | 0.924     |
|    ent_coef_loss   | -0.00185  |
|    learning_rate   | 0.000729  |
|    n_updates       | 2711257   |
----------------------------------
Eval num_timesteps=2720000, episode_reward=-19953.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2e+04   |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | 1.34e+03 |
|    critic_loss     | 1.56e+04 |
|    ent_coef        | 0.96     |
|    ent_coef_loss   | -0.0121  |
|    learning_rate   | 0.000728 |
|    n_updates       | 2721257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.55e+04 |
| time/              |           |
|    episodes        | 544       |
|    fps             | 16        |
|    time_elapsed    | 169208    |
|    total_timesteps | 2720000   |
----------------------------------
Eval num_timesteps=2730000, episode_reward=-20284.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.03e+04 |
| time/              |           |
|    total_timesteps | 2730000   |
| train/             |           |
|    actor_loss      | 1.07e+03  |
|    critic_loss     | 5.17e+03  |
|    ent_coef        | 1.08      |
|    ent_coef_loss   | 0.00995   |
|    learning_rate   | 0.000727  |
|    n_updates       | 2731257   |
----------------------------------
Eval num_timesteps=2740000, episode_reward=-8882.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.88e+03 |
| time/              |           |
|    total_timesteps | 2740000   |
| train/             |           |
|    actor_loss      | 1.06e+03  |
|    critic_loss     | 4.77e+05  |
|    ent_coef        | 1.15      |
|    ent_coef_loss   | 0.00648   |
|    learning_rate   | 0.000726  |
|    n_updates       | 2741257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.56e+04 |
| time/              |           |
|    episodes        | 548       |
|    fps             | 16        |
|    time_elapsed    | 170379    |
|    total_timesteps | 2740000   |
----------------------------------
Eval num_timesteps=2750000, episode_reward=-556.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | 984      |
|    critic_loss     | 1.09e+04 |
|    ent_coef        | 0.926    |
|    ent_coef_loss   | -0.0192  |
|    learning_rate   | 0.000725 |
|    n_updates       | 2751257  |
---------------------------------
New best mean reward!
Eval num_timesteps=2760000, episode_reward=-1427.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+03 |
| time/              |           |
|    total_timesteps | 2760000   |
| train/             |           |
|    actor_loss      | 979       |
|    critic_loss     | 6.05e+03  |
|    ent_coef        | 0.849     |
|    ent_coef_loss   | 0.0436    |
|    learning_rate   | 0.000724  |
|    n_updates       | 2761257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.54e+04 |
| time/              |           |
|    episodes        | 552       |
|    fps             | 16        |
|    time_elapsed    | 171556    |
|    total_timesteps | 2760000   |
----------------------------------
Eval num_timesteps=2770000, episode_reward=-1748.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.75e+03 |
| time/              |           |
|    total_timesteps | 2770000   |
| train/             |           |
|    actor_loss      | 1.06e+03  |
|    critic_loss     | 6.37e+03  |
|    ent_coef        | 0.826     |
|    ent_coef_loss   | 0.00757   |
|    learning_rate   | 0.000723  |
|    n_updates       | 2771257   |
----------------------------------
Eval num_timesteps=2780000, episode_reward=-948.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | 1.1e+03  |
|    critic_loss     | 4.12e+03 |
|    ent_coef        | 0.734    |
|    ent_coef_loss   | 0.144    |
|    learning_rate   | 0.000722 |
|    n_updates       | 2781257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.51e+04 |
| time/              |           |
|    episodes        | 556       |
|    fps             | 16        |
|    time_elapsed    | 172737    |
|    total_timesteps | 2780000   |
----------------------------------
Eval num_timesteps=2790000, episode_reward=-2670.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.67e+03 |
| time/              |           |
|    total_timesteps | 2790000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 3.52e+04  |
|    ent_coef        | 0.786     |
|    ent_coef_loss   | -0.0608   |
|    learning_rate   | 0.000721  |
|    n_updates       | 2791257   |
----------------------------------
Eval num_timesteps=2800000, episode_reward=-1052.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.05e+03 |
| time/              |           |
|    total_timesteps | 2800000   |
| train/             |           |
|    actor_loss      | 1.26e+03  |
|    critic_loss     | 1.31e+04  |
|    ent_coef        | 0.773     |
|    ent_coef_loss   | -0.0749   |
|    learning_rate   | 0.00072   |
|    n_updates       | 2801257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.47e+04 |
| time/              |           |
|    episodes        | 560       |
|    fps             | 16        |
|    time_elapsed    | 173911    |
|    total_timesteps | 2800000   |
----------------------------------
Eval num_timesteps=2810000, episode_reward=-16105.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 2810000   |
| train/             |           |
|    actor_loss      | 1.45e+03  |
|    critic_loss     | 1.87e+04  |
|    ent_coef        | 1.39      |
|    ent_coef_loss   | -0.129    |
|    learning_rate   | 0.000719  |
|    n_updates       | 2811257   |
----------------------------------
Eval num_timesteps=2820000, episode_reward=-16115.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+04 |
| time/              |           |
|    total_timesteps | 2820000   |
| train/             |           |
|    actor_loss      | 1.42e+03  |
|    critic_loss     | 1.39e+04  |
|    ent_coef        | 0.997     |
|    ent_coef_loss   | -0.00031  |
|    learning_rate   | 0.000718  |
|    n_updates       | 2821257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.46e+04 |
| time/              |           |
|    episodes        | 564       |
|    fps             | 16        |
|    time_elapsed    | 175082    |
|    total_timesteps | 2820000   |
----------------------------------
Eval num_timesteps=2830000, episode_reward=-18824.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.88e+04 |
| time/              |           |
|    total_timesteps | 2830000   |
| train/             |           |
|    actor_loss      | 1.31e+03  |
|    critic_loss     | 1.25e+04  |
|    ent_coef        | 1.74      |
|    ent_coef_loss   | 0.179     |
|    learning_rate   | 0.000717  |
|    n_updates       | 2831257   |
----------------------------------
Eval num_timesteps=2840000, episode_reward=-21851.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.19e+04 |
| time/              |           |
|    total_timesteps | 2840000   |
| train/             |           |
|    actor_loss      | 995       |
|    critic_loss     | 1.34e+04  |
|    ent_coef        | 1.82      |
|    ent_coef_loss   | 0.108     |
|    learning_rate   | 0.000716  |
|    n_updates       | 2841257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.47e+04 |
| time/              |           |
|    episodes        | 568       |
|    fps             | 16        |
|    time_elapsed    | 176248    |
|    total_timesteps | 2840000   |
----------------------------------
Eval num_timesteps=2850000, episode_reward=-16523.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.65e+04 |
| time/              |           |
|    total_timesteps | 2850000   |
| train/             |           |
|    actor_loss      | 1.18e+03  |
|    critic_loss     | 6.97e+03  |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | -0.00394  |
|    learning_rate   | 0.000715  |
|    n_updates       | 2851257   |
----------------------------------
Eval num_timesteps=2860000, episode_reward=-15041.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+04 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | 1.11e+03 |
|    critic_loss     | 7.47e+03 |
|    ent_coef        | 1.04     |
|    ent_coef_loss   | -0.0119  |
|    learning_rate   | 0.000714 |
|    n_updates       | 2861257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.48e+04 |
| time/              |           |
|    episodes        | 572       |
|    fps             | 16        |
|    time_elapsed    | 177412    |
|    total_timesteps | 2860000   |
----------------------------------
Eval num_timesteps=2870000, episode_reward=-2324.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.32e+03 |
| time/              |           |
|    total_timesteps | 2870000   |
| train/             |           |
|    actor_loss      | 1.47e+03  |
|    critic_loss     | 2.39e+05  |
|    ent_coef        | 0.91      |
|    ent_coef_loss   | 0.0157    |
|    learning_rate   | 0.000713  |
|    n_updates       | 2871257   |
----------------------------------
Eval num_timesteps=2880000, episode_reward=-13050.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.31e+04 |
| time/              |           |
|    total_timesteps | 2880000   |
| train/             |           |
|    actor_loss      | 1.02e+03  |
|    critic_loss     | 9.22e+03  |
|    ent_coef        | 0.972     |
|    ent_coef_loss   | 0.0044    |
|    learning_rate   | 0.000712  |
|    n_updates       | 2881257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.49e+04 |
| time/              |           |
|    episodes        | 576       |
|    fps             | 16        |
|    time_elapsed    | 178572    |
|    total_timesteps | 2880000   |
----------------------------------
Eval num_timesteps=2890000, episode_reward=-16693.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.67e+04 |
| time/              |           |
|    total_timesteps | 2890000   |
| train/             |           |
|    actor_loss      | 949       |
|    critic_loss     | 2.21e+04  |
|    ent_coef        | 0.607     |
|    ent_coef_loss   | -0.14     |
|    learning_rate   | 0.000711  |
|    n_updates       | 2891257   |
----------------------------------
Eval num_timesteps=2900000, episode_reward=-16544.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.65e+04 |
| time/              |           |
|    total_timesteps | 2900000   |
| train/             |           |
|    actor_loss      | 886       |
|    critic_loss     | 1.97e+03  |
|    ent_coef        | 0.719     |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 0.00071   |
|    n_updates       | 2901257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.49e+04 |
| time/              |           |
|    episodes        | 580       |
|    fps             | 16        |
|    time_elapsed    | 179732    |
|    total_timesteps | 2900000   |
----------------------------------
Eval num_timesteps=2910000, episode_reward=-11451.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.15e+04 |
| time/              |           |
|    total_timesteps | 2910000   |
| train/             |           |
|    actor_loss      | 962       |
|    critic_loss     | 3.46e+03  |
|    ent_coef        | 0.866     |
|    ent_coef_loss   | -0.0471   |
|    learning_rate   | 0.000709  |
|    n_updates       | 2911257   |
----------------------------------
Eval num_timesteps=2920000, episode_reward=-9426.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.43e+03 |
| time/              |           |
|    total_timesteps | 2920000   |
| train/             |           |
|    actor_loss      | 1.08e+03  |
|    critic_loss     | 6.15e+03  |
|    ent_coef        | 0.587     |
|    ent_coef_loss   | -0.0517   |
|    learning_rate   | 0.000708  |
|    n_updates       | 2921257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.48e+04 |
| time/              |           |
|    episodes        | 584       |
|    fps             | 16        |
|    time_elapsed    | 180897    |
|    total_timesteps | 2920000   |
----------------------------------
Eval num_timesteps=2930000, episode_reward=-16625.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.66e+04 |
| time/              |           |
|    total_timesteps | 2930000   |
| train/             |           |
|    actor_loss      | 1.22e+03  |
|    critic_loss     | 9.19e+04  |
|    ent_coef        | 0.647     |
|    ent_coef_loss   | -0.0212   |
|    learning_rate   | 0.000707  |
|    n_updates       | 2931257   |
----------------------------------
Eval num_timesteps=2940000, episode_reward=-14788.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+04 |
| time/              |           |
|    total_timesteps | 2940000   |
| train/             |           |
|    actor_loss      | 1.09e+03  |
|    critic_loss     | 3.81e+03  |
|    ent_coef        | 0.513     |
|    ent_coef_loss   | -0.18     |
|    learning_rate   | 0.000706  |
|    n_updates       | 2941257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.43e+04 |
| time/              |           |
|    episodes        | 588       |
|    fps             | 16        |
|    time_elapsed    | 182063    |
|    total_timesteps | 2940000   |
----------------------------------
Eval num_timesteps=2950000, episode_reward=-15620.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+04 |
| time/              |           |
|    total_timesteps | 2950000   |
| train/             |           |
|    actor_loss      | 1.05e+03  |
|    critic_loss     | 7.15e+03  |
|    ent_coef        | 0.514     |
|    ent_coef_loss   | -0.0764   |
|    learning_rate   | 0.000705  |
|    n_updates       | 2951257   |
----------------------------------
Eval num_timesteps=2960000, episode_reward=-14421.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+04 |
| time/              |           |
|    total_timesteps | 2960000   |
| train/             |           |
|    actor_loss      | 1.05e+03  |
|    critic_loss     | 4.18e+03  |
|    ent_coef        | 0.477     |
|    ent_coef_loss   | -0.0147   |
|    learning_rate   | 0.000704  |
|    n_updates       | 2961257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.37e+04 |
| time/              |           |
|    episodes        | 592       |
|    fps             | 16        |
|    time_elapsed    | 183231    |
|    total_timesteps | 2960000   |
----------------------------------
Eval num_timesteps=2970000, episode_reward=-16014.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.6e+04 |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | 1.22e+03 |
|    critic_loss     | 4.71e+04 |
|    ent_coef        | 0.491    |
|    ent_coef_loss   | -0.00239 |
|    learning_rate   | 0.000703 |
|    n_updates       | 2971257  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=-17493.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.75e+04 |
| time/              |           |
|    total_timesteps | 2980000   |
| train/             |           |
|    actor_loss      | 1.06e+03  |
|    critic_loss     | 3.4e+03   |
|    ent_coef        | 0.56      |
|    ent_coef_loss   | -0.123    |
|    learning_rate   | 0.000702  |
|    n_updates       | 2981257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.39e+04 |
| time/              |           |
|    episodes        | 596       |
|    fps             | 16        |
|    time_elapsed    | 184403    |
|    total_timesteps | 2980000   |
----------------------------------
Eval num_timesteps=2990000, episode_reward=-10052.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+04 |
| time/              |           |
|    total_timesteps | 2990000   |
| train/             |           |
|    actor_loss      | 1.22e+03  |
|    critic_loss     | 1e+04     |
|    ent_coef        | 0.511     |
|    ent_coef_loss   | 0.25      |
|    learning_rate   | 0.000701  |
|    n_updates       | 2991257   |
----------------------------------
Eval num_timesteps=3000000, episode_reward=-15395.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+04 |
| time/              |           |
|    total_timesteps | 3000000   |
| train/             |           |
|    actor_loss      | 924       |
|    critic_loss     | 1.9e+03   |
|    ent_coef        | 0.298     |
|    ent_coef_loss   | 0.355     |
|    learning_rate   | 0.0007    |
|    n_updates       | 3001257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.41e+04 |
| time/              |           |
|    episodes        | 600       |
|    fps             | 16        |
|    time_elapsed    | 185581    |
|    total_timesteps | 3000000   |
----------------------------------
Eval num_timesteps=3010000, episode_reward=-15845.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.58e+04 |
| time/              |           |
|    total_timesteps | 3010000   |
| train/             |           |
|    actor_loss      | 937       |
|    critic_loss     | 1.46e+04  |
|    ent_coef        | 0.309     |
|    ent_coef_loss   | 0.463     |
|    learning_rate   | 0.000699  |
|    n_updates       | 3011257   |
----------------------------------
Eval num_timesteps=3020000, episode_reward=-3666.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.67e+03 |
| time/              |           |
|    total_timesteps | 3020000   |
| train/             |           |
|    actor_loss      | 899       |
|    critic_loss     | 4.54e+03  |
|    ent_coef        | 0.453     |
|    ent_coef_loss   | 0.0811    |
|    learning_rate   | 0.000698  |
|    n_updates       | 3021257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.43e+04 |
| time/              |           |
|    episodes        | 604       |
|    fps             | 16        |
|    time_elapsed    | 186758    |
|    total_timesteps | 3020000   |
----------------------------------
Eval num_timesteps=3030000, episode_reward=-3963.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.96e+03 |
| time/              |           |
|    total_timesteps | 3030000   |
| train/             |           |
|    actor_loss      | 942       |
|    critic_loss     | 3.98e+03  |
|    ent_coef        | 1.34      |
|    ent_coef_loss   | -0.0315   |
|    learning_rate   | 0.000697  |
|    n_updates       | 3031257   |
----------------------------------
Eval num_timesteps=3040000, episode_reward=-25911.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.59e+04 |
| time/              |           |
|    total_timesteps | 3040000   |
| train/             |           |
|    actor_loss      | 762       |
|    critic_loss     | 9.11e+03  |
|    ent_coef        | 0.892     |
|    ent_coef_loss   | -0.0191   |
|    learning_rate   | 0.000696  |
|    n_updates       | 3041257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.43e+04 |
| time/              |           |
|    episodes        | 608       |
|    fps             | 16        |
|    time_elapsed    | 187928    |
|    total_timesteps | 3040000   |
----------------------------------
Eval num_timesteps=3050000, episode_reward=-12615.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.26e+04 |
| time/              |           |
|    total_timesteps | 3050000   |
| train/             |           |
|    actor_loss      | 441       |
|    critic_loss     | 2.91e+03  |
|    ent_coef        | 0.979     |
|    ent_coef_loss   | -0.00339  |
|    learning_rate   | 0.000695  |
|    n_updates       | 3051257   |
----------------------------------
Eval num_timesteps=3060000, episode_reward=-9882.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.88e+03 |
| time/              |           |
|    total_timesteps | 3060000   |
| train/             |           |
|    actor_loss      | 457       |
|    critic_loss     | 3.08e+03  |
|    ent_coef        | 0.784     |
|    ent_coef_loss   | 0.0106    |
|    learning_rate   | 0.000694  |
|    n_updates       | 3061257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.39e+04 |
| time/              |           |
|    episodes        | 612       |
|    fps             | 16        |
|    time_elapsed    | 189092    |
|    total_timesteps | 3060000   |
----------------------------------
Eval num_timesteps=3070000, episode_reward=-9720.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.72e+03 |
| time/              |           |
|    total_timesteps | 3070000   |
| train/             |           |
|    actor_loss      | 826       |
|    critic_loss     | 3.92e+03  |
|    ent_coef        | 0.627     |
|    ent_coef_loss   | -0.0943   |
|    learning_rate   | 0.000693  |
|    n_updates       | 3071257   |
----------------------------------
Eval num_timesteps=3080000, episode_reward=-986.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -986     |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | 921      |
|    critic_loss     | 4.82e+03 |
|    ent_coef        | 0.575    |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.000692 |
|    n_updates       | 3081257  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.33e+04 |
| time/              |           |
|    episodes        | 616       |
|    fps             | 16        |
|    time_elapsed    | 190250    |
|    total_timesteps | 3080000   |
----------------------------------
Eval num_timesteps=3090000, episode_reward=-9463.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.46e+03 |
| time/              |           |
|    total_timesteps | 3090000   |
| train/             |           |
|    actor_loss      | 992       |
|    critic_loss     | 4.18e+03  |
|    ent_coef        | 0.888     |
|    ent_coef_loss   | 0.00939   |
|    learning_rate   | 0.000691  |
|    n_updates       | 3091257   |
----------------------------------
Eval num_timesteps=3100000, episode_reward=-12672.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.27e+04 |
| time/              |           |
|    total_timesteps | 3100000   |
| train/             |           |
|    actor_loss      | 944       |
|    critic_loss     | 5.87e+03  |
|    ent_coef        | 0.649     |
|    ent_coef_loss   | -0.183    |
|    learning_rate   | 0.00069   |
|    n_updates       | 3101257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.3e+04 |
| time/              |          |
|    episodes        | 620      |
|    fps             | 16       |
|    time_elapsed    | 191410   |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=-16048.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.6e+04 |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | 961      |
|    critic_loss     | 2.8e+03  |
|    ent_coef        | 0.539    |
|    ent_coef_loss   | -0.11    |
|    learning_rate   | 0.000689 |
|    n_updates       | 3111257  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=-20419.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.04e+04 |
| time/              |           |
|    total_timesteps | 3120000   |
| train/             |           |
|    actor_loss      | 980       |
|    critic_loss     | 1.23e+04  |
|    ent_coef        | 0.959     |
|    ent_coef_loss   | -0.00756  |
|    learning_rate   | 0.000688  |
|    n_updates       | 3121257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+04 |
| time/              |           |
|    episodes        | 624       |
|    fps             | 16        |
|    time_elapsed    | 192670    |
|    total_timesteps | 3120000   |
----------------------------------
Eval num_timesteps=3130000, episode_reward=-16199.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+04 |
| time/              |           |
|    total_timesteps | 3130000   |
| train/             |           |
|    actor_loss      | 1.06e+03  |
|    critic_loss     | 9.08e+03  |
|    ent_coef        | 0.769     |
|    ent_coef_loss   | 0.0821    |
|    learning_rate   | 0.000687  |
|    n_updates       | 3131257   |
----------------------------------
Eval num_timesteps=3140000, episode_reward=-13909.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.39e+04 |
| time/              |           |
|    total_timesteps | 3140000   |
| train/             |           |
|    actor_loss      | 1.07e+03  |
|    critic_loss     | 1.78e+04  |
|    ent_coef        | 0.82      |
|    ent_coef_loss   | 0.0814    |
|    learning_rate   | 0.000686  |
|    n_updates       | 3141257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.47e+04 |
| time/              |           |
|    episodes        | 628       |
|    fps             | 16        |
|    time_elapsed    | 193951    |
|    total_timesteps | 3140000   |
----------------------------------
Eval num_timesteps=3150000, episode_reward=-14239.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+04 |
| time/              |           |
|    total_timesteps | 3150000   |
| train/             |           |
|    actor_loss      | 1.02e+03  |
|    critic_loss     | 1.54e+04  |
|    ent_coef        | 0.898     |
|    ent_coef_loss   | -0.0503   |
|    learning_rate   | 0.000685  |
|    n_updates       | 3151257   |
----------------------------------
Eval num_timesteps=3160000, episode_reward=-14259.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+04 |
| time/              |           |
|    total_timesteps | 3160000   |
| train/             |           |
|    actor_loss      | 1.03e+03  |
|    critic_loss     | 7.1e+03   |
|    ent_coef        | 1.3       |
|    ent_coef_loss   | 0.102     |
|    learning_rate   | 0.000684  |
|    n_updates       | 3161257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    episodes        | 632       |
|    fps             | 16        |
|    time_elapsed    | 195311    |
|    total_timesteps | 3160000   |
----------------------------------
Eval num_timesteps=3170000, episode_reward=-14176.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+04 |
| time/              |           |
|    total_timesteps | 3170000   |
| train/             |           |
|    actor_loss      | 543       |
|    critic_loss     | 1.76e+04  |
|    ent_coef        | 1.28      |
|    ent_coef_loss   | -0.00451  |
|    learning_rate   | 0.000683  |
|    n_updates       | 3171257   |
----------------------------------
Eval num_timesteps=3180000, episode_reward=-20669.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+04 |
| time/              |           |
|    total_timesteps | 3180000   |
| train/             |           |
|    actor_loss      | 732       |
|    critic_loss     | 9.24e+03  |
|    ent_coef        | 0.94      |
|    ent_coef_loss   | 0.00916   |
|    learning_rate   | 0.000682  |
|    n_updates       | 3181257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.28e+04 |
| time/              |           |
|    episodes        | 636       |
|    fps             | 16        |
|    time_elapsed    | 196522    |
|    total_timesteps | 3180000   |
----------------------------------
Eval num_timesteps=3190000, episode_reward=-21286.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.13e+04 |
| time/              |           |
|    total_timesteps | 3190000   |
| train/             |           |
|    actor_loss      | 843       |
|    critic_loss     | 8.23e+03  |
|    ent_coef        | 0.647     |
|    ent_coef_loss   | -0.0335   |
|    learning_rate   | 0.000681  |
|    n_updates       | 3191257   |
----------------------------------
Eval num_timesteps=3200000, episode_reward=-8834.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.83e+03 |
| time/              |           |
|    total_timesteps | 3200000   |
| train/             |           |
|    actor_loss      | 1.12e+03  |
|    critic_loss     | 2.71e+04  |
|    ent_coef        | 0.772     |
|    ent_coef_loss   | 0.00685   |
|    learning_rate   | 0.00068   |
|    n_updates       | 3201257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.3e+04 |
| time/              |          |
|    episodes        | 640      |
|    fps             | 16       |
|    time_elapsed    | 197704   |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=-15901.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+04 |
| time/              |           |
|    total_timesteps | 3210000   |
| train/             |           |
|    actor_loss      | 751       |
|    critic_loss     | 1.3e+04   |
|    ent_coef        | 1.02      |
|    ent_coef_loss   | 0.00647   |
|    learning_rate   | 0.000679  |
|    n_updates       | 3211257   |
----------------------------------
Eval num_timesteps=3220000, episode_reward=-19309.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.93e+04 |
| time/              |           |
|    total_timesteps | 3220000   |
| train/             |           |
|    actor_loss      | 708       |
|    critic_loss     | 5.91e+03  |
|    ent_coef        | 0.791     |
|    ent_coef_loss   | -0.00833  |
|    learning_rate   | 0.000678  |
|    n_updates       | 3221257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.3e+04 |
| time/              |          |
|    episodes        | 644      |
|    fps             | 16       |
|    time_elapsed    | 198886   |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=-22556.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.26e+04 |
| time/              |           |
|    total_timesteps | 3230000   |
| train/             |           |
|    actor_loss      | 473       |
|    critic_loss     | 2.74e+03  |
|    ent_coef        | 0.808     |
|    ent_coef_loss   | -0.0503   |
|    learning_rate   | 0.000677  |
|    n_updates       | 3231257   |
----------------------------------
Eval num_timesteps=3240000, episode_reward=-6515.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.52e+03 |
| time/              |           |
|    total_timesteps | 3240000   |
| train/             |           |
|    actor_loss      | 492       |
|    critic_loss     | 1.16e+04  |
|    ent_coef        | 1.14      |
|    ent_coef_loss   | 0.0164    |
|    learning_rate   | 0.000676  |
|    n_updates       | 3241257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.28e+04 |
| time/              |           |
|    episodes        | 648       |
|    fps             | 16        |
|    time_elapsed    | 200077    |
|    total_timesteps | 3240000   |
----------------------------------
Eval num_timesteps=3250000, episode_reward=-5592.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.59e+03 |
| time/              |           |
|    total_timesteps | 3250000   |
| train/             |           |
|    actor_loss      | 561       |
|    critic_loss     | 2.37e+04  |
|    ent_coef        | 1.31      |
|    ent_coef_loss   | 0.0216    |
|    learning_rate   | 0.000675  |
|    n_updates       | 3251257   |
----------------------------------
Eval num_timesteps=3260000, episode_reward=-7678.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.68e+03 |
| time/              |           |
|    total_timesteps | 3260000   |
| train/             |           |
|    actor_loss      | 604       |
|    critic_loss     | 1.87e+04  |
|    ent_coef        | 1.17      |
|    ent_coef_loss   | -0.0338   |
|    learning_rate   | 0.000674  |
|    n_updates       | 3261257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.28e+04 |
| time/              |           |
|    episodes        | 652       |
|    fps             | 16        |
|    time_elapsed    | 201278    |
|    total_timesteps | 3260000   |
----------------------------------
Eval num_timesteps=3270000, episode_reward=-8193.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.19e+03 |
| time/              |           |
|    total_timesteps | 3270000   |
| train/             |           |
|    actor_loss      | 716       |
|    critic_loss     | 1.54e+04  |
|    ent_coef        | 0.563     |
|    ent_coef_loss   | 0.072     |
|    learning_rate   | 0.000673  |
|    n_updates       | 3271257   |
----------------------------------
Eval num_timesteps=3280000, episode_reward=-11786.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.18e+04 |
| time/              |           |
|    total_timesteps | 3280000   |
| train/             |           |
|    actor_loss      | 713       |
|    critic_loss     | 9.02e+03  |
|    ent_coef        | 0.506     |
|    ent_coef_loss   | -0.0333   |
|    learning_rate   | 0.000672  |
|    n_updates       | 3281257   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.3e+04 |
| time/              |          |
|    episodes        | 656      |
|    fps             | 16       |
|    time_elapsed    | 202475   |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=-11117.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+04 |
| time/              |           |
|    total_timesteps | 3290000   |
| train/             |           |
|    actor_loss      | 728       |
|    critic_loss     | 8.73e+03  |
|    ent_coef        | 0.436     |
|    ent_coef_loss   | -0.11     |
|    learning_rate   | 0.000671  |
|    n_updates       | 3291257   |
----------------------------------
Eval num_timesteps=3300000, episode_reward=-20066.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.01e+04 |
| time/              |           |
|    total_timesteps | 3300000   |
| train/             |           |
|    actor_loss      | 841       |
|    critic_loss     | 1.54e+04  |
|    ent_coef        | 0.46      |
|    ent_coef_loss   | -0.0749   |
|    learning_rate   | 0.00067   |
|    n_updates       | 3301257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.34e+04 |
| time/              |           |
|    episodes        | 660       |
|    fps             | 16        |
|    time_elapsed    | 203681    |
|    total_timesteps | 3300000   |
----------------------------------
Eval num_timesteps=3310000, episode_reward=-8708.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.71e+03 |
| time/              |           |
|    total_timesteps | 3310000   |
| train/             |           |
|    actor_loss      | 778       |
|    critic_loss     | 7.36e+03  |
|    ent_coef        | 0.577     |
|    ent_coef_loss   | 0.0277    |
|    learning_rate   | 0.000669  |
|    n_updates       | 3311257   |
----------------------------------
Eval num_timesteps=3320000, episode_reward=-12642.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.26e+04 |
| time/              |           |
|    total_timesteps | 3320000   |
| train/             |           |
|    actor_loss      | 783       |
|    critic_loss     | 1.09e+04  |
|    ent_coef        | 0.542     |
|    ent_coef_loss   | -0.191    |
|    learning_rate   | 0.000668  |
|    n_updates       | 3321257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.35e+04 |
| time/              |           |
|    episodes        | 664       |
|    fps             | 16        |
|    time_elapsed    | 204884    |
|    total_timesteps | 3320000   |
----------------------------------
Eval num_timesteps=3330000, episode_reward=-7857.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.86e+03 |
| time/              |           |
|    total_timesteps | 3330000   |
| train/             |           |
|    actor_loss      | 706       |
|    critic_loss     | 6.23e+03  |
|    ent_coef        | 0.471     |
|    ent_coef_loss   | -0.0262   |
|    learning_rate   | 0.000667  |
|    n_updates       | 3331257   |
----------------------------------
Eval num_timesteps=3340000, episode_reward=-15361.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+04 |
| time/              |           |
|    total_timesteps | 3340000   |
| train/             |           |
|    actor_loss      | 734       |
|    critic_loss     | 1.31e+04  |
|    ent_coef        | 0.431     |
|    ent_coef_loss   | 0.0193    |
|    learning_rate   | 0.000666  |
|    n_updates       | 3341257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.32e+04 |
| time/              |           |
|    episodes        | 668       |
|    fps             | 16        |
|    time_elapsed    | 206064    |
|    total_timesteps | 3340000   |
----------------------------------
Eval num_timesteps=3350000, episode_reward=-11943.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+04 |
| time/              |           |
|    total_timesteps | 3350000   |
| train/             |           |
|    actor_loss      | 833       |
|    critic_loss     | 2.89e+04  |
|    ent_coef        | 0.556     |
|    ent_coef_loss   | -0.167    |
|    learning_rate   | 0.000665  |
|    n_updates       | 3351257   |
----------------------------------
Eval num_timesteps=3360000, episode_reward=-4047.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.05e+03 |
| time/              |           |
|    total_timesteps | 3360000   |
| train/             |           |
|    actor_loss      | 785       |
|    critic_loss     | 2.91e+03  |
|    ent_coef        | 0.678     |
|    ent_coef_loss   | -0.132    |
|    learning_rate   | 0.000664  |
|    n_updates       | 3361257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.29e+04 |
| time/              |           |
|    episodes        | 672       |
|    fps             | 16        |
|    time_elapsed    | 207244    |
|    total_timesteps | 3360000   |
----------------------------------
Eval num_timesteps=3370000, episode_reward=-3175.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+03 |
| time/              |           |
|    total_timesteps | 3370000   |
| train/             |           |
|    actor_loss      | 779       |
|    critic_loss     | 2.19e+04  |
|    ent_coef        | 0.371     |
|    ent_coef_loss   | 0.0707    |
|    learning_rate   | 0.000663  |
|    n_updates       | 3371257   |
----------------------------------
Eval num_timesteps=3380000, episode_reward=-6132.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.13e+03 |
| time/              |           |
|    total_timesteps | 3380000   |
| train/             |           |
|    actor_loss      | 720       |
|    critic_loss     | 2.82e+03  |
|    ent_coef        | 0.381     |
|    ent_coef_loss   | -0.225    |
|    learning_rate   | 0.000662  |
|    n_updates       | 3381257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.26e+04 |
| time/              |           |
|    episodes        | 676       |
|    fps             | 16        |
|    time_elapsed    | 208422    |
|    total_timesteps | 3380000   |
----------------------------------
Eval num_timesteps=3390000, episode_reward=-14818.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+04 |
| time/              |           |
|    total_timesteps | 3390000   |
| train/             |           |
|    actor_loss      | 743       |
|    critic_loss     | 1.63e+03  |
|    ent_coef        | 0.299     |
|    ent_coef_loss   | -0.125    |
|    learning_rate   | 0.000661  |
|    n_updates       | 3391257   |
----------------------------------
Eval num_timesteps=3400000, episode_reward=-6382.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.38e+03 |
| time/              |           |
|    total_timesteps | 3400000   |
| train/             |           |
|    actor_loss      | 722       |
|    critic_loss     | 1.76e+03  |
|    ent_coef        | 0.322     |
|    ent_coef_loss   | -0.419    |
|    learning_rate   | 0.00066   |
|    n_updates       | 3401257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    episodes        | 680       |
|    fps             | 16        |
|    time_elapsed    | 209625    |
|    total_timesteps | 3400000   |
----------------------------------
Eval num_timesteps=3410000, episode_reward=-6629.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.63e+03 |
| time/              |           |
|    total_timesteps | 3410000   |
| train/             |           |
|    actor_loss      | 617       |
|    critic_loss     | 1.68e+04  |
|    ent_coef        | 0.412     |
|    ent_coef_loss   | -0.165    |
|    learning_rate   | 0.000659  |
|    n_updates       | 3411257   |
----------------------------------
Eval num_timesteps=3420000, episode_reward=-12203.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+04 |
| time/              |           |
|    total_timesteps | 3420000   |
| train/             |           |
|    actor_loss      | 776       |
|    critic_loss     | 2.67e+03  |
|    ent_coef        | 0.321     |
|    ent_coef_loss   | 0.179     |
|    learning_rate   | 0.000658  |
|    n_updates       | 3421257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.26e+04 |
| time/              |           |
|    episodes        | 684       |
|    fps             | 16        |
|    time_elapsed    | 210821    |
|    total_timesteps | 3420000   |
----------------------------------
Eval num_timesteps=3430000, episode_reward=-4835.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.84e+03 |
| time/              |           |
|    total_timesteps | 3430000   |
| train/             |           |
|    actor_loss      | 813       |
|    critic_loss     | 4.31e+03  |
|    ent_coef        | 0.4       |
|    ent_coef_loss   | 0.26      |
|    learning_rate   | 0.000657  |
|    n_updates       | 3431257   |
----------------------------------
Eval num_timesteps=3440000, episode_reward=-11938.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+04 |
| time/              |           |
|    total_timesteps | 3440000   |
| train/             |           |
|    actor_loss      | 875       |
|    critic_loss     | 2.33e+03  |
|    ent_coef        | 0.372     |
|    ent_coef_loss   | -0.0499   |
|    learning_rate   | 0.000656  |
|    n_updates       | 3441257   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.26e+04 |
| time/              |           |
|    episodes        | 688       |
|    fps             | 16        |
|    time_elapsed    | 212029    |
|    total_timesteps | 3440000   |
