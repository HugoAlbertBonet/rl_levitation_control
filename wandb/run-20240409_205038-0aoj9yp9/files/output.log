Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 5gdl/SAC_105
Eval num_timesteps=10000, episode_reward=-4908.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 10000     |
| train/             |           |
|    actor_loss      | -3.86     |
|    critic_loss     | 0.557     |
|    ent_coef        | 0.065     |
|    ent_coef_loss   | 86.8      |
|    learning_rate   | 0.000999  |
|    n_updates       | 2989596   |
----------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-4933.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | 30.1      |
|    critic_loss     | 0.48      |
|    ent_coef        | 0.0043    |
|    ent_coef_loss   | -17.1     |
|    learning_rate   | 0.000998  |
|    n_updates       | 2999596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 33        |
|    time_elapsed    | 590       |
|    total_timesteps | 20000     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-4930.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 30000     |
| train/             |           |
|    actor_loss      | 64.2      |
|    critic_loss     | 0.365     |
|    ent_coef        | 0.00151   |
|    ent_coef_loss   | -11.1     |
|    learning_rate   | 0.000997  |
|    n_updates       | 3009596   |
----------------------------------
Eval num_timesteps=40000, episode_reward=-4912.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 40000     |
| train/             |           |
|    actor_loss      | 84.4      |
|    critic_loss     | 0.564     |
|    ent_coef        | 0.0013    |
|    ent_coef_loss   | 2.73      |
|    learning_rate   | 0.000996  |
|    n_updates       | 3019596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 33       |
|    time_elapsed    | 1198     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-4934.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 50000     |
| train/             |           |
|    actor_loss      | 94.3      |
|    critic_loss     | 0.478     |
|    ent_coef        | 0.00123   |
|    ent_coef_loss   | 25.9      |
|    learning_rate   | 0.000995  |
|    n_updates       | 3029596   |
----------------------------------
Eval num_timesteps=60000, episode_reward=-4937.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 60000     |
| train/             |           |
|    actor_loss      | 97.6      |
|    critic_loss     | 0.396     |
|    ent_coef        | 0.00103   |
|    ent_coef_loss   | -9.52     |
|    learning_rate   | 0.000994  |
|    n_updates       | 3039596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 33        |
|    time_elapsed    | 1766      |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-4928.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 70000     |
| train/             |           |
|    actor_loss      | 102       |
|    critic_loss     | 0.809     |
|    ent_coef        | 0.000968  |
|    ent_coef_loss   | 10.4      |
|    learning_rate   | 0.000993  |
|    n_updates       | 3049596   |
----------------------------------
Eval num_timesteps=80000, episode_reward=-4922.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 80000     |
| train/             |           |
|    actor_loss      | 105       |
|    critic_loss     | 0.461     |
|    ent_coef        | 0.0017    |
|    ent_coef_loss   | -14.4     |
|    learning_rate   | 0.000992  |
|    n_updates       | 3059596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 34        |
|    time_elapsed    | 2325      |
|    total_timesteps | 80000     |
----------------------------------
Eval num_timesteps=90000, episode_reward=-4902.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 105      |
|    critic_loss     | 0.343    |
|    ent_coef        | 0.00095  |
|    ent_coef_loss   | -7.3     |
|    learning_rate   | 0.000991 |
|    n_updates       | 3069596  |
---------------------------------
New best mean reward!
Eval num_timesteps=100000, episode_reward=-4921.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 100000    |
| train/             |           |
|    actor_loss      | 105       |
|    critic_loss     | 0.142     |
|    ent_coef        | 0.000688  |
|    ent_coef_loss   | 5.14      |
|    learning_rate   | 0.00099   |
|    n_updates       | 3079596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 34        |
|    time_elapsed    | 2884      |
|    total_timesteps | 100000    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-4921.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 110000    |
| train/             |           |
|    actor_loss      | 104       |
|    critic_loss     | 0.284     |
|    ent_coef        | 0.000564  |
|    ent_coef_loss   | 15.8      |
|    learning_rate   | 0.000989  |
|    n_updates       | 3089596   |
----------------------------------
Eval num_timesteps=120000, episode_reward=-4893.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 120000    |
| train/             |           |
|    actor_loss      | 104       |
|    critic_loss     | 0.0957    |
|    ent_coef        | 0.000491  |
|    ent_coef_loss   | 5.57      |
|    learning_rate   | 0.000988  |
|    n_updates       | 3099596   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 34        |
|    time_elapsed    | 3446      |
|    total_timesteps | 120000    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-4940.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 130000    |
| train/             |           |
|    actor_loss      | 103       |
|    critic_loss     | 0.116     |
|    ent_coef        | 0.0005    |
|    ent_coef_loss   | -18.4     |
|    learning_rate   | 0.000987  |
|    n_updates       | 3109596   |
----------------------------------
Eval num_timesteps=140000, episode_reward=-4916.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 140000    |
| train/             |           |
|    actor_loss      | 103       |
|    critic_loss     | 0.235     |
|    ent_coef        | 0.000529  |
|    ent_coef_loss   | 3.26      |
|    learning_rate   | 0.000986  |
|    n_updates       | 3119596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 34        |
|    time_elapsed    | 4013      |
|    total_timesteps | 140000    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-4944.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 150000    |
| train/             |           |
|    actor_loss      | 102       |
|    critic_loss     | 0.493     |
|    ent_coef        | 0.000444  |
|    ent_coef_loss   | 27        |
|    learning_rate   | 0.000985  |
|    n_updates       | 3129596   |
----------------------------------
Eval num_timesteps=160000, episode_reward=-4924.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 160000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0568    |
|    ent_coef        | 0.000547  |
|    ent_coef_loss   | -2        |
|    learning_rate   | 0.000984  |
|    n_updates       | 3139596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 34        |
|    time_elapsed    | 4574      |
|    total_timesteps | 160000    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-4924.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 170000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0465    |
|    ent_coef        | 0.000321  |
|    ent_coef_loss   | -28.5     |
|    learning_rate   | 0.000983  |
|    n_updates       | 3149596   |
----------------------------------
Eval num_timesteps=180000, episode_reward=-4875.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 180000    |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0456    |
|    ent_coef        | 0.000507  |
|    ent_coef_loss   | -29.6     |
|    learning_rate   | 0.000982  |
|    n_updates       | 3159596   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 36        |
|    fps             | 35        |
|    time_elapsed    | 5133      |
|    total_timesteps | 180000    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-4893.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 190000    |
| train/             |           |
|    actor_loss      | 98        |
|    critic_loss     | 0.167     |
|    ent_coef        | 0.000574  |
|    ent_coef_loss   | -0.714    |
|    learning_rate   | 0.000981  |
|    n_updates       | 3169596   |
----------------------------------
Eval num_timesteps=200000, episode_reward=-4880.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 200000    |
| train/             |           |
|    actor_loss      | 97.1      |
|    critic_loss     | 0.318     |
|    ent_coef        | 0.00067   |
|    ent_coef_loss   | -9.47     |
|    learning_rate   | 0.00098   |
|    n_updates       | 3179596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 40        |
|    fps             | 35        |
|    time_elapsed    | 5692      |
|    total_timesteps | 200000    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-4889.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 210000    |
| train/             |           |
|    actor_loss      | 96.1      |
|    critic_loss     | 0.9       |
|    ent_coef        | 0.000725  |
|    ent_coef_loss   | 32.8      |
|    learning_rate   | 0.000979  |
|    n_updates       | 3189596   |
----------------------------------
Eval num_timesteps=220000, episode_reward=-4944.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 220000    |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.257     |
|    ent_coef        | 0.000616  |
|    ent_coef_loss   | 20.9      |
|    learning_rate   | 0.000978  |
|    n_updates       | 3199596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 35        |
|    time_elapsed    | 6252      |
|    total_timesteps | 220000    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-4918.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 230000    |
| train/             |           |
|    actor_loss      | 99.2      |
|    critic_loss     | 1.18      |
|    ent_coef        | 0.000521  |
|    ent_coef_loss   | 2.37      |
|    learning_rate   | 0.000977  |
|    n_updates       | 3209596   |
----------------------------------
Eval num_timesteps=240000, episode_reward=-4934.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 240000    |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.681     |
|    ent_coef        | 0.000885  |
|    ent_coef_loss   | -5.92     |
|    learning_rate   | 0.000976  |
|    n_updates       | 3219596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 35        |
|    time_elapsed    | 6812      |
|    total_timesteps | 240000    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-4917.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 250000    |
| train/             |           |
|    actor_loss      | 98        |
|    critic_loss     | 0.338     |
|    ent_coef        | 0.000818  |
|    ent_coef_loss   | 15.3      |
|    learning_rate   | 0.000975  |
|    n_updates       | 3229596   |
----------------------------------
Eval num_timesteps=260000, episode_reward=-4895.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 97.9     |
|    critic_loss     | 0.649    |
|    ent_coef        | 0.000862 |
|    ent_coef_loss   | 5.99     |
|    learning_rate   | 0.000974 |
|    n_updates       | 3239596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 35        |
|    time_elapsed    | 7374      |
|    total_timesteps | 260000    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-4909.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 270000    |
| train/             |           |
|    actor_loss      | 96.4      |
|    critic_loss     | 0.204     |
|    ent_coef        | 0.00112   |
|    ent_coef_loss   | 1.03      |
|    learning_rate   | 0.000973  |
|    n_updates       | 3249596   |
----------------------------------
Eval num_timesteps=280000, episode_reward=-4907.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 280000    |
| train/             |           |
|    actor_loss      | 96.3      |
|    critic_loss     | 1.37      |
|    ent_coef        | 0.000719  |
|    ent_coef_loss   | 21.2      |
|    learning_rate   | 0.000972  |
|    n_updates       | 3259596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 35        |
|    time_elapsed    | 7936      |
|    total_timesteps | 280000    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-4917.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 290000    |
| train/             |           |
|    actor_loss      | 97.5      |
|    critic_loss     | 0.127     |
|    ent_coef        | 0.000664  |
|    ent_coef_loss   | 7.85      |
|    learning_rate   | 0.000971  |
|    n_updates       | 3269596   |
----------------------------------
Eval num_timesteps=300000, episode_reward=-4909.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 300000    |
| train/             |           |
|    actor_loss      | 97.4      |
|    critic_loss     | 0.425     |
|    ent_coef        | 0.0011    |
|    ent_coef_loss   | -1.15     |
|    learning_rate   | 0.00097   |
|    n_updates       | 3279596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 35        |
|    time_elapsed    | 8496      |
|    total_timesteps | 300000    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-4938.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 310000    |
| train/             |           |
|    actor_loss      | 93.6      |
|    critic_loss     | 0.664     |
|    ent_coef        | 0.00202   |
|    ent_coef_loss   | 14.9      |
|    learning_rate   | 0.000969  |
|    n_updates       | 3289596   |
----------------------------------
Eval num_timesteps=320000, episode_reward=-4940.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 320000    |
| train/             |           |
|    actor_loss      | 93.9      |
|    critic_loss     | 0.171     |
|    ent_coef        | 0.000564  |
|    ent_coef_loss   | 0.728     |
|    learning_rate   | 0.000968  |
|    n_updates       | 3299596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 64        |
|    fps             | 35        |
|    time_elapsed    | 9054      |
|    total_timesteps | 320000    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-4893.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 330000    |
| train/             |           |
|    actor_loss      | 95.9      |
|    critic_loss     | 0.161     |
|    ent_coef        | 0.000374  |
|    ent_coef_loss   | -2.9      |
|    learning_rate   | 0.000967  |
|    n_updates       | 3309596   |
----------------------------------
Eval num_timesteps=340000, episode_reward=-4906.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 340000    |
| train/             |           |
|    actor_loss      | 98.1      |
|    critic_loss     | 0.146     |
|    ent_coef        | 0.000315  |
|    ent_coef_loss   | 3.43      |
|    learning_rate   | 0.000966  |
|    n_updates       | 3319596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 35        |
|    time_elapsed    | 9612      |
|    total_timesteps | 340000    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-4865.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 350000    |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0623    |
|    ent_coef        | 0.000257  |
|    ent_coef_loss   | -8.49     |
|    learning_rate   | 0.000965  |
|    n_updates       | 3329596   |
----------------------------------
New best mean reward!
Eval num_timesteps=360000, episode_reward=-4883.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 360000    |
| train/             |           |
|    actor_loss      | 99.5      |
|    critic_loss     | 0.802     |
|    ent_coef        | 0.00017   |
|    ent_coef_loss   | 1.69      |
|    learning_rate   | 0.000964  |
|    n_updates       | 3339596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 72        |
|    fps             | 35        |
|    time_elapsed    | 10173     |
|    total_timesteps | 360000    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-4812.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.81e+03 |
| time/              |           |
|    total_timesteps | 370000    |
| train/             |           |
|    actor_loss      | 99.8      |
|    critic_loss     | 0.0223    |
|    ent_coef        | 0.000153  |
|    ent_coef_loss   | -20.9     |
|    learning_rate   | 0.000963  |
|    n_updates       | 3349596   |
----------------------------------
New best mean reward!
Eval num_timesteps=380000, episode_reward=-4860.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 380000    |
| train/             |           |
|    actor_loss      | 99.5      |
|    critic_loss     | 0.0571    |
|    ent_coef        | 0.000148  |
|    ent_coef_loss   | -10.7     |
|    learning_rate   | 0.000962  |
|    n_updates       | 3359596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 76        |
|    fps             | 35        |
|    time_elapsed    | 10727     |
|    total_timesteps | 380000    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-4837.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.84e+03 |
| time/              |           |
|    total_timesteps | 390000    |
| train/             |           |
|    actor_loss      | 99.7      |
|    critic_loss     | 0.0253    |
|    ent_coef        | 7.47e-05  |
|    ent_coef_loss   | 13.9      |
|    learning_rate   | 0.000961  |
|    n_updates       | 3369596   |
----------------------------------
Eval num_timesteps=400000, episode_reward=-4889.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 400000    |
| train/             |           |
|    actor_loss      | 99.7      |
|    critic_loss     | 0.42      |
|    ent_coef        | 8.16e-05  |
|    ent_coef_loss   | -24.8     |
|    learning_rate   | 0.00096   |
|    n_updates       | 3379596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 80        |
|    fps             | 35        |
|    time_elapsed    | 11283     |
|    total_timesteps | 400000    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-4881.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 410000    |
| train/             |           |
|    actor_loss      | 99.3      |
|    critic_loss     | 0.123     |
|    ent_coef        | 4.9e-05   |
|    ent_coef_loss   | 100       |
|    learning_rate   | 0.000959  |
|    n_updates       | 3389596   |
----------------------------------
Eval num_timesteps=420000, episode_reward=-4866.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 420000    |
| train/             |           |
|    actor_loss      | 99.5      |
|    critic_loss     | 0.0236    |
|    ent_coef        | 9.14e-05  |
|    ent_coef_loss   | -10.4     |
|    learning_rate   | 0.000958  |
|    n_updates       | 3399596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 84        |
|    fps             | 35        |
|    time_elapsed    | 11844     |
|    total_timesteps | 420000    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-4852.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.85e+03 |
| time/              |           |
|    total_timesteps | 430000    |
| train/             |           |
|    actor_loss      | 97.4      |
|    critic_loss     | 0.451     |
|    ent_coef        | 0.000496  |
|    ent_coef_loss   | 43.3      |
|    learning_rate   | 0.000957  |
|    n_updates       | 3409596   |
----------------------------------
Eval num_timesteps=440000, episode_reward=-4921.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 440000    |
| train/             |           |
|    actor_loss      | 94.1      |
|    critic_loss     | 0.0769    |
|    ent_coef        | 0.000462  |
|    ent_coef_loss   | 1.28      |
|    learning_rate   | 0.000956  |
|    n_updates       | 3419596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 35       |
|    time_elapsed    | 12402    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-4928.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 450000    |
| train/             |           |
|    actor_loss      | 95.8      |
|    critic_loss     | 0.0771    |
|    ent_coef        | 0.000568  |
|    ent_coef_loss   | 10.5      |
|    learning_rate   | 0.000955  |
|    n_updates       | 3429596   |
----------------------------------
Eval num_timesteps=460000, episode_reward=-4939.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 460000    |
| train/             |           |
|    actor_loss      | 96.1      |
|    critic_loss     | 0.031     |
|    ent_coef        | 0.000924  |
|    ent_coef_loss   | -4.36     |
|    learning_rate   | 0.000954  |
|    n_updates       | 3439596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 35       |
|    time_elapsed    | 12968    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-4948.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 470000    |
| train/             |           |
|    actor_loss      | 95.7      |
|    critic_loss     | 0.242     |
|    ent_coef        | 0.00109   |
|    ent_coef_loss   | 8.99      |
|    learning_rate   | 0.000953  |
|    n_updates       | 3449596   |
----------------------------------
Eval num_timesteps=480000, episode_reward=-4939.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 480000    |
| train/             |           |
|    actor_loss      | 96.7      |
|    critic_loss     | 0.0901    |
|    ent_coef        | 0.000871  |
|    ent_coef_loss   | 0.82      |
|    learning_rate   | 0.000952  |
|    n_updates       | 3459596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 96        |
|    fps             | 35        |
|    time_elapsed    | 13529     |
|    total_timesteps | 480000    |
----------------------------------
Eval num_timesteps=490000, episode_reward=-4775.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.78e+03 |
| time/              |           |
|    total_timesteps | 490000    |
| train/             |           |
|    actor_loss      | 97.4      |
|    critic_loss     | 0.0709    |
|    ent_coef        | 0.000386  |
|    ent_coef_loss   | 5.14      |
|    learning_rate   | 0.000951  |
|    n_updates       | 3469596   |
----------------------------------
New best mean reward!
Eval num_timesteps=500000, episode_reward=-4888.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 500000    |
| train/             |           |
|    actor_loss      | 97.6      |
|    critic_loss     | 0.0844    |
|    ent_coef        | 0.000369  |
|    ent_coef_loss   | -23.5     |
|    learning_rate   | 0.00095   |
|    n_updates       | 3479596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 100       |
|    fps             | 35        |
|    time_elapsed    | 14090     |
|    total_timesteps | 500000    |
----------------------------------
Eval num_timesteps=510000, episode_reward=-4865.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 510000    |
| train/             |           |
|    actor_loss      | 97.2      |
|    critic_loss     | 0.0768    |
|    ent_coef        | 0.000565  |
|    ent_coef_loss   | 9.96      |
|    learning_rate   | 0.000949  |
|    n_updates       | 3489596   |
----------------------------------
Eval num_timesteps=520000, episode_reward=-4884.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 520000    |
| train/             |           |
|    actor_loss      | 97.1      |
|    critic_loss     | 0.0574    |
|    ent_coef        | 0.000564  |
|    ent_coef_loss   | -10.7     |
|    learning_rate   | 0.000948  |
|    n_updates       | 3499596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 104       |
|    fps             | 35        |
|    time_elapsed    | 14651     |
|    total_timesteps | 520000    |
----------------------------------
Eval num_timesteps=530000, episode_reward=-4877.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 530000    |
| train/             |           |
|    actor_loss      | 95.4      |
|    critic_loss     | 0.0331    |
|    ent_coef        | 0.000994  |
|    ent_coef_loss   | -9.3      |
|    learning_rate   | 0.000947  |
|    n_updates       | 3509596   |
----------------------------------
Eval num_timesteps=540000, episode_reward=-4902.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 95.3     |
|    critic_loss     | 0.0423   |
|    ent_coef        | 0.000767 |
|    ent_coef_loss   | -7.45    |
|    learning_rate   | 0.000946 |
|    n_updates       | 3519596  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 35       |
|    time_elapsed    | 15213    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-4902.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 94.7     |
|    critic_loss     | 0.0344   |
|    ent_coef        | 0.000492 |
|    ent_coef_loss   | -7.13    |
|    learning_rate   | 0.000945 |
|    n_updates       | 3529596  |
---------------------------------
Eval num_timesteps=560000, episode_reward=-4847.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.85e+03 |
| time/              |           |
|    total_timesteps | 560000    |
| train/             |           |
|    actor_loss      | 95.3      |
|    critic_loss     | 0.0518    |
|    ent_coef        | 0.00072   |
|    ent_coef_loss   | -0.81     |
|    learning_rate   | 0.000944  |
|    n_updates       | 3539596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 35       |
|    time_elapsed    | 15785    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-4809.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.81e+03 |
| time/              |           |
|    total_timesteps | 570000    |
| train/             |           |
|    actor_loss      | 94.9      |
|    critic_loss     | 0.0776    |
|    ent_coef        | 0.000708  |
|    ent_coef_loss   | 3.89      |
|    learning_rate   | 0.000943  |
|    n_updates       | 3549596   |
----------------------------------
Eval num_timesteps=580000, episode_reward=-4807.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.81e+03 |
| time/              |           |
|    total_timesteps | 580000    |
| train/             |           |
|    actor_loss      | 95.6      |
|    critic_loss     | 0.0407    |
|    ent_coef        | 0.000901  |
|    ent_coef_loss   | 6.65      |
|    learning_rate   | 0.000942  |
|    n_updates       | 3559596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 35       |
|    time_elapsed    | 16348    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-4881.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 590000    |
| train/             |           |
|    actor_loss      | 95.6      |
|    critic_loss     | 0.0469    |
|    ent_coef        | 0.000435  |
|    ent_coef_loss   | -2.41     |
|    learning_rate   | 0.000941  |
|    n_updates       | 3569596   |
----------------------------------
Eval num_timesteps=600000, episode_reward=-4854.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.85e+03 |
| time/              |           |
|    total_timesteps | 600000    |
| train/             |           |
|    actor_loss      | 96.7      |
|    critic_loss     | 0.0659    |
|    ent_coef        | 0.000359  |
|    ent_coef_loss   | 15.1      |
|    learning_rate   | 0.00094   |
|    n_updates       | 3579596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 35       |
|    time_elapsed    | 16913    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-4876.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 610000    |
| train/             |           |
|    actor_loss      | 97.4      |
|    critic_loss     | 0.0449    |
|    ent_coef        | 0.000238  |
|    ent_coef_loss   | 12.9      |
|    learning_rate   | 0.000939  |
|    n_updates       | 3589596   |
----------------------------------
Eval num_timesteps=620000, episode_reward=-4833.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.83e+03 |
| time/              |           |
|    total_timesteps | 620000    |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0478    |
|    ent_coef        | 0.000242  |
|    ent_coef_loss   | -22.8     |
|    learning_rate   | 0.000938  |
|    n_updates       | 3599596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 124       |
|    fps             | 35        |
|    time_elapsed    | 17477     |
|    total_timesteps | 620000    |
----------------------------------
Eval num_timesteps=630000, episode_reward=-4933.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 630000    |
| train/             |           |
|    actor_loss      | 99        |
|    critic_loss     | 0.0885    |
|    ent_coef        | 0.000202  |
|    ent_coef_loss   | -9.62     |
|    learning_rate   | 0.000937  |
|    n_updates       | 3609596   |
----------------------------------
Eval num_timesteps=640000, episode_reward=-4909.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 640000    |
| train/             |           |
|    actor_loss      | 99.7      |
|    critic_loss     | 0.0848    |
|    ent_coef        | 0.000214  |
|    ent_coef_loss   | -3.21     |
|    learning_rate   | 0.000936  |
|    n_updates       | 3619596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 128       |
|    fps             | 35        |
|    time_elapsed    | 18044     |
|    total_timesteps | 640000    |
----------------------------------
Eval num_timesteps=650000, episode_reward=-4892.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 650000    |
| train/             |           |
|    actor_loss      | 99.4      |
|    critic_loss     | 0.0305    |
|    ent_coef        | 0.000345  |
|    ent_coef_loss   | 2.47      |
|    learning_rate   | 0.000935  |
|    n_updates       | 3629596   |
----------------------------------
Eval num_timesteps=660000, episode_reward=-4876.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 660000    |
| train/             |           |
|    actor_loss      | 99.1      |
|    critic_loss     | 0.0511    |
|    ent_coef        | 0.000389  |
|    ent_coef_loss   | -19.6     |
|    learning_rate   | 0.000934  |
|    n_updates       | 3639596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 132       |
|    fps             | 35        |
|    time_elapsed    | 18620     |
|    total_timesteps | 660000    |
----------------------------------
Eval num_timesteps=670000, episode_reward=-4897.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | 98.9     |
|    critic_loss     | 0.0347   |
|    ent_coef        | 0.000273 |
|    ent_coef_loss   | 5.99     |
|    learning_rate   | 0.000933 |
|    n_updates       | 3649596  |
---------------------------------
Eval num_timesteps=680000, episode_reward=-4763.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.76e+03 |
| time/              |           |
|    total_timesteps | 680000    |
| train/             |           |
|    actor_loss      | 99.3      |
|    critic_loss     | 0.0244    |
|    ent_coef        | 0.000217  |
|    ent_coef_loss   | 1.01      |
|    learning_rate   | 0.000932  |
|    n_updates       | 3659596   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 136       |
|    fps             | 35        |
|    time_elapsed    | 19199     |
|    total_timesteps | 680000    |
----------------------------------
Eval num_timesteps=690000, episode_reward=-4888.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 690000    |
| train/             |           |
|    actor_loss      | 99.2      |
|    critic_loss     | 0.0414    |
|    ent_coef        | 0.000188  |
|    ent_coef_loss   | -9.18     |
|    learning_rate   | 0.000931  |
|    n_updates       | 3669596   |
----------------------------------
Eval num_timesteps=700000, episode_reward=-4830.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.83e+03 |
| time/              |           |
|    total_timesteps | 700000    |
| train/             |           |
|    actor_loss      | 99.1      |
|    critic_loss     | 0.0471    |
|    ent_coef        | 0.000252  |
|    ent_coef_loss   | -17.2     |
|    learning_rate   | 0.00093   |
|    n_updates       | 3679596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 140       |
|    fps             | 35        |
|    time_elapsed    | 19770     |
|    total_timesteps | 700000    |
----------------------------------
Eval num_timesteps=710000, episode_reward=-4876.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 710000    |
| train/             |           |
|    actor_loss      | 99.2      |
|    critic_loss     | 0.0533    |
|    ent_coef        | 0.000226  |
|    ent_coef_loss   | -15.7     |
|    learning_rate   | 0.000929  |
|    n_updates       | 3689596   |
----------------------------------
Eval num_timesteps=720000, episode_reward=-4829.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.83e+03 |
| time/              |           |
|    total_timesteps | 720000    |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0647    |
|    ent_coef        | 0.000251  |
|    ent_coef_loss   | 22.5      |
|    learning_rate   | 0.000928  |
|    n_updates       | 3699596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 144       |
|    fps             | 35        |
|    time_elapsed    | 20336     |
|    total_timesteps | 720000    |
----------------------------------
Eval num_timesteps=730000, episode_reward=-4898.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | 98.4     |
|    critic_loss     | 0.0319   |
|    ent_coef        | 0.000357 |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.000927 |
|    n_updates       | 3709596  |
---------------------------------
Eval num_timesteps=740000, episode_reward=-4912.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 740000    |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0638    |
|    ent_coef        | 0.000326  |
|    ent_coef_loss   | 1.39      |
|    learning_rate   | 0.000926  |
|    n_updates       | 3719596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 148       |
|    fps             | 35        |
|    time_elapsed    | 20898     |
|    total_timesteps | 740000    |
----------------------------------
Eval num_timesteps=750000, episode_reward=-4938.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 750000    |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0259    |
|    ent_coef        | 0.000347  |
|    ent_coef_loss   | -28.2     |
|    learning_rate   | 0.000925  |
|    n_updates       | 3729596   |
----------------------------------
Eval num_timesteps=760000, episode_reward=-4938.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 760000    |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0396    |
|    ent_coef        | 0.000338  |
|    ent_coef_loss   | 4.5       |
|    learning_rate   | 0.000924  |
|    n_updates       | 3739596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 152       |
|    fps             | 35        |
|    time_elapsed    | 21451     |
|    total_timesteps | 760000    |
----------------------------------
Eval num_timesteps=770000, episode_reward=-4947.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 770000    |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0405    |
|    ent_coef        | 0.000386  |
|    ent_coef_loss   | -1.88     |
|    learning_rate   | 0.000923  |
|    n_updates       | 3749596   |
----------------------------------
Eval num_timesteps=780000, episode_reward=-4940.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 780000    |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0255    |
|    ent_coef        | 0.000246  |
|    ent_coef_loss   | -13.1     |
|    learning_rate   | 0.000922  |
|    n_updates       | 3759596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 156       |
|    fps             | 35        |
|    time_elapsed    | 22013     |
|    total_timesteps | 780000    |
----------------------------------
Eval num_timesteps=790000, episode_reward=-4945.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 790000    |
| train/             |           |
|    actor_loss      | 99.3      |
|    critic_loss     | 0.0368    |
|    ent_coef        | 0.000299  |
|    ent_coef_loss   | -0.672    |
|    learning_rate   | 0.000921  |
|    n_updates       | 3769596   |
----------------------------------
Eval num_timesteps=800000, episode_reward=-4912.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 800000    |
| train/             |           |
|    actor_loss      | 99.5      |
|    critic_loss     | 0.044     |
|    ent_coef        | 0.000185  |
|    ent_coef_loss   | 9.02      |
|    learning_rate   | 0.00092   |
|    n_updates       | 3779596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 160       |
|    fps             | 35        |
|    time_elapsed    | 22578     |
|    total_timesteps | 800000    |
----------------------------------
Eval num_timesteps=810000, episode_reward=-4925.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 810000    |
| train/             |           |
|    actor_loss      | 99.8      |
|    critic_loss     | 0.0214    |
|    ent_coef        | 8.05e-05  |
|    ent_coef_loss   | 11.4      |
|    learning_rate   | 0.000919  |
|    n_updates       | 3789596   |
----------------------------------
Eval num_timesteps=820000, episode_reward=-4917.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 820000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0462    |
|    ent_coef        | 0.000123  |
|    ent_coef_loss   | 2.51      |
|    learning_rate   | 0.000918  |
|    n_updates       | 3799596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 164       |
|    fps             | 35        |
|    time_elapsed    | 23138     |
|    total_timesteps | 820000    |
----------------------------------
Eval num_timesteps=830000, episode_reward=-4930.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 830000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0212    |
|    ent_coef        | 0.000162  |
|    ent_coef_loss   | -0.613    |
|    learning_rate   | 0.000917  |
|    n_updates       | 3809596   |
----------------------------------
Eval num_timesteps=840000, episode_reward=-4934.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 840000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.00623   |
|    ent_coef        | 0.000144  |
|    ent_coef_loss   | 15.1      |
|    learning_rate   | 0.000916  |
|    n_updates       | 3819596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 168       |
|    fps             | 35        |
|    time_elapsed    | 23711     |
|    total_timesteps | 840000    |
----------------------------------
Eval num_timesteps=850000, episode_reward=-4896.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | 100      |
|    critic_loss     | 0.0282   |
|    ent_coef        | 0.000133 |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.000915 |
|    n_updates       | 3829596  |
---------------------------------
Eval num_timesteps=860000, episode_reward=-4910.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 860000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0075    |
|    ent_coef        | 0.00015   |
|    ent_coef_loss   | 7.46      |
|    learning_rate   | 0.000914  |
|    n_updates       | 3839596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 172       |
|    fps             | 35        |
|    time_elapsed    | 24281     |
|    total_timesteps | 860000    |
----------------------------------
Eval num_timesteps=870000, episode_reward=-4933.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 870000    |
| train/             |           |
|    actor_loss      | 100       |
|    critic_loss     | 0.0143    |
|    ent_coef        | 0.000213  |
|    ent_coef_loss   | 4.39      |
|    learning_rate   | 0.000913  |
|    n_updates       | 3849596   |
----------------------------------
Eval num_timesteps=880000, episode_reward=-4943.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 880000    |
| train/             |           |
|    actor_loss      | 99.6      |
|    critic_loss     | 0.0691    |
|    ent_coef        | 0.000313  |
|    ent_coef_loss   | -3.13     |
|    learning_rate   | 0.000912  |
|    n_updates       | 3859596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 176       |
|    fps             | 35        |
|    time_elapsed    | 24846     |
|    total_timesteps | 880000    |
----------------------------------
Eval num_timesteps=890000, episode_reward=-4935.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 890000    |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0222    |
|    ent_coef        | 0.000298  |
|    ent_coef_loss   | -48.4     |
|    learning_rate   | 0.000911  |
|    n_updates       | 3869596   |
----------------------------------
Eval num_timesteps=900000, episode_reward=-4875.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 900000    |
| train/             |           |
|    actor_loss      | 98.1      |
|    critic_loss     | 0.0196    |
|    ent_coef        | 0.000333  |
|    ent_coef_loss   | 14.9      |
|    learning_rate   | 0.00091   |
|    n_updates       | 3879596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 180       |
|    fps             | 35        |
|    time_elapsed    | 25416     |
|    total_timesteps | 900000    |
----------------------------------
Eval num_timesteps=910000, episode_reward=-4879.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 910000    |
| train/             |           |
|    actor_loss      | 97.3      |
|    critic_loss     | 0.0837    |
|    ent_coef        | 0.000305  |
|    ent_coef_loss   | 3.98      |
|    learning_rate   | 0.000909  |
|    n_updates       | 3889596   |
----------------------------------
Eval num_timesteps=920000, episode_reward=-4862.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 920000    |
| train/             |           |
|    actor_loss      | 97        |
|    critic_loss     | 0.0342    |
|    ent_coef        | 0.000558  |
|    ent_coef_loss   | 10.8      |
|    learning_rate   | 0.000908  |
|    n_updates       | 3899596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 184       |
|    fps             | 35        |
|    time_elapsed    | 25976     |
|    total_timesteps | 920000    |
----------------------------------
Eval num_timesteps=930000, episode_reward=-4868.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 930000    |
| train/             |           |
|    actor_loss      | 96        |
|    critic_loss     | 0.0465    |
|    ent_coef        | 0.000779  |
|    ent_coef_loss   | 2.29      |
|    learning_rate   | 0.000907  |
|    n_updates       | 3909596   |
----------------------------------
Eval num_timesteps=940000, episode_reward=-4890.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 940000    |
| train/             |           |
|    actor_loss      | 94.5      |
|    critic_loss     | 0.045     |
|    ent_coef        | 0.00113   |
|    ent_coef_loss   | -10.2     |
|    learning_rate   | 0.000906  |
|    n_updates       | 3919596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 188       |
|    fps             | 35        |
|    time_elapsed    | 26548     |
|    total_timesteps | 940000    |
----------------------------------
Eval num_timesteps=950000, episode_reward=-4861.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 950000    |
| train/             |           |
|    actor_loss      | 93.2      |
|    critic_loss     | 0.0525    |
|    ent_coef        | 0.00131   |
|    ent_coef_loss   | -6.35     |
|    learning_rate   | 0.000905  |
|    n_updates       | 3929596   |
----------------------------------
Eval num_timesteps=960000, episode_reward=-4886.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 960000    |
| train/             |           |
|    actor_loss      | 92.7      |
|    critic_loss     | 0.0185    |
|    ent_coef        | 0.00125   |
|    ent_coef_loss   | -6.75     |
|    learning_rate   | 0.000904  |
|    n_updates       | 3939596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 192       |
|    fps             | 35        |
|    time_elapsed    | 27115     |
|    total_timesteps | 960000    |
----------------------------------
Eval num_timesteps=970000, episode_reward=-4864.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 970000    |
| train/             |           |
|    actor_loss      | 93        |
|    critic_loss     | 0.066     |
|    ent_coef        | 0.00062   |
|    ent_coef_loss   | -9.91     |
|    learning_rate   | 0.000903  |
|    n_updates       | 3949596   |
----------------------------------
Eval num_timesteps=980000, episode_reward=-4920.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 980000    |
| train/             |           |
|    actor_loss      | 94.9      |
|    critic_loss     | 0.0473    |
|    ent_coef        | 0.000298  |
|    ent_coef_loss   | -11.8     |
|    learning_rate   | 0.000902  |
|    n_updates       | 3959596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 196       |
|    fps             | 35        |
|    time_elapsed    | 27678     |
|    total_timesteps | 980000    |
----------------------------------
Eval num_timesteps=990000, episode_reward=-4915.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 990000    |
| train/             |           |
|    actor_loss      | 96.4      |
|    critic_loss     | 0.0572    |
|    ent_coef        | 0.000112  |
|    ent_coef_loss   | 30.7      |
|    learning_rate   | 0.000901  |
|    n_updates       | 3969596   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=-4917.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1000000   |
| train/             |           |
|    actor_loss      | 97.7      |
|    critic_loss     | 0.315     |
|    ent_coef        | 8.09e-05  |
|    ent_coef_loss   | -7.55     |
|    learning_rate   | 0.0009    |
|    n_updates       | 3979596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 200       |
|    fps             | 35        |
|    time_elapsed    | 28246     |
|    total_timesteps | 1000000   |
----------------------------------
Eval num_timesteps=1010000, episode_reward=-4910.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1010000   |
| train/             |           |
|    actor_loss      | 98        |
|    critic_loss     | 0.318     |
|    ent_coef        | 0.000101  |
|    ent_coef_loss   | 69.3      |
|    learning_rate   | 0.000899  |
|    n_updates       | 3989596   |
----------------------------------
Eval num_timesteps=1020000, episode_reward=-4883.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1020000   |
| train/             |           |
|    actor_loss      | 97.3      |
|    critic_loss     | 0.105     |
|    ent_coef        | 0.000221  |
|    ent_coef_loss   | -17       |
|    learning_rate   | 0.000898  |
|    n_updates       | 3999596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 204       |
|    fps             | 35        |
|    time_elapsed    | 28822     |
|    total_timesteps | 1020000   |
----------------------------------
Eval num_timesteps=1030000, episode_reward=-4871.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1030000   |
| train/             |           |
|    actor_loss      | 94.2      |
|    critic_loss     | 0.425     |
|    ent_coef        | 0.000623  |
|    ent_coef_loss   | 13.8      |
|    learning_rate   | 0.000897  |
|    n_updates       | 4009596   |
----------------------------------
Eval num_timesteps=1040000, episode_reward=-4912.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1040000   |
| train/             |           |
|    actor_loss      | 92.9      |
|    critic_loss     | 0.0982    |
|    ent_coef        | 0.00141   |
|    ent_coef_loss   | 1.2       |
|    learning_rate   | 0.000896  |
|    n_updates       | 4019596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 208       |
|    fps             | 35        |
|    time_elapsed    | 29408     |
|    total_timesteps | 1040000   |
----------------------------------
Eval num_timesteps=1050000, episode_reward=-4874.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1050000   |
| train/             |           |
|    actor_loss      | 76.7      |
|    critic_loss     | 0.0452    |
|    ent_coef        | 0.00297   |
|    ent_coef_loss   | 4.18      |
|    learning_rate   | 0.000895  |
|    n_updates       | 4029596   |
----------------------------------
Eval num_timesteps=1060000, episode_reward=-4921.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1060000   |
| train/             |           |
|    actor_loss      | 81.2      |
|    critic_loss     | 0.0469    |
|    ent_coef        | 0.000377  |
|    ent_coef_loss   | 4.23      |
|    learning_rate   | 0.000894  |
|    n_updates       | 4039596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 212       |
|    fps             | 35        |
|    time_elapsed    | 29972     |
|    total_timesteps | 1060000   |
----------------------------------
Eval num_timesteps=1070000, episode_reward=-4920.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1070000   |
| train/             |           |
|    actor_loss      | 86.8      |
|    critic_loss     | 0.158     |
|    ent_coef        | 0.000408  |
|    ent_coef_loss   | 8.01      |
|    learning_rate   | 0.000893  |
|    n_updates       | 4049596   |
----------------------------------
Eval num_timesteps=1080000, episode_reward=-4897.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | 90.1     |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.000892 |
|    n_updates       | 4059596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 216       |
|    fps             | 35        |
|    time_elapsed    | 30538     |
|    total_timesteps | 1080000   |
----------------------------------
Eval num_timesteps=1090000, episode_reward=-4905.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1090000   |
| train/             |           |
|    actor_loss      | 92.4      |
|    critic_loss     | 0.0564    |
|    ent_coef        | 0.000416  |
|    ent_coef_loss   | 2.12      |
|    learning_rate   | 0.000891  |
|    n_updates       | 4069596   |
----------------------------------
Eval num_timesteps=1100000, episode_reward=-4905.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1100000   |
| train/             |           |
|    actor_loss      | 93.5      |
|    critic_loss     | 0.164     |
|    ent_coef        | 0.000225  |
|    ent_coef_loss   | -5.68     |
|    learning_rate   | 0.00089   |
|    n_updates       | 4079596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 220       |
|    fps             | 35        |
|    time_elapsed    | 31109     |
|    total_timesteps | 1100000   |
----------------------------------
Eval num_timesteps=1110000, episode_reward=-4767.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.77e+03 |
| time/              |           |
|    total_timesteps | 1110000   |
| train/             |           |
|    actor_loss      | 95.9      |
|    critic_loss     | 0.0791    |
|    ent_coef        | 0.000392  |
|    ent_coef_loss   | -5.94     |
|    learning_rate   | 0.000889  |
|    n_updates       | 4089596   |
----------------------------------
Eval num_timesteps=1120000, episode_reward=-4866.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1120000   |
| train/             |           |
|    actor_loss      | 93.9      |
|    critic_loss     | 0.101     |
|    ent_coef        | 0.000502  |
|    ent_coef_loss   | -29.2     |
|    learning_rate   | 0.000888  |
|    n_updates       | 4099596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 224       |
|    fps             | 35        |
|    time_elapsed    | 31688     |
|    total_timesteps | 1120000   |
----------------------------------
Eval num_timesteps=1130000, episode_reward=-4875.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1130000   |
| train/             |           |
|    actor_loss      | 93.3      |
|    critic_loss     | 0.0911    |
|    ent_coef        | 0.000605  |
|    ent_coef_loss   | -7.06     |
|    learning_rate   | 0.000887  |
|    n_updates       | 4109596   |
----------------------------------
Eval num_timesteps=1140000, episode_reward=-4871.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1140000   |
| train/             |           |
|    actor_loss      | 92.4      |
|    critic_loss     | 0.103     |
|    ent_coef        | 0.00128   |
|    ent_coef_loss   | 2.44      |
|    learning_rate   | 0.000886  |
|    n_updates       | 4119596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 228       |
|    fps             | 35        |
|    time_elapsed    | 32267     |
|    total_timesteps | 1140000   |
----------------------------------
Eval num_timesteps=1150000, episode_reward=-4873.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1150000   |
| train/             |           |
|    actor_loss      | 90        |
|    critic_loss     | 0.0677    |
|    ent_coef        | 0.00135   |
|    ent_coef_loss   | -0.826    |
|    learning_rate   | 0.000885  |
|    n_updates       | 4129596   |
----------------------------------
Eval num_timesteps=1160000, episode_reward=-4900.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | 91.7     |
|    critic_loss     | 0.0395   |
|    ent_coef        | 0.000416 |
|    ent_coef_loss   | -5.72    |
|    learning_rate   | 0.000884 |
|    n_updates       | 4139596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 232       |
|    fps             | 35        |
|    time_elapsed    | 32839     |
|    total_timesteps | 1160000   |
----------------------------------
Eval num_timesteps=1170000, episode_reward=-4898.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | 93.6     |
|    critic_loss     | 0.0716   |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000883 |
|    n_updates       | 4149596  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=-4899.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | 94.7     |
|    critic_loss     | 0.0371   |
|    ent_coef        | 0.000474 |
|    ent_coef_loss   | 16.4     |
|    learning_rate   | 0.000882 |
|    n_updates       | 4159596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 236       |
|    fps             | 35        |
|    time_elapsed    | 33416     |
|    total_timesteps | 1180000   |
----------------------------------
Eval num_timesteps=1190000, episode_reward=-4908.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1190000   |
| train/             |           |
|    actor_loss      | 94.9      |
|    critic_loss     | 0.0914    |
|    ent_coef        | 0.000437  |
|    ent_coef_loss   | -10.7     |
|    learning_rate   | 0.000881  |
|    n_updates       | 4169596   |
----------------------------------
Eval num_timesteps=1200000, episode_reward=-4905.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1200000   |
| train/             |           |
|    actor_loss      | 95.8      |
|    critic_loss     | 0.0857    |
|    ent_coef        | 0.000233  |
|    ent_coef_loss   | -15.5     |
|    learning_rate   | 0.00088   |
|    n_updates       | 4179596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 35       |
|    time_elapsed    | 33995    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=-4931.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1210000   |
| train/             |           |
|    actor_loss      | 96.2      |
|    critic_loss     | 0.0924    |
|    ent_coef        | 0.000267  |
|    ent_coef_loss   | 2.88      |
|    learning_rate   | 0.000879  |
|    n_updates       | 4189596   |
----------------------------------
Eval num_timesteps=1220000, episode_reward=-4891.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1220000   |
| train/             |           |
|    actor_loss      | 96.9      |
|    critic_loss     | 0.0739    |
|    ent_coef        | 0.000253  |
|    ent_coef_loss   | -2.36     |
|    learning_rate   | 0.000878  |
|    n_updates       | 4199596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 35       |
|    time_elapsed    | 34574    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=-4891.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1230000   |
| train/             |           |
|    actor_loss      | 97.8      |
|    critic_loss     | 0.0744    |
|    ent_coef        | 0.000315  |
|    ent_coef_loss   | -14       |
|    learning_rate   | 0.000877  |
|    n_updates       | 4209596   |
----------------------------------
Eval num_timesteps=1240000, episode_reward=-4889.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1240000   |
| train/             |           |
|    actor_loss      | 97.6      |
|    critic_loss     | 0.093     |
|    ent_coef        | 0.000459  |
|    ent_coef_loss   | -24.5     |
|    learning_rate   | 0.000876  |
|    n_updates       | 4219596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 35       |
|    time_elapsed    | 35148    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=-4880.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1250000   |
| train/             |           |
|    actor_loss      | 97        |
|    critic_loss     | 0.0608    |
|    ent_coef        | 0.000537  |
|    ent_coef_loss   | 3.35      |
|    learning_rate   | 0.000875  |
|    n_updates       | 4229596   |
----------------------------------
Eval num_timesteps=1260000, episode_reward=-4893.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1260000   |
| train/             |           |
|    actor_loss      | 94.8      |
|    critic_loss     | 0.0776    |
|    ent_coef        | 0.000817  |
|    ent_coef_loss   | 2.29      |
|    learning_rate   | 0.000874  |
|    n_updates       | 4239596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 35       |
|    time_elapsed    | 35731    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=-4895.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | 95.1     |
|    critic_loss     | 0.0608   |
|    ent_coef        | 0.000549 |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.000873 |
|    n_updates       | 4249596  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=-4897.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | 95.9     |
|    critic_loss     | 0.0755   |
|    ent_coef        | 0.000369 |
|    ent_coef_loss   | 5.36     |
|    learning_rate   | 0.000872 |
|    n_updates       | 4259596  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 35       |
|    time_elapsed    | 36313    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=-4896.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | 95.7     |
|    critic_loss     | 0.0539   |
|    ent_coef        | 0.000523 |
|    ent_coef_loss   | -7.12    |
|    learning_rate   | 0.000871 |
|    n_updates       | 4269596  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=-4914.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1300000   |
| train/             |           |
|    actor_loss      | 95.4      |
|    critic_loss     | 0.0794    |
|    ent_coef        | 0.000496  |
|    ent_coef_loss   | -11.5     |
|    learning_rate   | 0.00087   |
|    n_updates       | 4279596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 35       |
|    time_elapsed    | 36887    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=-4896.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | 95.5     |
|    critic_loss     | 0.0516   |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.000869 |
|    n_updates       | 4289596  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=-4899.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | 96.1     |
|    critic_loss     | 0.0643   |
|    ent_coef        | 0.000418 |
|    ent_coef_loss   | -22.6    |
|    learning_rate   | 0.000868 |
|    n_updates       | 4299596  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 35       |
|    time_elapsed    | 37467    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=-4891.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1330000   |
| train/             |           |
|    actor_loss      | 96.7      |
|    critic_loss     | 0.0782    |
|    ent_coef        | 0.000258  |
|    ent_coef_loss   | 6.19      |
|    learning_rate   | 0.000867  |
|    n_updates       | 4309596   |
----------------------------------
Eval num_timesteps=1340000, episode_reward=-4945.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 1340000   |
| train/             |           |
|    actor_loss      | 96.8      |
|    critic_loss     | 0.083     |
|    ent_coef        | 0.000366  |
|    ent_coef_loss   | 1.5       |
|    learning_rate   | 0.000866  |
|    n_updates       | 4319596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 35       |
|    time_elapsed    | 38049    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=-4873.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1350000   |
| train/             |           |
|    actor_loss      | 97.1      |
|    critic_loss     | 0.0525    |
|    ent_coef        | 0.000522  |
|    ent_coef_loss   | 7.69      |
|    learning_rate   | 0.000865  |
|    n_updates       | 4329596   |
----------------------------------
Eval num_timesteps=1360000, episode_reward=-4900.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | 96       |
|    critic_loss     | 0.081    |
|    ent_coef        | 0.000675 |
|    ent_coef_loss   | 8.62     |
|    learning_rate   | 0.000864 |
|    n_updates       | 4339596  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 35       |
|    time_elapsed    | 38638    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=-4902.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | 95.7     |
|    critic_loss     | 0.0294   |
|    ent_coef        | 0.000921 |
|    ent_coef_loss   | 1.91     |
|    learning_rate   | 0.000863 |
|    n_updates       | 4349596  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=-4886.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1380000   |
| train/             |           |
|    actor_loss      | 94        |
|    critic_loss     | 0.0605    |
|    ent_coef        | 0.00105   |
|    ent_coef_loss   | -10.3     |
|    learning_rate   | 0.000862  |
|    n_updates       | 4359596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 276       |
|    fps             | 35        |
|    time_elapsed    | 39223     |
|    total_timesteps | 1380000   |
----------------------------------
Eval num_timesteps=1390000, episode_reward=-4845.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.85e+03 |
| time/              |           |
|    total_timesteps | 1390000   |
| train/             |           |
|    actor_loss      | 94.5      |
|    critic_loss     | 0.052     |
|    ent_coef        | 0.000536  |
|    ent_coef_loss   | 0.943     |
|    learning_rate   | 0.000861  |
|    n_updates       | 4369596   |
----------------------------------
Eval num_timesteps=1400000, episode_reward=-4886.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1400000   |
| train/             |           |
|    actor_loss      | 94.8      |
|    critic_loss     | 0.0785    |
|    ent_coef        | 0.000516  |
|    ent_coef_loss   | 4.86      |
|    learning_rate   | 0.00086   |
|    n_updates       | 4379596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 280       |
|    fps             | 35        |
|    time_elapsed    | 39805     |
|    total_timesteps | 1400000   |
----------------------------------
Eval num_timesteps=1410000, episode_reward=-4888.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1410000   |
| train/             |           |
|    actor_loss      | 96        |
|    critic_loss     | 0.0643    |
|    ent_coef        | 0.000597  |
|    ent_coef_loss   | 3.96      |
|    learning_rate   | 0.000859  |
|    n_updates       | 4389596   |
----------------------------------
Eval num_timesteps=1420000, episode_reward=-4890.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1420000   |
| train/             |           |
|    actor_loss      | 95.6      |
|    critic_loss     | 0.0544    |
|    ent_coef        | 0.000644  |
|    ent_coef_loss   | -8.58     |
|    learning_rate   | 0.000858  |
|    n_updates       | 4399596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 284       |
|    fps             | 35        |
|    time_elapsed    | 40388     |
|    total_timesteps | 1420000   |
----------------------------------
Eval num_timesteps=1430000, episode_reward=-4904.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | 95.6     |
|    critic_loss     | 0.0632   |
|    ent_coef        | 0.000665 |
|    ent_coef_loss   | -4.25    |
|    learning_rate   | 0.000857 |
|    n_updates       | 4409596  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=-4908.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1440000   |
| train/             |           |
|    actor_loss      | 95.3      |
|    critic_loss     | 0.0616    |
|    ent_coef        | 0.000638  |
|    ent_coef_loss   | 1.36      |
|    learning_rate   | 0.000856  |
|    n_updates       | 4419596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 288       |
|    fps             | 35        |
|    time_elapsed    | 40959     |
|    total_timesteps | 1440000   |
----------------------------------
Eval num_timesteps=1450000, episode_reward=-4912.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1450000   |
| train/             |           |
|    actor_loss      | 95.2      |
|    critic_loss     | 0.103     |
|    ent_coef        | 0.000538  |
|    ent_coef_loss   | 8.38      |
|    learning_rate   | 0.000855  |
|    n_updates       | 4429596   |
----------------------------------
Eval num_timesteps=1460000, episode_reward=-4866.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1460000   |
| train/             |           |
|    actor_loss      | 95.8      |
|    critic_loss     | 0.063     |
|    ent_coef        | 0.000516  |
|    ent_coef_loss   | -3.17     |
|    learning_rate   | 0.000854  |
|    n_updates       | 4439596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 292       |
|    fps             | 35        |
|    time_elapsed    | 41527     |
|    total_timesteps | 1460000   |
----------------------------------
Eval num_timesteps=1470000, episode_reward=-4931.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1470000   |
| train/             |           |
|    actor_loss      | 95.4      |
|    critic_loss     | 0.0283    |
|    ent_coef        | 0.000353  |
|    ent_coef_loss   | 7.26      |
|    learning_rate   | 0.000853  |
|    n_updates       | 4449596   |
----------------------------------
Eval num_timesteps=1480000, episode_reward=-4872.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1480000   |
| train/             |           |
|    actor_loss      | 95.9      |
|    critic_loss     | 0.0566    |
|    ent_coef        | 0.000386  |
|    ent_coef_loss   | -13       |
|    learning_rate   | 0.000852  |
|    n_updates       | 4459596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 35       |
|    time_elapsed    | 42094    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=-4897.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | 96.5     |
|    critic_loss     | 0.0278   |
|    ent_coef        | 0.000294 |
|    ent_coef_loss   | 10.9     |
|    learning_rate   | 0.000851 |
|    n_updates       | 4469596  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=-4855.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 1500000   |
| train/             |           |
|    actor_loss      | 96.7      |
|    critic_loss     | 0.0289    |
|    ent_coef        | 0.000421  |
|    ent_coef_loss   | -12.9     |
|    learning_rate   | 0.00085   |
|    n_updates       | 4479596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 35       |
|    time_elapsed    | 42652    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=-4890.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1510000   |
| train/             |           |
|    actor_loss      | 95.7      |
|    critic_loss     | 0.0377    |
|    ent_coef        | 0.000618  |
|    ent_coef_loss   | -0.784    |
|    learning_rate   | 0.000849  |
|    n_updates       | 4489596   |
----------------------------------
Eval num_timesteps=1520000, episode_reward=-4828.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.83e+03 |
| time/              |           |
|    total_timesteps | 1520000   |
| train/             |           |
|    actor_loss      | 95.9      |
|    critic_loss     | 0.0317    |
|    ent_coef        | 0.000656  |
|    ent_coef_loss   | -12.9     |
|    learning_rate   | 0.000848  |
|    n_updates       | 4499596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 304       |
|    fps             | 35        |
|    time_elapsed    | 43222     |
|    total_timesteps | 1520000   |
----------------------------------
Eval num_timesteps=1530000, episode_reward=-4801.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.8e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | 95.7     |
|    critic_loss     | 0.0725   |
|    ent_coef        | 0.000804 |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000847 |
|    n_updates       | 4509596  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=-4882.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1540000   |
| train/             |           |
|    actor_loss      | 94.9      |
|    critic_loss     | 0.0508    |
|    ent_coef        | 0.000844  |
|    ent_coef_loss   | 5.82      |
|    learning_rate   | 0.000846  |
|    n_updates       | 4519596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 308       |
|    fps             | 35        |
|    time_elapsed    | 43793     |
|    total_timesteps | 1540000   |
----------------------------------
Eval num_timesteps=1550000, episode_reward=-4903.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | 94.7     |
|    critic_loss     | 0.0576   |
|    ent_coef        | 0.000707 |
|    ent_coef_loss   | 2.15     |
|    learning_rate   | 0.000845 |
|    n_updates       | 4529596  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=-4876.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1560000   |
| train/             |           |
|    actor_loss      | 94.9      |
|    critic_loss     | 0.0325    |
|    ent_coef        | 0.000733  |
|    ent_coef_loss   | 7.77      |
|    learning_rate   | 0.000844  |
|    n_updates       | 4539596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 312       |
|    fps             | 35        |
|    time_elapsed    | 44362     |
|    total_timesteps | 1560000   |
----------------------------------
Eval num_timesteps=1570000, episode_reward=-4873.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1570000   |
| train/             |           |
|    actor_loss      | 94.8      |
|    critic_loss     | 0.0736    |
|    ent_coef        | 0.000791  |
|    ent_coef_loss   | 3.81      |
|    learning_rate   | 0.000843  |
|    n_updates       | 4549596   |
----------------------------------
Eval num_timesteps=1580000, episode_reward=-4879.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1580000   |
| train/             |           |
|    actor_loss      | 95.1      |
|    critic_loss     | 0.0332    |
|    ent_coef        | 0.000776  |
|    ent_coef_loss   | -6.59     |
|    learning_rate   | 0.000842  |
|    n_updates       | 4559596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 316       |
|    fps             | 35        |
|    time_elapsed    | 44927     |
|    total_timesteps | 1580000   |
----------------------------------
Eval num_timesteps=1590000, episode_reward=-4889.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1590000   |
| train/             |           |
|    actor_loss      | 95.2      |
|    critic_loss     | 0.0365    |
|    ent_coef        | 0.0008    |
|    ent_coef_loss   | 3.7       |
|    learning_rate   | 0.000841  |
|    n_updates       | 4569596   |
----------------------------------
Eval num_timesteps=1600000, episode_reward=-4873.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1600000   |
| train/             |           |
|    actor_loss      | 94.4      |
|    critic_loss     | 0.111     |
|    ent_coef        | 0.00079   |
|    ent_coef_loss   | -2.15     |
|    learning_rate   | 0.00084   |
|    n_updates       | 4579596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 320       |
|    fps             | 35        |
|    time_elapsed    | 45500     |
|    total_timesteps | 1600000   |
----------------------------------
Eval num_timesteps=1610000, episode_reward=-4901.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | 94.7     |
|    critic_loss     | 0.0198   |
|    ent_coef        | 0.000766 |
|    ent_coef_loss   | 7.52     |
|    learning_rate   | 0.000839 |
|    n_updates       | 4589596  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=-4894.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 1620000   |
| train/             |           |
|    actor_loss      | 94.2      |
|    critic_loss     | 0.0372    |
|    ent_coef        | 0.000723  |
|    ent_coef_loss   | -10.3     |
|    learning_rate   | 0.000838  |
|    n_updates       | 4599596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 324       |
|    fps             | 35        |
|    time_elapsed    | 46076     |
|    total_timesteps | 1620000   |
----------------------------------
Eval num_timesteps=1630000, episode_reward=-4838.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.84e+03 |
| time/              |           |
|    total_timesteps | 1630000   |
| train/             |           |
|    actor_loss      | 93.9      |
|    critic_loss     | 0.071     |
|    ent_coef        | 0.000694  |
|    ent_coef_loss   | 3.81      |
|    learning_rate   | 0.000837  |
|    n_updates       | 4609596   |
----------------------------------
Eval num_timesteps=1640000, episode_reward=-4882.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1640000   |
| train/             |           |
|    actor_loss      | 94.4      |
|    critic_loss     | 0.0363    |
|    ent_coef        | 0.000838  |
|    ent_coef_loss   | -11       |
|    learning_rate   | 0.000836  |
|    n_updates       | 4619596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 328       |
|    fps             | 35        |
|    time_elapsed    | 46652     |
|    total_timesteps | 1640000   |
----------------------------------
Eval num_timesteps=1650000, episode_reward=-4908.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1650000   |
| train/             |           |
|    actor_loss      | 93.5      |
|    critic_loss     | 0.0672    |
|    ent_coef        | 0.000845  |
|    ent_coef_loss   | 2.7       |
|    learning_rate   | 0.000835  |
|    n_updates       | 4629596   |
----------------------------------
Eval num_timesteps=1660000, episode_reward=-4904.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | 93.3     |
|    critic_loss     | 0.0775   |
|    ent_coef        | 0.000953 |
|    ent_coef_loss   | 4.1      |
|    learning_rate   | 0.000834 |
|    n_updates       | 4639596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 332       |
|    fps             | 35        |
|    time_elapsed    | 47232     |
|    total_timesteps | 1660000   |
----------------------------------
Eval num_timesteps=1670000, episode_reward=-4866.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1670000   |
| train/             |           |
|    actor_loss      | 92.5      |
|    critic_loss     | 0.1       |
|    ent_coef        | 0.00104   |
|    ent_coef_loss   | 3.71      |
|    learning_rate   | 0.000833  |
|    n_updates       | 4649596   |
----------------------------------
Eval num_timesteps=1680000, episode_reward=-4878.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1680000   |
| train/             |           |
|    actor_loss      | 92.2      |
|    critic_loss     | 0.0727    |
|    ent_coef        | 0.00109   |
|    ent_coef_loss   | -4.77     |
|    learning_rate   | 0.000832  |
|    n_updates       | 4659596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 336       |
|    fps             | 35        |
|    time_elapsed    | 47813     |
|    total_timesteps | 1680000   |
----------------------------------
Eval num_timesteps=1690000, episode_reward=-4917.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1690000   |
| train/             |           |
|    actor_loss      | 91.8      |
|    critic_loss     | 0.0712    |
|    ent_coef        | 0.00138   |
|    ent_coef_loss   | 4.68      |
|    learning_rate   | 0.000831  |
|    n_updates       | 4669596   |
----------------------------------
Eval num_timesteps=1700000, episode_reward=-4826.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.83e+03 |
| time/              |           |
|    total_timesteps | 1700000   |
| train/             |           |
|    actor_loss      | 90.4      |
|    critic_loss     | 0.0573    |
|    ent_coef        | 0.00138   |
|    ent_coef_loss   | 0.368     |
|    learning_rate   | 0.00083   |
|    n_updates       | 4679596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 340       |
|    fps             | 35        |
|    time_elapsed    | 48397     |
|    total_timesteps | 1700000   |
----------------------------------
Eval num_timesteps=1710000, episode_reward=-4921.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1710000   |
| train/             |           |
|    actor_loss      | 90.2      |
|    critic_loss     | 0.0737    |
|    ent_coef        | 0.00149   |
|    ent_coef_loss   | 5.31      |
|    learning_rate   | 0.000829  |
|    n_updates       | 4689596   |
----------------------------------
Eval num_timesteps=1720000, episode_reward=-4874.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 1720000   |
| train/             |           |
|    actor_loss      | 89.9      |
|    critic_loss     | 0.0743    |
|    ent_coef        | 0.00139   |
|    ent_coef_loss   | 8.11      |
|    learning_rate   | 0.000828  |
|    n_updates       | 4699596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 344       |
|    fps             | 35        |
|    time_elapsed    | 48976     |
|    total_timesteps | 1720000   |
----------------------------------
Eval num_timesteps=1730000, episode_reward=-4921.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1730000   |
| train/             |           |
|    actor_loss      | 88.9      |
|    critic_loss     | 0.035     |
|    ent_coef        | 0.00149   |
|    ent_coef_loss   | -0.735    |
|    learning_rate   | 0.000827  |
|    n_updates       | 4709596   |
----------------------------------
Eval num_timesteps=1740000, episode_reward=-4935.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 1740000   |
| train/             |           |
|    actor_loss      | 89.3      |
|    critic_loss     | 0.0713    |
|    ent_coef        | 0.00152   |
|    ent_coef_loss   | -4.95     |
|    learning_rate   | 0.000826  |
|    n_updates       | 4719596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.88e+03 |
| time/              |           |
|    episodes        | 348       |
|    fps             | 35        |
|    time_elapsed    | 49555     |
|    total_timesteps | 1740000   |
----------------------------------
Eval num_timesteps=1750000, episode_reward=-4818.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.82e+03 |
| time/              |           |
|    total_timesteps | 1750000   |
| train/             |           |
|    actor_loss      | 89.6      |
|    critic_loss     | 0.0663    |
|    ent_coef        | 0.00118   |
|    ent_coef_loss   | 5.92      |
|    learning_rate   | 0.000825  |
|    n_updates       | 4729596   |
----------------------------------
Eval num_timesteps=1760000, episode_reward=-4917.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1760000   |
| train/             |           |
|    actor_loss      | 90.6      |
|    critic_loss     | 0.0465    |
|    ent_coef        | 0.000744  |
|    ent_coef_loss   | 1.06      |
|    learning_rate   | 0.000824  |
|    n_updates       | 4739596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 352       |
|    fps             | 35        |
|    time_elapsed    | 50129     |
|    total_timesteps | 1760000   |
----------------------------------
Eval num_timesteps=1770000, episode_reward=-4933.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1770000   |
| train/             |           |
|    actor_loss      | 92.7      |
|    critic_loss     | 0.0278    |
|    ent_coef        | 0.00036   |
|    ent_coef_loss   | 12        |
|    learning_rate   | 0.000823  |
|    n_updates       | 4749596   |
----------------------------------
Eval num_timesteps=1780000, episode_reward=-4862.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.86e+03 |
| time/              |           |
|    total_timesteps | 1780000   |
| train/             |           |
|    actor_loss      | 93.7      |
|    critic_loss     | 0.0728    |
|    ent_coef        | 0.000233  |
|    ent_coef_loss   | -30.6     |
|    learning_rate   | 0.000822  |
|    n_updates       | 4759596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 356       |
|    fps             | 35        |
|    time_elapsed    | 50703     |
|    total_timesteps | 1780000   |
----------------------------------
Eval num_timesteps=1790000, episode_reward=-4922.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1790000   |
| train/             |           |
|    actor_loss      | 95.9      |
|    critic_loss     | 0.0862    |
|    ent_coef        | 0.000228  |
|    ent_coef_loss   | 16.2      |
|    learning_rate   | 0.000821  |
|    n_updates       | 4769596   |
----------------------------------
Eval num_timesteps=1800000, episode_reward=-4911.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1800000   |
| train/             |           |
|    actor_loss      | 96.2      |
|    critic_loss     | 0.104     |
|    ent_coef        | 0.000197  |
|    ent_coef_loss   | -3.58     |
|    learning_rate   | 0.00082   |
|    n_updates       | 4779596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 360       |
|    fps             | 35        |
|    time_elapsed    | 51276     |
|    total_timesteps | 1800000   |
----------------------------------
Eval num_timesteps=1810000, episode_reward=-4925.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1810000   |
| train/             |           |
|    actor_loss      | 97.4      |
|    critic_loss     | 0.0392    |
|    ent_coef        | 0.000134  |
|    ent_coef_loss   | 4.19      |
|    learning_rate   | 0.000819  |
|    n_updates       | 4789596   |
----------------------------------
Eval num_timesteps=1820000, episode_reward=-4922.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1820000   |
| train/             |           |
|    actor_loss      | 97.1      |
|    critic_loss     | 0.0461    |
|    ent_coef        | 0.000971  |
|    ent_coef_loss   | 9.96      |
|    learning_rate   | 0.000818  |
|    n_updates       | 4799596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 364       |
|    fps             | 35        |
|    time_elapsed    | 51844     |
|    total_timesteps | 1820000   |
----------------------------------
Eval num_timesteps=1830000, episode_reward=-4922.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1830000   |
| train/             |           |
|    actor_loss      | 94.4      |
|    critic_loss     | 0.096     |
|    ent_coef        | 0.000916  |
|    ent_coef_loss   | 7.39      |
|    learning_rate   | 0.000817  |
|    n_updates       | 4809596   |
----------------------------------
Eval num_timesteps=1840000, episode_reward=-4895.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | 95.6     |
|    critic_loss     | 0.0307   |
|    ent_coef        | 2.97e-07 |
|    ent_coef_loss   | -196     |
|    learning_rate   | 0.000816 |
|    n_updates       | 4819596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 368       |
|    fps             | 35        |
|    time_elapsed    | 52423     |
|    total_timesteps | 1840000   |
----------------------------------
Eval num_timesteps=1850000, episode_reward=-4920.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1850000   |
| train/             |           |
|    actor_loss      | 97        |
|    critic_loss     | 0.0534    |
|    ent_coef        | 9.09e-10  |
|    ent_coef_loss   | 50.7      |
|    learning_rate   | 0.000815  |
|    n_updates       | 4829596   |
----------------------------------
Eval num_timesteps=1860000, episode_reward=-4902.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | 97.8     |
|    critic_loss     | 0.0472   |
|    ent_coef        | 4.05e-12 |
|    ent_coef_loss   | -207     |
|    learning_rate   | 0.000814 |
|    n_updates       | 4839596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 372       |
|    fps             | 35        |
|    time_elapsed    | 53017     |
|    total_timesteps | 1860000   |
----------------------------------
Eval num_timesteps=1870000, episode_reward=-4905.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1870000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0944    |
|    ent_coef        | 1.07e-15  |
|    ent_coef_loss   | -278      |
|    learning_rate   | 0.000813  |
|    n_updates       | 4849596   |
----------------------------------
Eval num_timesteps=1880000, episode_reward=-4911.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1880000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0753    |
|    ent_coef        | 5.73e-17  |
|    ent_coef_loss   | 229       |
|    learning_rate   | 0.000812  |
|    n_updates       | 4859596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 376       |
|    fps             | 35        |
|    time_elapsed    | 53600     |
|    total_timesteps | 1880000   |
----------------------------------
Eval num_timesteps=1890000, episode_reward=-4882.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 1890000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0709    |
|    ent_coef        | 1.05e-12  |
|    ent_coef_loss   | 1.73e+03  |
|    learning_rate   | 0.000811  |
|    n_updates       | 4869596   |
----------------------------------
Eval num_timesteps=1900000, episode_reward=-4947.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 1900000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0866    |
|    ent_coef        | 1.09e-09  |
|    ent_coef_loss   | 161       |
|    learning_rate   | 0.00081   |
|    n_updates       | 4879596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 380       |
|    fps             | 35        |
|    time_elapsed    | 54171     |
|    total_timesteps | 1900000   |
----------------------------------
Eval num_timesteps=1910000, episode_reward=-4932.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1910000   |
| train/             |           |
|    actor_loss      | 98.2      |
|    critic_loss     | 0.0395    |
|    ent_coef        | 3.65e-06  |
|    ent_coef_loss   | 107       |
|    learning_rate   | 0.000809  |
|    n_updates       | 4889596   |
----------------------------------
Eval num_timesteps=1920000, episode_reward=-4943.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 1920000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0444    |
|    ent_coef        | 3.55e-05  |
|    ent_coef_loss   | 354       |
|    learning_rate   | 0.000808  |
|    n_updates       | 4899596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 384       |
|    fps             | 35        |
|    time_elapsed    | 54741     |
|    total_timesteps | 1920000   |
----------------------------------
Eval num_timesteps=1930000, episode_reward=-4911.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1930000   |
| train/             |           |
|    actor_loss      | 98.2      |
|    critic_loss     | 0.113     |
|    ent_coef        | 0.00107   |
|    ent_coef_loss   | 4.87      |
|    learning_rate   | 0.000807  |
|    n_updates       | 4909596   |
----------------------------------
Eval num_timesteps=1940000, episode_reward=-4934.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 1940000   |
| train/             |           |
|    actor_loss      | 97.5      |
|    critic_loss     | 0.0747    |
|    ent_coef        | 0.000875  |
|    ent_coef_loss   | -4.4      |
|    learning_rate   | 0.000806  |
|    n_updates       | 4919596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 388       |
|    fps             | 35        |
|    time_elapsed    | 55318     |
|    total_timesteps | 1940000   |
----------------------------------
Eval num_timesteps=1950000, episode_reward=-4898.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | 97.9     |
|    critic_loss     | 0.0933   |
|    ent_coef        | 0.00061  |
|    ent_coef_loss   | 0.244    |
|    learning_rate   | 0.000805 |
|    n_updates       | 4929596  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=-4938.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 1960000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0495    |
|    ent_coef        | 2.34e-06  |
|    ent_coef_loss   | -34.8     |
|    learning_rate   | 0.000804  |
|    n_updates       | 4939596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 392       |
|    fps             | 35        |
|    time_elapsed    | 55899     |
|    total_timesteps | 1960000   |
----------------------------------
Eval num_timesteps=1970000, episode_reward=-4913.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1970000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0885    |
|    ent_coef        | 6.76e-08  |
|    ent_coef_loss   | -4.28     |
|    learning_rate   | 0.000803  |
|    n_updates       | 4949596   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=-4910.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 1980000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0534    |
|    ent_coef        | 4.69e-06  |
|    ent_coef_loss   | -3.73     |
|    learning_rate   | 0.000802  |
|    n_updates       | 4959596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 396       |
|    fps             | 35        |
|    time_elapsed    | 56470     |
|    total_timesteps | 1980000   |
----------------------------------
Eval num_timesteps=1990000, episode_reward=-4922.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 1990000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.084     |
|    ent_coef        | 1.01e-07  |
|    ent_coef_loss   | -38.7     |
|    learning_rate   | 0.000801  |
|    n_updates       | 4969596   |
----------------------------------
Eval num_timesteps=2000000, episode_reward=-4892.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 2000000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0927    |
|    ent_coef        | 6.4e-07   |
|    ent_coef_loss   | 365       |
|    learning_rate   | 0.0008    |
|    n_updates       | 4979596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 400       |
|    fps             | 35        |
|    time_elapsed    | 57069     |
----------------------------------
Eval num_timesteps=2010000, episode_reward=-4903.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | 98.7     |
|    critic_loss     | 0.0898   |
|    ent_coef        | 4.18e-07 |
|    ent_coef_loss   | -45.9    |
|    learning_rate   | 0.000799 |
|    n_updates       | 4989596  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=-4917.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2020000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0708    |
|    ent_coef        | 1.83e-06  |
|    ent_coef_loss   | -19.8     |
|    learning_rate   | 0.000798  |
|    n_updates       | 4999596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.89e+03 |
| time/              |           |
|    episodes        | 404       |
|    fps             | 35        |
|    time_elapsed    | 57644     |
|    total_timesteps | 2020000   |
----------------------------------
Eval num_timesteps=2030000, episode_reward=-4945.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 2030000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.025     |
|    ent_coef        | 8.33e-06  |
|    ent_coef_loss   | 1.12      |
|    learning_rate   | 0.000797  |
|    n_updates       | 5009596   |
----------------------------------
Eval num_timesteps=2040000, episode_reward=-4929.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2040000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.121     |
|    ent_coef        | 0.000132  |
|    ent_coef_loss   | 27.3      |
|    learning_rate   | 0.000796  |
|    n_updates       | 5019596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 35       |
|    time_elapsed    | 58206    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=-4930.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2050000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0852    |
|    ent_coef        | 0.000691  |
|    ent_coef_loss   | -7.26     |
|    learning_rate   | 0.000795  |
|    n_updates       | 5029596   |
----------------------------------
Eval num_timesteps=2060000, episode_reward=-4905.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2060000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0761    |
|    ent_coef        | 0.000214  |
|    ent_coef_loss   | -6.41     |
|    learning_rate   | 0.000794  |
|    n_updates       | 5039596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 35       |
|    time_elapsed    | 58774    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=-4935.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2070000   |
| train/             |           |
|    actor_loss      | 98.9      |
|    critic_loss     | 0.0535    |
|    ent_coef        | 3.33e-06  |
|    ent_coef_loss   | 31.4      |
|    learning_rate   | 0.000793  |
|    n_updates       | 5049596   |
----------------------------------
Eval num_timesteps=2080000, episode_reward=-4932.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2080000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.044     |
|    ent_coef        | 4.26e-05  |
|    ent_coef_loss   | 68.9      |
|    learning_rate   | 0.000792  |
|    n_updates       | 5059596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 35       |
|    time_elapsed    | 59339    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=-4926.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2090000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0811    |
|    ent_coef        | 2.32e-07  |
|    ent_coef_loss   | -48       |
|    learning_rate   | 0.000791  |
|    n_updates       | 5069596   |
----------------------------------
Eval num_timesteps=2100000, episode_reward=-4940.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2100000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0781    |
|    ent_coef        | 1.11e-06  |
|    ent_coef_loss   | 13.1      |
|    learning_rate   | 0.00079   |
|    n_updates       | 5079596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 35       |
|    time_elapsed    | 59918    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=-4937.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2110000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.114     |
|    ent_coef        | 1.58e-05  |
|    ent_coef_loss   | 168       |
|    learning_rate   | 0.000789  |
|    n_updates       | 5089596   |
----------------------------------
Eval num_timesteps=2120000, episode_reward=-4937.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2120000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0373    |
|    ent_coef        | 2.12e-05  |
|    ent_coef_loss   | 35.6      |
|    learning_rate   | 0.000788  |
|    n_updates       | 5099596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 35       |
|    time_elapsed    | 60486    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=-4916.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2130000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0181    |
|    ent_coef        | 9.33e-06  |
|    ent_coef_loss   | -30.4     |
|    learning_rate   | 0.000787  |
|    n_updates       | 5109596   |
----------------------------------
Eval num_timesteps=2140000, episode_reward=-4932.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2140000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0597    |
|    ent_coef        | 1.14e-08  |
|    ent_coef_loss   | -9.09     |
|    learning_rate   | 0.000786  |
|    n_updates       | 5119596   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -4.9e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 35       |
|    time_elapsed    | 61050    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=-4914.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2150000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0857    |
|    ent_coef        | 9.22e-08  |
|    ent_coef_loss   | 203       |
|    learning_rate   | 0.000785  |
|    n_updates       | 5129596   |
----------------------------------
Eval num_timesteps=2160000, episode_reward=-4941.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2160000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0727    |
|    ent_coef        | 2.88e-07  |
|    ent_coef_loss   | -42.2     |
|    learning_rate   | 0.000784  |
|    n_updates       | 5139596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 432       |
|    fps             | 35        |
|    time_elapsed    | 61618     |
|    total_timesteps | 2160000   |
----------------------------------
Eval num_timesteps=2170000, episode_reward=-4937.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2170000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0336    |
|    ent_coef        | 1.14e-06  |
|    ent_coef_loss   | -27.4     |
|    learning_rate   | 0.000783  |
|    n_updates       | 5149596   |
----------------------------------
Eval num_timesteps=2180000, episode_reward=-4924.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2180000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.054     |
|    ent_coef        | 0.000158  |
|    ent_coef_loss   | -22.3     |
|    learning_rate   | 0.000782  |
|    n_updates       | 5159596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 436       |
|    fps             | 35        |
|    time_elapsed    | 62192     |
|    total_timesteps | 2180000   |
----------------------------------
Eval num_timesteps=2190000, episode_reward=-4878.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.88e+03 |
| time/              |           |
|    total_timesteps | 2190000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0674    |
|    ent_coef        | 2.6e-07   |
|    ent_coef_loss   | -44.3     |
|    learning_rate   | 0.000781  |
|    n_updates       | 5169596   |
----------------------------------
Eval num_timesteps=2200000, episode_reward=-4929.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2200000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0338    |
|    ent_coef        | 2.77e-06  |
|    ent_coef_loss   | -37.6     |
|    learning_rate   | 0.00078   |
|    n_updates       | 5179596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 440       |
|    fps             | 35        |
|    time_elapsed    | 62771     |
|    total_timesteps | 2200000   |
----------------------------------
Eval num_timesteps=2210000, episode_reward=-4915.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2210000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0118    |
|    ent_coef        | 6.22e-05  |
|    ent_coef_loss   | 80.5      |
|    learning_rate   | 0.000779  |
|    n_updates       | 5189596   |
----------------------------------
Eval num_timesteps=2220000, episode_reward=-4913.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2220000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0288    |
|    ent_coef        | 0.00108   |
|    ent_coef_loss   | 3.99      |
|    learning_rate   | 0.000778  |
|    n_updates       | 5199596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 444       |
|    fps             | 35        |
|    time_elapsed    | 63353     |
|    total_timesteps | 2220000   |
----------------------------------
Eval num_timesteps=2230000, episode_reward=-4931.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2230000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0703    |
|    ent_coef        | 0.000475  |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 0.000777  |
|    n_updates       | 5209596   |
----------------------------------
Eval num_timesteps=2240000, episode_reward=-4922.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2240000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.121     |
|    ent_coef        | 0.00113   |
|    ent_coef_loss   | 6.1       |
|    learning_rate   | 0.000776  |
|    n_updates       | 5219596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 448       |
|    fps             | 35        |
|    time_elapsed    | 63927     |
|    total_timesteps | 2240000   |
----------------------------------
Eval num_timesteps=2250000, episode_reward=-4923.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2250000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0336    |
|    ent_coef        | 0.00122   |
|    ent_coef_loss   | 1.51      |
|    learning_rate   | 0.000775  |
|    n_updates       | 5229596   |
----------------------------------
Eval num_timesteps=2260000, episode_reward=-4931.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2260000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0687    |
|    ent_coef        | 0.000622  |
|    ent_coef_loss   | -0.872    |
|    learning_rate   | 0.000774  |
|    n_updates       | 5239596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 452       |
|    fps             | 35        |
|    time_elapsed    | 64494     |
|    total_timesteps | 2260000   |
----------------------------------
Eval num_timesteps=2270000, episode_reward=-4924.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2270000   |
| train/             |           |
|    actor_loss      | 97.8      |
|    critic_loss     | 0.0428    |
|    ent_coef        | 0.000991  |
|    ent_coef_loss   | 3.52      |
|    learning_rate   | 0.000773  |
|    n_updates       | 5249596   |
----------------------------------
Eval num_timesteps=2280000, episode_reward=-4919.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2280000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0757    |
|    ent_coef        | 2.36e-05  |
|    ent_coef_loss   | -48.9     |
|    learning_rate   | 0.000772  |
|    n_updates       | 5259596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 456       |
|    fps             | 35        |
|    time_elapsed    | 65062     |
|    total_timesteps | 2280000   |
----------------------------------
Eval num_timesteps=2290000, episode_reward=-4925.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2290000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0335    |
|    ent_coef        | 3.17e-05  |
|    ent_coef_loss   | -0.904    |
|    learning_rate   | 0.000771  |
|    n_updates       | 5269596   |
----------------------------------
Eval num_timesteps=2300000, episode_reward=-4901.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | 98.9     |
|    critic_loss     | 0.041    |
|    ent_coef        | 7.09e-06 |
|    ent_coef_loss   | 81.7     |
|    learning_rate   | 0.00077  |
|    n_updates       | 5279596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 460       |
|    fps             | 35        |
|    time_elapsed    | 65643     |
|    total_timesteps | 2300000   |
----------------------------------
Eval num_timesteps=2310000, episode_reward=-4938.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2310000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.081     |
|    ent_coef        | 4.47e-06  |
|    ent_coef_loss   | -43       |
|    learning_rate   | 0.000769  |
|    n_updates       | 5289596   |
----------------------------------
Eval num_timesteps=2320000, episode_reward=-4928.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2320000   |
| train/             |           |
|    actor_loss      | 99        |
|    critic_loss     | 0.0414    |
|    ent_coef        | 5.31e-09  |
|    ent_coef_loss   | -121      |
|    learning_rate   | 0.000768  |
|    n_updates       | 5299596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 464       |
|    fps             | 35        |
|    time_elapsed    | 66214     |
|    total_timesteps | 2320000   |
----------------------------------
Eval num_timesteps=2330000, episode_reward=-4897.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | 99       |
|    critic_loss     | 0.0442   |
|    ent_coef        | 1.12e-06 |
|    ent_coef_loss   | 184      |
|    learning_rate   | 0.000767 |
|    n_updates       | 5309596  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=-4904.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | 98.7     |
|    critic_loss     | 0.0626   |
|    ent_coef        | 7.64e-09 |
|    ent_coef_loss   | -188     |
|    learning_rate   | 0.000766 |
|    n_updates       | 5319596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 468       |
|    fps             | 35        |
|    time_elapsed    | 66778     |
|    total_timesteps | 2340000   |
----------------------------------
Eval num_timesteps=2350000, episode_reward=-4914.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2350000   |
| train/             |           |
|    actor_loss      | 98.9      |
|    critic_loss     | 0.0478    |
|    ent_coef        | 1.06e-11  |
|    ent_coef_loss   | -72.7     |
|    learning_rate   | 0.000765  |
|    n_updates       | 5329596   |
----------------------------------
Eval num_timesteps=2360000, episode_reward=-4908.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2360000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0371    |
|    ent_coef        | 4.32e-15  |
|    ent_coef_loss   | -180      |
|    learning_rate   | 0.000764  |
|    n_updates       | 5339596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 472       |
|    fps             | 35        |
|    time_elapsed    | 67343     |
|    total_timesteps | 2360000   |
----------------------------------
Eval num_timesteps=2370000, episode_reward=-4874.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.87e+03 |
| time/              |           |
|    total_timesteps | 2370000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.149     |
|    ent_coef        | 2.33e-18  |
|    ent_coef_loss   | -195      |
|    learning_rate   | 0.000763  |
|    n_updates       | 5349596   |
----------------------------------
Eval num_timesteps=2380000, episode_reward=-4924.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2380000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0807    |
|    ent_coef        | 3.17e-18  |
|    ent_coef_loss   | 520       |
|    learning_rate   | 0.000762  |
|    n_updates       | 5359596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 476       |
|    fps             | 35        |
|    time_elapsed    | 67908     |
|    total_timesteps | 2380000   |
----------------------------------
Eval num_timesteps=2390000, episode_reward=-4938.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2390000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.169     |
|    ent_coef        | 8.86e-15  |
|    ent_coef_loss   | 764       |
|    learning_rate   | 0.000761  |
|    n_updates       | 5369596   |
----------------------------------
Eval num_timesteps=2400000, episode_reward=-4892.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.89e+03 |
| time/              |           |
|    total_timesteps | 2400000   |
| train/             |           |
|    actor_loss      | 98.9      |
|    critic_loss     | 0.0665    |
|    ent_coef        | 1.44e-11  |
|    ent_coef_loss   | 581       |
|    learning_rate   | 0.00076   |
|    n_updates       | 5379596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 480       |
|    fps             | 35        |
|    time_elapsed    | 68471     |
|    total_timesteps | 2400000   |
----------------------------------
Eval num_timesteps=2410000, episode_reward=-4921.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2410000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0518    |
|    ent_coef        | 7.84e-09  |
|    ent_coef_loss   | 61.9      |
|    learning_rate   | 0.000759  |
|    n_updates       | 5389596   |
----------------------------------
Eval num_timesteps=2420000, episode_reward=-4941.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2420000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0259    |
|    ent_coef        | 2.38e-07  |
|    ent_coef_loss   | -40.1     |
|    learning_rate   | 0.000758  |
|    n_updates       | 5399596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 484       |
|    fps             | 35        |
|    time_elapsed    | 69038     |
|    total_timesteps | 2420000   |
----------------------------------
Eval num_timesteps=2430000, episode_reward=-4932.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2430000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.27      |
|    ent_coef        | 9.63e-07  |
|    ent_coef_loss   | -38.9     |
|    learning_rate   | 0.000757  |
|    n_updates       | 5409596   |
----------------------------------
Eval num_timesteps=2440000, episode_reward=-4907.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2440000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0661    |
|    ent_coef        | 0.000214  |
|    ent_coef_loss   | 9.06      |
|    learning_rate   | 0.000756  |
|    n_updates       | 5419596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 488       |
|    fps             | 35        |
|    time_elapsed    | 69608     |
|    total_timesteps | 2440000   |
----------------------------------
Eval num_timesteps=2450000, episode_reward=-4921.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2450000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.234     |
|    ent_coef        | 4.84e-05  |
|    ent_coef_loss   | 3.57      |
|    learning_rate   | 0.000755  |
|    n_updates       | 5429596   |
----------------------------------
Eval num_timesteps=2460000, episode_reward=-4906.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2460000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0357    |
|    ent_coef        | 7.43e-08  |
|    ent_coef_loss   | -142      |
|    learning_rate   | 0.000754  |
|    n_updates       | 5439596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 492       |
|    fps             | 35        |
|    time_elapsed    | 70152     |
|    total_timesteps | 2460000   |
----------------------------------
Eval num_timesteps=2470000, episode_reward=-4931.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2470000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0263    |
|    ent_coef        | 1.27e-09  |
|    ent_coef_loss   | 578       |
|    learning_rate   | 0.000753  |
|    n_updates       | 5449596   |
----------------------------------
Eval num_timesteps=2480000, episode_reward=-4934.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2480000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0377    |
|    ent_coef        | 2.1e-06   |
|    ent_coef_loss   | 257       |
|    learning_rate   | 0.000752  |
|    n_updates       | 5459596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 496       |
|    fps             | 35        |
|    time_elapsed    | 70684     |
|    total_timesteps | 2480000   |
----------------------------------
Eval num_timesteps=2490000, episode_reward=-4947.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 2490000   |
| train/             |           |
|    actor_loss      | 99.1      |
|    critic_loss     | 0.0496    |
|    ent_coef        | 9.48e-05  |
|    ent_coef_loss   | -8        |
|    learning_rate   | 0.000751  |
|    n_updates       | 5469596   |
----------------------------------
Eval num_timesteps=2500000, episode_reward=-4899.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | 98.8     |
|    critic_loss     | 0.071    |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | 18.7     |
|    learning_rate   | 0.00075  |
|    n_updates       | 5479596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 500       |
|    fps             | 35        |
|    time_elapsed    | 71197     |
|    total_timesteps | 2500000   |
----------------------------------
Eval num_timesteps=2510000, episode_reward=-4936.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2510000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.205     |
|    ent_coef        | 0.00014   |
|    ent_coef_loss   | 3.45      |
|    learning_rate   | 0.000749  |
|    n_updates       | 5489596   |
----------------------------------
Eval num_timesteps=2520000, episode_reward=-4931.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2520000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0156    |
|    ent_coef        | 0.000156  |
|    ent_coef_loss   | 13.2      |
|    learning_rate   | 0.000748  |
|    n_updates       | 5499596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 504       |
|    fps             | 35        |
|    time_elapsed    | 71706     |
|    total_timesteps | 2520000   |
----------------------------------
Eval num_timesteps=2530000, episode_reward=-4905.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2530000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0431    |
|    ent_coef        | 2.77e-07  |
|    ent_coef_loss   | -186      |
|    learning_rate   | 0.000747  |
|    n_updates       | 5509596   |
----------------------------------
Eval num_timesteps=2540000, episode_reward=-4945.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.95e+03 |
| time/              |           |
|    total_timesteps | 2540000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0163    |
|    ent_coef        | 2.55e-09  |
|    ent_coef_loss   | -5.5      |
|    learning_rate   | 0.000746  |
|    n_updates       | 5519596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 508       |
|    fps             | 35        |
|    time_elapsed    | 72224     |
|    total_timesteps | 2540000   |
----------------------------------
Eval num_timesteps=2550000, episode_reward=-4922.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2550000   |
| train/             |           |
|    actor_loss      | 98.9      |
|    critic_loss     | 0.0208    |
|    ent_coef        | 2.5e-07   |
|    ent_coef_loss   | 5.17      |
|    learning_rate   | 0.000745  |
|    n_updates       | 5529596   |
----------------------------------
Eval num_timesteps=2560000, episode_reward=-4935.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2560000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0389    |
|    ent_coef        | 4.49e-06  |
|    ent_coef_loss   | 296       |
|    learning_rate   | 0.000744  |
|    n_updates       | 5539596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 512       |
|    fps             | 35        |
|    time_elapsed    | 72735     |
|    total_timesteps | 2560000   |
----------------------------------
Eval num_timesteps=2570000, episode_reward=-4936.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2570000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0329    |
|    ent_coef        | 1.55e-08  |
|    ent_coef_loss   | -151      |
|    learning_rate   | 0.000743  |
|    n_updates       | 5549596   |
----------------------------------
Eval num_timesteps=2580000, episode_reward=-4936.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2580000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0303    |
|    ent_coef        | 2.67e-10  |
|    ent_coef_loss   | 291       |
|    learning_rate   | 0.000742  |
|    n_updates       | 5559596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 516       |
|    fps             | 35        |
|    time_elapsed    | 73245     |
|    total_timesteps | 2580000   |
----------------------------------
Eval num_timesteps=2590000, episode_reward=-4895.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | 98.9     |
|    critic_loss     | 0.0517   |
|    ent_coef        | 1.29e-08 |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000741 |
|    n_updates       | 5569596  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=-4899.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.9e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | 98.8     |
|    critic_loss     | 0.0353   |
|    ent_coef        | 1.57e-05 |
|    ent_coef_loss   | 92.6     |
|    learning_rate   | 0.00074  |
|    n_updates       | 5579596  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 520       |
|    fps             | 35        |
|    time_elapsed    | 73753     |
|    total_timesteps | 2600000   |
----------------------------------
Eval num_timesteps=2610000, episode_reward=-4925.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2610000   |
| train/             |           |
|    actor_loss      | 98.8      |
|    critic_loss     | 0.0529    |
|    ent_coef        | 3.69e-08  |
|    ent_coef_loss   | -140      |
|    learning_rate   | 0.000739  |
|    n_updates       | 5589596   |
----------------------------------
Eval num_timesteps=2620000, episode_reward=-4921.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2620000   |
| train/             |           |
|    actor_loss      | 98.9      |
|    critic_loss     | 0.0412    |
|    ent_coef        | 7.17e-09  |
|    ent_coef_loss   | 609       |
|    learning_rate   | 0.000738  |
|    n_updates       | 5599596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 524       |
|    fps             | 35        |
|    time_elapsed    | 74263     |
|    total_timesteps | 2620000   |
----------------------------------
Eval num_timesteps=2630000, episode_reward=-4933.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2630000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0109    |
|    ent_coef        | 1.09e-05  |
|    ent_coef_loss   | 204       |
|    learning_rate   | 0.000737  |
|    n_updates       | 5609596   |
----------------------------------
Eval num_timesteps=2640000, episode_reward=-4933.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2640000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0295    |
|    ent_coef        | 6.42e-06  |
|    ent_coef_loss   | 5.37      |
|    learning_rate   | 0.000736  |
|    n_updates       | 5619596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 528       |
|    fps             | 35        |
|    time_elapsed    | 74772     |
|    total_timesteps | 2640000   |
----------------------------------
Eval num_timesteps=2650000, episode_reward=-4940.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+03 |
| time/              |           |
|    total_timesteps | 2650000   |
| train/             |           |
|    actor_loss      | 98.7      |
|    critic_loss     | 0.0171    |
|    ent_coef        | 2.71e-05  |
|    ent_coef_loss   | 6.94      |
|    learning_rate   | 0.000735  |
|    n_updates       | 5629596   |
----------------------------------
Eval num_timesteps=2660000, episode_reward=-4930.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2660000   |
| train/             |           |
|    actor_loss      | 98.6      |
|    critic_loss     | 0.0749    |
|    ent_coef        | 2.27e-08  |
|    ent_coef_loss   | -178      |
|    learning_rate   | 0.000734  |
|    n_updates       | 5639596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 532       |
|    fps             | 35        |
|    time_elapsed    | 75280     |
|    total_timesteps | 2660000   |
----------------------------------
Eval num_timesteps=2670000, episode_reward=-4930.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2670000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0308    |
|    ent_coef        | 1.49e-11  |
|    ent_coef_loss   | -255      |
|    learning_rate   | 0.000733  |
|    n_updates       | 5649596   |
----------------------------------
Eval num_timesteps=2680000, episode_reward=-4932.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.93e+03 |
| time/              |           |
|    total_timesteps | 2680000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0761    |
|    ent_coef        | 1.01e-14  |
|    ent_coef_loss   | -324      |
|    learning_rate   | 0.000732  |
|    n_updates       | 5659596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 536       |
|    fps             | 35        |
|    time_elapsed    | 75793     |
|    total_timesteps | 2680000   |
----------------------------------
Eval num_timesteps=2690000, episode_reward=-4920.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2690000   |
| train/             |           |
|    actor_loss      | 98.5      |
|    critic_loss     | 0.0144    |
|    ent_coef        | 6.51e-18  |
|    ent_coef_loss   | -432      |
|    learning_rate   | 0.000731  |
|    n_updates       | 5669596   |
----------------------------------
Eval num_timesteps=2700000, episode_reward=-4910.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2700000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0337    |
|    ent_coef        | 4.34e-21  |
|    ent_coef_loss   | -497      |
|    learning_rate   | 0.00073   |
|    n_updates       | 5679596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 540       |
|    fps             | 35        |
|    time_elapsed    | 76306     |
|    total_timesteps | 2700000   |
----------------------------------
Eval num_timesteps=2710000, episode_reward=-4924.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.92e+03 |
| time/              |           |
|    total_timesteps | 2710000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0148    |
|    ent_coef        | 2.94e-24  |
|    ent_coef_loss   | -591      |
|    learning_rate   | 0.000729  |
|    n_updates       | 5689596   |
----------------------------------
Eval num_timesteps=2720000, episode_reward=-4913.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2720000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0155    |
|    ent_coef        | 2.03e-27  |
|    ent_coef_loss   | -666      |
|    learning_rate   | 0.000728  |
|    n_updates       | 5699596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.92e+03 |
| time/              |           |
|    episodes        | 544       |
|    fps             | 35        |
|    time_elapsed    | 76821     |
|    total_timesteps | 2720000   |
----------------------------------
Eval num_timesteps=2730000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2730000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0223    |
|    ent_coef        | 1.42e-30  |
|    ent_coef_loss   | -713      |
|    learning_rate   | 0.000727  |
|    n_updates       | 5709596   |
----------------------------------
Eval num_timesteps=2740000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2740000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0254    |
|    ent_coef        | 9.95e-34  |
|    ent_coef_loss   | -816      |
|    learning_rate   | 0.000726  |
|    n_updates       | 5719596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 548       |
|    fps             | 35        |
|    time_elapsed    | 77333     |
|    total_timesteps | 2740000   |
----------------------------------
Eval num_timesteps=2750000, episode_reward=-4910.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2750000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0239    |
|    ent_coef        | 7.06e-37  |
|    ent_coef_loss   | -847      |
|    learning_rate   | 0.000725  |
|    n_updates       | 5729596   |
----------------------------------
Eval num_timesteps=2760000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2760000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0201    |
|    ent_coef        | 5.05e-40  |
|    ent_coef_loss   | -886      |
|    learning_rate   | 0.000724  |
|    n_updates       | 5739596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 552       |
|    fps             | 35        |
|    time_elapsed    | 77845     |
|    total_timesteps | 2760000   |
----------------------------------
Eval num_timesteps=2770000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2770000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.00565   |
|    ent_coef        | 3.66e-43  |
|    ent_coef_loss   | -984      |
|    learning_rate   | 0.000723  |
|    n_updates       | 5749596   |
----------------------------------
Eval num_timesteps=2780000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2780000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0241    |
|    ent_coef        | 0         |
|    ent_coef_loss   | -1.13e+03 |
|    learning_rate   | 0.000722  |
|    n_updates       | 5759596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 556       |
|    fps             | 35        |
|    time_elapsed    | 78359     |
|    total_timesteps | 2780000   |
----------------------------------
Eval num_timesteps=2790000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2790000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0609    |
|    ent_coef        | 0         |
|    ent_coef_loss   | -1.17e+03 |
|    learning_rate   | 0.000721  |
|    n_updates       | 5769596   |
----------------------------------
Eval num_timesteps=2800000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2800000   |
| train/             |           |
|    actor_loss      | 98.3      |
|    critic_loss     | 0.0386    |
|    ent_coef        | 0         |
|    ent_coef_loss   | -1.25e+03 |
|    learning_rate   | 0.00072   |
|    n_updates       | 5779596   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.91e+03 |
| time/              |           |
|    episodes        | 560       |
|    fps             | 35        |
|    time_elapsed    | 78869     |
|    total_timesteps | 2800000   |
----------------------------------
Eval num_timesteps=2810000, episode_reward=-4910.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.91e+03 |
| time/              |           |
|    total_timesteps | 2810000   |
| train/             |           |
|    actor_loss      | 98.4      |
|    critic_loss     | 0.0285    |
|    ent_coef        | 0         |
|    ent_coef_loss   | -1.35e+03 |
|    learning_rate   | 0.000719  |
|    n_updates       | 5789596   |
