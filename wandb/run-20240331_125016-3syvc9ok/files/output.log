Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_57
Eval num_timesteps=10000, episode_reward=1700.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -27.2    |
|    critic_loss     | 0.0439   |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | 0.038    |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=1238.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.031    |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | -4.93    |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 39       |
|    time_elapsed    | 505      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2009.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | -4.6     |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=1210.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.0349   |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | -4.43    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 40       |
|    time_elapsed    | 978      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1679.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 0.0421   |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | -0.559   |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=1686.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 0.0256   |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -3.05    |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 41       |
|    time_elapsed    | 1441     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1674.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.00228  |
|    ent_coef_loss   | -3.05    |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=1607.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.0644   |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | 0.00845  |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 42       |
|    time_elapsed    | 1901     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1287.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.0475   |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | 0.682    |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=1250.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 0.0434   |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -2.97    |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 42       |
|    time_elapsed    | 2370     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1779.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.044    |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=2045.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 0.0661   |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | 3.65     |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 42       |
|    time_elapsed    | 2841     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=2042.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.0683   |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
Eval num_timesteps=140000, episode_reward=2044.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 0.153    |
|    ent_coef        | 0.00251  |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 42       |
|    time_elapsed    | 3312     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2035.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
Eval num_timesteps=160000, episode_reward=1677.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 1.11     |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 42       |
|    time_elapsed    | 3783     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1673.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 0.0393   |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | -4.11    |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=1672.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.0908   |
|    ent_coef        | 0.00222  |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 42       |
|    time_elapsed    | 4250     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1251.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.0636   |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=2037.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.0952   |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 6.16     |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 40       |
|    fps             | 42       |
|    time_elapsed    | 4714     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1969.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.0725   |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
Eval num_timesteps=220000, episode_reward=1268.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | -1.09    |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 42       |
|    time_elapsed    | 5187     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=1664.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.098    |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | 0.2      |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=1662.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 2.08     |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 42       |
|    time_elapsed    | 5662     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2408.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.0864   |
|    ent_coef        | 0.00282  |
|    ent_coef_loss   | -0.662   |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
New best mean reward!
Eval num_timesteps=260000, episode_reward=1969.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.0914   |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -0.289   |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 42       |
|    time_elapsed    | 6135     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=1935.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=1243.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 0.0723   |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 5.63     |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.69e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 42       |
|    time_elapsed    | 6610     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=919.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -33.5    |
|    critic_loss     | 0.104    |
|    ent_coef        | 0.0022   |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=967.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 0.0632   |
|    ent_coef        | 0.00285  |
|    ent_coef_loss   | 0.891    |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 60       |
|    fps             | 42       |
|    time_elapsed    | 7086     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=2029.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 0.159    |
|    ent_coef        | 0.00299  |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
Eval num_timesteps=320000, episode_reward=2005.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 0.0781   |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | -5.58    |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 64       |
|    fps             | 42       |
|    time_elapsed    | 7561     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2016.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 0.0464   |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=2019.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | 0.824    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 42       |
|    time_elapsed    | 8035     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=1636.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -33.6    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00319  |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=2418.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 0.0458   |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | 0.579    |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.74e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 42       |
|    time_elapsed    | 8509     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=2424.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 0.0643   |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | 3.16     |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
New best mean reward!
Eval num_timesteps=380000, episode_reward=2433.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -34.3    |
|    critic_loss     | 0.063    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | 0.252    |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 42       |
|    time_elapsed    | 8984     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=2432.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=2425.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 80       |
|    fps             | 42       |
|    time_elapsed    | 9456     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2422.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -34.9    |
|    critic_loss     | 0.0769   |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | -6.61    |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
Eval num_timesteps=420000, episode_reward=2457.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -2.78    |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 42       |
|    time_elapsed    | 9927     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2067.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -34.7    |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.00249  |
|    ent_coef_loss   | 2.55     |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=1043.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -33.9    |
|    critic_loss     | 0.0657   |
|    ent_coef        | 0.0026   |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 42       |
|    time_elapsed    | 10401    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2434.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -34.7    |
|    critic_loss     | 0.05     |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | -3.39    |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=1057.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -35      |
|    critic_loss     | 0.239    |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | 2.55     |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 42       |
|    time_elapsed    | 10873    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2024.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -0.991   |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=2428.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -35      |
|    critic_loss     | 0.0786   |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | -3.15    |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 42       |
|    time_elapsed    | 11353    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=1750.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 0.0881   |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | -5.53    |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=2448.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -34      |
|    critic_loss     | 0.0757   |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | 1.5      |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 42       |
|    time_elapsed    | 11824    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=2086.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00351  |
|    ent_coef_loss   | -0.911   |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=2433.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | 4.69     |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 104      |
|    fps             | 42       |
|    time_elapsed    | 12303    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2428.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00355  |
|    ent_coef_loss   | 5.55     |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
Eval num_timesteps=540000, episode_reward=2056.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 0.0633   |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 42       |
|    time_elapsed    | 12772    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2370.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 1.33     |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | 3.51     |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=2424.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -36.2    |
|    critic_loss     | 0.0823   |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | -0.693   |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.97e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 42       |
|    time_elapsed    | 13244    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=2429.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.0709   |
|    ent_coef        | 0.00361  |
|    ent_coef_loss   | 1.16     |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=2432.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 0.169    |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 0.315    |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 42       |
|    time_elapsed    | 13721    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2431.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 0.0934   |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -0.338   |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=2433.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.148    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -5.41    |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 42       |
|    time_elapsed    | 14192    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2430.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.168    |
|    ent_coef        | 0.00346  |
|    ent_coef_loss   | -0.724   |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=2442.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 1.78     |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | 1.4      |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 42       |
|    time_elapsed    | 14666    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2455.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.19     |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | 7.33     |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=2512.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.158    |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | -0.623   |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 42       |
|    time_elapsed    | 15137    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2429.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.183    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | -0.0365  |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=2119.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.213    |
|    ent_coef        | 0.00425  |
|    ent_coef_loss   | -0.736   |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 42       |
|    time_elapsed    | 15609    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=1679.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | -3.04    |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=2430.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.166    |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | 0.73     |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    episodes        | 136      |
|    fps             | 42       |
|    time_elapsed    | 16082    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2425.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 0.158    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 4.6      |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=1381.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -40.3    |
|    critic_loss     | 0.145    |
|    ent_coef        | 0.00392  |
|    ent_coef_loss   | -2.13    |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 42       |
|    time_elapsed    | 16551    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=2416.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -41.4    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=2430.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.0951   |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | 0.612    |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 42       |
|    time_elapsed    | 17028    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=2419.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.00382  |
|    ent_coef_loss   | -2.02    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=1482.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 0.181    |
|    ent_coef        | 0.00325  |
|    ent_coef_loss   | 0.546    |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 42       |
|    time_elapsed    | 17504    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1264.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.0862   |
|    ent_coef        | 0.00299  |
|    ent_coef_loss   | 5.97     |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
Eval num_timesteps=760000, episode_reward=1331.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 0.0822   |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 42       |
|    time_elapsed    | 17985    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2083.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00322  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=1401.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | 1.94     |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 42       |
|    time_elapsed    | 18461    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2368.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.0645   |
|    ent_coef        | 0.00361  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=1441.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 0.1      |
|    ent_coef        | 0.00375  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 42       |
|    time_elapsed    | 18935    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=1564.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.0386   |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | -0.236   |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=2471.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | 2.71     |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 42       |
|    time_elapsed    | 19408    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2026.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.121    |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2037.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.0247   |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 42       |
|    time_elapsed    | 19884    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2062.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.0674   |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=2073.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 0.18     |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 42       |
|    time_elapsed    | 20361    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2484.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -36.8    |
|    critic_loss     | 0.0307   |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 2.91     |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=2444.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.00382  |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 42       |
|    time_elapsed    | 20845    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=1551.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 0.0819   |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -0.322   |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=1613.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -36.8    |
|    critic_loss     | 0.0745   |
|    ent_coef        | 0.00422  |
|    ent_coef_loss   | 0.797    |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 42       |
|    time_elapsed    | 21323    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=832.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -36.2    |
|    critic_loss     | 0.216    |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=2059.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 0.124    |
|    ent_coef        | 0.00424  |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 42       |
|    time_elapsed    | 21802    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2437.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2058.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00403  |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 42       |
|    time_elapsed    | 22275    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2094.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=2076.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.211    |
|    ent_coef        | 0.00346  |
|    ent_coef_loss   | 0.662    |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 42       |
|    time_elapsed    | 22750    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=2125.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 0.232    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | -0.451   |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2429.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 42       |
|    time_elapsed    | 23221    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=2043.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | 1.14     |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=2082.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 0.782    |
|    ent_coef        | 0.00431  |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 42       |
|    time_elapsed    | 23691    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=2036.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.448    |
|    ent_coef        | 0.00426  |
|    ent_coef_loss   | -0.586   |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=1618.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.274    |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | 0.619    |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 42       |
|    time_elapsed    | 24167    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=2039.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00403  |
|    ent_coef_loss   | 0.197    |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=2058.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -36.8    |
|    critic_loss     | 1.19     |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 42       |
|    time_elapsed    | 24638    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=2058.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00438  |
|    ent_coef_loss   | 1.74     |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=2108.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.0851   |
|    ent_coef        | 0.00393  |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 42       |
|    time_elapsed    | 25111    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=1714.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 3.5      |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -7.45    |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=1367.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.181    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 0.252    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 42       |
|    time_elapsed    | 25589    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2066.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00375  |
|    ent_coef_loss   | 0.781    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=1397.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.0471   |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 42       |
|    time_elapsed    | 26066    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=1765.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.042    |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | -0.589   |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=2028.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.067    |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 42       |
|    time_elapsed    | 26551    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2088.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.174    |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=1802.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.0363   |
|    ent_coef        | 0.00292  |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 42       |
|    time_elapsed    | 27105    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=2067.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 0.415    |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2077.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 0.0992   |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    episodes        | 232      |
|    fps             | 41       |
|    time_elapsed    | 27696    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2056.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00354  |
|    ent_coef_loss   | -0.94    |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2061.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 0.0681   |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | 1.18     |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 41       |
|    time_elapsed    | 28284    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2040.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2141.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 0.121    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -4.14    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 41       |
|    time_elapsed    | 28872    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=2076.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -39.9    |
|    critic_loss     | 0.0537   |
|    ent_coef        | 0.00356  |
|    ent_coef_loss   | -2.87    |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=1815.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 0.334    |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -0.901   |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 41       |
|    time_elapsed    | 29480    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2066.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.161    |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | 4.56     |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=2432.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.198    |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 41       |
|    time_elapsed    | 30086    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=1684.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.0784   |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 0.447    |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=1271.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.0705   |
|    ent_coef        | 0.00501  |
|    ent_coef_loss   | -4.62    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 41       |
|    time_elapsed    | 30690    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2432.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 0.225    |
|    ent_coef        | 0.00374  |
|    ent_coef_loss   | -0.663   |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=2068.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 2.07     |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | -2.59    |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 40       |
|    time_elapsed    | 31292    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=2500.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.194    |
|    ent_coef        | 0.00334  |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2059.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.0625   |
|    ent_coef        | 0.00299  |
|    ent_coef_loss   | 0.084    |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 40       |
|    time_elapsed    | 31897    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=2086.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.0516   |
|    ent_coef        | 0.00323  |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=2132.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 0.0831   |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | 0.51     |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 40       |
|    time_elapsed    | 32500    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2495.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 0.717    |
|    ent_coef        | 0.0025   |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2095.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 40       |
|    time_elapsed    | 33100    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2094.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | -0.748   |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2090.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.395    |
|    ent_coef        | 0.00213  |
|    ent_coef_loss   | 4.07     |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 40       |
|    time_elapsed    | 33708    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2057.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.0702   |
|    ent_coef        | 0.00261  |
|    ent_coef_loss   | -0.0615  |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2072.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 0.0873   |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 40       |
|    time_elapsed    | 34310    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2055.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 0.064    |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | 1.63     |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2052.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.0761   |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | -2.46    |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 40       |
|    time_elapsed    | 34913    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=2053.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 0.0857   |
|    ent_coef        | 0.00269  |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2064.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.129    |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | 0.0834   |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 39       |
|    time_elapsed    | 35518    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=1684.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | -0.683   |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=2070.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | 1.18     |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 39       |
|    time_elapsed    | 36120    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2433.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.00356  |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2438.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 0.0781   |
|    ent_coef        | 0.00295  |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 39       |
|    time_elapsed    | 36713    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2066.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -38.9    |
|    critic_loss     | 0.193    |
|    ent_coef        | 0.00303  |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2063.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.0359   |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | -0.838   |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 39       |
|    time_elapsed    | 37309    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2076.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00299  |
|    ent_coef_loss   | 4.93     |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2060.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | -3.22    |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 39       |
|    time_elapsed    | 37910    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2092.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | 0.934    |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=2077.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 39       |
|    time_elapsed    | 38515    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=1216.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.388    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | 1.66     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=1956.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.0688   |
|    ent_coef        | 0.00292  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 39       |
|    time_elapsed    | 39121    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2064.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.0763   |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | 3.27     |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2070.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.129    |
|    ent_coef        | 0.00311  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 39       |
|    time_elapsed    | 39723    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2078.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.243    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -3.45    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2072.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -3.92    |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 39       |
|    time_elapsed    | 40326    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=2061.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.094    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1271.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.212    |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | -3.83    |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 39       |
|    time_elapsed    | 40931    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=2076.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=2067.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.208    |
|    ent_coef        | 0.00294  |
|    ent_coef_loss   | -0.302   |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 38       |
|    time_elapsed    | 41539    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2068.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.16     |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | 2.29     |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2436.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.0292   |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -5.53    |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 38       |
|    time_elapsed    | 42142    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=1754.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.0799   |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | -3.85    |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2063.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 0.0398   |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -3.93    |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 38       |
|    time_elapsed    | 42723    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2082.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -1.77    |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2086.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 38       |
|    time_elapsed    | 43305    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2414.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | 2.54     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2449.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.183    |
|    ent_coef        | 0.00299  |
|    ent_coef_loss   | -2.71    |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 38       |
|    time_elapsed    | 43879    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=1705.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | 3.98     |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2434.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00305  |
|    ent_coef_loss   | -1.11    |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 38       |
|    time_elapsed    | 44457    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2451.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.321    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=2353.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.33     |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 38       |
|    time_elapsed    | 45028    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=1781.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.0312   |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=2428.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.289    |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 38       |
|    time_elapsed    | 45595    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2432.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.128    |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2062.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.082    |
|    ent_coef        | 0.0026   |
|    ent_coef_loss   | -4.63    |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 38       |
|    time_elapsed    | 46168    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2047.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 0.0946   |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | -1.6     |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2072.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.0748   |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 38       |
|    time_elapsed    | 46751    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2435.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.261    |
|    ent_coef        | 0.00297  |
|    ent_coef_loss   | 5.91     |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2428.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.0964   |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | -0.29    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 38       |
|    time_elapsed    | 47319    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=2025.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.259    |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2215.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.109    |
|    ent_coef        | 0.00296  |
|    ent_coef_loss   | -0.685   |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 38       |
|    time_elapsed    | 47870    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2512.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1860000, episode_reward=2087.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.1      |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | -0.83    |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 38       |
|    time_elapsed    | 48421    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2107.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.0442   |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | -0.646   |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=2087.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 38       |
|    time_elapsed    | 48985    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2065.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.0709   |
|    ent_coef        | 0.00341  |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=1225.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00422  |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 38       |
|    time_elapsed    | 49562    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=1690.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.145    |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | -4.14    |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=1261.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.243    |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | -3.92    |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 38       |
|    time_elapsed    | 50148    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=1049.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00502  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2074.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.169    |
|    ent_coef        | 0.00571  |
|    ent_coef_loss   | -0.499   |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 38       |
|    time_elapsed    | 50731    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2061.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2428.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.395    |
|    ent_coef        | 0.00463  |
|    ent_coef_loss   | 3.75     |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    episodes        | 392      |
|    fps             | 38       |
|    time_elapsed    | 51311    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=2424.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 0.362    |
|    ent_coef        | 0.00462  |
|    ent_coef_loss   | -3.23    |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2411.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -35.2    |
|    critic_loss     | 0.655    |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | -6.04    |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 38       |
|    time_elapsed    | 51892    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=2416.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 0.194    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -4.93    |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2425.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.0711   |
|    ent_coef        | 0.00366  |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 38       |
|    time_elapsed    | 52469    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2431.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | -3.31    |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=2429.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 1.1      |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 38       |
|    time_elapsed    | 53055    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2433.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.405    |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | -0.198   |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=2430.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.541    |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | 2.94     |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 38       |
|    time_elapsed    | 53624    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2426.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.159    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | 4.28     |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2425.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | 0.746    |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 38       |
|    time_elapsed    | 54203    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2431.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 0.0987   |
|    ent_coef        | 0.00442  |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=2435.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00457  |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 37       |
|    time_elapsed    | 54790    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=2414.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.1      |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | 0.162    |
|    learning_rate   | 0.000791 |
|    n_updates       | 3434324  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=2423.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00367  |
|    ent_coef_loss   | 0.445    |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 37       |
|    time_elapsed    | 55377    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2491.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.218    |
|    ent_coef        | 0.00375  |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=2059.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 0.381    |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.000788 |
|    n_updates       | 3464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 37       |
|    time_elapsed    | 55958    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2102.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=2067.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.199    |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 37       |
|    time_elapsed    | 56536    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=2067.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.193    |
|    ent_coef        | 0.0046   |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=2061.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.322    |
|    ent_coef        | 0.0048   |
|    ent_coef_loss   | 0.887    |
|    learning_rate   | 0.000784 |
|    n_updates       | 3504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 37       |
|    time_elapsed    | 57046    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=2044.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 0.2      |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.000783 |
|    n_updates       | 3514324  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=2440.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 0.303    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | -5       |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 37       |
|    time_elapsed    | 57527    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=2444.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | 0.639    |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=1990.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.217    |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 6.59     |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 37       |
|    time_elapsed    | 58011    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=2425.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -34.9    |
|    critic_loss     | 0.217    |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=1217.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 0.455    |
|    ent_coef        | 0.00526  |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 0.000778 |
|    n_updates       | 3564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 37       |
|    time_elapsed    | 58499    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2410.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | 1.06     |
|    learning_rate   | 0.000777 |
|    n_updates       | 3574324  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2492.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -35.4    |
|    critic_loss     | 0.164    |
|    ent_coef        | 0.0053   |
|    ent_coef_loss   | -0.473   |
|    learning_rate   | 0.000776 |
|    n_updates       | 3584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 37       |
|    time_elapsed    | 58995    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2439.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -36.7    |
|    critic_loss     | 0.209    |
|    ent_coef        | 0.00471  |
|    ent_coef_loss   | 1.6      |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2441.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.354    |
|    ent_coef        | 0.00414  |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 37       |
|    time_elapsed    | 59491    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2441.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.305    |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=2432.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.00597  |
|    ent_coef_loss   | 0.564    |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 38       |
|    time_elapsed    | 59985    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2429.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 0.143    |
|    ent_coef        | 0.00552  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=2414.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 38       |
|    time_elapsed    | 60473    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=2294.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.44     |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2340.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.219    |
|    ent_coef        | 0.00567  |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 38       |
|    time_elapsed    | 60965    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=2426.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 0.427    |
|    ent_coef        | 0.00502  |
|    ent_coef_loss   | -0.386   |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=2415.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.209    |
|    ent_coef        | 0.00433  |
|    ent_coef_loss   | 2.02     |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.12e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 38       |
|    time_elapsed    | 61456    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=2422.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 1.17     |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -0.201   |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=2432.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.00402  |
|    ent_coef_loss   | 0.31     |
|    learning_rate   | 0.000764 |
|    n_updates       | 3704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 38       |
|    time_elapsed    | 61947    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=2421.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.396    |
|    ent_coef        | 0.00412  |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=2065.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -36.8    |
|    critic_loss     | 0.805    |
|    ent_coef        | 0.0053   |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 38       |
|    time_elapsed    | 62436    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=2062.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 0.585    |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2422.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.469    |
|    ent_coef        | 0.00661  |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 38       |
|    time_elapsed    | 62920    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=2432.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=2444.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -35.4    |
|    critic_loss     | 0.187    |
|    ent_coef        | 0.00671  |
|    ent_coef_loss   | 0.0967   |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 38       |
|    time_elapsed    | 63401    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2423.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -35.4    |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.00739  |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=2453.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -34.9    |
|    critic_loss     | 0.429    |
|    ent_coef        | 0.00712  |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 38       |
|    time_elapsed    | 63887    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2429.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 0.611    |
|    ent_coef        | 0.00641  |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=1752.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 0.617    |
|    ent_coef        | 0.00611  |
|    ent_coef_loss   | 0.873    |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.24e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 38       |
|    time_elapsed    | 64364    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=2437.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 2.34     |
|    ent_coef        | 0.00654  |
|    ent_coef_loss   | -0.281   |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=2007.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 5.2      |
|    ent_coef        | 0.00624  |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 38       |
|    time_elapsed    | 64853    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=2021.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000751 |
|    n_updates       | 3834324  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2445.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.00529  |
|    ent_coef_loss   | 4.18     |
|    learning_rate   | 0.00075  |
|    n_updates       | 3844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 38       |
|    time_elapsed    | 65345    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=2121.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.00495  |
|    ent_coef_loss   | -1.96    |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=2455.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.3      |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 38       |
|    time_elapsed    | 65829    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=2108.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 2.43     |
|    ent_coef        | 0.00481  |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=2066.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 0.225    |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | 5.93     |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    episodes        | 508      |
|    fps             | 38       |
|    time_elapsed    | 66321    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2106.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.38     |
|    ent_coef        | 0.00596  |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=1288.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.109    |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 38       |
|    time_elapsed    | 66814    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=1283.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 0.136    |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2014.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.173    |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | 4.2      |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 38       |
|    time_elapsed    | 67304    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=2025.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 0.726    |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 0.762    |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2483.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 0.168    |
|    ent_coef        | 0.00534  |
|    ent_coef_loss   | 0.199    |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 38       |
|    time_elapsed    | 67795    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=2545.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.0633   |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 0.548    |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
New best mean reward!
Eval num_timesteps=2620000, episode_reward=2082.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 0.261    |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 1.97     |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 38       |
|    time_elapsed    | 68290    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=2151.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.905    |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | 0.651    |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=2494.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | 4.3      |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 38       |
|    time_elapsed    | 68781    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2076.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -38.3    |
|    critic_loss     | 0.416    |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2130.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00513  |
|    ent_coef_loss   | 4.73     |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 38       |
|    time_elapsed    | 69273    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=2079.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.0794   |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 0.487    |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=1119.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 0.881    |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | 0.273    |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 38       |
|    time_elapsed    | 69766    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=2128.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.123    |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | 7.37     |
|    learning_rate   | 0.000731 |
|    n_updates       | 4034324  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2465.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | -0.606   |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 540      |
|    fps             | 38       |
|    time_elapsed    | 70252    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=2480.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -40.1    |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.0061   |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.000729 |
|    n_updates       | 4054324  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=2549.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 38       |
|    time_elapsed    | 70742    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=2485.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000727 |
|    n_updates       | 4074324  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=2465.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | -0.588   |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 38       |
|    time_elapsed    | 71223    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2127.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -39.8    |
|    critic_loss     | 0.226    |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | -2.26    |
|    learning_rate   | 0.000725 |
|    n_updates       | 4094324  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=2492.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -40.9    |
|    critic_loss     | 0.191    |
|    ent_coef        | 0.0054   |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    episodes        | 552      |
|    fps             | 38       |
|    time_elapsed    | 71701    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=2082.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00658  |
|    ent_coef_loss   | -0.503   |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2079.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.264    |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | 1.2      |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 556      |
|    fps             | 38       |
|    time_elapsed    | 72188    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=2481.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2078.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -38.9    |
|    critic_loss     | 0.132    |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | 2.59     |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.2e+03  |
| time/              |          |
|    episodes        | 560      |
|    fps             | 38       |
|    time_elapsed    | 72670    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=2061.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | 0.208    |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=2086.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000718 |
|    n_updates       | 4164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 38       |
|    time_elapsed    | 73161    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=2070.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 0.0478   |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | 0.81     |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=2072.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -39.8    |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00332  |
|    ent_coef_loss   | 4.3      |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 568      |
|    fps             | 38       |
|    time_elapsed    | 73652    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2048.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 0.0788   |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=2049.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.0047   |
|    ent_coef_loss   | 3.45     |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 572      |
|    fps             | 38       |
|    time_elapsed    | 74147    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=2491.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -41      |
|    critic_loss     | 0.244    |
|    ent_coef        | 0.00608  |
|    ent_coef_loss   | -2.51    |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=2078.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.108    |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 576      |
|    fps             | 38       |
|    time_elapsed    | 74634    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=2075.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -40.6    |
|    critic_loss     | 0.18     |
|    ent_coef        | 0.00579  |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=2072.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -40.5    |
|    critic_loss     | 0.492    |
|    ent_coef        | 0.00468  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 580      |
|    fps             | 38       |
|    time_elapsed    | 75126    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=2084.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.0899   |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------