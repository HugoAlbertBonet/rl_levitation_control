Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_52
Eval num_timesteps=10000, episode_reward=-968.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -969     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -16.9    |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-969.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -970     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -6.03    |
|    critic_loss     | 0.069    |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | -7.99    |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -968     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 31       |
|    time_elapsed    | 634      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-964.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -965     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 4.74     |
|    critic_loss     | 0.218    |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=-964.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -964     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 11       |
|    critic_loss     | 0.0855   |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | -2.51    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -962     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 31       |
|    time_elapsed    | 1271     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-964.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -964     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 16.2     |
|    critic_loss     | 0.0517   |
|    ent_coef        | 0.00184  |
|    ent_coef_loss   | -3.13    |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=-953.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -953     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 17.5     |
|    critic_loss     | 0.0969   |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | 2.98     |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -958     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 31       |
|    time_elapsed    | 1889     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=541.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 15.8     |
|    critic_loss     | 0.0599   |
|    ent_coef        | 0.0014   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=-960.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -960     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 13.3     |
|    critic_loss     | 0.0562   |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | 2.5      |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -576     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 32       |
|    time_elapsed    | 2489     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=558.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 10.9     |
|    critic_loss     | 0.0595   |
|    ent_coef        | 0.00208  |
|    ent_coef_loss   | -2.98    |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
New best mean reward!
Eval num_timesteps=100000, episode_reward=-957.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -958     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 10.9     |
|    critic_loss     | 0.0883   |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -448     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 32       |
|    time_elapsed    | 3088     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-957.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 10.4     |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -4.01    |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=-950.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 8.63     |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -533     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 32       |
|    time_elapsed    | 3683     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1012.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 6.06     |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
New best mean reward!
Eval num_timesteps=140000, episode_reward=-956.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -956     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 6.74     |
|    critic_loss     | 0.0521   |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -435     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 32       |
|    time_elapsed    | 4273     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-957.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -958     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 7.3      |
|    critic_loss     | 0.057    |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
Eval num_timesteps=160000, episode_reward=1078.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 6.71     |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.00148  |
|    ent_coef_loss   | 6.75     |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -439     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 33       |
|    time_elapsed    | 4841     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1127.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 4.55     |
|    critic_loss     | 0.167    |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | 1.11     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
New best mean reward!
Eval num_timesteps=180000, episode_reward=-944.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 4.67     |
|    critic_loss     | 0.104    |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -6.92    |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -314     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 33       |
|    time_elapsed    | 5395     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-960.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -961     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 4.29     |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | 0.951    |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=1070.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 4.05     |
|    critic_loss     | 0.128    |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -0.516   |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 33       |
|    time_elapsed    | 5971     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2010.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -1.87    |
|    critic_loss     | 0.167    |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | 6.17     |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
New best mean reward!
Eval num_timesteps=220000, episode_reward=1400.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 0.201    |
|    critic_loss     | 0.0511   |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -22.8    |
| time/              |          |
|    episodes        | 44       |
|    fps             | 33       |
|    time_elapsed    | 6554     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2001.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.134    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00155  |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=1693.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -4.46    |
|    critic_loss     | 0.0402   |
|    ent_coef        | 0.00161  |
|    ent_coef_loss   | 9.02     |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 142      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 33       |
|    time_elapsed    | 7139     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-970.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -970     |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -4.42    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
Eval num_timesteps=260000, episode_reward=928.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -3.09    |
|    critic_loss     | 0.103    |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | 2.82     |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 183      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 33       |
|    time_elapsed    | 7727     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=2010.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 0.0511   |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | -3.98    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=2010.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 0.0904   |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | -4.17    |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 297      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 33       |
|    time_elapsed    | 8321     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1956.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -4.38    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.00259  |
|    ent_coef_loss   | -2.57    |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=2010.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.0721   |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | -4.25    |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 411      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 33       |
|    time_elapsed    | 8898     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=2010.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -8.71    |
|    critic_loss     | 0.0599   |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | 4.81     |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
Eval num_timesteps=320000, episode_reward=2010.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.0848   |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 0.965    |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 511      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 33       |
|    time_elapsed    | 9419     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2010.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -9.04    |
|    critic_loss     | 0.0476   |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -5.36    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=2010.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -6.44    |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.00285  |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 600      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 34       |
|    time_elapsed    | 9888     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=2000.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.0437   |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=2010.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.0647   |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | -3.82    |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 678      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 34       |
|    time_elapsed    | 10352    |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=2010.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.042    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
Eval num_timesteps=380000, episode_reward=2011.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0635   |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | 0.516    |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 747      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 35       |
|    time_elapsed    | 10822    |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=2011.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -15.2    |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00292  |
|    ent_coef_loss   | -0.629   |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=2010.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -9.7     |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | -8.01    |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 811      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 35       |
|    time_elapsed    | 11295    |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2011.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | 0.053    |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
New best mean reward!
Eval num_timesteps=420000, episode_reward=2011.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -15.7    |
|    critic_loss     | 0.195    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 868      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 35       |
|    time_elapsed    | 11769    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2010.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -0.64    |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=2010.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -15.8    |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00303  |
|    ent_coef_loss   | -6.04    |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 35       |
|    time_elapsed    | 12245    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2005.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -18.2    |
|    critic_loss     | 0.0314   |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | -4.75    |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=2011.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -18.3    |
|    critic_loss     | 0.0675   |
|    ent_coef        | 0.00294  |
|    ent_coef_loss   | 5.55     |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 954      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 36       |
|    time_elapsed    | 12719    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-967.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -16.9    |
|    critic_loss     | 0.036    |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=2011.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -14.4    |
|    critic_loss     | 0.0838   |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | -2.54    |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 967      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 36       |
|    time_elapsed    | 13194    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=2011.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -15.5    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00256  |
|    ent_coef_loss   | -3.73    |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=1480.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -17.6    |
|    critic_loss     | 0.206    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | -0.755   |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 988      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 36       |
|    time_elapsed    | 13666    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=640.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -19.1    |
|    critic_loss     | 0.0486   |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 1.91     |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=2004.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -20.1    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | 0.245    |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 36       |
|    time_elapsed    | 14141    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2012.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -18.4    |
|    critic_loss     | 0.0732   |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | 0.712    |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
New best mean reward!
Eval num_timesteps=540000, episode_reward=2010.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -16.2    |
|    critic_loss     | 0.0769   |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | -0.7     |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 36       |
|    time_elapsed    | 14616    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2012.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 0.0539   |
|    ent_coef        | 0.00355  |
|    ent_coef_loss   | -0.0717  |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=2012.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -15.4    |
|    critic_loss     | 0.0836   |
|    ent_coef        | 0.0033   |
|    ent_coef_loss   | -4.44    |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    episodes        | 112      |
|    fps             | 37       |
|    time_elapsed    | 15087    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=2012.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 0.0467   |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | 3.07     |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
New best mean reward!
Eval num_timesteps=580000, episode_reward=2012.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -19.3    |
|    critic_loss     | 0.0237   |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 37       |
|    time_elapsed    | 15559    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2009.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 0.0208   |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=2012.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -16.9    |
|    critic_loss     | 0.0266   |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | -2.48    |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 37       |
|    time_elapsed    | 16032    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2012.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -22.6    |
|    critic_loss     | 0.0221   |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=1039.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -20.5    |
|    critic_loss     | 0.0846   |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | -4.57    |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 37       |
|    time_elapsed    | 16503    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2011.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=1726.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -23.4    |
|    critic_loss     | 0.056    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | 3.55     |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 37       |
|    time_elapsed    | 16975    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2012.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 0.0529   |
|    ent_coef        | 0.00267  |
|    ent_coef_loss   | 0.478    |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=990.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 132      |
|    fps             | 37       |
|    time_elapsed    | 17447    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=2012.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.0451   |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | 3.79     |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=2012.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -18.7    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 37       |
|    time_elapsed    | 17919    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2012.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -20.5    |
|    critic_loss     | 0.0582   |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | -7.49    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=2011.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 0.0111   |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 140      |
|    fps             | 38       |
|    time_elapsed    | 18392    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=2010.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -20.4    |
|    critic_loss     | 0.0226   |
|    ent_coef        | 0.00261  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=2011.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 0.0732   |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -0.436   |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 38       |
|    time_elapsed    | 18867    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=2010.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.0346   |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | 0.197    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=2011.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -21.5    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | 9.4      |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 38       |
|    time_elapsed    | 19339    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2012.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -23.5    |
|    critic_loss     | 0.0457   |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | -8.43    |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
Eval num_timesteps=760000, episode_reward=2011.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -22.5    |
|    critic_loss     | 0.0386   |
|    ent_coef        | 0.00305  |
|    ent_coef_loss   | 1        |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 38       |
|    time_elapsed    | 19814    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2010.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.149    |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | -4.06    |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=2010.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 0.041    |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | -1.97    |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 38       |
|    time_elapsed    | 20291    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2010.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -21.6    |
|    critic_loss     | 0.0228   |
|    ent_coef        | 0.00302  |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=2012.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -25.5    |
|    critic_loss     | 0.0506   |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -3.75    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 38       |
|    time_elapsed    | 20764    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-967.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 0.0731   |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=-963.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -964     |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 0.0169   |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | 1        |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 38       |
|    time_elapsed    | 21236    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2010.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 0.0751   |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -1.33    |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2010.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.034    |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -0.941   |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 38       |
|    time_elapsed    | 21711    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-966.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -25.3    |
|    critic_loss     | 0.0757   |
|    ent_coef        | 0.00302  |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=-967.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -22.2    |
|    critic_loss     | 0.0256   |
|    ent_coef        | 0.00289  |
|    ent_coef_loss   | -1.73    |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 38       |
|    time_elapsed    | 22186    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-966.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.127    |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=-958.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -959     |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 0.0274   |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | 0.548    |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 38       |
|    time_elapsed    | 22658    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2011.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 0.0348   |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | -4.62    |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2011.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -20.6    |
|    critic_loss     | 0.0174   |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | -0.567   |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 38       |
|    time_elapsed    | 23134    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2011.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 0.0727   |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | -3.84    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=2011.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -19.7    |
|    critic_loss     | 0.0751   |
|    ent_coef        | 0.00235  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 38       |
|    time_elapsed    | 23607    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2011.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -22.6    |
|    critic_loss     | 0.0247   |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2011.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -22.5    |
|    critic_loss     | 0.0436   |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -3.85    |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 39       |
|    time_elapsed    | 24081    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-966.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.0466   |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | -2.39    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=2010.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.0177   |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | 0.646    |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 39       |
|    time_elapsed    | 24561    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=-968.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -969     |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -19.8    |
|    critic_loss     | 0.053    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | -4.57    |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=-967.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -23.1    |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | 0.0562   |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 39       |
|    time_elapsed    | 25041    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=-967.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -21.7    |
|    critic_loss     | 0.0211   |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | -0.698   |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=-967.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.0116   |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -1.39    |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 39       |
|    time_elapsed    | 25529    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=-967.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 0.0131   |
|    ent_coef        | 0.00237  |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=-965.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -19.4    |
|    critic_loss     | 0.566    |
|    ent_coef        | 0.00237  |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 39       |
|    time_elapsed    | 26008    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=-967.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -21.5    |
|    critic_loss     | 0.0402   |
|    ent_coef        | 0.00237  |
|    ent_coef_loss   | 2.02     |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=2010.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.0731   |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | 4.37     |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 208      |
|    fps             | 39       |
|    time_elapsed    | 26485    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=867.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.0944   |
|    ent_coef        | 0.00223  |
|    ent_coef_loss   | 0.965    |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=1042.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -24.4    |
|    critic_loss     | 0.00993  |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 5.47     |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 212      |
|    fps             | 39       |
|    time_elapsed    | 26960    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2010.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -22.2    |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00196  |
|    ent_coef_loss   | -2.7     |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=2011.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 0.0178   |
|    ent_coef        | 0.00215  |
|    ent_coef_loss   | -6.77    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 39       |
|    time_elapsed    | 27434    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2012.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -24.7    |
|    critic_loss     | 0.0328   |
|    ent_coef        | 0.00243  |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=2001.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -23.2    |
|    critic_loss     | 0.0159   |
|    ent_coef        | 0.00184  |
|    ent_coef_loss   | 1.79     |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 39       |
|    time_elapsed    | 27906    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=2002.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.0205   |
|    ent_coef        | 0.00186  |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=2001.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -24.4    |
|    critic_loss     | 5.85     |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 39       |
|    time_elapsed    | 28376    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2001.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -26.2    |
|    critic_loss     | 0.0457   |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=2001.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 0.0199   |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 3.33     |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 39       |
|    time_elapsed    | 28847    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=2001.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 0.0385   |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2001.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -26.2    |
|    critic_loss     | 0.0286   |
|    ent_coef        | 0.00216  |
|    ent_coef_loss   | -0.00124 |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 39       |
|    time_elapsed    | 29318    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2011.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.0113   |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2011.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.00514  |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 39       |
|    time_elapsed    | 29788    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2012.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.003    |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | -0.0227  |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2012.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.0115   |
|    ent_coef        | 0.00148  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 39       |
|    time_elapsed    | 30259    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=2011.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 0.00393  |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -3.25    |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=2013.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.0433   |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -4.91    |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 39       |
|    time_elapsed    | 30733    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2012.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 6.32     |
|    ent_coef        | 0.00128  |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=993.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.00494  |
|    ent_coef        | 0.00105  |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 248      |
|    fps             | 39       |
|    time_elapsed    | 31203    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=2012.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 0.00359  |
|    ent_coef        | 0.00115  |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=2013.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.0406   |
|    ent_coef        | 0.00141  |
|    ent_coef_loss   | -5.76    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 39       |
|    time_elapsed    | 31678    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2011.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 0.0022   |
|    ent_coef        | 0.00119  |
|    ent_coef_loss   | -0.415   |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=2012.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 0.00487  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -2.21    |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 39       |
|    time_elapsed    | 32155    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=2002.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.00672  |
|    ent_coef        | 0.00121  |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2013.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -27.2    |
|    critic_loss     | 0.0824   |
|    ent_coef        | 0.0011   |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 39       |
|    time_elapsed    | 32628    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=589.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.0332   |
|    ent_coef        | 0.000732 |
|    ent_coef_loss   | 0.693    |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=2003.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -27      |
|    critic_loss     | 0.00302  |
|    ent_coef        | 0.000696 |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 39       |
|    time_elapsed    | 33099    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2014.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.0098   |
|    ent_coef        | 0.000826 |
|    ent_coef_loss   | -5.4     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1340000, episode_reward=2013.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 0.0118   |
|    ent_coef        | 0.00114  |
|    ent_coef_loss   | -2.7     |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 39       |
|    time_elapsed    | 33566    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2014.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.0035   |
|    ent_coef        | 0.000706 |
|    ent_coef_loss   | -7.41    |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2012.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 0.0561   |
|    ent_coef        | 0.00121  |
|    ent_coef_loss   | 5.76     |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 39       |
|    time_elapsed    | 34036    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2012.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 0.0478   |
|    ent_coef        | 0.000905 |
|    ent_coef_loss   | -7.99    |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2018.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 0.00788  |
|    ent_coef        | 0.000683 |
|    ent_coef_loss   | 2.86     |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 39       |
|    time_elapsed    | 34508    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2003.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 0.0104   |
|    ent_coef        | 0.00107  |
|    ent_coef_loss   | 3.7      |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2002.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 0.173    |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -0.00832 |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 40       |
|    time_elapsed    | 34981    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=1989.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 0.0349   |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=1833.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.00959  |
|    ent_coef        | 0.00122  |
|    ent_coef_loss   | 14.8     |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 40       |
|    time_elapsed    | 35451    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=533.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 0.006    |
|    ent_coef        | 0.000842 |
|    ent_coef_loss   | -3.31    |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=1990.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.0254   |
|    ent_coef        | 0.00119  |
|    ent_coef_loss   | -7.22    |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 40       |
|    time_elapsed    | 35919    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2001.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.01     |
|    ent_coef        | 0.000979 |
|    ent_coef_loss   | -5.73    |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2001.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 3.69     |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -5.61    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 40       |
|    time_elapsed    | 36385    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2000.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.00279  |
|    ent_coef        | 0.00114  |
|    ent_coef_loss   | -4.06    |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2000.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.0211   |
|    ent_coef        | 0.00125  |
|    ent_coef_loss   | -7.43    |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 40       |
|    time_elapsed    | 36856    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2000.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -27.3    |
|    critic_loss     | 0.00549  |
|    ent_coef        | 0.00108  |
|    ent_coef_loss   | -5.53    |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=-786.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -786     |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.00592  |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 40       |
|    time_elapsed    | 37331    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2000.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | 8.54     |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=1990.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -5.23    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 40       |
|    time_elapsed    | 37805    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=1995.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 0.0407   |
|    ent_coef        | 0.000894 |
|    ent_coef_loss   | -0.346   |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2001.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.0434   |
|    ent_coef        | 0.000986 |
|    ent_coef_loss   | -4.42    |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 40       |
|    time_elapsed    | 38281    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2003.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 0.0525   |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | 17.8     |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2002.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.00944  |
|    ent_coef        | 0.000924 |
|    ent_coef_loss   | -12.3    |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 40       |
|    time_elapsed    | 38754    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2001.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.0441   |
|    ent_coef        | 0.000764 |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2005.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.0669   |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | 9.07     |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 316      |
|    fps             | 40       |
|    time_elapsed    | 39221    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=535.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 0.0189   |
|    ent_coef        | 0.00119  |
|    ent_coef_loss   | -15      |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1107.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.0384   |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | 5.88     |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 40       |
|    time_elapsed    | 39693    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=1106.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.0116   |
|    ent_coef        | 0.00106  |
|    ent_coef_loss   | 10.9     |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=2003.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 0.00269  |
|    ent_coef        | 0.000989 |
|    ent_coef_loss   | -12      |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 40       |
|    time_elapsed    | 40160    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2005.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 0.0132   |
|    ent_coef        | 0.000813 |
|    ent_coef_loss   | -9.65    |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2006.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 0.00494  |
|    ent_coef        | 0.000917 |
|    ent_coef_loss   | 2.18     |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 40       |
|    time_elapsed    | 40625    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2004.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 0.0195   |
|    ent_coef        | 0.000768 |
|    ent_coef_loss   | 4.78     |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2012.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.0131   |
|    ent_coef        | 0.0013   |
|    ent_coef_loss   | 8.13     |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 40       |
|    time_elapsed    | 41092    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=1952.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2011.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.0154   |
|    ent_coef        | 0.00137  |
|    ent_coef_loss   | -2.93    |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 40       |
|    time_elapsed    | 41555    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2011.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.00907  |
|    ent_coef        | 0.00131  |
|    ent_coef_loss   | -0.127   |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2000.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.0122   |
|    ent_coef        | 0.00128  |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 40       |
|    time_elapsed    | 42022    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2000.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.0119   |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2000.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.042    |
|    ent_coef        | 0.00131  |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 40       |
|    time_elapsed    | 42488    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2000.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.0108   |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | -4.59    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=2000.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 0.0064   |
|    ent_coef        | 0.00141  |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 40       |
|    time_elapsed    | 42954    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=617.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.00707  |
|    ent_coef        | 0.00145  |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=1831.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 0.0262   |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | -4.23    |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 40       |
|    time_elapsed    | 43420    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=1948.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.0214   |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2010.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.0274   |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | -0.16    |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 40       |
|    time_elapsed    | 43888    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2010.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.0191   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | -4.07    |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2000.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.0345   |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 40       |
|    time_elapsed    | 44351    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=1090.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 0.0662   |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | 0.397    |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=1074.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | -6.27    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 40       |
|    time_elapsed    | 44816    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=2012.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | -4.19    |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2015.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.387    |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | -6.77    |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 40       |
|    time_elapsed    | 45284    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2005.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 0.274    |
|    ent_coef        | 0.00296  |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=1995.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.266    |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 40       |
|    time_elapsed    | 45754    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2004.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.0618   |
|    ent_coef        | 0.00473  |
|    ent_coef_loss   | 2.03     |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=2016.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | -0.33    |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 40       |
|    time_elapsed    | 46222    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2015.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.0885   |
|    ent_coef        | 0.00421  |
|    ent_coef_loss   | -1.39    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2017.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 40       |
|    time_elapsed    | 46689    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=2015.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 0.0874   |
|    ent_coef        | 0.00426  |
|    ent_coef_loss   | -2.68    |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=2015.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.183    |
|    ent_coef        | 0.0039   |
|    ent_coef_loss   | -0.585   |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 40       |
|    time_elapsed    | 47160    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2017.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.0749   |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | -0.0816  |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2015.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.0722   |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 4.11     |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 40       |
|    time_elapsed    | 47633    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2016.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 0.747    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -0.529   |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2016.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 0.207    |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 40       |
|    time_elapsed    | 48110    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=2016.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.0501   |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2016.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 0.06     |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 40       |
|    time_elapsed    | 48587    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=2016.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -32.5    |
|    critic_loss     | 0.136    |
|    ent_coef        | 0.00357  |
|    ent_coef_loss   | 4.68     |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2016.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -34.1    |
|    critic_loss     | 0.209    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 40       |
|    time_elapsed    | 49061    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2009.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 0.0476   |
|    ent_coef        | 0.00461  |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=2008.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 0.222    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -0.7     |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 404      |
|    fps             | 40       |
|    time_elapsed    | 49540    |
|    total_timesteps | 2020000  |
