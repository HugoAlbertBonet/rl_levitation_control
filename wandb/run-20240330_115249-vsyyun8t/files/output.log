Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_50
Eval num_timesteps=10000, episode_reward=36.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.5     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=36.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.9     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -17.4    |
|    critic_loss     | 0.058    |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 37.4     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 45       |
|    time_elapsed    | 443      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2115.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -9.62    |
|    critic_loss     | 0.0645   |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | -3.47    |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=58.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 58.7     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0824   |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | -4.23    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 227      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 45       |
|    time_elapsed    | 885      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=36.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 37       |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -7.63    |
|    critic_loss     | 0.0554   |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=37.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 37.1     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -7.08    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -2.63    |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 166      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 45       |
|    time_elapsed    | 1305     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=34.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 34.4     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -4.03    |
|    critic_loss     | 0.198    |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -10      |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=74.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 74.9     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -5.05    |
|    critic_loss     | 0.079    |
|    ent_coef        | 0.0021   |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 135      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 45       |
|    time_elapsed    | 1745     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1533.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -5.33    |
|    critic_loss     | 0.0828   |
|    ent_coef        | 0.00171  |
|    ent_coef_loss   | -0.486   |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=52.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 52.9     |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 0.0821   |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | 5.44     |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 344      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 46       |
|    time_elapsed    | 2170     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=33.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 33.7     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -8.93    |
|    critic_loss     | 0.0387   |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -0.522   |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=1578.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.161    |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | 5.88     |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 441      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 46       |
|    time_elapsed    | 2588     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=47.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 47.2     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 0.0739   |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | -6.29    |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
Eval num_timesteps=140000, episode_reward=34.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 34       |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.0385   |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -4.69    |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 495      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 46       |
|    time_elapsed    | 3005     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=57.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57.2     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.165    |
|    ent_coef        | 0.00167  |
|    ent_coef_loss   | 5.33     |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
Eval num_timesteps=160000, episode_reward=66.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 66.8     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 438      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 46       |
|    time_elapsed    | 3423     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1681.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | 4.41     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=2098.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.0749   |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | 0.677    |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 482      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 46       |
|    time_elapsed    | 3840     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=71.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 72       |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.177    |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=1695.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | -6.22    |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 531      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 46       |
|    time_elapsed    | 4256     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2083.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.093    |
|    ent_coef        | 0.0018   |
|    ent_coef_loss   | 6.29     |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
Eval num_timesteps=220000, episode_reward=2099.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00165  |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 676      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 47       |
|    time_elapsed    | 4671     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2102.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0492   |
|    ent_coef        | 0.00153  |
|    ent_coef_loss   | 0.887    |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=2104.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 2.74     |
|    ent_coef        | 0.00151  |
|    ent_coef_loss   | 4.4      |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 702      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 47       |
|    time_elapsed    | 5088     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2109.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.4      |
|    ent_coef        | 0.00126  |
|    ent_coef_loss   | 6.61     |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
Eval num_timesteps=260000, episode_reward=1560.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -17.9    |
|    critic_loss     | 0.0587   |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | -4.62    |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 790      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 47       |
|    time_elapsed    | 5505     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=1973.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -16.3    |
|    critic_loss     | 0.0961   |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | 2.92     |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=2141.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -17.9    |
|    critic_loss     | 0.0525   |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 0.713    |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 872      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 47       |
|    time_elapsed    | 5921     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=40.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 40.5     |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -18.9    |
|    critic_loss     | 0.0332   |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=53.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 53.4     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.24     |
|    ent_coef        | 0.0018   |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 853      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 47       |
|    time_elapsed    | 6336     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=2159.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -14.7    |
|    critic_loss     | 0.0582   |
|    ent_coef        | 0.00196  |
|    ent_coef_loss   | -1.96    |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
New best mean reward!
Eval num_timesteps=320000, episode_reward=2131.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.0997   |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 2.75     |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 901      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 47       |
|    time_elapsed    | 6751     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2138.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 0.0877   |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=69.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 69.4     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 0.0781   |
|    ent_coef        | 0.00202  |
|    ent_coef_loss   | -4.36    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 973      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 47       |
|    time_elapsed    | 7166     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=65.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 65.7     |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -19.3    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | -0.216   |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=3019.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -18.2    |
|    critic_loss     | 0.0818   |
|    ent_coef        | 0.00204  |
|    ent_coef_loss   | -2.21    |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 953      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 47       |
|    time_elapsed    | 7582     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=73.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 73.8     |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -20.2    |
|    critic_loss     | 0.374    |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
Eval num_timesteps=380000, episode_reward=74.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 74.8     |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -18.9    |
|    critic_loss     | 0.127    |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 962      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 47       |
|    time_elapsed    | 7998     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=2495.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -17.7    |
|    critic_loss     | 0.0694   |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | 0.708    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=94.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 94       |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.122    |
|    ent_coef        | 0.00251  |
|    ent_coef_loss   | -0.75    |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 979      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 47       |
|    time_elapsed    | 8413     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=3038.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | 5.83     |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
New best mean reward!
Eval num_timesteps=420000, episode_reward=2176.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -19.1    |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00231  |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 47       |
|    time_elapsed    | 8829     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2166.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -19.6    |
|    critic_loss     | 0.0791   |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | -0.701   |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=3014.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -21.4    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 47       |
|    time_elapsed    | 9246     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=103.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 103      |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -20.6    |
|    critic_loss     | 0.17     |
|    ent_coef        | 0.0025   |
|    ent_coef_loss   | 3.74     |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=77.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 77.3     |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 0.327    |
|    ent_coef        | 0.00229  |
|    ent_coef_loss   | 6.89     |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 47       |
|    time_elapsed    | 9662     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2182.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -20      |
|    critic_loss     | 0.602    |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | -3.57    |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=60.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 60.3     |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.228    |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | 2.85     |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 47       |
|    time_elapsed    | 10077    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=143.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -18.3    |
|    critic_loss     | 0.174    |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -5.1     |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=76.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 76.5     |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -20.5    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 0.867    |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 47       |
|    time_elapsed    | 10494    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=2451.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -21.7    |
|    critic_loss     | 0.226    |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | 3.63     |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=2174.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 0.555    |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 5.85     |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 47       |
|    time_elapsed    | 10910    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=80.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 80.1     |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -18.5    |
|    critic_loss     | 0.0808   |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | -7.12    |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
Eval num_timesteps=540000, episode_reward=94.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 94.7     |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -20.5    |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -4.64    |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 47       |
|    time_elapsed    | 11326    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=142.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 0.183    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -0.153   |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=2211.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 0.394    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 47       |
|    time_elapsed    | 11742    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=2324.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -18.2    |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -0.243   |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=2242.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 0.188    |
|    ent_coef        | 0.00285  |
|    ent_coef_loss   | -2.91    |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 47       |
|    time_elapsed    | 12158    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2234.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.1      |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | -0.0513  |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=2171.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -22.1    |
|    critic_loss     | 0.419    |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | 5.27     |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 47       |
|    time_elapsed    | 12574    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2164.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.392    |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | -4.35    |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=3050.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -7.82    |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 47       |
|    time_elapsed    | 12990    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2154.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -24.2    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | 4.19     |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=3045.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -22.2    |
|    critic_loss     | 0.174    |
|    ent_coef        | 0.00267  |
|    ent_coef_loss   | -0.0827  |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 47       |
|    time_elapsed    | 13405    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2261.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=3035.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.328    |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | 2.91     |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 47       |
|    time_elapsed    | 13821    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=74.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 74.8     |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | 1.82     |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=2185.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.253    |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | 1.48     |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 47       |
|    time_elapsed    | 14238    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=108.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -22.8    |
|    critic_loss     | 0.601    |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=2199.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -23.1    |
|    critic_loss     | 0.577    |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 47       |
|    time_elapsed    | 14653    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=3057.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 0.416    |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | -0.984   |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
New best mean reward!
Eval num_timesteps=720000, episode_reward=83.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 83.5     |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 0.733    |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 47       |
|    time_elapsed    | 15069    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=81.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 81.7     |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 0.562    |
|    ent_coef        | 0.00365  |
|    ent_coef_loss   | 0.859    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=3037.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 0.6      |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 47       |
|    time_elapsed    | 15484    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=3006.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | -3.88    |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
Eval num_timesteps=760000, episode_reward=2101.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.549    |
|    ent_coef        | 0.00346  |
|    ent_coef_loss   | 6.31     |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 47       |
|    time_elapsed    | 15900    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=161.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 0.36     |
|    ent_coef        | 0.00351  |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=3020.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.255    |
|    ent_coef        | 0.00392  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 156      |
|    fps             | 47       |
|    time_elapsed    | 16315    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=3028.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 0.217    |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | 7.4      |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=1941.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.867    |
|    ent_coef        | 0.00473  |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 47       |
|    time_elapsed    | 16731    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=3020.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 1.17     |
|    ent_coef        | 0.00486  |
|    ent_coef_loss   | 4.87     |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=36.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.4     |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -33.6    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | -3.87    |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 47       |
|    time_elapsed    | 17146    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2156.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 0.797    |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=54.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 55       |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 7.78     |
|    ent_coef        | 0.00689  |
|    ent_coef_loss   | 3.46     |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 47       |
|    time_elapsed    | 17562    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2165.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00633  |
|    ent_coef_loss   | -0.397   |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=70.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 70.7     |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -21.5    |
|    critic_loss     | 0.657    |
|    ent_coef        | 0.00583  |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 47       |
|    time_elapsed    | 17978    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2162.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -16.7    |
|    critic_loss     | 1.12     |
|    ent_coef        | 0.00627  |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=3028.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -19.8    |
|    critic_loss     | 0.818    |
|    ent_coef        | 0.00752  |
|    ent_coef_loss   | -4.6     |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 47       |
|    time_elapsed    | 18395    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=51.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 51.7     |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -32.5    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00881  |
|    ent_coef_loss   | 1.97     |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=48.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 48.7     |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -25      |
|    critic_loss     | 1.84     |
|    ent_coef        | 0.00816  |
|    ent_coef_loss   | -1.29    |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 47       |
|    time_elapsed    | 18813    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=91.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 91.5     |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 2.41     |
|    ent_coef        | 0.00774  |
|    ent_coef_loss   | 0.352    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=3019.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -20.9    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | -4.27    |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 47       |
|    time_elapsed    | 19228    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=32.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.5     |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 3.11     |
|    ent_coef        | 0.00666  |
|    ent_coef_loss   | -3.1     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2078.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -24.6    |
|    critic_loss     | 2.07     |
|    ent_coef        | 0.0048   |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 47       |
|    time_elapsed    | 19645    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=3007.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -19.5    |
|    critic_loss     | 0.922    |
|    ent_coef        | 0.00508  |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=61.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 61.2     |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 0.551    |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | 4.19     |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 47       |
|    time_elapsed    | 20063    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=63.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 63.2     |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -33.9    |
|    critic_loss     | 3.39     |
|    ent_coef        | 0.00528  |
|    ent_coef_loss   | 1.55     |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=51.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 51.4     |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.00621  |
|    ent_coef_loss   | 4.26     |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 47       |
|    time_elapsed    | 20480    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=2159.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -18.8    |
|    critic_loss     | 0.838    |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | -4.99    |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=3011.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -24.5    |
|    critic_loss     | 8.56     |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | 0.17     |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 47       |
|    time_elapsed    | 20902    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=3012.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 6.83     |
|    ent_coef        | 0.00506  |
|    ent_coef_loss   | 2.9      |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=3002.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 0.285    |
|    ent_coef        | 0.00477  |
|    ent_coef_loss   | -0.744   |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 47       |
|    time_elapsed    | 21315    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=3014.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -40.6    |
|    critic_loss     | 0.334    |
|    ent_coef        | 0.00498  |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=3011.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -32.8    |
|    critic_loss     | 0.358    |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | 0.0258   |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.59e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 47       |
|    time_elapsed    | 21728    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=32.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.7     |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 0.508    |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | 3.22     |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=33.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 33.5     |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | 0.274    |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 47       |
|    time_elapsed    | 22141    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=1657.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.476    |
|    ent_coef        | 0.00612  |
|    ent_coef_loss   | 1.2      |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=28.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.606    |
|    ent_coef        | 0.00667  |
|    ent_coef_loss   | 0.728    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 216      |
|    fps             | 47       |
|    time_elapsed    | 22551    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=35.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35       |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -19.7    |
|    critic_loss     | 0.646    |
|    ent_coef        | 0.00702  |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=34.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35       |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.00728  |
|    ent_coef_loss   | -3.41    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 47       |
|    time_elapsed    | 22958    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=32.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.5     |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -21.6    |
|    critic_loss     | 0.339    |
|    ent_coef        | 0.00754  |
|    ent_coef_loss   | -3.69    |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=3014.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -25      |
|    critic_loss     | 0.724    |
|    ent_coef        | 0.00837  |
|    ent_coef_loss   | -1.97    |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 47       |
|    time_elapsed    | 23368    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=3015.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 1.35     |
|    ent_coef        | 0.00809  |
|    ent_coef_loss   | 4.76     |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=2714.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 1.15     |
|    ent_coef        | 0.00984  |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 47       |
|    time_elapsed    | 23777    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=3009.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 2.28     |
|    ent_coef        | 0.00866  |
|    ent_coef_loss   | -0.0174  |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2995.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -36.8    |
|    critic_loss     | 1.25     |
|    ent_coef        | 0.00834  |
|    ent_coef_loss   | 0.339    |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 47       |
|    time_elapsed    | 24187    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2215.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.00947  |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2115.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.00859  |
|    ent_coef_loss   | -0.719   |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 47       |
|    time_elapsed    | 24595    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2121.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 1.88     |
|    ent_coef        | 0.00804  |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2146.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 12.6     |
|    ent_coef        | 0.00772  |
|    ent_coef_loss   | -3.24    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 47       |
|    time_elapsed    | 25004    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=3006.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.523    |
|    ent_coef        | 0.006    |
|    ent_coef_loss   | 4.41     |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=2337.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.537    |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 3.49     |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 48       |
|    time_elapsed    | 25413    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2082.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.404    |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | 4.46     |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=3004.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.523    |
|    ent_coef        | 0.00475  |
|    ent_coef_loss   | 0.164    |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 48       |
|    time_elapsed    | 25824    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=1559.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.606    |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=3016.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.44     |
|    ent_coef        | 0.0051   |
|    ent_coef_loss   | 0.342    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 48       |
|    time_elapsed    | 26233    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=58.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 58.6     |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.423    |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | -1.97    |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=2975.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -27.2    |
|    critic_loss     | 0.327    |
|    ent_coef        | 0.00526  |
|    ent_coef_loss   | 0.0741   |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 48       |
|    time_elapsed    | 26643    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=42.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 42       |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 0.296    |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=59.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 59.8     |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 2.08     |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 48       |
|    time_elapsed    | 27051    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=3018.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.00569  |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=38.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 38.7     |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.367    |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 264      |
|    fps             | 48       |
|    time_elapsed    | 27460    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2941.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.0055   |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2135.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.549    |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | -3.44    |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 48       |
|    time_elapsed    | 27868    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2782.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.00664  |
|    ent_coef_loss   | -2.63    |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=44.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 44.4     |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 1.5      |
|    ent_coef        | 0.0074   |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 272      |
|    fps             | 48       |
|    time_elapsed    | 28278    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=1529.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 51.4     |
|    ent_coef        | 0.00882  |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=36.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.1     |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 9.34     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 48       |
|    time_elapsed    | 28686    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2890.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 16.2     |
|    ent_coef        | 0.0163   |
|    ent_coef_loss   | 2.21     |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2233.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -24.3    |
|    critic_loss     | 11.2     |
|    ent_coef        | 0.019    |
|    ent_coef_loss   | -0.346   |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 48       |
|    time_elapsed    | 29094    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=2118.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 5.51     |
|    ent_coef        | 0.0218   |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=3003.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 9        |
|    ent_coef        | 0.0273   |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 284      |
|    fps             | 48       |
|    time_elapsed    | 29503    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=3010.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -18      |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.0265   |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=44.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 44.7     |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -16.1    |
|    critic_loss     | 5.34     |
|    ent_coef        | 0.0231   |
|    ent_coef_loss   | -5.9     |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 48       |
|    time_elapsed    | 29911    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=54.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 54.6     |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -19.2    |
|    critic_loss     | 9.13     |
|    ent_coef        | 0.0199   |
|    ent_coef_loss   | 1.06     |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=34.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 34.8     |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 12.1     |
|    ent_coef        | 0.0176   |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 48       |
|    time_elapsed    | 30322    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=51.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 51.8     |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 13.6     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=57.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57.5     |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 1.69     |
|    ent_coef        | 0.0164   |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 48       |
|    time_elapsed    | 30774    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=47.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 47.6     |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -24.2    |
|    critic_loss     | 4.08     |
|    ent_coef        | 0.0133   |
|    ent_coef_loss   | -2.88    |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=48.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 48.3     |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -21.6    |
|    critic_loss     | 6.47     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -0.305   |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.74e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 47       |
|    time_elapsed    | 31411    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=46.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 46.7     |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -18.6    |
|    critic_loss     | 2.51     |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | -4.47    |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=40.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 40.3     |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -20      |
|    critic_loss     | 0.898    |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | -4.52    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 47       |
|    time_elapsed    | 32052    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=2031.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -19.6    |
|    critic_loss     | 4.01     |
|    ent_coef        | 0.00992  |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2058.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -22.5    |
|    critic_loss     | 0.788    |
|    ent_coef        | 0.0099   |
|    ent_coef_loss   | -0.478   |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 47       |
|    time_elapsed    | 32674    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=1972.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -25.9    |
|    critic_loss     | 1.18     |
|    ent_coef        | 0.00932  |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2920.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -24.7    |
|    critic_loss     | 1.15     |
|    ent_coef        | 0.00859  |
|    ent_coef_loss   | 0.221    |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 312      |
|    fps             | 46       |
|    time_elapsed    | 33277    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=3002.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 5.88     |
|    ent_coef        | 0.00797  |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=3019.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00779  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 46       |
|    time_elapsed    | 33879    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=77.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 77.7     |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.839    |
|    ent_coef        | 0.00746  |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=59.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 59.5     |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.00728  |
|    ent_coef_loss   | 0.182    |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 46       |
|    time_elapsed    | 34478    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=46.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 46.9     |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.591    |
|    ent_coef        | 0.00589  |
|    ent_coef_loss   | -0.0776  |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=2999.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.00534  |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 46       |
|    time_elapsed    | 35072    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2995.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | 3.02     |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2997.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.822    |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 46       |
|    time_elapsed    | 35641    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2994.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -27.8    |
|    critic_loss     | 0.47     |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | 1.52     |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2993.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.486    |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | 0.395    |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 45       |
|    time_elapsed    | 36199    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2994.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 0.809    |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -1.58    |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2992.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -33.9    |
|    critic_loss     | 0.228    |
|    ent_coef        | 0.00355  |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 45       |
|    time_elapsed    | 36776    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2993.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -34.1    |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2992.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 0.165    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 45       |
|    time_elapsed    | 37359    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2996.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.445    |
|    ent_coef        | 0.0046   |
|    ent_coef_loss   | 6.22     |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2990.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00589  |
|    ent_coef_loss   | 0.0769   |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 45       |
|    time_elapsed    | 37949    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2991.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.00626  |
|    ent_coef_loss   | 0.711    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=1570.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 2.24     |
|    ent_coef        | 0.00676  |
|    ent_coef_loss   | 6.82     |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 45       |
|    time_elapsed    | 38540    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2992.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00684  |
|    ent_coef_loss   | 3.53     |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=1778.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 2.36     |
|    ent_coef        | 0.00715  |
|    ent_coef_loss   | -2.68    |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 44       |
|    time_elapsed    | 39134    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=1558.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00904  |
|    ent_coef_loss   | 0.431    |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2995.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.00891  |
|    ent_coef_loss   | -1.13    |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 44       |
|    time_elapsed    | 39715    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2993.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 1.3      |
|    ent_coef        | 0.00933  |
|    ent_coef_loss   | -0.441   |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2989.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 2.42     |
|    ent_coef        | 0.00855  |
|    ent_coef_loss   | 2.6      |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 44       |
|    time_elapsed    | 40228    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2979.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -41.3    |
|    critic_loss     | 4.47     |
|    ent_coef        | 0.00808  |
|    ent_coef_loss   | 6.41     |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=37.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 37.6     |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.015    |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 44       |
|    time_elapsed    | 40697    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=45.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 45       |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 3.36     |
|    ent_coef        | 0.016    |
|    ent_coef_loss   | -0.915   |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=50.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 50.8     |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 10.8     |
|    ent_coef        | 0.0165   |
|    ent_coef_loss   | -0.684   |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 44       |
|    time_elapsed    | 41164    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=46.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 46.4     |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -34.5    |
|    critic_loss     | 4.7      |
|    ent_coef        | 0.0178   |
|    ent_coef_loss   | -1.33    |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=2993.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.0159   |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 44       |
|    time_elapsed    | 41639    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=1621.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 6.79     |
|    ent_coef        | 0.0176   |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=1592.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 5.69     |
|    ent_coef        | 0.0149   |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 44       |
|    time_elapsed    | 42114    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2516.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 8.01     |
|    ent_coef        | 0.0202   |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2890.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -41.2    |
|    critic_loss     | 4.05     |
|    ent_coef        | 0.0213   |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 44       |
|    time_elapsed    | 42588    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=2883.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 15.8     |
|    ent_coef        | 0.0251   |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=2931.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -33.9    |
|    critic_loss     | 2.97     |
|    ent_coef        | 0.0223   |
|    ent_coef_loss   | -0.412   |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 44       |
|    time_elapsed    | 43062    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2739.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 8.09     |
|    ent_coef        | 0.0176   |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=3001.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 10.5     |
|    ent_coef        | 0.0149   |
|    ent_coef_loss   | 4.85     |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 44       |
|    time_elapsed    | 43537    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2936.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 6.78     |
|    ent_coef        | 0.0138   |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=42.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 42.3     |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -39.9    |
|    critic_loss     | 5.25     |
|    ent_coef        | 0.0122   |
|    ent_coef_loss   | -0.625   |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 44       |
|    time_elapsed    | 44016    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=34.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35       |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 2.08     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | 0.355    |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=28.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28       |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 9.55     |
|    ent_coef        | 0.0145   |
|    ent_coef_loss   | 0.396    |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 44       |
|    time_elapsed    | 44491    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=275.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -40.8    |
|    critic_loss     | 5.15     |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | 5.38     |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2595.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 1.88     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -0.5     |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.05e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 44       |
|    time_elapsed    | 44966    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=30.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 30.8     |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -41.4    |
|    critic_loss     | 3.18     |
|    ent_coef        | 0.0135   |
|    ent_coef_loss   | -3.62    |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=27.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27       |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -46.2    |
|    critic_loss     | 3.25     |
|    ent_coef        | 0.0157   |
|    ent_coef_loss   | -0.968   |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 44       |
|    time_elapsed    | 45442    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=28.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 5.39     |
|    ent_coef        | 0.0199   |
|    ent_coef_loss   | 3.31     |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=29.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 9.31     |
|    ent_coef        | 0.0175   |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 44       |
|    time_elapsed    | 45914    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=1532.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 6.94     |
|    ent_coef        | 0.015    |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2041.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 31.2     |
|    ent_coef        | 0.0142   |
|    ent_coef_loss   | 5.65     |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 44       |
|    time_elapsed    | 46387    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2122.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.0178   |
|    ent_coef_loss   | 2.91     |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=2083.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -35.1    |
|    critic_loss     | 14.2     |
|    ent_coef        | 0.0206   |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.97e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 44       |
|    time_elapsed    | 46861    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=1532.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -88.5    |
|    critic_loss     | 48.3     |
|    ent_coef        | 0.0557   |
|    ent_coef_loss   | 3.63     |
|    learning_rate   | 0.000791 |
|    n_updates       | 3434324  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=2096.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -97      |
|    critic_loss     | 27       |
|    ent_coef        | 0.0539   |
|    ent_coef_loss   | -0.438   |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 44       |
|    time_elapsed    | 47337    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=1804.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -95.3    |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.0416   |
|    ent_coef_loss   | 2.91     |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=2990.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 19.5     |
|    ent_coef        | 0.027    |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000788 |
|    n_updates       | 3464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 44       |
|    time_elapsed    | 47814    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2991.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.0283   |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=1556.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 53.9     |
|    ent_coef        | 0.0384   |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 44       |
|    time_elapsed    | 48287    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=2990.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 132      |
|    ent_coef        | 0.0558   |
|    ent_coef_loss   | 0.997    |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=2993.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 172      |
|    ent_coef        | 0.0804   |
|    ent_coef_loss   | 2.23     |
|    learning_rate   | 0.000784 |
|    n_updates       | 3504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 44       |
|    time_elapsed    | 48761    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=34.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 34       |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -219     |
|    critic_loss     | 390      |
|    ent_coef        | 0.128    |
|    ent_coef_loss   | 2.39     |
|    learning_rate   | 0.000783 |
|    n_updates       | 3514324  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=31.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 31.6     |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -262     |
|    critic_loss     | 122      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -1.4     |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 44       |
|    time_elapsed    | 49236    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=2073.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -212     |
|    critic_loss     | 268      |
|    ent_coef        | 0.113    |
|    ent_coef_loss   | 0.0809   |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=1775.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -178     |
|    critic_loss     | 185      |
|    ent_coef        | 0.0838   |
|    ent_coef_loss   | 0.733    |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 44       |
|    time_elapsed    | 49708    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=1601.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -149     |
|    critic_loss     | 132      |
|    ent_coef        | 0.0783   |
|    ent_coef_loss   | 0.128    |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=2927.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 39       |
|    ent_coef        | 0.0553   |
|    ent_coef_loss   | -0.317   |
|    learning_rate   | 0.000778 |
|    n_updates       | 3564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 44       |
|    time_elapsed    | 50185    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2994.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -80.3    |
|    critic_loss     | 113      |
|    ent_coef        | 0.056    |
|    ent_coef_loss   | -2.01    |
|    learning_rate   | 0.000777 |
|    n_updates       | 3574324  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2995.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -80.2    |
|    critic_loss     | 107      |
|    ent_coef        | 0.0488   |
|    ent_coef_loss   | -0.599   |
|    learning_rate   | 0.000776 |
|    n_updates       | 3584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 44       |
|    time_elapsed    | 50663    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2995.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -58.1    |
|    critic_loss     | 27.8     |
|    ent_coef        | 0.0434   |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2994.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 22.2     |
|    ent_coef        | 0.0409   |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 44       |
|    time_elapsed    | 51140    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2991.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -65.4    |
|    critic_loss     | 29.4     |
|    ent_coef        | 0.0449   |
|    ent_coef_loss   | -1.11    |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=2990.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -68.5    |
|    critic_loss     | 79.5     |
|    ent_coef        | 0.0393   |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 456      |
|    fps             | 44       |
|    time_elapsed    | 51612    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2991.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -56.8    |
|    critic_loss     | 26.8     |
|    ent_coef        | 0.04     |
|    ent_coef_loss   | -0.377   |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=2992.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 21.8     |
|    ent_coef        | 0.0365   |
|    ent_coef_loss   | 0.656    |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 44       |
|    time_elapsed    | 52087    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=2113.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 7.87     |
|    ent_coef        | 0.0334   |
|    ent_coef_loss   | -0.742   |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2123.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -46.7    |
|    critic_loss     | 52.1     |
|    ent_coef        | 0.0302   |
|    ent_coef_loss   | -0.791   |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 44       |
|    time_elapsed    | 52562    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=1527.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 40.3     |
|    ent_coef        | 0.032    |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=2992.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 31.5     |
|    ent_coef        | 0.0296   |
|    ent_coef_loss   | 0.718    |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 44       |
|    time_elapsed    | 53039    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=2995.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 9.53     |
|    ent_coef        | 0.0273   |
|    ent_coef_loss   | -0.192   |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=2995.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -34.1    |
|    critic_loss     | 12.3     |
|    ent_coef        | 0.0251   |
|    ent_coef_loss   | 0.95     |
|    learning_rate   | 0.000764 |
|    n_updates       | 3704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 44       |
|    time_elapsed    | 53518    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=2991.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -30      |
|    critic_loss     | 19.2     |
|    ent_coef        | 0.0271   |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=2990.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.0327   |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 44       |
|    time_elapsed    | 53994    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=2990.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 6.18     |
|    ent_coef        | 0.0296   |
|    ent_coef_loss   | -0.687   |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2993.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 7.31     |
|    ent_coef        | 0.0279   |
|    ent_coef_loss   | 0.508    |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 44       |
|    time_elapsed    | 54472    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=2820.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 10.8     |
|    ent_coef        | 0.0257   |
|    ent_coef_loss   | 0.804    |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=2779.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 34.5     |
|    ent_coef        | 0.0309   |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 44       |
|    time_elapsed    | 54953    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2898.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 14.8     |
|    ent_coef        | 0.0324   |
|    ent_coef_loss   | -0.119   |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=1537.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -41.4    |
|    critic_loss     | 7.27     |
|    ent_coef        | 0.0305   |
|    ent_coef_loss   | -1.93    |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 44       |
|    time_elapsed    | 55435    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2960.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 21.1     |
|    ent_coef        | 0.03     |
|    ent_coef_loss   | 0.637    |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=2994.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -61.8    |
|    critic_loss     | 56.8     |
|    ent_coef        | 0.0357   |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 43       |
|    time_elapsed    | 55916    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=52.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 52.6     |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 34       |
|    ent_coef        | 0.0484   |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=1527.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -78.4    |
|    critic_loss     | 116      |
|    ent_coef        | 0.0768   |
|    ent_coef_loss   | 0.157    |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 43       |
|    time_elapsed    | 56407    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=49.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 49.5     |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -125     |
|    critic_loss     | 67.4     |
|    ent_coef        | 0.0896   |
|    ent_coef_loss   | 0.917    |
|    learning_rate   | 0.000751 |
|    n_updates       | 3834324  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=32.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.9     |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -125     |
|    critic_loss     | 37.8     |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | -0.17    |
|    learning_rate   | 0.00075  |
|    n_updates       | 3844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 43       |
|    time_elapsed    | 56882    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=28.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28.6     |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -218     |
|    critic_loss     | 99.8     |
|    ent_coef        | 0.127    |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=26.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -196     |
|    critic_loss     | 58.7     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -0.0299  |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 43       |
|    time_elapsed    | 57357    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=26.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 26.3     |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -107     |
|    critic_loss     | 116      |
|    ent_coef        | 0.0842   |
|    ent_coef_loss   | 0.833    |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=29.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 20.4     |
|    ent_coef        | 0.0737   |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 43       |
|    time_elapsed    | 57831    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=28.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28       |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -80.5    |
|    critic_loss     | 22.9     |
|    ent_coef        | 0.0586   |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=41.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 41.4     |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 24.2     |
|    ent_coef        | 0.0485   |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 43       |
|    time_elapsed    | 58306    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=27.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.2     |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -94.3    |
|    critic_loss     | 65.3     |
|    ent_coef        | 0.0477   |
|    ent_coef_loss   | -0.095   |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=27.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.9     |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 112      |
|    ent_coef        | 0.0529   |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 43       |
|    time_elapsed    | 58779    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=1720.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -96.6    |
|    critic_loss     | 85.9     |
|    ent_coef        | 0.0509   |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2238.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -67.9    |
|    critic_loss     | 35.6     |
|    ent_coef        | 0.0585   |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 520      |
|    fps             | 43       |
|    time_elapsed    | 59251    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=3000.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -59.8    |
|    critic_loss     | 118      |
|    ent_coef        | 0.0674   |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=2989.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -68.4    |
|    critic_loss     | 31.3     |
|    ent_coef        | 0.0624   |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 43       |
|    time_elapsed    | 59726    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=26.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -74.5    |
|    critic_loss     | 18.9     |
|    ent_coef        | 0.0509   |
|    ent_coef_loss   | 0.41     |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=27.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.1     |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -75.5    |
|    critic_loss     | 152      |
|    ent_coef        | 0.046    |
|    ent_coef_loss   | 3.14     |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 43       |
|    time_elapsed    | 60198    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=30.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 30.9     |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 71.2     |
|    ent_coef        | 0.0483   |
|    ent_coef_loss   | 0.938    |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2990.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -59.3    |
|    critic_loss     | 24.5     |
|    ent_coef        | 0.0453   |
|    ent_coef_loss   | -1.42    |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 43       |
|    time_elapsed    | 60667    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=56.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 56.1     |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 180      |
|    ent_coef        | 0.0429   |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=40.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 40.4     |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.044    |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 43       |
|    time_elapsed    | 61139    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=1958.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -56.6    |
|    critic_loss     | 29.8     |
|    ent_coef        | 0.0445   |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.000731 |
|    n_updates       | 4034324  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=48.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 48.5     |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 131      |
|    ent_coef        | 0.0488   |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 540      |
|    fps             | 43       |
|    time_elapsed    | 61614    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=35.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35.6     |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -41.1    |
|    critic_loss     | 20.1     |
|    ent_coef        | 0.0451   |
|    ent_coef_loss   | 0.566    |
|    learning_rate   | 0.000729 |
|    n_updates       | 4054324  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=39.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 39.7     |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 19.2     |
|    ent_coef        | 0.0446   |
|    ent_coef_loss   | -0.491   |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 43       |
|    time_elapsed    | 62088    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=40.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 40.4     |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -45.8    |
|    critic_loss     | 25.1     |
|    ent_coef        | 0.0483   |
|    ent_coef_loss   | 0.595    |
|    learning_rate   | 0.000727 |
|    n_updates       | 4074324  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=2997.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 18.2     |
|    ent_coef        | 0.0512   |
|    ent_coef_loss   | -2.72    |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 43       |
|    time_elapsed    | 62563    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2991.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 19.8     |
|    ent_coef        | 0.0482   |
|    ent_coef_loss   | -1.93    |
|    learning_rate   | 0.000725 |
|    n_updates       | 4094324  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=2990.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 101      |
|    ent_coef        | 0.0422   |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 552      |
|    fps             | 43       |
|    time_elapsed    | 63039    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=2991.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.0467   |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2991.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 54.6     |
|    ent_coef        | 0.0466   |
|    ent_coef_loss   | -0.0611  |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 556      |
|    fps             | 43       |
|    time_elapsed    | 63511    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=2997.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 27.4     |
|    ent_coef        | 0.0414   |
|    ent_coef_loss   | 0.529    |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=3002.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 54.6     |
|    ent_coef        | 0.0407   |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 43       |
|    time_elapsed    | 63981    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=29.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 30.9     |
|    ent_coef        | 0.0421   |
|    ent_coef_loss   | 2.98     |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=32.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.4     |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 45.6     |
|    ent_coef        | 0.0431   |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000718 |
|    n_updates       | 4164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 43       |
|    time_elapsed    | 64452    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=36.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.3     |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -90.1    |
|    critic_loss     | 37.4     |
|    ent_coef        | 0.0467   |
|    ent_coef_loss   | 3.68     |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=35.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35.2     |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -102     |
|    critic_loss     | 57.8     |
|    ent_coef        | 0.0462   |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 568      |
|    fps             | 43       |
|    time_elapsed    | 64927    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=31.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 31.5     |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -85.8    |
|    critic_loss     | 56.3     |
|    ent_coef        | 0.0572   |
|    ent_coef_loss   | 0.321    |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=34.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 34.2     |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -134     |
|    critic_loss     | 108      |
|    ent_coef        | 0.0838   |
|    ent_coef_loss   | 1.18     |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    episodes        | 572      |
|    fps             | 43       |
|    time_elapsed    | 65400    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=33.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 33.4     |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -172     |
|    critic_loss     | 144      |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.888    |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=38.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 38.9     |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -218     |
|    critic_loss     | 61.6     |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    episodes        | 576      |
|    fps             | 43       |
|    time_elapsed    | 65871    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=43.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 43.5     |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -226     |
|    critic_loss     | 172      |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=32.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 32.8     |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -132     |
|    critic_loss     | 184      |
|    ent_coef        | 0.0975   |
|    ent_coef_loss   | 1.89     |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 580      |
|    fps             | 43       |
|    time_elapsed    | 66342    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=38.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 38.2     |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 56.4     |
|    ent_coef        | 0.0838   |
|    ent_coef_loss   | -0.941   |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=47.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 47.8     |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -136     |
|    critic_loss     | 77       |
|    ent_coef        | 0.0784   |
|    ent_coef_loss   | -0.206   |
|    learning_rate   | 0.000708 |
|    n_updates       | 4264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 994      |
| time/              |          |
|    episodes        | 584      |
|    fps             | 43       |
|    time_elapsed    | 66811    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=42.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 42.3     |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -94.2    |
|    critic_loss     | 83.1     |
|    ent_coef        | 0.0757   |
|    ent_coef_loss   | -3.61    |
|    learning_rate   | 0.000707 |
|    n_updates       | 4274324  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=33.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 33.5     |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -194     |
|    critic_loss     | 230      |
|    ent_coef        | 0.0861   |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000706 |
|    n_updates       | 4284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 887      |
| time/              |          |
|    episodes        | 588      |
|    fps             | 43       |
|    time_elapsed    | 67282    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=35.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 35.3     |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -187     |
|    critic_loss     | 42.6     |
|    ent_coef        | 0.079    |
|    ent_coef_loss   | -0.0448  |
|    learning_rate   | 0.000705 |
|    n_updates       | 4294324  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=38.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 38.2     |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -208     |
|    critic_loss     | 114      |
|    ent_coef        | 0.124    |
|    ent_coef_loss   | 0.0319   |
|    learning_rate   | 0.000704 |
|    n_updates       | 4304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 784      |
| time/              |          |
|    episodes        | 592      |
|    fps             | 43       |
|    time_elapsed    | 67755    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=43.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 43.6     |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -267     |
|    critic_loss     | 155      |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | -0.625   |
|    learning_rate   | 0.000703 |
|    n_updates       | 4314324  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=71.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 71.3     |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -291     |
|    critic_loss     | 168      |
|    ent_coef        | 0.143    |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000702 |
|    n_updates       | 4324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 715      |
| time/              |          |
|    episodes        | 596      |
|    fps             | 43       |
|    time_elapsed    | 68229    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=2108.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -269     |
|    critic_loss     | 371      |
|    ent_coef        | 0.146    |
|    ent_coef_loss   | 0.484    |
|    learning_rate   | 0.000701 |
|    n_updates       | 4334324  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=2992.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -204     |
|    critic_loss     | 185      |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | 0.968    |
|    learning_rate   | 0.0007   |
|    n_updates       | 4344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 756      |
| time/              |          |
|    episodes        | 600      |
|    fps             | 43       |
|    time_elapsed    | 68705    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=60.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 60.5     |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -135     |
|    critic_loss     | 94.3     |
|    ent_coef        | 0.0955   |
|    ent_coef_loss   | -0.821   |
|    learning_rate   | 0.000699 |
|    n_updates       | 4354324  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=61.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 61.4     |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -147     |
|    critic_loss     | 40.7     |
|    ent_coef        | 0.0864   |
|    ent_coef_loss   | -0.156   |
|    learning_rate   | 0.000698 |
|    n_updates       | 4364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 807      |
| time/              |          |
|    episodes        | 604      |
|    fps             | 43       |
|    time_elapsed    | 69180    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=52.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 52.4     |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -100     |
|    critic_loss     | 100      |
|    ent_coef        | 0.0757   |
|    ent_coef_loss   | -0.915   |
|    learning_rate   | 0.000697 |
|    n_updates       | 4374324  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=53.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 53.2     |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -87.1    |
|    critic_loss     | 77.5     |
|    ent_coef        | 0.0787   |
|    ent_coef_loss   | -0.65    |
|    learning_rate   | 0.000696 |
|    n_updates       | 4384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 808      |
| time/              |          |
|    episodes        | 608      |
|    fps             | 43       |
|    time_elapsed    | 69654    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=56.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 56       |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -85.1    |
|    critic_loss     | 90.1     |
|    ent_coef        | 0.0864   |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000695 |
|    n_updates       | 4394324  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=65.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 65       |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 32.7     |
|    ent_coef        | 0.0836   |
|    ent_coef_loss   | -0.931   |
|    learning_rate   | 0.000694 |
|    n_updates       | 4404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 809      |
| time/              |          |
|    episodes        | 612      |
|    fps             | 43       |
|    time_elapsed    | 70126    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=57.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57.2     |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -58.6    |
|    critic_loss     | 81.2     |
|    ent_coef        | 0.0893   |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000693 |
|    n_updates       | 4414324  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=2994.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 67.2     |
|    ent_coef        | 0.0839   |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.000692 |
|    n_updates       | 4424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 810      |
| time/              |          |
|    episodes        | 616      |
|    fps             | 43       |
|    time_elapsed    | 70597    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=2996.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 135      |
|    ent_coef        | 0.0832   |
|    ent_coef_loss   | -0.632   |
|    learning_rate   | 0.000691 |
|    n_updates       | 4434324  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=3003.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 51.2     |
|    ent_coef        | 0.0634   |
|    ent_coef_loss   | -0.644   |
|    learning_rate   | 0.00069  |
|    n_updates       | 4444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 878      |
| time/              |          |
|    episodes        | 620      |
|    fps             | 43       |
|    time_elapsed    | 71065    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=3001.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -35.2    |
|    critic_loss     | 38.6     |
|    ent_coef        | 0.0658   |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000689 |
|    n_updates       | 4454324  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=2122.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 122      |
|    ent_coef        | 0.0557   |
|    ent_coef_loss   | -0.337   |
|    learning_rate   | 0.000688 |
|    n_updates       | 4464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 878      |
| time/              |          |
|    episodes        | 624      |
|    fps             | 43       |
|    time_elapsed    | 71536    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=29.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 42.9     |
|    ent_coef        | 0.059    |
|    ent_coef_loss   | 0.362    |
|    learning_rate   | 0.000687 |
|    n_updates       | 4474324  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=3000.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -41.3    |
|    critic_loss     | 48       |
|    ent_coef        | 0.0575   |
|    ent_coef_loss   | -2.2     |
|    learning_rate   | 0.000686 |
|    n_updates       | 4484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 844      |
| time/              |          |
|    episodes        | 628      |
|    fps             | 43       |
|    time_elapsed    | 72003    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=29.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.8     |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 64.2     |
|    ent_coef        | 0.0602   |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000685 |
|    n_updates       | 4494324  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=28.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 28.1     |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 107      |
|    ent_coef        | 0.0518   |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000684 |
|    n_updates       | 4504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 903      |
| time/              |          |
|    episodes        | 632      |
|    fps             | 43       |
|    time_elapsed    | 72468    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=30.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 30       |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 42.4     |
|    ent_coef        | 0.0473   |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.000683 |
|    n_updates       | 4514324  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=2694.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 24.6     |
|    ent_coef        | 0.0518   |
|    ent_coef_loss   | -1.9     |
|    learning_rate   | 0.000682 |
|    n_updates       | 4524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 873      |
| time/              |          |
|    episodes        | 636      |
|    fps             | 43       |
|    time_elapsed    | 72938    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=29.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 27.9     |
|    ent_coef        | 0.0528   |
|    ent_coef_loss   | -1.51    |
|    learning_rate   | 0.000681 |
|    n_updates       | 4534324  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=2777.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 30.2     |
|    ent_coef        | 0.0499   |
|    ent_coef_loss   | -0.537   |
|    learning_rate   | 0.00068  |
|    n_updates       | 4544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 972      |
| time/              |          |
|    episodes        | 640      |
|    fps             | 43       |
|    time_elapsed    | 73404    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=2750.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 50       |
|    ent_coef        | 0.047    |
|    ent_coef_loss   | 0.523    |
|    learning_rate   | 0.000679 |
|    n_updates       | 4554324  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=31.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 31.7     |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -40.8    |
|    critic_loss     | 25.5     |
|    ent_coef        | 0.0469   |
|    ent_coef_loss   | 0.0341   |
|    learning_rate   | 0.000678 |
|    n_updates       | 4564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 644      |
|    fps             | 43       |
|    time_elapsed    | 73872    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=3000.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -59.9    |
|    critic_loss     | 52.2     |
|    ent_coef        | 0.0417   |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000677 |
|    n_updates       | 4574324  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=45.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 45.3     |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -24.2    |
|    critic_loss     | 17.8     |
|    ent_coef        | 0.0528   |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000676 |
|    n_updates       | 4584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    episodes        | 648      |
|    fps             | 43       |
|    time_elapsed    | 74342    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=47.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 47.9     |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -67      |
|    critic_loss     | 371      |
|    ent_coef        | 0.062    |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000675 |
|    n_updates       | 4594324  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=1795.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 70.9     |
|    ent_coef        | 0.0515   |
|    ent_coef_loss   | -0.969   |
|    learning_rate   | 0.000674 |
|    n_updates       | 4604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 652      |
|    fps             | 43       |
|    time_elapsed    | 74809    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=1261.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 29.6     |
|    ent_coef        | 0.0439   |
|    ent_coef_loss   | -0.551   |
|    learning_rate   | 0.000673 |
|    n_updates       | 4614324  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=2931.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 19.2     |
|    ent_coef        | 0.0447   |
|    ent_coef_loss   | -0.491   |
|    learning_rate   | 0.000672 |
|    n_updates       | 4624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 989      |
| time/              |          |
|    episodes        | 656      |
|    fps             | 43       |
|    time_elapsed    | 75275    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=44.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 44.7     |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 37.4     |
|    ent_coef        | 0.0394   |
|    ent_coef_loss   | 0.425    |
|    learning_rate   | 0.000671 |
|    n_updates       | 4634324  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=36.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 36.4     |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 71.3     |
|    ent_coef        | 0.0394   |
|    ent_coef_loss   | -0.171   |
|    learning_rate   | 0.00067  |
|    n_updates       | 4644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 919      |
| time/              |          |
|    episodes        | 660      |
|    fps             | 43       |
|    time_elapsed    | 75745    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=2539.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 28.8     |
|    ent_coef        | 0.0349   |
|    ent_coef_loss   | 2.87     |
|    learning_rate   | 0.000669 |
|    n_updates       | 4654324  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=29.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 40.8     |
|    ent_coef        | 0.0357   |
|    ent_coef_loss   | -0.337   |
|    learning_rate   | 0.000668 |
|    n_updates       | 4664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 882      |
| time/              |          |
|    episodes        | 664      |
|    fps             | 43       |
|    time_elapsed    | 76214    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=27.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 27.7     |
|    ent_coef        | 0.0409   |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.000667 |
|    n_updates       | 4674324  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=27.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.4     |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -37.8    |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.0385   |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000666 |
|    n_updates       | 4684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 882      |
| time/              |          |
|    episodes        | 668      |
|    fps             | 43       |
|    time_elapsed    | 76683    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=29.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.2     |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 32.2     |
|    ent_coef        | 0.0417   |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.000665 |
|    n_updates       | 4694324  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=26.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -15.1    |
|    critic_loss     | 22.3     |
|    ent_coef        | 0.0345   |
|    ent_coef_loss   | 0.489    |
|    learning_rate   | 0.000664 |
|    n_updates       | 4704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 881      |
| time/              |          |
|    episodes        | 672      |
|    fps             | 43       |
|    time_elapsed    | 77155    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=26.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 26.4     |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -24.2    |
|    critic_loss     | 155      |
|    ent_coef        | 0.0276   |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.000663 |
|    n_updates       | 4714324  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=29.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.9     |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -8.16    |
|    critic_loss     | 29.1     |
|    ent_coef        | 0.0303   |
|    ent_coef_loss   | 0.118    |
|    learning_rate   | 0.000662 |
|    n_updates       | 4724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 881      |
| time/              |          |
|    episodes        | 676      |
|    fps             | 43       |
|    time_elapsed    | 77629    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=27.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.1     |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -23.5    |
|    critic_loss     | 23.6     |
|    ent_coef        | 0.0333   |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000661 |
|    n_updates       | 4734324  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=29.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29.1     |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 25.8     |
|    ent_coef        | 0.0304   |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.00066  |
|    n_updates       | 4744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 880      |
| time/              |          |
|    episodes        | 680      |
|    fps             | 43       |
|    time_elapsed    | 78103    |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=3000.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 43.2     |
|    ent_coef        | 0.0475   |
|    ent_coef_loss   | 0.943    |
|    learning_rate   | 0.000659 |
|    n_updates       | 4754324  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=3000.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 28       |
|    ent_coef        | 0.0486   |
|    ent_coef_loss   | 2.45     |
|    learning_rate   | 0.000658 |
|    n_updates       | 4764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 956      |
| time/              |          |
|    episodes        | 684      |
|    fps             | 43       |
|    time_elapsed    | 78575    |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=29.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 29       |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 61.6     |
|    ent_coef        | 0.0505   |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000657 |
|    n_updates       | 4774324  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=27.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.2     |
| time/              |          |
|    total_timesteps | 3440000  |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 10.6     |
|    ent_coef        | 0.0505   |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.000656 |
|    n_updates       | 4784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 688      |
|    fps             | 43       |
|    time_elapsed    | 79050    |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=27.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.2     |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -16.3    |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.054    |
|    ent_coef_loss   | 0.833    |
|    learning_rate   | 0.000655 |
|    n_updates       | 4794324  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=27.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 27.3     |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 13.1     |
|    ent_coef        | 0.0541   |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000654 |
|    n_updates       | 4804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 692      |
|    fps             | 43       |
|    time_elapsed    | 79527    |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=3000.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 23.1     |
|    ent_coef        | 0.0463   |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000653 |
|    n_updates       | 4814324  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=38.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 38.2     |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -41.9    |
|    critic_loss     | 45.2     |
|    ent_coef        | 0.057    |
|    ent_coef_loss   | 1.58     |
|    learning_rate   | 0.000652 |
|    n_updates       | 4824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 696      |
|    fps             | 43       |
|    time_elapsed    | 80005    |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=1550.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 19.7     |
|    ent_coef        | 0.0569   |
|    ent_coef_loss   | 0.469    |
|    learning_rate   | 0.000651 |
|    n_updates       | 4834324  |
