Using cuda device
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_28
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=407.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 64.3     |
|    critic_loss     | 0.0323   |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | -4.84    |
|    learning_rate   | 0.000999 |
|    n_updates       | 9899     |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=434.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 31.3     |
|    critic_loss     | 0.0191   |
|    ent_coef        | 0.000957 |
|    ent_coef_loss   | -3.6     |
|    learning_rate   | 0.000998 |
|    n_updates       | 19899    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 417      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 51       |
|    time_elapsed    | 384      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=72.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 72.1     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 15.6     |
|    critic_loss     | 0.00763  |
|    ent_coef        | 0.000604 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.000997 |
|    n_updates       | 29899    |
---------------------------------
Eval num_timesteps=40000, episode_reward=409.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 5.34     |
|    critic_loss     | 0.0266   |
|    ent_coef        | 0.000543 |
|    ent_coef_loss   | -7.64    |
|    learning_rate   | 0.000996 |
|    n_updates       | 39899    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 363      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 52       |
|    time_elapsed    | 762      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=109.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 0.813    |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.000581 |
|    ent_coef_loss   | 21.9     |
|    learning_rate   | 0.000995 |
|    n_updates       | 49899    |
---------------------------------
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 5gdl/SAC_29
Eval num_timesteps=10000, episode_reward=45.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 45.6     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -3.29    |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00051  |
|    ent_coef_loss   | 18.9     |
|    learning_rate   | 0.000999 |
|    n_updates       | 64526    |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=4601.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.6e+03  |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -5.81    |
|    critic_loss     | 0.352    |
|    ent_coef        | 0.000976 |
|    ent_coef_loss   | -9.83    |
|    learning_rate   | 0.000998 |
|    n_updates       | 74526    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 670      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 52       |
|    time_elapsed    | 384      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2117.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -19.1    |
|    critic_loss     | 0.396    |
|    ent_coef        | 0.0058   |
|    ent_coef_loss   | -1.29    |
|    learning_rate   | 0.000997 |
|    n_updates       | 84526    |
---------------------------------
Eval num_timesteps=40000, episode_reward=2644.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.287    |
|    ent_coef        | 0.0051   |
|    ent_coef_loss   | 6.27     |
|    learning_rate   | 0.000996 |
|    n_updates       | 94526    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 52       |
|    time_elapsed    | 763      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1776.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -27      |
|    critic_loss     | 1.8      |
|    ent_coef        | 0.00424  |
|    ent_coef_loss   | 7.24     |
|    learning_rate   | 0.000995 |
|    n_updates       | 104526   |
---------------------------------
Eval num_timesteps=60000, episode_reward=3658.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.66e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 0.807    |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000994 |
|    n_updates       | 114526   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 52       |
|    time_elapsed    | 1144     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=4599.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.6e+03  |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 1        |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | 5.93     |
|    learning_rate   | 0.000993 |
|    n_updates       | 124526   |
---------------------------------
Eval num_timesteps=80000, episode_reward=4304.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.3e+03  |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | -5.95    |
|    learning_rate   | 0.000992 |
|    n_updates       | 134526   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 52       |
|    time_elapsed    | 1512     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=4136.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.14e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -34      |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00728  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000991 |
|    n_updates       | 144526   |
---------------------------------
Eval num_timesteps=100000, episode_reward=1757.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 1.98     |
|    ent_coef        | 0.00604  |
|    ent_coef_loss   | -2.61    |
|    learning_rate   | 0.00099  |
|    n_updates       | 154526   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 53       |
|    time_elapsed    | 1875     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=4220.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.22e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.00582  |
|    ent_coef_loss   | -0.628   |
|    learning_rate   | 0.000989 |
|    n_updates       | 164526   |
---------------------------------
Eval num_timesteps=120000, episode_reward=4919.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.92e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -22.3    |
|    critic_loss     | 3.99     |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000988 |
|    n_updates       | 174526   |
---------------------------------
New best mean reward!
Stopping training because the mean reward 4919.70  is above the threshold 4900
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 5gdl/SAC_30
Eval num_timesteps=10000, episode_reward=3887.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.89e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -21.7    |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.00517  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000999 |
|    n_updates       | 184425   |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=1437.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -21.4    |
|    critic_loss     | 2.34     |
|    ent_coef        | 0.00498  |
|    ent_coef_loss   | 3.8      |
|    learning_rate   | 0.000998 |
|    n_updates       | 194425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 4        |
|    fps             | 51       |
|    time_elapsed    | 388      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2777.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | 4.17     |
|    learning_rate   | 0.000997 |
|    n_updates       | 204425   |
---------------------------------
Eval num_timesteps=40000, episode_reward=1355.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -22.1    |
|    critic_loss     | 1.32     |
|    ent_coef        | 0.00543  |
|    ent_coef_loss   | -0.452   |
|    learning_rate   | 0.000996 |
|    n_updates       | 214425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 8        |
|    fps             | 51       |
|    time_elapsed    | 772      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1843.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -25.5    |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.00506  |
|    ent_coef_loss   | 8.16     |
|    learning_rate   | 0.000995 |
|    n_updates       | 224425   |
---------------------------------
Eval num_timesteps=60000, episode_reward=3346.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.35e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -19.9    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | -3.96    |
|    learning_rate   | 0.000994 |
|    n_updates       | 234425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 51       |
|    time_elapsed    | 1156     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1352.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -19.5    |
|    critic_loss     | 2.29     |
|    ent_coef        | 0.00615  |
|    ent_coef_loss   | 5.57     |
|    learning_rate   | 0.000993 |
|    n_updates       | 244425   |
---------------------------------
Eval num_timesteps=80000, episode_reward=816.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -22.1    |
|    critic_loss     | 2.06     |
|    ent_coef        | 0.00637  |
|    ent_coef_loss   | -0.557   |
|    learning_rate   | 0.000992 |
|    n_updates       | 254425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 52       |
|    time_elapsed    | 1525     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=470.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00572  |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.000991 |
|    n_updates       | 264425   |
---------------------------------
Eval num_timesteps=100000, episode_reward=2044.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00565  |
|    ent_coef_loss   | 0.781    |
|    learning_rate   | 0.00099  |
|    n_updates       | 274425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 20       |
|    fps             | 52       |
|    time_elapsed    | 1891     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2445.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -20.5    |
|    critic_loss     | 1.07     |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.000989 |
|    n_updates       | 284425   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1966.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -20.6    |
|    critic_loss     | 1.42     |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | -6.29    |
|    learning_rate   | 0.000988 |
|    n_updates       | 294425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.59e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 53       |
|    time_elapsed    | 2255     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=3865.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.87e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -19.9    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | -3.74    |
|    learning_rate   | 0.000987 |
|    n_updates       | 304425   |
---------------------------------
Eval num_timesteps=140000, episode_reward=2557.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.00585  |
|    ent_coef_loss   | 1.83     |
|    learning_rate   | 0.000986 |
|    n_updates       | 314425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 53       |
|    time_elapsed    | 2620     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2197.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | -3.11    |
|    learning_rate   | 0.000985 |
|    n_updates       | 324425   |
---------------------------------
Eval num_timesteps=160000, episode_reward=4464.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.46e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 1.5      |
|    ent_coef        | 0.00627  |
|    ent_coef_loss   | -0.0257  |
|    learning_rate   | 0.000984 |
|    n_updates       | 334425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 53       |
|    time_elapsed    | 2984     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=4663.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.66e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 2.37     |
|    ent_coef        | 0.00685  |
|    ent_coef_loss   | 0.344    |
|    learning_rate   | 0.000983 |
|    n_updates       | 344425   |
---------------------------------
New best mean reward!
Eval num_timesteps=180000, episode_reward=4757.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.76e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00654  |
|    ent_coef_loss   | 6.68     |
|    learning_rate   | 0.000982 |
|    n_updates       | 354425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 53       |
|    time_elapsed    | 3349     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=2665.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -34.5    |
|    critic_loss     | 2.26     |
|    ent_coef        | 0.00711  |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000981 |
|    n_updates       | 364425   |
---------------------------------
Eval num_timesteps=200000, episode_reward=4380.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.38e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 1.02     |
|    ent_coef        | 0.00706  |
|    ent_coef_loss   | -4.11    |
|    learning_rate   | 0.00098  |
|    n_updates       | 374425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.02e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 53       |
|    time_elapsed    | 3714     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=4448.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.45e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00746  |
|    ent_coef_loss   | -3.29    |
|    learning_rate   | 0.000979 |
|    n_updates       | 384425   |
---------------------------------
Eval num_timesteps=220000, episode_reward=576.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.0066   |
|    ent_coef_loss   | 6.3      |
|    learning_rate   | 0.000978 |
|    n_updates       | 394425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 53       |
|    time_elapsed    | 4081     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=606.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 2.78     |
|    ent_coef        | 0.00839  |
|    ent_coef_loss   | -5.26    |
|    learning_rate   | 0.000977 |
|    n_updates       | 404425   |
---------------------------------
Eval num_timesteps=240000, episode_reward=495.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 3.28     |
|    ent_coef        | 0.00891  |
|    ent_coef_loss   | 3.66     |
|    learning_rate   | 0.000976 |
|    n_updates       | 414425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 53       |
|    time_elapsed    | 4447     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=4303.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.3e+03  |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 2.93     |
|    ent_coef        | 0.00736  |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000975 |
|    n_updates       | 424425   |
---------------------------------
Eval num_timesteps=260000, episode_reward=2559.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 2.29     |
|    ent_coef        | 0.00707  |
|    ent_coef_loss   | 5.64     |
|    learning_rate   | 0.000974 |
|    n_updates       | 434425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 54       |
|    time_elapsed    | 4813     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=4183.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.18e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -39.9    |
|    critic_loss     | 1.69     |
|    ent_coef        | 0.00599  |
|    ent_coef_loss   | -0.255   |
|    learning_rate   | 0.000973 |
|    n_updates       | 444425   |
---------------------------------
Eval num_timesteps=280000, episode_reward=3818.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.82e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -35.2    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00683  |
|    ent_coef_loss   | 1.76     |
|    learning_rate   | 0.000972 |
|    n_updates       | 454425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 54       |
|    time_elapsed    | 5179     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=2836.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 0.968    |
|    ent_coef        | 0.00704  |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.000971 |
|    n_updates       | 464425   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2102.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00701  |
|    ent_coef_loss   | 3.32     |
|    learning_rate   | 0.00097  |
|    n_updates       | 474425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    episodes        | 60       |
|    fps             | 54       |
|    time_elapsed    | 5544     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=1982.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -41.6    |
|    critic_loss     | 1.75     |
|    ent_coef        | 0.0069   |
|    ent_coef_loss   | 1.63     |
|    learning_rate   | 0.000969 |
|    n_updates       | 484425   |
---------------------------------
Eval num_timesteps=320000, episode_reward=4289.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.29e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 2.08     |
|    ent_coef        | 0.00652  |
|    ent_coef_loss   | 9.34     |
|    learning_rate   | 0.000968 |
|    n_updates       | 494425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 54       |
|    time_elapsed    | 5909     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=4036.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.04e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | 4.35     |
|    learning_rate   | 0.000967 |
|    n_updates       | 504425   |
---------------------------------
Eval num_timesteps=340000, episode_reward=2365.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | 2.29     |
|    learning_rate   | 0.000966 |
|    n_updates       | 514425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 54       |
|    time_elapsed    | 6275     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=2761.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 1.74     |
|    ent_coef        | 0.00517  |
|    ent_coef_loss   | 3.48     |
|    learning_rate   | 0.000965 |
|    n_updates       | 524425   |
---------------------------------
Eval num_timesteps=360000, episode_reward=671.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -33.6    |
|    critic_loss     | 3.19     |
|    ent_coef        | 0.00575  |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.000964 |
|    n_updates       | 534425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 54       |
|    time_elapsed    | 6640     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=3875.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.88e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 2.61     |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | 4.68     |
|    learning_rate   | 0.000963 |
|    n_updates       | 544425   |
---------------------------------
Eval num_timesteps=380000, episode_reward=3060.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000962 |
|    n_updates       | 554425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.22e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 54       |
|    time_elapsed    | 7006     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=3514.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.51e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -34.3    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | 6.44     |
|    learning_rate   | 0.000961 |
|    n_updates       | 564425   |
---------------------------------
Eval num_timesteps=400000, episode_reward=2096.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 4.69     |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.00096  |
|    n_updates       | 574425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 54       |
|    time_elapsed    | 7372     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2349.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00599  |
|    ent_coef_loss   | 4.65     |
|    learning_rate   | 0.000959 |
|    n_updates       | 584425   |
---------------------------------
Eval num_timesteps=420000, episode_reward=962.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 2.39     |
|    ent_coef        | 0.00543  |
|    ent_coef_loss   | -3.87    |
|    learning_rate   | 0.000958 |
|    n_updates       | 594425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 54       |
|    time_elapsed    | 7739     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=1071.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -36.7    |
|    critic_loss     | 1.27     |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | 6.05     |
|    learning_rate   | 0.000957 |
|    n_updates       | 604425   |
---------------------------------
Eval num_timesteps=440000, episode_reward=2952.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.00493  |
|    ent_coef_loss   | -3.03    |
|    learning_rate   | 0.000956 |
|    n_updates       | 614425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 54       |
|    time_elapsed    | 8105     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=3103.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000955 |
|    n_updates       | 624425   |
---------------------------------
Eval num_timesteps=460000, episode_reward=3337.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -39.5    |
|    critic_loss     | 1.87     |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | 8.55     |
|    learning_rate   | 0.000954 |
|    n_updates       | 634425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 54       |
|    time_elapsed    | 8471     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2490.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -44.3    |
|    critic_loss     | 3.47     |
|    ent_coef        | 0.0061   |
|    ent_coef_loss   | 6.74     |
|    learning_rate   | 0.000953 |
|    n_updates       | 644425   |
---------------------------------
Eval num_timesteps=480000, episode_reward=2430.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | -0.531   |
|    learning_rate   | 0.000952 |
|    n_updates       | 654425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 54       |
|    time_elapsed    | 8836     |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=2091.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -42.3    |
|    critic_loss     | 3.03     |
|    ent_coef        | 0.00631  |
|    ent_coef_loss   | -4.72    |
|    learning_rate   | 0.000951 |
|    n_updates       | 664425   |
---------------------------------
Eval num_timesteps=500000, episode_reward=2610.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.00095  |
|    n_updates       | 674425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 54       |
|    time_elapsed    | 9201     |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=3722.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.72e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 2.83     |
|    ent_coef        | 0.00655  |
|    ent_coef_loss   | 6.83     |
|    learning_rate   | 0.000949 |
|    n_updates       | 684425   |
---------------------------------
Eval num_timesteps=520000, episode_reward=2981.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 3.24     |
|    ent_coef        | 0.00645  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000948 |
|    n_updates       | 694425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 54       |
|    time_elapsed    | 9566     |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=4557.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.56e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 4.36     |
|    ent_coef        | 0.00703  |
|    ent_coef_loss   | 1.19     |
|    learning_rate   | 0.000947 |
|    n_updates       | 704425   |
---------------------------------
Eval num_timesteps=540000, episode_reward=4751.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.75e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -44.1    |
|    critic_loss     | 3.68     |
|    ent_coef        | 0.00712  |
|    ent_coef_loss   | -0.912   |
|    learning_rate   | 0.000946 |
|    n_updates       | 714425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 54       |
|    time_elapsed    | 9932     |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=4417.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.42e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 2.51     |
|    ent_coef        | 0.00776  |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.000945 |
|    n_updates       | 724425   |
---------------------------------
Eval num_timesteps=560000, episode_reward=4293.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.29e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -44.1    |
|    critic_loss     | 1.55     |
|    ent_coef        | 0.00683  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000944 |
|    n_updates       | 734425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 54       |
|    time_elapsed    | 10297    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=3514.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.51e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.006    |
|    ent_coef_loss   | -4.08    |
|    learning_rate   | 0.000943 |
|    n_updates       | 744425   |
---------------------------------
Eval num_timesteps=580000, episode_reward=4204.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.2e+03  |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 2.2      |
|    ent_coef        | 0.0058   |
|    ent_coef_loss   | 0.741    |
|    learning_rate   | 0.000942 |
|    n_updates       | 754425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 54       |
|    time_elapsed    | 10662    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2508.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 2.57     |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.000941 |
|    n_updates       | 764425   |
---------------------------------
Eval num_timesteps=600000, episode_reward=3546.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.55e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 1.12     |
|    ent_coef        | 0.00512  |
|    ent_coef_loss   | 5.36     |
|    learning_rate   | 0.00094  |
|    n_updates       | 774425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 54       |
|    time_elapsed    | 11028    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=4679.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.68e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -44.8    |
|    critic_loss     | 1.7      |
|    ent_coef        | 0.00485  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000939 |
|    n_updates       | 784425   |
---------------------------------
Eval num_timesteps=620000, episode_reward=4944.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.94e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 2.37     |
|    ent_coef        | 0.00591  |
|    ent_coef_loss   | 7.94     |
|    learning_rate   | 0.000938 |
|    n_updates       | 794425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 54       |
|    time_elapsed    | 11394    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=3915.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.92e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00572  |
|    ent_coef_loss   | 0.658    |
|    learning_rate   | 0.000937 |
|    n_updates       | 804425   |
---------------------------------
Eval num_timesteps=640000, episode_reward=4870.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.87e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -42.2    |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | 3.56     |
|    learning_rate   | 0.000936 |
|    n_updates       | 814425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 54       |
|    time_elapsed    | 11759    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=4617.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.62e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 1.67     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | 4.56     |
|    learning_rate   | 0.000935 |
|    n_updates       | 824425   |
---------------------------------
Eval num_timesteps=660000, episode_reward=4685.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.69e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.00583  |
|    ent_coef_loss   | -2.23    |
|    learning_rate   | 0.000934 |
|    n_updates       | 834425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 54       |
|    time_elapsed    | 12125    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=3653.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.65e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 2.6      |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | -3.79    |
|    learning_rate   | 0.000933 |
|    n_updates       | 844425   |
---------------------------------
Eval num_timesteps=680000, episode_reward=4481.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.48e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -46.7    |
|    critic_loss     | 3.33     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | 1.88     |
|    learning_rate   | 0.000932 |
|    n_updates       | 854425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 54       |
|    time_elapsed    | 12491    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=4568.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.57e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 2.48     |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | -5.01    |
|    learning_rate   | 0.000931 |
|    n_updates       | 864425   |
---------------------------------
Eval num_timesteps=700000, episode_reward=4066.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.07e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -45.9    |
|    critic_loss     | 3.25     |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | -0.299   |
|    learning_rate   | 0.00093  |
|    n_updates       | 874425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 54       |
|    time_elapsed    | 12857    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=4173.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.17e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 2.06     |
|    ent_coef        | 0.00636  |
|    ent_coef_loss   | 4.35     |
|    learning_rate   | 0.000929 |
|    n_updates       | 884425   |
---------------------------------
Eval num_timesteps=720000, episode_reward=4658.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.66e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00637  |
|    ent_coef_loss   | -0.411   |
|    learning_rate   | 0.000928 |
|    n_updates       | 894425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.02e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 54       |
|    time_elapsed    | 13223    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=4544.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.54e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.00711  |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.000927 |
|    n_updates       | 904425   |
---------------------------------
Eval num_timesteps=740000, episode_reward=4797.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.8e+03  |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 3.4      |
|    ent_coef        | 0.00771  |
|    ent_coef_loss   | -3.54    |
|    learning_rate   | 0.000926 |
|    n_updates       | 914425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 54       |
|    time_elapsed    | 13590    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=4271.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 5.74     |
|    ent_coef        | 0.0079   |
|    ent_coef_loss   | -4       |
|    learning_rate   | 0.000925 |
|    n_updates       | 924425   |
---------------------------------
Eval num_timesteps=760000, episode_reward=4606.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.61e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 2.54     |
|    ent_coef        | 0.00712  |
|    ent_coef_loss   | 0.869    |
|    learning_rate   | 0.000924 |
|    n_updates       | 934425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.3e+03  |
| time/              |          |
|    episodes        | 152      |
|    fps             | 54       |
|    time_elapsed    | 13956    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=4347.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.35e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -56.7    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.00783  |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.000923 |
|    n_updates       | 944425   |
---------------------------------
Eval num_timesteps=780000, episode_reward=4497.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.5e+03  |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 1.84     |
|    ent_coef        | 0.0078   |
|    ent_coef_loss   | -1.11    |
|    learning_rate   | 0.000922 |
|    n_updates       | 954425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.37e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 54       |
|    time_elapsed    | 14322    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=4685.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.69e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 1.43     |
|    ent_coef        | 0.00758  |
|    ent_coef_loss   | 3.37     |
|    learning_rate   | 0.000921 |
|    n_updates       | 964425   |
---------------------------------
Eval num_timesteps=800000, episode_reward=4174.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.17e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.0065   |
|    ent_coef_loss   | -3.21    |
|    learning_rate   | 0.00092  |
|    n_updates       | 974425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.4e+03  |
| time/              |          |
|    episodes        | 160      |
|    fps             | 54       |
|    time_elapsed    | 14689    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=4065.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.07e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.00708  |
|    ent_coef_loss   | -9.63    |
|    learning_rate   | 0.000919 |
|    n_updates       | 984425   |
---------------------------------
Eval num_timesteps=820000, episode_reward=4030.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.03e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 2.04     |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | -1.58    |
|    learning_rate   | 0.000918 |
|    n_updates       | 994425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.51e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 54       |
|    time_elapsed    | 15056    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=4285.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.29e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00663  |
|    ent_coef_loss   | -5.03    |
|    learning_rate   | 0.000917 |
|    n_updates       | 1004425  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2385.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 2.02     |
|    ent_coef        | 0.00643  |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.000916 |
|    n_updates       | 1014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.55e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 54       |
|    time_elapsed    | 15421    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=4672.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.67e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -60.8    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.00642  |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.000915 |
|    n_updates       | 1024425  |
---------------------------------
Eval num_timesteps=860000, episode_reward=4727.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.73e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 3.54     |
|    ent_coef        | 0.00687  |
|    ent_coef_loss   | -0.631   |
|    learning_rate   | 0.000914 |
|    n_updates       | 1034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.63e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 54       |
|    time_elapsed    | 15787    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=4909.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.91e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -59.1    |
|    critic_loss     | 2        |
|    ent_coef        | 0.00755  |
|    ent_coef_loss   | 0.68     |
|    learning_rate   | 0.000913 |
|    n_updates       | 1044425  |
---------------------------------
Eval num_timesteps=880000, episode_reward=4656.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.66e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -60.5    |
|    critic_loss     | 2.56     |
|    ent_coef        | 0.00731  |
|    ent_coef_loss   | -0.836   |
|    learning_rate   | 0.000912 |
|    n_updates       | 1054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.63e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 54       |
|    time_elapsed    | 16154    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=4842.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.84e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -61.3    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | -2.66    |
|    learning_rate   | 0.000911 |
|    n_updates       | 1064425  |
---------------------------------
Eval num_timesteps=900000, episode_reward=4912.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.91e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 2.04     |
|    ent_coef        | 0.00699  |
|    ent_coef_loss   | 0.806    |
|    learning_rate   | 0.00091  |
|    n_updates       | 1074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.68e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 54       |
|    time_elapsed    | 16519    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=4020.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.02e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.00764  |
|    ent_coef_loss   | 3.08     |
|    learning_rate   | 0.000909 |
|    n_updates       | 1084425  |
---------------------------------
Eval num_timesteps=920000, episode_reward=4424.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.42e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -62.9    |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.00708  |
|    ent_coef_loss   | 7.41     |
|    learning_rate   | 0.000908 |
|    n_updates       | 1094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.76e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 54       |
|    time_elapsed    | 16886    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=3611.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.61e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -65      |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.0072   |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000907 |
|    n_updates       | 1104425  |
---------------------------------
Eval num_timesteps=940000, episode_reward=4348.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.35e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 2.58     |
|    ent_coef        | 0.00692  |
|    ent_coef_loss   | 1.49     |
|    learning_rate   | 0.000906 |
|    n_updates       | 1114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.79e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 54       |
|    time_elapsed    | 17252    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=4737.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.74e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.00691  |
|    ent_coef_loss   | -5.49    |
|    learning_rate   | 0.000905 |
|    n_updates       | 1124425  |
---------------------------------
Eval num_timesteps=960000, episode_reward=4594.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.59e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.0068   |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 0.000904 |
|    n_updates       | 1134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.82e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 54       |
|    time_elapsed    | 17617    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=3655.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.66e+03 |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -66.3    |
|    critic_loss     | 2.62     |
|    ent_coef        | 0.00753  |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.000903 |
|    n_updates       | 1144425  |
---------------------------------
Eval num_timesteps=980000, episode_reward=4353.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.35e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 6.53     |
|    ent_coef        | 0.00665  |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000902 |
|    n_updates       | 1154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.84e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 54       |
|    time_elapsed    | 17982    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=3966.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.97e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -67.7    |
|    critic_loss     | 2.54     |
|    ent_coef        | 0.00666  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000901 |
|    n_updates       | 1164425  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=4592.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.59e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -68.5    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | 2.21     |
|    learning_rate   | 0.0009   |
|    n_updates       | 1174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.93e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 54       |
|    time_elapsed    | 18364    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=3416.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.42e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -75.9    |
|    critic_loss     | 1.4      |
|    ent_coef        | 0.00569  |
|    ent_coef_loss   | -2.76    |
|    learning_rate   | 0.000899 |
|    n_updates       | 1184425  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=838.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -71.8    |
|    critic_loss     | 1.55     |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | -0.648   |
|    learning_rate   | 0.000898 |
|    n_updates       | 1194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.85e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 54       |
|    time_elapsed    | 18750    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=828.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -71.2    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | -5.17    |
|    learning_rate   | 0.000897 |
|    n_updates       | 1204425  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=662.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | -0.209   |
|    learning_rate   | 0.000896 |
|    n_updates       | 1214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.73e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 54       |
|    time_elapsed    | 19127    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=933.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 2.24     |
|    ent_coef        | 0.00503  |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.000895 |
|    n_updates       | 1224425  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=647.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 2.35     |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | 6.93     |
|    learning_rate   | 0.000894 |
|    n_updates       | 1234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.59e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 54       |
|    time_elapsed    | 19492    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=3326.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -68.3    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.00649  |
|    ent_coef_loss   | -7.19    |
|    learning_rate   | 0.000893 |
|    n_updates       | 1244425  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=675.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -67      |
|    critic_loss     | 1.5      |
|    ent_coef        | 0.00577  |
|    ent_coef_loss   | 4.02     |
|    learning_rate   | 0.000892 |
|    n_updates       | 1254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.57e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 54       |
|    time_elapsed    | 19857    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=727.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -70.7    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | 0.971    |
|    learning_rate   | 0.000891 |
|    n_updates       | 1264425  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=2498.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -69.9    |
|    critic_loss     | 1.5      |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | -4.82    |
|    learning_rate   | 0.00089  |
|    n_updates       | 1274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.55e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 54       |
|    time_elapsed    | 20222    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=4754.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.75e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 1.94     |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 6.93     |
|    learning_rate   | 0.000889 |
|    n_updates       | 1284425  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=4525.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.53e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -68.5    |
|    critic_loss     | 1.13     |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | -0.402   |
|    learning_rate   | 0.000888 |
|    n_updates       | 1294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.6e+03  |
| time/              |          |
|    episodes        | 224      |
|    fps             | 54       |
|    time_elapsed    | 20588    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=4358.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.36e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 1.24     |
|    ent_coef        | 0.00484  |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.000887 |
|    n_updates       | 1304425  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=4683.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.68e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -68.4    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.00443  |
|    ent_coef_loss   | -3.9     |
|    learning_rate   | 0.000886 |
|    n_updates       | 1314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.69e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 54       |
|    time_elapsed    | 20952    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=4780.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.78e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -67.6    |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | -10.2    |
|    learning_rate   | 0.000885 |
|    n_updates       | 1324425  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=4254.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.25e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -71.2    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | -7.35    |
|    learning_rate   | 0.000884 |
|    n_updates       | 1334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.71e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 54       |
|    time_elapsed    | 21317    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=4942.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.94e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -67.5    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00442  |
|    ent_coef_loss   | 9.37     |
|    learning_rate   | 0.000883 |
|    n_updates       | 1344425  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=4766.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.77e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -71.6    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00443  |
|    ent_coef_loss   | 1.94     |
|    learning_rate   | 0.000882 |
|    n_updates       | 1354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.7e+03  |
| time/              |          |
|    episodes        | 236      |
|    fps             | 54       |
|    time_elapsed    | 21682    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=4924.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.92e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -71      |
|    critic_loss     | 1.3      |
|    ent_coef        | 0.00456  |
|    ent_coef_loss   | 3.48     |
|    learning_rate   | 0.000881 |
|    n_updates       | 1364425  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=4755.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.76e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -74.3    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00475  |
|    ent_coef_loss   | 8.7      |
|    learning_rate   | 0.00088  |
|    n_updates       | 1374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.71e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 54       |
|    time_elapsed    | 22047    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=4520.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.52e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -75.5    |
|    critic_loss     | 0.875    |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | -7.99    |
|    learning_rate   | 0.000879 |
|    n_updates       | 1384425  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=4373.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.37e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -78.6    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.00522  |
|    ent_coef_loss   | 4.86     |
|    learning_rate   | 0.000878 |
|    n_updates       | 1394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.71e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 54       |
|    time_elapsed    | 22413    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=4968.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.97e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -78.2    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00498  |
|    ent_coef_loss   | 2.86     |
|    learning_rate   | 0.000877 |
|    n_updates       | 1404425  |
---------------------------------
New best mean reward!
Eval num_timesteps=1240000, episode_reward=4627.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.63e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -80.4    |
|    critic_loss     | 0.655    |
|    ent_coef        | 0.00464  |
|    ent_coef_loss   | 3.51     |
|    learning_rate   | 0.000876 |
|    n_updates       | 1414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.69e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 54       |
|    time_elapsed    | 22779    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=4733.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.73e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -80.5    |
|    critic_loss     | 0.922    |
|    ent_coef        | 0.00461  |
|    ent_coef_loss   | 9.68     |
|    learning_rate   | 0.000875 |
|    n_updates       | 1424425  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=933.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -78.6    |
|    critic_loss     | 0.715    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -8.36    |
|    learning_rate   | 0.000874 |
|    n_updates       | 1434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.64e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 54       |
|    time_elapsed    | 23142    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=3971.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.97e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -79      |
|    critic_loss     | 0.615    |
|    ent_coef        | 0.00382  |
|    ent_coef_loss   | -0.361   |
|    learning_rate   | 0.000873 |
|    n_updates       | 1444425  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=903.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -75.2    |
|    critic_loss     | 0.831    |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | 4.35     |
|    learning_rate   | 0.000872 |
|    n_updates       | 1454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.51e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 54       |
|    time_elapsed    | 23507    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=4389.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.39e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -74.9    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | -2.4     |
|    learning_rate   | 0.000871 |
|    n_updates       | 1464425  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=693.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -76.3    |
|    critic_loss     | 0.891    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 3.57     |
|    learning_rate   | 0.00087  |
|    n_updates       | 1474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.42e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 54       |
|    time_elapsed    | 23873    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=1038.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -76.1    |
|    critic_loss     | 2.05     |
|    ent_coef        | 0.00453  |
|    ent_coef_loss   | 9.79     |
|    learning_rate   | 0.000869 |
|    n_updates       | 1484425  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=2646.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -76.7    |
|    critic_loss     | 1.48     |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000868 |
|    n_updates       | 1494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.32e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 54       |
|    time_elapsed    | 24238    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2301.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -72.7    |
|    critic_loss     | 1.4      |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000867 |
|    n_updates       | 1504425  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2478.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -79.2    |
|    critic_loss     | 1.43     |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | 5.44     |
|    learning_rate   | 0.000866 |
|    n_updates       | 1514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 54       |
|    time_elapsed    | 24602    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=4006.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.01e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -77.9    |
|    critic_loss     | 1.1      |
|    ent_coef        | 0.00691  |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.000865 |
|    n_updates       | 1524425  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=578.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -76.3    |
|    critic_loss     | 2.81     |
|    ent_coef        | 0.00579  |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000864 |
|    n_updates       | 1534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 272      |
|    fps             | 54       |
|    time_elapsed    | 24968    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2208.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -74.9    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00527  |
|    ent_coef_loss   | 0.514    |
|    learning_rate   | 0.000863 |
|    n_updates       | 1544425  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2265.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -75      |
|    critic_loss     | 3.95     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.000862 |
|    n_updates       | 1554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.11e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 54       |
|    time_elapsed    | 25333    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2126.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -71.9    |
|    critic_loss     | 2.52     |
|    ent_coef        | 0.00626  |
|    ent_coef_loss   | -0.937   |
|    learning_rate   | 0.000861 |
|    n_updates       | 1564425  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2043.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -72.3    |
|    critic_loss     | 2.11     |
|    ent_coef        | 0.00657  |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.00086  |
|    n_updates       | 1574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.03e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 54       |
|    time_elapsed    | 25698    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=529.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -74.7    |
|    critic_loss     | 2.52     |
|    ent_coef        | 0.00675  |
|    ent_coef_loss   | 4.11     |
|    learning_rate   | 0.000859 |
|    n_updates       | 1584425  |
---------------------------------
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 5gdl/SAC_31
Eval num_timesteps=10000, episode_reward=424.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -71.1    |
|    critic_loss     | 2.12     |
|    ent_coef        | 0.00663  |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000999 |
|    n_updates       | 1597953  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=3768.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.77e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -73.6    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.00664  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.000998 |
|    n_updates       | 1607953  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 51       |
|    time_elapsed    | 390      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2173.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -73.3    |
|    critic_loss     | 4.58     |
|    ent_coef        | 0.00641  |
|    ent_coef_loss   | -2.39    |
|    learning_rate   | 0.000997 |
|    n_updates       | 1617953  |
---------------------------------
Eval num_timesteps=40000, episode_reward=644.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -74.6    |
|    critic_loss     | 2.41     |
|    ent_coef        | 0.00632  |
|    ent_coef_loss   | -3.6     |
|    learning_rate   | 0.000996 |
|    n_updates       | 1627953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 51       |
|    time_elapsed    | 769      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=3064.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -61.6    |
|    critic_loss     | 4.46     |
|    ent_coef        | 0.0069   |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 0.000995 |
|    n_updates       | 1637953  |
---------------------------------
Eval num_timesteps=60000, episode_reward=957.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -71.3    |
|    critic_loss     | 3        |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000994 |
|    n_updates       | 1647953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 12       |
|    fps             | 52       |
|    time_elapsed    | 1136     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=693.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 3.09     |
|    ent_coef        | 0.00644  |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000993 |
|    n_updates       | 1657953  |
---------------------------------
Eval num_timesteps=80000, episode_reward=3958.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.96e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -68.4    |
|    critic_loss     | 4.52     |
|    ent_coef        | 0.00591  |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1667953  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 53       |
|    time_elapsed    | 1506     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1748.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -65.4    |
|    critic_loss     | 3.31     |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 4.45     |
|    learning_rate   | 0.000991 |
|    n_updates       | 1677953  |
---------------------------------
Eval num_timesteps=100000, episode_reward=3930.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.93e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 3.54     |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | -5.51    |
|    learning_rate   | 0.00099  |
|    n_updates       | 1687953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 53       |
|    time_elapsed    | 1874     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=3476.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -66.9    |
|    critic_loss     | 4.21     |
|    ent_coef        | 0.00483  |
|    ent_coef_loss   | -0.685   |
|    learning_rate   | 0.000989 |
|    n_updates       | 1697953  |
---------------------------------
Eval num_timesteps=120000, episode_reward=3132.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00446  |
|    ent_coef_loss   | 1.28     |
|    learning_rate   | 0.000988 |
|    n_updates       | 1707953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 53       |
|    time_elapsed    | 2242     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=3760.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.76e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -63.6    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00438  |
|    ent_coef_loss   | -6.4     |
|    learning_rate   | 0.000987 |
|    n_updates       | 1717953  |
---------------------------------
Eval num_timesteps=140000, episode_reward=4476.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.48e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00446  |
|    ent_coef_loss   | -4.68    |
|    learning_rate   | 0.000986 |
|    n_updates       | 1727953  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 53       |
|    time_elapsed    | 2610     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=4462.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.46e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -62.9    |
|    critic_loss     | 2.16     |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | 7.95     |
|    learning_rate   | 0.000985 |
|    n_updates       | 1737953  |
---------------------------------
Eval num_timesteps=160000, episode_reward=4255.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.26e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -61.1    |
|    critic_loss     | 5.28     |
|    ent_coef        | 0.00375  |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000984 |
|    n_updates       | 1747953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 53       |
|    time_elapsed    | 2979     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=4057.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.06e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -65.1    |
|    critic_loss     | 2.28     |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000983 |
|    n_updates       | 1757953  |
---------------------------------
Eval num_timesteps=180000, episode_reward=4390.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.39e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 2.16     |
|    ent_coef        | 0.0043   |
|    ent_coef_loss   | -6.47    |
|    learning_rate   | 0.000982 |
|    n_updates       | 1767953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 53       |
|    time_elapsed    | 3346     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=4277.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.28e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 1.34     |
|    ent_coef        | 0.00462  |
|    ent_coef_loss   | 0.307    |
|    learning_rate   | 0.000981 |
|    n_updates       | 1777953  |
---------------------------------
Eval num_timesteps=200000, episode_reward=2113.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -66      |
|    critic_loss     | 2.17     |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -1       |
|    learning_rate   | 0.00098  |
|    n_updates       | 1787953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 53       |
|    time_elapsed    | 3713     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=3778.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.78e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -68.3    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00467  |
|    ent_coef_loss   | -5.25    |
|    learning_rate   | 0.000979 |
|    n_updates       | 1797953  |
---------------------------------
Eval num_timesteps=220000, episode_reward=2265.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -68.7    |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000978 |
|    n_updates       | 1807953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 44       |
|    fps             | 53       |
|    time_elapsed    | 4082     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=4273.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.27e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -63.9    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000977 |
|    n_updates       | 1817953  |
---------------------------------
Eval num_timesteps=240000, episode_reward=2076.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -65.1    |
|    critic_loss     | 3.92     |
|    ent_coef        | 0.0054   |
|    ent_coef_loss   | 8.17     |
|    learning_rate   | 0.000976 |
|    n_updates       | 1827953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 53       |
|    time_elapsed    | 4449     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=915.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -70.3    |
|    critic_loss     | 1.46     |
|    ent_coef        | 0.00609  |
|    ent_coef_loss   | 0.827    |
|    learning_rate   | 0.000975 |
|    n_updates       | 1837953  |
---------------------------------
Eval num_timesteps=260000, episode_reward=2937.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -71.1    |
|    critic_loss     | 2.56     |
|    ent_coef        | 0.00606  |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.000974 |
|    n_updates       | 1847953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 53       |
|    time_elapsed    | 4816     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=3796.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.8e+03  |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -68.2    |
|    critic_loss     | 3.09     |
|    ent_coef        | 0.00611  |
|    ent_coef_loss   | -2.77    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1857953  |
---------------------------------
Eval num_timesteps=280000, episode_reward=4050.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.05e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -63.9    |
|    critic_loss     | 3.31     |
|    ent_coef        | 0.00625  |
|    ent_coef_loss   | -0.754   |
|    learning_rate   | 0.000972 |
|    n_updates       | 1867953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 54       |
|    time_elapsed    | 5183     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=755.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 3.98     |
|    ent_coef        | 0.00644  |
|    ent_coef_loss   | -4.2     |
|    learning_rate   | 0.000971 |
|    n_updates       | 1877953  |
---------------------------------
Eval num_timesteps=300000, episode_reward=2267.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -66.5    |
|    critic_loss     | 4.14     |
|    ent_coef        | 0.00705  |
|    ent_coef_loss   | 6.03     |
|    learning_rate   | 0.00097  |
|    n_updates       | 1887953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 60       |
|    fps             | 54       |
|    time_elapsed    | 5551     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=4535.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.54e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -64.3    |
|    critic_loss     | 2.82     |
|    ent_coef        | 0.00761  |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000969 |
|    n_updates       | 1897953  |
---------------------------------
New best mean reward!
Eval num_timesteps=320000, episode_reward=764.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -68.5    |
|    critic_loss     | 3.87     |
|    ent_coef        | 0.00781  |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.000968 |
|    n_updates       | 1907953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 54       |
|    time_elapsed    | 5918     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=713.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 3.71     |
|    ent_coef        | 0.00706  |
|    ent_coef_loss   | -7.59    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1917953  |
---------------------------------
Eval num_timesteps=340000, episode_reward=4124.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.12e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -65.5    |
|    critic_loss     | 4.35     |
|    ent_coef        | 0.00662  |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1927953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 54       |
|    time_elapsed    | 6286     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=4713.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.71e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -64.7    |
|    critic_loss     | 2.2      |
|    ent_coef        | 0.00773  |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000965 |
|    n_updates       | 1937953  |
---------------------------------
New best mean reward!
Eval num_timesteps=360000, episode_reward=4253.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.25e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -68.7    |
|    critic_loss     | 1.87     |
|    ent_coef        | 0.00766  |
|    ent_coef_loss   | -3.02    |
|    learning_rate   | 0.000964 |
|    n_updates       | 1947953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 54       |
|    time_elapsed    | 6654     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=4703.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.7e+03  |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -67.7    |
|    critic_loss     | 1.88     |
|    ent_coef        | 0.00755  |
|    ent_coef_loss   | -4.2     |
|    learning_rate   | 0.000963 |
|    n_updates       | 1957953  |
---------------------------------
Eval num_timesteps=380000, episode_reward=4597.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.6e+03  |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 4.21     |
|    ent_coef        | 0.00661  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000962 |
|    n_updates       | 1967953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 54       |
|    time_elapsed    | 7026     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=681.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -69.2    |
|    critic_loss     | 2.53     |
|    ent_coef        | 0.00675  |
|    ent_coef_loss   | -5.29    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1977953  |
---------------------------------
Eval num_timesteps=400000, episode_reward=4549.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.55e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -70.1    |
|    critic_loss     | 2.8      |
|    ent_coef        | 0.00625  |
|    ent_coef_loss   | -4.12    |
|    learning_rate   | 0.00096  |
|    n_updates       | 1987953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 54       |
|    time_elapsed    | 7391     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=623.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 2.93     |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | 2.77     |
|    learning_rate   | 0.000959 |
|    n_updates       | 1997953  |
---------------------------------
Eval num_timesteps=420000, episode_reward=433.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 2.49     |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | 3.01     |
|    learning_rate   | 0.000958 |
|    n_updates       | 2007953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 54       |
|    time_elapsed    | 7758     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=3886.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.89e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -58.7    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00541  |
|    ent_coef_loss   | 2.29     |
|    learning_rate   | 0.000957 |
|    n_updates       | 2017953  |
---------------------------------
Eval num_timesteps=440000, episode_reward=3778.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.78e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 3.17     |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | 4.37     |
|    learning_rate   | 0.000956 |
|    n_updates       | 2027953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 54       |
|    time_elapsed    | 8123     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=4308.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.31e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -58      |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.006    |
|    ent_coef_loss   | 2.16     |
|    learning_rate   | 0.000955 |
|    n_updates       | 2037953  |
---------------------------------
Eval num_timesteps=460000, episode_reward=4780.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.78e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -59.2    |
|    critic_loss     | 3.21     |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | -5.95    |
|    learning_rate   | 0.000954 |
|    n_updates       | 2047953  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 54       |
|    time_elapsed    | 8488     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=4926.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.93e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -59      |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | -2.57    |
|    learning_rate   | 0.000953 |
|    n_updates       | 2057953  |
---------------------------------
New best mean reward!
Eval num_timesteps=480000, episode_reward=4931.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.93e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -58.9    |
|    critic_loss     | 3.86     |
|    ent_coef        | 0.00743  |
|    ent_coef_loss   | 3.95     |
|    learning_rate   | 0.000952 |
|    n_updates       | 2067953  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 54       |
|    time_elapsed    | 8853     |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=4499.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.5e+03  |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 1.43     |
|    ent_coef        | 0.00663  |
|    ent_coef_loss   | -4.86    |
|    learning_rate   | 0.000951 |
|    n_updates       | 2077953  |
---------------------------------
Eval num_timesteps=500000, episode_reward=3913.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.91e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -63.4    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.00675  |
|    ent_coef_loss   | 0.0348   |
|    learning_rate   | 0.00095  |
|    n_updates       | 2087953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 54       |
|    time_elapsed    | 9217     |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=941.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 2.42     |
|    ent_coef        | 0.0071   |
|    ent_coef_loss   | -4.66    |
|    learning_rate   | 0.000949 |
|    n_updates       | 2097953  |
---------------------------------
Eval num_timesteps=520000, episode_reward=457.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -58.5    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.00633  |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000948 |
|    n_updates       | 2107953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 54       |
|    time_elapsed    | 9582     |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=760.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 2.41     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | -3.85    |
|    learning_rate   | 0.000947 |
|    n_updates       | 2117953  |
---------------------------------
Eval num_timesteps=540000, episode_reward=546.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -56.8    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | -3.72    |
|    learning_rate   | 0.000946 |
|    n_updates       | 2127953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 54       |
|    time_elapsed    | 9948     |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=3688.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.69e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.00585  |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.000945 |
|    n_updates       | 2137953  |
---------------------------------
Eval num_timesteps=560000, episode_reward=4538.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.54e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 1.86     |
|    ent_coef        | 0.00643  |
|    ent_coef_loss   | 0.395    |
|    learning_rate   | 0.000944 |
|    n_updates       | 2147953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 54       |
|    time_elapsed    | 10313    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=1262.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -58.6    |
|    critic_loss     | 2.29     |
|    ent_coef        | 0.00618  |
|    ent_coef_loss   | 0.354    |
|    learning_rate   | 0.000943 |
|    n_updates       | 2157953  |
---------------------------------
Eval num_timesteps=580000, episode_reward=4077.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.08e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 2.81     |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | -2.93    |
|    learning_rate   | 0.000942 |
|    n_updates       | 2167953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 54       |
|    time_elapsed    | 10679    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=4220.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.22e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -58.3    |
|    critic_loss     | 3.06     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | 0.318    |
|    learning_rate   | 0.000941 |
|    n_updates       | 2177953  |
---------------------------------
Eval num_timesteps=600000, episode_reward=3135.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 3.44     |
|    ent_coef        | 0.00569  |
|    ent_coef_loss   | 0.51     |
|    learning_rate   | 0.00094  |
|    n_updates       | 2187953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 54       |
|    time_elapsed    | 11043    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2419.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | 5.76     |
|    learning_rate   | 0.000939 |
|    n_updates       | 2197953  |
---------------------------------
Eval num_timesteps=620000, episode_reward=820.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -58.3    |
|    critic_loss     | 2.87     |
|    ent_coef        | 0.00527  |
|    ent_coef_loss   | 3.22     |
|    learning_rate   | 0.000938 |
|    n_updates       | 2207953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 54       |
|    time_elapsed    | 11410    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2208.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 3.5      |
|    ent_coef        | 0.00476  |
|    ent_coef_loss   | -7.93    |
|    learning_rate   | 0.000937 |
|    n_updates       | 2217953  |
---------------------------------
Eval num_timesteps=640000, episode_reward=3666.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.67e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 2.84     |
|    ent_coef        | 0.00506  |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.000936 |
|    n_updates       | 2227953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 54       |
|    time_elapsed    | 11775    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=3486.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.49e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -61.3    |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000935 |
|    n_updates       | 2237953  |
---------------------------------
Eval num_timesteps=660000, episode_reward=2663.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -60.6    |
|    critic_loss     | 4.66     |
|    ent_coef        | 0.00479  |
|    ent_coef_loss   | 4.55     |
|    learning_rate   | 0.000934 |
|    n_updates       | 2247953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 54       |
|    time_elapsed    | 12141    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=768.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -59      |
|    critic_loss     | 2.53     |
|    ent_coef        | 0.00521  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000933 |
|    n_updates       | 2257953  |
---------------------------------
Eval num_timesteps=680000, episode_reward=4551.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.55e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 3.89     |
|    ent_coef        | 0.00532  |
|    ent_coef_loss   | -6.13    |
|    learning_rate   | 0.000932 |
|    n_updates       | 2267953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 54       |
|    time_elapsed    | 12506    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=680.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 2.74     |
|    ent_coef        | 0.0055   |
|    ent_coef_loss   | -7.56    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2277953  |
---------------------------------
Eval num_timesteps=700000, episode_reward=4080.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.08e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 2.46     |
|    ent_coef        | 0.00567  |
|    ent_coef_loss   | -18      |
|    learning_rate   | 0.00093  |
|    n_updates       | 2287953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 54       |
|    time_elapsed    | 12872    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=883.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 5.64     |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | 8.98     |
|    learning_rate   | 0.000929 |
|    n_updates       | 2297953  |
---------------------------------
Eval num_timesteps=720000, episode_reward=4097.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.1e+03  |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -61      |
|    critic_loss     | 5.03     |
|    ent_coef        | 0.00606  |
|    ent_coef_loss   | 3.19     |
|    learning_rate   | 0.000928 |
|    n_updates       | 2307953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 54       |
|    time_elapsed    | 13238    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=441.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 3.71     |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2317953  |
---------------------------------
Eval num_timesteps=740000, episode_reward=817.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 4.03     |
|    ent_coef        | 0.0064   |
|    ent_coef_loss   | -5.54    |
|    learning_rate   | 0.000926 |
|    n_updates       | 2327953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 54       |
|    time_elapsed    | 13603    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1785.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 4.02     |
|    ent_coef        | 0.00585  |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.000925 |
|    n_updates       | 2337953  |
---------------------------------
Eval num_timesteps=760000, episode_reward=4098.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.1e+03  |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 5.57     |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | 2.85     |
|    learning_rate   | 0.000924 |
|    n_updates       | 2347953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 54       |
|    time_elapsed    | 13969    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=3762.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.76e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -59.1    |
|    critic_loss     | 3.44     |
|    ent_coef        | 0.00545  |
|    ent_coef_loss   | -3.88    |
|    learning_rate   | 0.000923 |
|    n_updates       | 2357953  |
---------------------------------
Eval num_timesteps=780000, episode_reward=1496.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 1.86     |
|    ent_coef        | 0.00625  |
|    ent_coef_loss   | -0.162   |
|    learning_rate   | 0.000922 |
|    n_updates       | 2367953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 54       |
|    time_elapsed    | 14334    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=4183.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.18e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 3.74     |
|    ent_coef        | 0.00646  |
|    ent_coef_loss   | -9.69    |
|    learning_rate   | 0.000921 |
|    n_updates       | 2377953  |
---------------------------------
Eval num_timesteps=800000, episode_reward=3329.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -58.9    |
|    critic_loss     | 3.11     |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.00092  |
|    n_updates       | 2387953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 54       |
|    time_elapsed    | 14702    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=4911.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.91e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 4.87     |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | 5.9      |
|    learning_rate   | 0.000919 |
|    n_updates       | 2397953  |
---------------------------------
Eval num_timesteps=820000, episode_reward=4306.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.31e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 3.47     |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | -5.03    |
|    learning_rate   | 0.000918 |
|    n_updates       | 2407953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 54       |
|    time_elapsed    | 15067    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=4614.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.61e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -46.6    |
|    critic_loss     | 5.27     |
|    ent_coef        | 0.00662  |
|    ent_coef_loss   | 0.89     |
|    learning_rate   | 0.000917 |
|    n_updates       | 2417953  |
---------------------------------
Eval num_timesteps=840000, episode_reward=1592.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 5.2      |
|    ent_coef        | 0.00691  |
|    ent_coef_loss   | 6.07     |
|    learning_rate   | 0.000916 |
|    n_updates       | 2427953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 54       |
|    time_elapsed    | 15433    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=906.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -55.7    |
|    critic_loss     | 4.83     |
|    ent_coef        | 0.0075   |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000915 |
|    n_updates       | 2437953  |
---------------------------------
Eval num_timesteps=860000, episode_reward=2877.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -43.2    |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.00691  |
|    ent_coef_loss   | 3.95     |
|    learning_rate   | 0.000914 |
|    n_updates       | 2447953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 54       |
|    time_elapsed    | 15800    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=3495.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.5e+03  |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 4.66     |
|    ent_coef        | 0.00767  |
|    ent_coef_loss   | -3.55    |
|    learning_rate   | 0.000913 |
|    n_updates       | 2457953  |
---------------------------------
Eval num_timesteps=880000, episode_reward=887.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -43.6    |
|    critic_loss     | 6.41     |
|    ent_coef        | 0.00728  |
|    ent_coef_loss   | 4.61     |
|    learning_rate   | 0.000912 |
|    n_updates       | 2467953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 54       |
|    time_elapsed    | 16165    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2408.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 4.59     |
|    ent_coef        | 0.00704  |
|    ent_coef_loss   | 6.1      |
|    learning_rate   | 0.000911 |
|    n_updates       | 2477953  |
---------------------------------
Eval num_timesteps=900000, episode_reward=693.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 8.19     |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | 4.8      |
|    learning_rate   | 0.00091  |
|    n_updates       | 2487953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 54       |
|    time_elapsed    | 16531    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2233.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 7.19     |
|    ent_coef        | 0.0085   |
|    ent_coef_loss   | 5.42     |
|    learning_rate   | 0.000909 |
|    n_updates       | 2497953  |
---------------------------------
Eval num_timesteps=920000, episode_reward=702.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 5.03     |
|    ent_coef        | 0.00974  |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000908 |
|    n_updates       | 2507953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 54       |
|    time_elapsed    | 16896    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=601.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 602      |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 5.29     |
|    ent_coef        | 0.00952  |
|    ent_coef_loss   | 4.42     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2517953  |
---------------------------------
Eval num_timesteps=940000, episode_reward=619.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 4.75     |
|    ent_coef        | 0.00932  |
|    ent_coef_loss   | -0.217   |
|    learning_rate   | 0.000906 |
|    n_updates       | 2527953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 54       |
|    time_elapsed    | 17262    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1031.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 9.17     |
|    ent_coef        | 0.00997  |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2537953  |
---------------------------------
Eval num_timesteps=960000, episode_reward=964.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -55.3    |
|    critic_loss     | 10.6     |
|    ent_coef        | 0.0109   |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000904 |
|    n_updates       | 2547953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 54       |
|    time_elapsed    | 17628    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=718.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -58.8    |
|    critic_loss     | 5.23     |
|    ent_coef        | 0.0104   |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000903 |
|    n_updates       | 2557953  |
---------------------------------
Eval num_timesteps=980000, episode_reward=4044.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.04e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 5.56     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000902 |
|    n_updates       | 2567953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 54       |
|    time_elapsed    | 17993    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=2343.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 5.97     |
|    ent_coef        | 0.00936  |
|    ent_coef_loss   | 4.88     |
|    learning_rate   | 0.000901 |
|    n_updates       | 2577953  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=773.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 6.01     |
|    ent_coef        | 0.00854  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.0009   |
|    n_updates       | 2587953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 54       |
|    time_elapsed    | 18358    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=671.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 7.69     |
|    ent_coef        | 0.00849  |
|    ent_coef_loss   | -0.72    |
|    learning_rate   | 0.000899 |
|    n_updates       | 2597953  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=3268.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 6.7      |
|    ent_coef        | 0.00698  |
|    ent_coef_loss   | -0.718   |
|    learning_rate   | 0.000898 |
|    n_updates       | 2607953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 204      |
|    fps             | 54       |
|    time_elapsed    | 18724    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=4451.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.45e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 5.6      |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | 8.14     |
|    learning_rate   | 0.000897 |
|    n_updates       | 2617953  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=1990.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 4.53     |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | -7.65    |
|    learning_rate   | 0.000896 |
|    n_updates       | 2627953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 54       |
|    time_elapsed    | 19100    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=715.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 6.48     |
|    ent_coef        | 0.00717  |
|    ent_coef_loss   | 6.22     |
|    learning_rate   | 0.000895 |
|    n_updates       | 2637953  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=618.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 6.73     |
|    ent_coef        | 0.0067   |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000894 |
|    n_updates       | 2647953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 54       |
|    time_elapsed    | 19468    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2532.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 2.86     |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | 5        |
|    learning_rate   | 0.000893 |
|    n_updates       | 2657953  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=4412.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.41e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 7.43     |
|    ent_coef        | 0.00665  |
|    ent_coef_loss   | -1.61    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2667953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 54       |
|    time_elapsed    | 19833    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=4454.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.45e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 2.52     |
|    ent_coef        | 0.00715  |
|    ent_coef_loss   | -0.723   |
|    learning_rate   | 0.000891 |
|    n_updates       | 2677953  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=4060.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.06e+03 |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -45.2    |
|    critic_loss     | 3.68     |
|    ent_coef        | 0.00682  |
|    ent_coef_loss   | -5.28    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2687953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 54       |
|    time_elapsed    | 20198    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=2384.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 3.06     |
|    ent_coef        | 0.00658  |
|    ent_coef_loss   | 4.22     |
|    learning_rate   | 0.000889 |
|    n_updates       | 2697953  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=983.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -46.6    |
|    critic_loss     | 6.24     |
|    ent_coef        | 0.00591  |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.000888 |
|    n_updates       | 2707953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 54       |
|    time_elapsed    | 20564    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2325.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 4.06     |
|    ent_coef        | 0.0067   |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000887 |
|    n_updates       | 2717953  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=1279.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 3.65     |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | -6.6     |
|    learning_rate   | 0.000886 |
|    n_updates       | 2727953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 54       |
|    time_elapsed    | 20931    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=827.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 3.46     |
|    ent_coef        | 0.00603  |
|    ent_coef_loss   | -4.94    |
|    learning_rate   | 0.000885 |
|    n_updates       | 2737953  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=792.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 4.16     |
|    ent_coef        | 0.00657  |
|    ent_coef_loss   | 10.5     |
|    learning_rate   | 0.000884 |
|    n_updates       | 2747953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 54       |
|    time_elapsed    | 21296    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=929.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -42.8    |
|    critic_loss     | 3.54     |
|    ent_coef        | 0.00646  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.000883 |
|    n_updates       | 2757953  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=1869.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -45.1    |
|    critic_loss     | 5.32     |
|    ent_coef        | 0.00574  |
|    ent_coef_loss   | 3.69     |
|    learning_rate   | 0.000882 |
|    n_updates       | 2767953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 54       |
|    time_elapsed    | 21663    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2404.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 6.59     |
|    ent_coef        | 0.00578  |
|    ent_coef_loss   | 8.75     |
|    learning_rate   | 0.000881 |
|    n_updates       | 2777953  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2846.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -41.4    |
|    critic_loss     | 5.69     |
|    ent_coef        | 0.00645  |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2787953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 54       |
|    time_elapsed    | 22028    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=1654.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 4.1      |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | -9.03    |
|    learning_rate   | 0.000879 |
|    n_updates       | 2797953  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=4502.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.5e+03  |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 3.54     |
|    ent_coef        | 0.00618  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.000878 |
|    n_updates       | 2807953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 244      |
|    fps             | 54       |
|    time_elapsed    | 22393    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=4107.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.11e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 2.15     |
|    ent_coef        | 0.00624  |
|    ent_coef_loss   | -7.33    |
|    learning_rate   | 0.000877 |
|    n_updates       | 2817953  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=4372.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.37e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 4.18     |
|    ent_coef        | 0.00815  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000876 |
|    n_updates       | 2827953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.69e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 54       |
|    time_elapsed    | 22758    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=4645.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.65e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -45.2    |
|    critic_loss     | 4.8      |
|    ent_coef        | 0.0095   |
|    ent_coef_loss   | 3.73     |
|    learning_rate   | 0.000875 |
|    n_updates       | 2837953  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=4624.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.62e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 6.19     |
|    ent_coef        | 0.00945  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2847953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.71e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 54       |
|    time_elapsed    | 23123    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2036.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 2.74     |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | 6.35     |
|    learning_rate   | 0.000873 |
|    n_updates       | 2857953  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=4687.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.69e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 3.86     |
|    ent_coef        | 0.00713  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.000872 |
|    n_updates       | 2867953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 54       |
|    time_elapsed    | 23488    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=4761.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.76e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 2.23     |
|    ent_coef        | 0.00702  |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.000871 |
|    n_updates       | 2877953  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=735.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 3.76     |
|    ent_coef        | 0.00719  |
|    ent_coef_loss   | 4.02     |
|    learning_rate   | 0.00087  |
|    n_updates       | 2887953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 54       |
|    time_elapsed    | 23855    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=4574.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.57e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -43.5    |
|    critic_loss     | 4.91     |
|    ent_coef        | 0.0078   |
|    ent_coef_loss   | 2.03     |
|    learning_rate   | 0.000869 |
|    n_updates       | 2897953  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=3724.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.72e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 4.22     |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.000868 |
|    n_updates       | 2907953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.74e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 54       |
|    time_elapsed    | 24220    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=4070.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.07e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 3.51     |
|    ent_coef        | 0.00595  |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2917953  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=4166.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.17e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 2.97     |
|    ent_coef        | 0.00535  |
|    ent_coef_loss   | -8.04    |
|    learning_rate   | 0.000866 |
|    n_updates       | 2927953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 54       |
|    time_elapsed    | 24584    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=423.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 1.8      |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | 0.347    |
|    learning_rate   | 0.000865 |
|    n_updates       | 2937953  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2051.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 2.49     |
|    ent_coef        | 0.00572  |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.000864 |
|    n_updates       | 2947953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 54       |
|    time_elapsed    | 24952    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=611.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 2.7      |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | -6.67    |
|    learning_rate   | 0.000863 |
|    n_updates       | 2957953  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=1952.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -47.5    |
|    critic_loss     | 4.45     |
|    ent_coef        | 0.00666  |
|    ent_coef_loss   | -3.86    |
|    learning_rate   | 0.000862 |
|    n_updates       | 2967953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 54       |
|    time_elapsed    | 25316    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=827.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -41.9    |
|    critic_loss     | 2.02     |
|    ent_coef        | 0.00642  |
|    ent_coef_loss   | -4.58    |
|    learning_rate   | 0.000861 |
|    n_updates       | 2977953  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=674.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 3.51     |
|    ent_coef        | 0.00651  |
|    ent_coef_loss   | -3.42    |
|    learning_rate   | 0.00086  |
|    n_updates       | 2987953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 54       |
|    time_elapsed    | 25682    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=319.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 3.8      |
|    ent_coef        | 0.00762  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000859 |
|    n_updates       | 2997953  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2066.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 4.34     |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.000858 |
|    n_updates       | 3007953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 54       |
|    time_elapsed    | 26047    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=638.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 5.52     |
|    ent_coef        | 0.00791  |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.000857 |
|    n_updates       | 3017953  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=588.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 3.25     |
|    ent_coef        | 0.00687  |
|    ent_coef_loss   | -4.79    |
|    learning_rate   | 0.000856 |
|    n_updates       | 3027953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 288      |
|    fps             | 54       |
|    time_elapsed    | 26413    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=660.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 4.04     |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | -4.14    |
|    learning_rate   | 0.000855 |
|    n_updates       | 3037953  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=1273.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 3.65     |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.000854 |
|    n_updates       | 3047953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 54       |
|    time_elapsed    | 26779    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=1613.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.00891  |
|    ent_coef_loss   | -6.85    |
|    learning_rate   | 0.000853 |
|    n_updates       | 3057953  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=893.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 3.16     |
|    ent_coef        | 0.01     |
|    ent_coef_loss   | -2.57    |
|    learning_rate   | 0.000852 |
|    n_updates       | 3067953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 54       |
|    time_elapsed    | 27145    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2479.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 3.1      |
|    ent_coef        | 0.00862  |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.000851 |
|    n_updates       | 3077953  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2308.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 2.2      |
|    ent_coef        | 0.00838  |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.00085  |
|    n_updates       | 3087953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 54       |
|    time_elapsed    | 27509    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2477.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 3.39     |
|    ent_coef        | 0.00701  |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.000849 |
|    n_updates       | 3097953  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=4790.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.79e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -42.9    |
|    critic_loss     | 2.44     |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000848 |
|    n_updates       | 3107953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 54       |
|    time_elapsed    | 27873    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=696.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -41.1    |
|    critic_loss     | 4.24     |
|    ent_coef        | 0.00545  |
|    ent_coef_loss   | -0.28    |
|    learning_rate   | 0.000847 |
|    n_updates       | 3117953  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2025.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -39.7    |
|    critic_loss     | 2.7      |
|    ent_coef        | 0.00613  |
|    ent_coef_loss   | -7.87    |
|    learning_rate   | 0.000846 |
|    n_updates       | 3127953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 54       |
|    time_elapsed    | 28239    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=4087.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.09e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 9.86     |
|    ent_coef        | 0.00636  |
|    ent_coef_loss   | 4.26     |
|    learning_rate   | 0.000845 |
|    n_updates       | 3137953  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=4367.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.37e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -38.5    |
|    critic_loss     | 3.89     |
|    ent_coef        | 0.00629  |
|    ent_coef_loss   | 2.48     |
|    learning_rate   | 0.000844 |
|    n_updates       | 3147953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 54       |
|    time_elapsed    | 28605    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=567.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 2.78     |
|    ent_coef        | 0.00689  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000843 |
|    n_updates       | 3157953  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=450.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 3.1      |
|    ent_coef        | 0.00641  |
|    ent_coef_loss   | -0.425   |
|    learning_rate   | 0.000842 |
|    n_updates       | 3167953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 54       |
|    time_elapsed    | 28974    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=437.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 3.18     |
|    ent_coef        | 0.00651  |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.000841 |
|    n_updates       | 3177953  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1503.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 6.79     |
|    ent_coef        | 0.0074   |
|    ent_coef_loss   | 2.21     |
|    learning_rate   | 0.00084  |
|    n_updates       | 3187953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 54       |
|    time_elapsed    | 29342    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=1372.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 2.34     |
|    ent_coef        | 0.00788  |
|    ent_coef_loss   | -2.94    |
|    learning_rate   | 0.000839 |
|    n_updates       | 3197953  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=1928.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 3.42     |
|    ent_coef        | 0.00805  |
|    ent_coef_loss   | 3.46     |
|    learning_rate   | 0.000838 |
|    n_updates       | 3207953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 54       |
|    time_elapsed    | 29708    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2093.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -40.9    |
|    critic_loss     | 2.69     |
|    ent_coef        | 0.00819  |
|    ent_coef_loss   | -8.31    |
|    learning_rate   | 0.000837 |
|    n_updates       | 3217953  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=1785.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 4.02     |
|    ent_coef        | 0.00814  |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.000836 |
|    n_updates       | 3227953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 328      |
|    fps             | 54       |
|    time_elapsed    | 30074    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=659.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 2.53     |
|    ent_coef        | 0.00758  |
|    ent_coef_loss   | 3.68     |
|    learning_rate   | 0.000835 |
|    n_updates       | 3237953  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=4517.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.52e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.0061   |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.000834 |
|    n_updates       | 3247953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 54       |
|    time_elapsed    | 30440    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=432.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -34.5    |
|    critic_loss     | 3.41     |
|    ent_coef        | 0.00592  |
|    ent_coef_loss   | 9.57     |
|    learning_rate   | 0.000833 |
|    n_updates       | 3257953  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=693.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 2.47     |
|    ent_coef        | 0.00592  |
|    ent_coef_loss   | -3.12    |
|    learning_rate   | 0.000832 |
|    n_updates       | 3267953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 54       |
|    time_elapsed    | 30808    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=905.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 1.77     |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | 2.05     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3277953  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=813.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 2.57     |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | -0.482   |
|    learning_rate   | 0.00083  |
|    n_updates       | 3287953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 54       |
|    time_elapsed    | 31175    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=599.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.00806  |
|    ent_coef_loss   | -3.73    |
|    learning_rate   | 0.000829 |
|    n_updates       | 3297953  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=1972.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.00751  |
|    ent_coef_loss   | -0.662   |
|    learning_rate   | 0.000828 |
|    n_updates       | 3307953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 54       |
|    time_elapsed    | 31540    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=1029.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -34.3    |
|    critic_loss     | 2.2      |
|    ent_coef        | 0.00638  |
|    ent_coef_loss   | 7.14     |
|    learning_rate   | 0.000827 |
|    n_updates       | 3317953  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=659.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 5.69     |
|    ent_coef        | 0.0059   |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3327953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 54       |
|    time_elapsed    | 31905    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=593.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | -0.746   |
|    learning_rate   | 0.000825 |
|    n_updates       | 3337953  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=718.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 2.47     |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | -0.0973  |
|    learning_rate   | 0.000824 |
|    n_updates       | 3347953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 54       |
|    time_elapsed    | 32271    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=847.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.0064   |
|    ent_coef_loss   | -5.42    |
|    learning_rate   | 0.000823 |
|    n_updates       | 3357953  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=666.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 3.83     |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | 7.27     |
|    learning_rate   | 0.000822 |
|    n_updates       | 3367953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 54       |
|    time_elapsed    | 32636    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=507.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 2.25     |
|    ent_coef        | 0.00626  |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.000821 |
|    n_updates       | 3377953  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=517.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -33.5    |
|    critic_loss     | 1.83     |
|    ent_coef        | 0.00599  |
|    ent_coef_loss   | 0.866    |
|    learning_rate   | 0.00082  |
|    n_updates       | 3387953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 54       |
|    time_elapsed    | 33003    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=4060.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.06e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 2.29     |
|    ent_coef        | 0.00638  |
|    ent_coef_loss   | 5.25     |
|    learning_rate   | 0.000819 |
|    n_updates       | 3397953  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=1090.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 2.38     |
|    ent_coef        | 0.00698  |
|    ent_coef_loss   | -4.93    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3407953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 54       |
|    time_elapsed    | 33369    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=633.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -33.5    |
|    critic_loss     | 3.06     |
|    ent_coef        | 0.00884  |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000817 |
|    n_updates       | 3417953  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2409.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 4.18     |
|    ent_coef        | 0.00871  |
|    ent_coef_loss   | 4.99     |
|    learning_rate   | 0.000816 |
|    n_updates       | 3427953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 54       |
|    time_elapsed    | 33735    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2235.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 2.46     |
|    ent_coef        | 0.007    |
|    ent_coef_loss   | 2.66     |
|    learning_rate   | 0.000815 |
|    n_updates       | 3437953  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=1067.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 4.3      |
|    ent_coef        | 0.00764  |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.000814 |
|    n_updates       | 3447953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 372      |
|    fps             | 54       |
|    time_elapsed    | 34100    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2302.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 6.97     |
|    ent_coef        | 0.00756  |
|    ent_coef_loss   | 8.62     |
|    learning_rate   | 0.000813 |
|    n_updates       | 3457953  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=699.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 3.47     |
|    ent_coef        | 0.00785  |
|    ent_coef_loss   | -0.155   |
|    learning_rate   | 0.000812 |
|    n_updates       | 3467953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 54       |
|    time_elapsed    | 34465    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=4573.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.57e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 2.77     |
|    ent_coef        | 0.008    |
|    ent_coef_loss   | -6.24    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3477953  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=992.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -35.2    |
|    critic_loss     | 8.63     |
|    ent_coef        | 0.00859  |
|    ent_coef_loss   | 5.78     |
|    learning_rate   | 0.00081  |
|    n_updates       | 3487953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 54       |
|    time_elapsed    | 34832    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=1913.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 2.52     |
|    ent_coef        | 0.00919  |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000809 |
|    n_updates       | 3497953  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=769.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 2.99     |
|    ent_coef        | 0.00901  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000808 |
|    n_updates       | 3507953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 54       |
|    time_elapsed    | 35198    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=4897.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.9e+03  |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -34.5    |
|    critic_loss     | 4.01     |
|    ent_coef        | 0.00824  |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000807 |
|    n_updates       | 3517953  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=448.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.00763  |
|    ent_coef_loss   | -0.74    |
|    learning_rate   | 0.000806 |
|    n_updates       | 3527953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 54       |
|    time_elapsed    | 35565    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=441.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -38.2    |
|    critic_loss     | 2.52     |
|    ent_coef        | 0.00785  |
|    ent_coef_loss   | 3.49     |
|    learning_rate   | 0.000805 |
|    n_updates       | 3537953  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=465.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -36.2    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.00751  |
|    ent_coef_loss   | -4.37    |
|    learning_rate   | 0.000804 |
|    n_updates       | 3547953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 392      |
|    fps             | 54       |
|    time_elapsed    | 35932    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=801.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -40.3    |
|    critic_loss     | 3.58     |
|    ent_coef        | 0.0071   |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.000803 |
|    n_updates       | 3557953  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=677.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 4.13     |
|    ent_coef        | 0.00773  |
|    ent_coef_loss   | 9.45     |
|    learning_rate   | 0.000802 |
|    n_updates       | 3567953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 396      |
|    fps             | 54       |
|    time_elapsed    | 36300    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=760.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -40.9    |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.00757  |
|    ent_coef_loss   | 0.051    |
|    learning_rate   | 0.000801 |
|    n_updates       | 3577953  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2784.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.0008   |
|    n_updates       | 3587953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 54       |
|    time_elapsed    | 36666    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2420.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 2.42     |
|    ent_coef        | 0.00736  |
|    ent_coef_loss   | -3.66    |
|    learning_rate   | 0.000799 |
|    n_updates       | 3597953  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=711.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 2.34     |
|    ent_coef        | 0.0072   |
|    ent_coef_loss   | 15       |
|    learning_rate   | 0.000798 |
|    n_updates       | 3607953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 54       |
|    time_elapsed    | 37032    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=1244.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -36.7    |
|    critic_loss     | 3.56     |
|    ent_coef        | 0.00711  |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.000797 |
|    n_updates       | 3617953  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=4258.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.26e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 2.22     |
|    ent_coef        | 0.0063   |
|    ent_coef_loss   | 1.89     |
|    learning_rate   | 0.000796 |
|    n_updates       | 3627953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 54       |
|    time_elapsed    | 37396    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=717.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 2.14     |
|    ent_coef        | 0.0064   |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000795 |
|    n_updates       | 3637953  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=623.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 1.34     |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 0.000794 |
|    n_updates       | 3647953  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 54       |
|    time_elapsed    | 37763    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=511.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00609  |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.000793 |
|    n_updates       | 3657953  |
---------------------------------
Logging to 5gdl/SAC_32
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=7699, episode_reward=1694.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 7699     |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | -7.53    |
|    learning_rate   | 0.000999 |
|    n_updates       | 3667852  |
---------------------------------
Eval num_timesteps=17699, episode_reward=518.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 17699    |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 2.46     |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | 8.14     |
|    learning_rate   | 0.000998 |
|    n_updates       | 3677852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 52       |
|    time_elapsed    | 429      |
|    total_timesteps | 22699    |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 2.13     |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | 8.1      |
|    learning_rate   | 0.000998 |
|    n_updates       | 3682852  |
---------------------------------
Eval num_timesteps=27699, episode_reward=2927.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 27699    |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 2.39     |
|    ent_coef        | 0.00591  |
|    ent_coef_loss   | 0.911    |
|    learning_rate   | 0.000997 |
|    n_updates       | 3687852  |
---------------------------------
Eval num_timesteps=37699, episode_reward=1373.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 37699    |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.00575  |
|    ent_coef_loss   | -0.527   |
|    learning_rate   | 0.000996 |
|    n_updates       | 3697852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 52       |
|    time_elapsed    | 819      |
|    total_timesteps | 42699    |
| train/             |          |
|    actor_loss      | -27.8    |
|    critic_loss     | 3.68     |
|    ent_coef        | 0.00577  |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000996 |
|    n_updates       | 3702852  |
---------------------------------
Eval num_timesteps=47699, episode_reward=3097.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 47699    |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00611  |
|    ent_coef_loss   | 0.673    |
|    learning_rate   | 0.000995 |
|    n_updates       | 3707852  |
---------------------------------
Eval num_timesteps=57699, episode_reward=4684.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.68e+03 |
| time/              |          |
|    total_timesteps | 57699    |
| train/             |          |
|    actor_loss      | -32.5    |
|    critic_loss     | 2.02     |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | 0.702    |
|    learning_rate   | 0.000994 |
|    n_updates       | 3717852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 52       |
|    time_elapsed    | 1189     |
|    total_timesteps | 62699    |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 4.05     |
|    ent_coef        | 0.0063   |
|    ent_coef_loss   | 5.71     |
|    learning_rate   | 0.000994 |
|    n_updates       | 3722852  |
---------------------------------
Eval num_timesteps=67699, episode_reward=2160.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 67699    |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 2.81     |
|    ent_coef        | 0.00631  |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.000993 |
|    n_updates       | 3727852  |
---------------------------------
Eval num_timesteps=77699, episode_reward=3131.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 77699    |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 2.17     |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | 0.0783   |
|    learning_rate   | 0.000992 |
|    n_updates       | 3737852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 53       |
|    time_elapsed    | 1557     |
|    total_timesteps | 82699    |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 5.81     |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 0.000992 |
|    n_updates       | 3742852  |
---------------------------------
Eval num_timesteps=87699, episode_reward=4620.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.62e+03 |
| time/              |          |
|    total_timesteps | 87699    |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 2        |
|    ent_coef        | 0.00593  |
|    ent_coef_loss   | 0.0238   |
|    learning_rate   | 0.000991 |
|    n_updates       | 3747852  |
---------------------------------
Eval num_timesteps=97699, episode_reward=2691.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 97699    |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 2.54     |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | -8.86    |
|    learning_rate   | 0.00099  |
|    n_updates       | 3757852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 53       |
|    time_elapsed    | 1925     |
|    total_timesteps | 102699   |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 3.08     |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.00099  |
|    n_updates       | 3762852  |
---------------------------------
Eval num_timesteps=107699, episode_reward=2080.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 107699   |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 3        |
|    ent_coef        | 0.00567  |
|    ent_coef_loss   | 0.755    |
|    learning_rate   | 0.000989 |
|    n_updates       | 3767852  |
---------------------------------
Eval num_timesteps=117699, episode_reward=4574.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.57e+03 |
| time/              |          |
|    total_timesteps | 117699   |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 2.98     |
|    ent_coef        | 0.0059   |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000988 |
|    n_updates       | 3777852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 53       |
|    time_elapsed    | 2294     |
|    total_timesteps | 122699   |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 3.2      |
|    ent_coef        | 0.00621  |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000988 |
|    n_updates       | 3782852  |
---------------------------------
Eval num_timesteps=127699, episode_reward=4524.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.52e+03 |
| time/              |          |
|    total_timesteps | 127699   |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 1.84     |
|    ent_coef        | 0.00583  |
|    ent_coef_loss   | 2.69     |
|    learning_rate   | 0.000987 |
|    n_updates       | 3787852  |
---------------------------------
Eval num_timesteps=137699, episode_reward=4722.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.72e+03 |
| time/              |          |
|    total_timesteps | 137699   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 3.03     |
|    ent_coef        | 0.00558  |
|    ent_coef_loss   | -3.51    |
|    learning_rate   | 0.000986 |
|    n_updates       | 3797852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    episodes        | 28       |
|    fps             | 53       |
|    time_elapsed    | 2661     |
|    total_timesteps | 142699   |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 2.71     |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.000986 |
|    n_updates       | 3802852  |
---------------------------------
Eval num_timesteps=147699, episode_reward=3811.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.81e+03 |
| time/              |          |
|    total_timesteps | 147699   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 2.98     |
|    ent_coef        | 0.00574  |
|    ent_coef_loss   | 2.45     |
|    learning_rate   | 0.000985 |
|    n_updates       | 3807852  |
---------------------------------
Eval num_timesteps=157699, episode_reward=1656.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 157699   |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 3.29     |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000984 |
|    n_updates       | 3817852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 53       |
|    time_elapsed    | 3030     |
|    total_timesteps | 162699   |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 4.46     |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | 8.79     |
|    learning_rate   | 0.000984 |
|    n_updates       | 3822852  |
---------------------------------
Eval num_timesteps=167699, episode_reward=3598.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.6e+03  |
| time/              |          |
|    total_timesteps | 167699   |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | -5.77    |
|    learning_rate   | 0.000983 |
|    n_updates       | 3827852  |
---------------------------------
Eval num_timesteps=177699, episode_reward=2614.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 177699   |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 2.22     |
|    ent_coef        | 0.00535  |
|    ent_coef_loss   | -5.48    |
|    learning_rate   | 0.000982 |
|    n_updates       | 3837852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 53       |
|    time_elapsed    | 3400     |
|    total_timesteps | 182699   |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.00516  |
|    ent_coef_loss   | 11.7     |
|    learning_rate   | 0.000982 |
|    n_updates       | 3842852  |
---------------------------------
Eval num_timesteps=187699, episode_reward=2303.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 187699   |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.000981 |
|    n_updates       | 3847852  |
---------------------------------
Eval num_timesteps=197699, episode_reward=828.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 197699   |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 2.27     |
|    ent_coef        | 0.00514  |
|    ent_coef_loss   | 3.84     |
|    learning_rate   | 0.00098  |
|    n_updates       | 3857852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 53       |
|    time_elapsed    | 3769     |
|    total_timesteps | 202699   |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00496  |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.00098  |
|    n_updates       | 3862852  |
---------------------------------
Eval num_timesteps=207699, episode_reward=4366.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.37e+03 |
| time/              |          |
|    total_timesteps | 207699   |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 2.17     |
|    ent_coef        | 0.00486  |
|    ent_coef_loss   | 0.8      |
|    learning_rate   | 0.000979 |
|    n_updates       | 3867852  |
---------------------------------
Eval num_timesteps=217699, episode_reward=2799.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 217699   |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 2.24     |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 0.976    |
|    learning_rate   | 0.000978 |
|    n_updates       | 3877852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 53       |
|    time_elapsed    | 4137     |
|    total_timesteps | 222699   |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 1.79     |
|    ent_coef        | 0.00545  |
|    ent_coef_loss   | 5.56     |
|    learning_rate   | 0.000978 |
|    n_updates       | 3882852  |
---------------------------------
Eval num_timesteps=227699, episode_reward=2243.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 227699   |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 3.15     |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | -2.5     |
|    learning_rate   | 0.000977 |
|    n_updates       | 3887852  |
---------------------------------
Eval num_timesteps=237699, episode_reward=2377.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 237699   |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 3.01     |
|    ent_coef        | 0.0057   |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.000976 |
|    n_updates       | 3897852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 53       |
|    time_elapsed    | 4504     |
|    total_timesteps | 242699   |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 1.79     |
|    ent_coef        | 0.00615  |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000976 |
|    n_updates       | 3902852  |
---------------------------------
Eval num_timesteps=247699, episode_reward=4110.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.11e+03 |
| time/              |          |
|    total_timesteps | 247699   |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | 1.65     |
|    learning_rate   | 0.000975 |
|    n_updates       | 3907852  |
---------------------------------
Eval num_timesteps=257699, episode_reward=908.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 257699   |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 2.83     |
|    ent_coef        | 0.00616  |
|    ent_coef_loss   | 8.75     |
|    learning_rate   | 0.000974 |
|    n_updates       | 3917852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 53       |
|    time_elapsed    | 4874     |
|    total_timesteps | 262699   |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 3.07     |
|    ent_coef        | 0.00604  |
|    ent_coef_loss   | 5.69     |
|    learning_rate   | 0.000974 |
|    n_updates       | 3922852  |
---------------------------------
Eval num_timesteps=267699, episode_reward=746.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 267699   |
| train/             |          |
|    actor_loss      | -34.1    |
|    critic_loss     | 5.3      |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.000973 |
|    n_updates       | 3927852  |
---------------------------------
Eval num_timesteps=277699, episode_reward=1809.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 277699   |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 2.41     |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | -0.00164 |
|    learning_rate   | 0.000972 |
|    n_updates       | 3937852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 53       |
|    time_elapsed    | 5243     |
|    total_timesteps | 282699   |
| train/             |          |
|    actor_loss      | -32.5    |
|    critic_loss     | 1.7      |
|    ent_coef        | 0.0053   |
|    ent_coef_loss   | -3.19    |
|    learning_rate   | 0.000972 |
|    n_updates       | 3942852  |
---------------------------------
Eval num_timesteps=287699, episode_reward=2567.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 287699   |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00532  |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000971 |
|    n_updates       | 3947852  |
---------------------------------
Eval num_timesteps=297699, episode_reward=4539.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.54e+03 |
| time/              |          |
|    total_timesteps | 297699   |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 3.45     |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.00097  |
|    n_updates       | 3957852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 60       |
|    fps             | 53       |
|    time_elapsed    | 5628     |
|    total_timesteps | 302699   |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 1.75     |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | 0.706    |
|    learning_rate   | 0.00097  |
|    n_updates       | 3962852  |
---------------------------------
Eval num_timesteps=307699, episode_reward=957.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 307699   |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 4.2      |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.000969 |
|    n_updates       | 3967852  |
---------------------------------
Eval num_timesteps=317699, episode_reward=2089.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 317699   |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 4.72     |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | -2.64    |
|    learning_rate   | 0.000968 |
|    n_updates       | 3977852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 53       |
|    time_elapsed    | 6011     |
|    total_timesteps | 322699   |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 1.2      |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | -2.83    |
|    learning_rate   | 0.000968 |
|    n_updates       | 3982852  |
---------------------------------
Eval num_timesteps=327699, episode_reward=2047.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 327699   |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 4.26     |
|    ent_coef        | 0.00556  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000967 |
|    n_updates       | 3987852  |
---------------------------------
Eval num_timesteps=337699, episode_reward=4768.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.77e+03 |
| time/              |          |
|    total_timesteps | 337699   |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | -2.76    |
|    learning_rate   | 0.000966 |
|    n_updates       | 3997852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 53       |
|    time_elapsed    | 6389     |
|    total_timesteps | 342699   |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 2.27     |
|    ent_coef        | 0.00487  |
|    ent_coef_loss   | 0.573    |
|    learning_rate   | 0.000966 |
|    n_updates       | 4002852  |
---------------------------------
Eval num_timesteps=347699, episode_reward=4850.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.85e+03 |
| time/              |          |
|    total_timesteps | 347699   |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 1.97     |
|    ent_coef        | 0.00506  |
|    ent_coef_loss   | -8.28    |
|    learning_rate   | 0.000965 |
|    n_updates       | 4007852  |
---------------------------------
Eval num_timesteps=357699, episode_reward=2253.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 357699   |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 2.02     |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | 9.03     |
|    learning_rate   | 0.000964 |
|    n_updates       | 4017852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 53       |
|    time_elapsed    | 6755     |
|    total_timesteps | 362699   |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 1.5      |
|    ent_coef        | 0.0051   |
|    ent_coef_loss   | -2.42    |
|    learning_rate   | 0.000964 |
|    n_updates       | 4022852  |
---------------------------------
Eval num_timesteps=367699, episode_reward=1726.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 367699   |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.00531  |
|    ent_coef_loss   | 2.57     |
|    learning_rate   | 0.000963 |
|    n_updates       | 4027852  |
---------------------------------
Eval num_timesteps=377699, episode_reward=2666.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 377699   |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00483  |
|    ent_coef_loss   | -4.28    |
|    learning_rate   | 0.000962 |
|    n_updates       | 4037852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 53       |
|    time_elapsed    | 7141     |
|    total_timesteps | 382699   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | 8.34     |
|    learning_rate   | 0.000962 |
|    n_updates       | 4042852  |
---------------------------------
Eval num_timesteps=387699, episode_reward=1858.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 387699   |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 2.06     |
|    ent_coef        | 0.00514  |
|    ent_coef_loss   | 0.547    |
|    learning_rate   | 0.000961 |
|    n_updates       | 4047852  |
---------------------------------
Eval num_timesteps=397699, episode_reward=1954.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 397699   |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 2.49     |
|    ent_coef        | 0.00532  |
|    ent_coef_loss   | 3.33     |
|    learning_rate   | 0.00096  |
|    n_updates       | 4057852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 53       |
|    time_elapsed    | 7527     |
|    total_timesteps | 402699   |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 1.43     |
|    ent_coef        | 0.00495  |
|    ent_coef_loss   | -3.97    |
|    learning_rate   | 0.00096  |
|    n_updates       | 4062852  |
---------------------------------
Eval num_timesteps=407699, episode_reward=1971.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 407699   |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | 0.966    |
|    learning_rate   | 0.000959 |
|    n_updates       | 4067852  |
---------------------------------
Eval num_timesteps=417699, episode_reward=1753.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 417699   |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 2.02     |
|    ent_coef        | 0.00531  |
|    ent_coef_loss   | -4.44    |
|    learning_rate   | 0.000958 |
|    n_updates       | 4077852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 53       |
|    time_elapsed    | 7897     |
|    total_timesteps | 422699   |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 4.34     |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.000958 |
|    n_updates       | 4082852  |
---------------------------------
Eval num_timesteps=427699, episode_reward=4800.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.8e+03  |
| time/              |          |
|    total_timesteps | 427699   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 1.76     |
|    ent_coef        | 0.006    |
|    ent_coef_loss   | -0.81    |
|    learning_rate   | 0.000957 |
|    n_updates       | 4087852  |
---------------------------------
Eval num_timesteps=437699, episode_reward=4960.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.96e+03 |
| time/              |          |
|    total_timesteps | 437699   |
| train/             |          |
|    actor_loss      | -34.3    |
|    critic_loss     | 2.81     |
|    ent_coef        | 0.00614  |
|    ent_coef_loss   | -0.245   |
|    learning_rate   | 0.000956 |
|    n_updates       | 4097852  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 53       |
|    time_elapsed    | 8285     |
|    total_timesteps | 442699   |
| train/             |          |
|    actor_loss      | -34.7    |
|    critic_loss     | 2.34     |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | 6.58     |
|    learning_rate   | 0.000956 |
|    n_updates       | 4102852  |
---------------------------------
Eval num_timesteps=447699, episode_reward=2235.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 447699   |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 2.29     |
|    ent_coef        | 0.00606  |
|    ent_coef_loss   | 2.5      |
|    learning_rate   | 0.000955 |
|    n_updates       | 4107852  |
---------------------------------
Eval num_timesteps=457699, episode_reward=2001.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 457699   |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 1.7      |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000954 |
|    n_updates       | 4117852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 53       |
|    time_elapsed    | 8669     |
|    total_timesteps | 462699   |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 1.62     |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | 3.43     |
|    learning_rate   | 0.000954 |
|    n_updates       | 4122852  |
---------------------------------
Eval num_timesteps=467699, episode_reward=2052.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 467699   |
| train/             |          |
|    actor_loss      | -35.7    |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.00554  |
|    ent_coef_loss   | -3.42    |
|    learning_rate   | 0.000953 |
|    n_updates       | 4127852  |
---------------------------------
Eval num_timesteps=477699, episode_reward=1847.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 477699   |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000952 |
|    n_updates       | 4137852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    episodes        | 96       |
|    fps             | 53       |
|    time_elapsed    | 9046     |
|    total_timesteps | 482699   |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 1.81     |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | 5.22     |
|    learning_rate   | 0.000952 |
|    n_updates       | 4142852  |
---------------------------------
Eval num_timesteps=487699, episode_reward=1825.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 487699   |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 1.7      |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000951 |
|    n_updates       | 4147852  |
---------------------------------
Eval num_timesteps=497699, episode_reward=1908.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 497699   |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 3.28     |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | -0.0441  |
|    learning_rate   | 0.00095  |
|    n_updates       | 4157852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 53       |
|    time_elapsed    | 9412     |
|    total_timesteps | 502699   |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.00493  |
|    ent_coef_loss   | -3.89    |
|    learning_rate   | 0.00095  |
|    n_updates       | 4162852  |
---------------------------------
Eval num_timesteps=507699, episode_reward=4703.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.7e+03  |
| time/              |          |
|    total_timesteps | 507699   |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 1.71     |
|    ent_coef        | 0.00462  |
|    ent_coef_loss   | 0.0416   |
|    learning_rate   | 0.000949 |
|    n_updates       | 4167852  |
---------------------------------
Eval num_timesteps=517699, episode_reward=2118.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 517699   |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.00459  |
|    ent_coef_loss   | 3.86     |
|    learning_rate   | 0.000948 |
|    n_updates       | 4177852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 53       |
|    time_elapsed    | 9777     |
|    total_timesteps | 522699   |
| train/             |          |
|    actor_loss      | -36.5    |
|    critic_loss     | 1.53     |
|    ent_coef        | 0.00482  |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000948 |
|    n_updates       | 4182852  |
---------------------------------
Eval num_timesteps=527699, episode_reward=1859.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 527699   |
| train/             |          |
|    actor_loss      | -38      |
|    critic_loss     | 2        |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | 3.32     |
|    learning_rate   | 0.000947 |
|    n_updates       | 4187852  |
---------------------------------
Eval num_timesteps=537699, episode_reward=4629.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.63e+03 |
| time/              |          |
|    total_timesteps | 537699   |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.005    |
|    ent_coef_loss   | -3.44    |
|    learning_rate   | 0.000946 |
|    n_updates       | 4197852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 53       |
|    time_elapsed    | 10143    |
|    total_timesteps | 542699   |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 2.05     |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000946 |
|    n_updates       | 4202852  |
---------------------------------
Eval num_timesteps=547699, episode_reward=1757.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 547699   |
| train/             |          |
|    actor_loss      | -35.4    |
|    critic_loss     | 1.42     |
|    ent_coef        | 0.00486  |
|    ent_coef_loss   | 5.94     |
|    learning_rate   | 0.000945 |
|    n_updates       | 4207852  |
---------------------------------
Eval num_timesteps=557699, episode_reward=2134.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 557699   |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00464  |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.000944 |
|    n_updates       | 4217852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 53       |
|    time_elapsed    | 10508    |
|    total_timesteps | 562699   |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 1.3      |
|    ent_coef        | 0.0049   |
|    ent_coef_loss   | -2.42    |
|    learning_rate   | 0.000944 |
|    n_updates       | 4222852  |
---------------------------------
Eval num_timesteps=567699, episode_reward=1985.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 567699   |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.00491  |
|    ent_coef_loss   | 1.26     |
|    learning_rate   | 0.000943 |
|    n_updates       | 4227852  |
---------------------------------
Eval num_timesteps=577699, episode_reward=1823.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 577699   |
| train/             |          |
|    actor_loss      | -35.4    |
|    critic_loss     | 2.44     |
|    ent_coef        | 0.00501  |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.000942 |
|    n_updates       | 4237852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 53       |
|    time_elapsed    | 10872    |
|    total_timesteps | 582699   |
| train/             |          |
|    actor_loss      | -34.9    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00531  |
|    ent_coef_loss   | -4.07    |
|    learning_rate   | 0.000942 |
|    n_updates       | 4242852  |
---------------------------------
Eval num_timesteps=587699, episode_reward=3317.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 587699   |
| train/             |          |
|    actor_loss      | -37.7    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00508  |
|    ent_coef_loss   | 10.5     |
|    learning_rate   | 0.000941 |
|    n_updates       | 4247852  |
---------------------------------
Eval num_timesteps=597699, episode_reward=3271.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 597699   |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 1.87     |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.00094  |
|    n_updates       | 4257852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 53       |
|    time_elapsed    | 11234    |
|    total_timesteps | 602699   |
| train/             |          |
|    actor_loss      | -35      |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.0054   |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.00094  |
|    n_updates       | 4262852  |
---------------------------------
Eval num_timesteps=607699, episode_reward=2456.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 607699   |
| train/             |          |
|    actor_loss      | -36.4    |
|    critic_loss     | 1.76     |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000939 |
|    n_updates       | 4267852  |
---------------------------------
Eval num_timesteps=617699, episode_reward=3873.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.87e+03 |
| time/              |          |
|    total_timesteps | 617699   |
| train/             |          |
|    actor_loss      | -37.6    |
|    critic_loss     | 1.15     |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | -5.99    |
|    learning_rate   | 0.000938 |
|    n_updates       | 4277852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 53       |
|    time_elapsed    | 11598    |
|    total_timesteps | 622699   |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.000938 |
|    n_updates       | 4282852  |
---------------------------------
Eval num_timesteps=627699, episode_reward=2090.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 627699   |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | 11.9     |
|    learning_rate   | 0.000937 |
|    n_updates       | 4287852  |
---------------------------------
Eval num_timesteps=637699, episode_reward=3531.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.53e+03 |
| time/              |          |
|    total_timesteps | 637699   |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 2        |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | -6.24    |
|    learning_rate   | 0.000936 |
|    n_updates       | 4297852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 53       |
|    time_elapsed    | 11961    |
|    total_timesteps | 642699   |
| train/             |          |
|    actor_loss      | -40.7    |
|    critic_loss     | 3.3      |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 0.0893   |
|    learning_rate   | 0.000936 |
|    n_updates       | 4302852  |
---------------------------------
Eval num_timesteps=647699, episode_reward=3612.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.61e+03 |
| time/              |          |
|    total_timesteps | 647699   |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 2.25     |
|    ent_coef        | 0.00573  |
|    ent_coef_loss   | -6.72    |
|    learning_rate   | 0.000935 |
|    n_updates       | 4307852  |
---------------------------------
Eval num_timesteps=657699, episode_reward=4449.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.45e+03 |
| time/              |          |
|    total_timesteps | 657699   |
| train/             |          |
|    actor_loss      | -36.7    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | -2.98    |
|    learning_rate   | 0.000934 |
|    n_updates       | 4317852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 53       |
|    time_elapsed    | 12324    |
|    total_timesteps | 662699   |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | 0.564    |
|    learning_rate   | 0.000934 |
|    n_updates       | 4322852  |
---------------------------------
Eval num_timesteps=667699, episode_reward=4448.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.45e+03 |
| time/              |          |
|    total_timesteps | 667699   |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00579  |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000933 |
|    n_updates       | 4327852  |
---------------------------------
Eval num_timesteps=677699, episode_reward=2045.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 677699   |
| train/             |          |
|    actor_loss      | -39.4    |
|    critic_loss     | 0.975    |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.000932 |
|    n_updates       | 4337852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.28e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 53       |
|    time_elapsed    | 12688    |
|    total_timesteps | 682699   |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.00554  |
|    ent_coef_loss   | 2.08     |
|    learning_rate   | 0.000932 |
|    n_updates       | 4342852  |
---------------------------------
Eval num_timesteps=687699, episode_reward=4391.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.39e+03 |
| time/              |          |
|    total_timesteps | 687699   |
| train/             |          |
|    actor_loss      | -38.4    |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | -5.59    |
|    learning_rate   | 0.000931 |
|    n_updates       | 4347852  |
---------------------------------
Eval num_timesteps=697699, episode_reward=4054.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.05e+03 |
| time/              |          |
|    total_timesteps | 697699   |
| train/             |          |
|    actor_loss      | -41.5    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00572  |
|    ent_coef_loss   | -4.21    |
|    learning_rate   | 0.00093  |
|    n_updates       | 4357852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 53       |
|    time_elapsed    | 13049    |
|    total_timesteps | 702699   |
| train/             |          |
|    actor_loss      | -41.1    |
|    critic_loss     | 2.72     |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | 1.06     |
|    learning_rate   | 0.00093  |
|    n_updates       | 4362852  |
---------------------------------
Eval num_timesteps=707699, episode_reward=2267.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 707699   |
| train/             |          |
|    actor_loss      | -41      |
|    critic_loss     | 1.91     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | -0.887   |
|    learning_rate   | 0.000929 |
|    n_updates       | 4367852  |
---------------------------------
Eval num_timesteps=717699, episode_reward=3140.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 717699   |
| train/             |          |
|    actor_loss      | -43.4    |
|    critic_loss     | 2.95     |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | -1.84    |
|    learning_rate   | 0.000928 |
|    n_updates       | 4377852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.31e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 53       |
|    time_elapsed    | 13413    |
|    total_timesteps | 722699   |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | 3.94     |
|    learning_rate   | 0.000928 |
|    n_updates       | 4382852  |
---------------------------------
Eval num_timesteps=727699, episode_reward=3722.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.72e+03 |
| time/              |          |
|    total_timesteps | 727699   |
| train/             |          |
|    actor_loss      | -42      |
|    critic_loss     | 1.21     |
|    ent_coef        | 0.00493  |
|    ent_coef_loss   | 2.11     |
|    learning_rate   | 0.000927 |
|    n_updates       | 4387852  |
---------------------------------
Eval num_timesteps=737699, episode_reward=1254.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 737699   |
| train/             |          |
|    actor_loss      | -44.1    |
|    critic_loss     | 0.697    |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000926 |
|    n_updates       | 4397852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 53       |
|    time_elapsed    | 13777    |
|    total_timesteps | 742699   |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.00577  |
|    ent_coef_loss   | -6.9     |
|    learning_rate   | 0.000926 |
|    n_updates       | 4402852  |
---------------------------------
Eval num_timesteps=747699, episode_reward=1438.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 747699   |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 3.36     |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | 6.03     |
|    learning_rate   | 0.000925 |
|    n_updates       | 4407852  |
---------------------------------
Eval num_timesteps=757699, episode_reward=2417.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 757699   |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 0.798    |
|    ent_coef        | 0.00654  |
|    ent_coef_loss   | -6.62    |
|    learning_rate   | 0.000924 |
|    n_updates       | 4417852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 53       |
|    time_elapsed    | 14140    |
|    total_timesteps | 762699   |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.00593  |
|    ent_coef_loss   | -0.69    |
|    learning_rate   | 0.000924 |
|    n_updates       | 4422852  |
---------------------------------
Eval num_timesteps=767699, episode_reward=3723.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.72e+03 |
| time/              |          |
|    total_timesteps | 767699   |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 2.26     |
|    ent_coef        | 0.0058   |
|    ent_coef_loss   | 4.03     |
|    learning_rate   | 0.000923 |
|    n_updates       | 4427852  |
---------------------------------
Eval num_timesteps=777699, episode_reward=1177.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 777699   |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.48     |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | -5.3     |
|    learning_rate   | 0.000922 |
|    n_updates       | 4437852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 53       |
|    time_elapsed    | 14503    |
|    total_timesteps | 782699   |
| train/             |          |
|    actor_loss      | -45      |
|    critic_loss     | 2.32     |
|    ent_coef        | 0.00558  |
|    ent_coef_loss   | 1.67     |
|    learning_rate   | 0.000922 |
|    n_updates       | 4442852  |
---------------------------------
Eval num_timesteps=787699, episode_reward=1022.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 787699   |
| train/             |          |
|    actor_loss      | -45.1    |
|    critic_loss     | 1.24     |
|    ent_coef        | 0.00603  |
|    ent_coef_loss   | -6.01    |
|    learning_rate   | 0.000921 |
|    n_updates       | 4447852  |
---------------------------------
Eval num_timesteps=797699, episode_reward=1114.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 797699   |
| train/             |          |
|    actor_loss      | -45.2    |
|    critic_loss     | 1.91     |
|    ent_coef        | 0.0057   |
|    ent_coef_loss   | -4.34    |
|    learning_rate   | 0.00092  |
|    n_updates       | 4457852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 53       |
|    time_elapsed    | 14868    |
|    total_timesteps | 802699   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 0.92     |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.00092  |
|    n_updates       | 4462852  |
---------------------------------
Eval num_timesteps=807699, episode_reward=1280.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 807699   |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 0.911    |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | -4.84    |
|    learning_rate   | 0.000919 |
|    n_updates       | 4467852  |
---------------------------------
Eval num_timesteps=817699, episode_reward=1291.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 817699   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | -0.0418  |
|    learning_rate   | 0.000918 |
|    n_updates       | 4477852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 54       |
|    time_elapsed    | 15232    |
|    total_timesteps | 822699   |
| train/             |          |
|    actor_loss      | -45.8    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.00552  |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.000918 |
|    n_updates       | 4482852  |
---------------------------------
Eval num_timesteps=827699, episode_reward=2092.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 827699   |
| train/             |          |
|    actor_loss      | -44.3    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00568  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000917 |
|    n_updates       | 4487852  |
---------------------------------
Eval num_timesteps=837699, episode_reward=2161.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 837699   |
| train/             |          |
|    actor_loss      | -44      |
|    critic_loss     | 0.968    |
|    ent_coef        | 0.00596  |
|    ent_coef_loss   | 3.62     |
|    learning_rate   | 0.000916 |
|    n_updates       | 4497852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 54       |
|    time_elapsed    | 15595    |
|    total_timesteps | 842699   |
| train/             |          |
|    actor_loss      | -44.4    |
|    critic_loss     | 3.65     |
|    ent_coef        | 0.00582  |
|    ent_coef_loss   | 7.26     |
|    learning_rate   | 0.000916 |
|    n_updates       | 4502852  |
---------------------------------
Eval num_timesteps=847699, episode_reward=3951.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.95e+03 |
| time/              |          |
|    total_timesteps | 847699   |
| train/             |          |
|    actor_loss      | -47.1    |
|    critic_loss     | 2.68     |
|    ent_coef        | 0.00601  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000915 |
|    n_updates       | 4507852  |
---------------------------------
Eval num_timesteps=857699, episode_reward=3620.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.62e+03 |
| time/              |          |
|    total_timesteps | 857699   |
| train/             |          |
|    actor_loss      | -43.6    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00638  |
|    ent_coef_loss   | -4.67    |
|    learning_rate   | 0.000914 |
|    n_updates       | 4517852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 54       |
|    time_elapsed    | 15958    |
|    total_timesteps | 862699   |
| train/             |          |
|    actor_loss      | -43.2    |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00632  |
|    ent_coef_loss   | -3.11    |
|    learning_rate   | 0.000914 |
|    n_updates       | 4522852  |
---------------------------------
Eval num_timesteps=867699, episode_reward=3612.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.61e+03 |
| time/              |          |
|    total_timesteps | 867699   |
| train/             |          |
|    actor_loss      | -44.7    |
|    critic_loss     | 2.06     |
|    ent_coef        | 0.00629  |
|    ent_coef_loss   | -7.03    |
|    learning_rate   | 0.000913 |
|    n_updates       | 4527852  |
---------------------------------
Eval num_timesteps=877699, episode_reward=3737.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.74e+03 |
| time/              |          |
|    total_timesteps | 877699   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 2.15     |
|    ent_coef        | 0.00621  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000912 |
|    n_updates       | 4537852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.51e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 54       |
|    time_elapsed    | 16321    |
|    total_timesteps | 882699   |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 2.49     |
|    ent_coef        | 0.00649  |
|    ent_coef_loss   | 7.56     |
|    learning_rate   | 0.000912 |
|    n_updates       | 4542852  |
---------------------------------
Eval num_timesteps=887699, episode_reward=3951.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.95e+03 |
| time/              |          |
|    total_timesteps | 887699   |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 1.21     |
|    ent_coef        | 0.00643  |
|    ent_coef_loss   | -0.527   |
|    learning_rate   | 0.000911 |
|    n_updates       | 4547852  |
---------------------------------
Eval num_timesteps=897699, episode_reward=1671.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 897699   |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 2.89     |
|    ent_coef        | 0.00629  |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.00091  |
|    n_updates       | 4557852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 54       |
|    time_elapsed    | 16685    |
|    total_timesteps | 902699   |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 2.6      |
|    ent_coef        | 0.0066   |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.00091  |
|    n_updates       | 4562852  |
---------------------------------
Eval num_timesteps=907699, episode_reward=2055.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 907699   |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 4.38     |
|    ent_coef        | 0.00639  |
|    ent_coef_loss   | -0.897   |
|    learning_rate   | 0.000909 |
|    n_updates       | 4567852  |
---------------------------------
Eval num_timesteps=917699, episode_reward=1663.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 917699   |
| train/             |          |
|    actor_loss      | -45.4    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000908 |
|    n_updates       | 4577852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 184      |
|    fps             | 54       |
|    time_elapsed    | 17049    |
|    total_timesteps | 922699   |
| train/             |          |
|    actor_loss      | -44.8    |
|    critic_loss     | 3.41     |
|    ent_coef        | 0.00619  |
|    ent_coef_loss   | 4.57     |
|    learning_rate   | 0.000908 |
|    n_updates       | 4582852  |
---------------------------------
Eval num_timesteps=927699, episode_reward=1882.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 927699   |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | 8.68     |
|    learning_rate   | 0.000907 |
|    n_updates       | 4587852  |
---------------------------------
Eval num_timesteps=937699, episode_reward=2351.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 937699   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 2.62     |
|    ent_coef        | 0.00589  |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000906 |
|    n_updates       | 4597852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 188      |
|    fps             | 54       |
|    time_elapsed    | 17412    |
|    total_timesteps | 942699   |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 2.48     |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000906 |
|    n_updates       | 4602852  |
---------------------------------
Eval num_timesteps=947699, episode_reward=3726.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.73e+03 |
| time/              |          |
|    total_timesteps | 947699   |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 3.53     |
|    ent_coef        | 0.00604  |
|    ent_coef_loss   | 0.365    |
|    learning_rate   | 0.000905 |
|    n_updates       | 4607852  |
---------------------------------
Eval num_timesteps=957699, episode_reward=2315.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 957699   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00626  |
|    ent_coef_loss   | -8.7     |
|    learning_rate   | 0.000904 |
|    n_updates       | 4617852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 54       |
|    time_elapsed    | 17776    |
|    total_timesteps | 962699   |
| train/             |          |
|    actor_loss      | -46.1    |
|    critic_loss     | 0.969    |
|    ent_coef        | 0.0062   |
|    ent_coef_loss   | -3.86    |
|    learning_rate   | 0.000904 |
|    n_updates       | 4622852  |
---------------------------------
Eval num_timesteps=967699, episode_reward=2067.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 967699   |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.0061   |
|    ent_coef_loss   | -3.46    |
|    learning_rate   | 0.000903 |
|    n_updates       | 4627852  |
---------------------------------
Eval num_timesteps=977699, episode_reward=3493.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.49e+03 |
| time/              |          |
|    total_timesteps | 977699   |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 2.68     |
|    ent_coef        | 0.00621  |
|    ent_coef_loss   | -0.0719  |
|    learning_rate   | 0.000902 |
|    n_updates       | 4637852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 196      |
|    fps             | 54       |
|    time_elapsed    | 18138    |
|    total_timesteps | 982699   |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 0.000902 |
|    n_updates       | 4642852  |
---------------------------------
Eval num_timesteps=987699, episode_reward=1538.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 987699   |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00603  |
|    ent_coef_loss   | 0.96     |
|    learning_rate   | 0.000901 |
|    n_updates       | 4647852  |
---------------------------------
Eval num_timesteps=997699, episode_reward=2762.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 997699   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 2.39     |
|    ent_coef        | 0.0059   |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.0009   |
|    n_updates       | 4657852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 54       |
|    time_elapsed    | 18501    |
|    total_timesteps | 1002699  |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | 0.455    |
|    learning_rate   | 0.0009   |
|    n_updates       | 4662852  |
---------------------------------
Eval num_timesteps=1007699, episode_reward=893.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 1007699  |
| train/             |          |
|    actor_loss      | -45      |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.0062   |
|    ent_coef_loss   | 3.58     |
|    learning_rate   | 0.000899 |
|    n_updates       | 4667852  |
---------------------------------
Eval num_timesteps=1017699, episode_reward=1690.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 1017699  |
| train/             |          |
|    actor_loss      | -43.3    |
|    critic_loss     | 3.06     |
|    ent_coef        | 0.00674  |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000898 |
|    n_updates       | 4677852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 54       |
|    time_elapsed    | 18865    |
|    total_timesteps | 1022699  |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 1.64     |
|    ent_coef        | 0.00646  |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.000898 |
|    n_updates       | 4682852  |
---------------------------------
Eval num_timesteps=1027699, episode_reward=1304.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1027699  |
| train/             |          |
|    actor_loss      | -44.8    |
|    critic_loss     | 1.46     |
|    ent_coef        | 0.00754  |
|    ent_coef_loss   | 3.52     |
|    learning_rate   | 0.000897 |
|    n_updates       | 4687852  |
---------------------------------
Eval num_timesteps=1037699, episode_reward=4580.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.58e+03 |
| time/              |          |
|    total_timesteps | 1037699  |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00746  |
|    ent_coef_loss   | -0.41    |
|    learning_rate   | 0.000896 |
|    n_updates       | 4697852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 54       |
|    time_elapsed    | 19228    |
|    total_timesteps | 1042699  |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 0.901    |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | 0.724    |
|    learning_rate   | 0.000896 |
|    n_updates       | 4702852  |
---------------------------------
Eval num_timesteps=1047699, episode_reward=4532.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.53e+03 |
| time/              |          |
|    total_timesteps | 1047699  |
| train/             |          |
|    actor_loss      | -44.9    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00742  |
|    ent_coef_loss   | -0.678   |
|    learning_rate   | 0.000895 |
|    n_updates       | 4707852  |
---------------------------------
Eval num_timesteps=1057699, episode_reward=3829.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.83e+03 |
| time/              |          |
|    total_timesteps | 1057699  |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 2.69     |
|    ent_coef        | 0.00773  |
|    ent_coef_loss   | -4.65    |
|    learning_rate   | 0.000894 |
|    n_updates       | 4717852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 54       |
|    time_elapsed    | 19591    |
|    total_timesteps | 1062699  |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00789  |
|    ent_coef_loss   | -0.582   |
|    learning_rate   | 0.000894 |
|    n_updates       | 4722852  |
---------------------------------
Eval num_timesteps=1067699, episode_reward=3708.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.71e+03 |
| time/              |          |
|    total_timesteps | 1067699  |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 1.61     |
|    ent_coef        | 0.00806  |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.000893 |
|    n_updates       | 4727852  |
---------------------------------
Eval num_timesteps=1077699, episode_reward=3668.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.67e+03 |
| time/              |          |
|    total_timesteps | 1077699  |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.61     |
|    ent_coef        | 0.00771  |
|    ent_coef_loss   | 5.88     |
|    learning_rate   | 0.000892 |
|    n_updates       | 4737852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 54       |
|    time_elapsed    | 19954    |
|    total_timesteps | 1082699  |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 1.4      |
|    ent_coef        | 0.00757  |
|    ent_coef_loss   | 0.377    |
|    learning_rate   | 0.000892 |
|    n_updates       | 4742852  |
---------------------------------
Eval num_timesteps=1087699, episode_reward=4105.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.11e+03 |
| time/              |          |
|    total_timesteps | 1087699  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 1.67     |
|    ent_coef        | 0.00777  |
|    ent_coef_loss   | -0.577   |
|    learning_rate   | 0.000891 |
|    n_updates       | 4747852  |
---------------------------------
Eval num_timesteps=1097699, episode_reward=3716.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.72e+03 |
| time/              |          |
|    total_timesteps | 1097699  |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 0.972    |
|    ent_coef        | 0.00756  |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.00089  |
|    n_updates       | 4757852  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 54       |
|    time_elapsed    | 20318    |
|    total_timesteps | 1102699  |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 0.975    |
|    ent_coef        | 0.00768  |
|    ent_coef_loss   | 2.57     |
|    learning_rate   | 0.00089  |
|    n_updates       | 4762852  |
---------------------------------
Eval num_timesteps=1107699, episode_reward=3601.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.6e+03  |
| time/              |          |
|    total_timesteps | 1107699  |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 3.15     |
|    ent_coef        | 0.00726  |
|    ent_coef_loss   | 9.43     |
|    learning_rate   | 0.000889 |
|    n_updates       | 4767852  |
---------------------------------
Eval num_timesteps=1117699, episode_reward=3656.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.66e+03 |
| time/              |          |
|    total_timesteps | 1117699  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 3.18     |
|    ent_coef        | 0.00753  |
|    ent_coef_loss   | 1.8      |
|    learning_rate   | 0.000888 |
|    n_updates       | 4777852  |
