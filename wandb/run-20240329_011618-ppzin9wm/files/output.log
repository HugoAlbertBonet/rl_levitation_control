Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_43
Eval num_timesteps=10000, episode_reward=2409.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | -5.6     |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=1758.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00828  |
|    ent_coef_loss   | 0.791    |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.3e+03  |
| time/              |          |
|    episodes        | 4        |
|    fps             | 40       |
|    time_elapsed    | 495      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1749.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.288    |
|    ent_coef        | 0.00941  |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
Eval num_timesteps=40000, episode_reward=1874.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.225    |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.09e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 39       |
|    time_elapsed    | 1009     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=2377.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 0.391    |
|    ent_coef        | 0.00597  |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=1855.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -43.7    |
|    critic_loss     | 0.402    |
|    ent_coef        | 0.00659  |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 39       |
|    time_elapsed    | 1503     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1897.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -40.5    |
|    critic_loss     | 0.36     |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=1923.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -39.9    |
|    critic_loss     | 0.413    |
|    ent_coef        | 0.0055   |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 40       |
|    time_elapsed    | 1999     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1914.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -37      |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.00656  |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=2474.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 0.412    |
|    ent_coef        | 0.00629  |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 40       |
|    time_elapsed    | 2493     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1745.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 2.94     |
|    ent_coef        | 0.00627  |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=1766.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -37.2    |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.0059   |
|    ent_coef_loss   | -1.77    |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 40       |
|    time_elapsed    | 2989     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1746.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -37.4    |
|    critic_loss     | 0.323    |
|    ent_coef        | 0.00627  |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
Eval num_timesteps=140000, episode_reward=1898.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.483    |
|    ent_coef        | 0.00685  |
|    ent_coef_loss   | 0.616    |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 40       |
|    time_elapsed    | 3486     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=1817.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -37.3    |
|    critic_loss     | 0.33     |
|    ent_coef        | 0.00685  |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
Eval num_timesteps=160000, episode_reward=2370.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 0.718    |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | -0.611   |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 40       |
|    time_elapsed    | 3980     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=2448.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -35.9    |
|    critic_loss     | 0.261    |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | 1.49     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=2466.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -37.5    |
|    critic_loss     | 0.371    |
|    ent_coef        | 0.00503  |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.07e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 40       |
|    time_elapsed    | 4477     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=2359.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.288    |
|    ent_coef        | 0.00541  |
|    ent_coef_loss   | 0.86     |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=2482.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -38.8    |
|    critic_loss     | 0.732    |
|    ent_coef        | 0.00634  |
|    ent_coef_loss   | -0.0819  |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 40       |
|    time_elapsed    | 4974     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2724.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.599    |
|    ent_coef        | 0.00616  |
|    ent_coef_loss   | 0.00174  |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
New best mean reward!
Eval num_timesteps=220000, episode_reward=2495.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -39      |
|    critic_loss     | 0.459    |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | 0.0356   |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 40       |
|    time_elapsed    | 5472     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2467.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -40.9    |
|    critic_loss     | 0.348    |
|    ent_coef        | 0.00567  |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=2475.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -40.4    |
|    critic_loss     | 0.564    |
|    ent_coef        | 0.0055   |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 40       |
|    time_elapsed    | 5971     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2476.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -43      |
|    critic_loss     | 0.276    |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 3.37     |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
Eval num_timesteps=260000, episode_reward=3385.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.39e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -42.5    |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.26e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 40       |
|    time_elapsed    | 6470     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=1940.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -42.9    |
|    critic_loss     | 0.421    |
|    ent_coef        | 0.00653  |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=2517.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -44      |
|    critic_loss     | 0.526    |
|    ent_coef        | 0.00598  |
|    ent_coef_loss   | 0.0199   |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 40       |
|    time_elapsed    | 6968     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=3013.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -44.8    |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.00628  |
|    ent_coef_loss   | 2.97     |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=1856.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 0.472    |
|    ent_coef        | 0.00616  |
|    ent_coef_loss   | 3.99     |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 60       |
|    fps             | 40       |
|    time_elapsed    | 7466     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=3465.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -43.5    |
|    critic_loss     | 0.694    |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | -3.63    |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
New best mean reward!
Eval num_timesteps=320000, episode_reward=3476.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -45.8    |
|    critic_loss     | 0.742    |
|    ent_coef        | 0.00693  |
|    ent_coef_loss   | -7       |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 40       |
|    time_elapsed    | 7966     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=3310.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -46.1    |
|    critic_loss     | 0.898    |
|    ent_coef        | 0.00801  |
|    ent_coef_loss   | -3.32    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=3039.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 0.855    |
|    ent_coef        | 0.00755  |
|    ent_coef_loss   | -4.69    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 40       |
|    time_elapsed    | 8465     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=3078.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 0.585    |
|    ent_coef        | 0.00826  |
|    ent_coef_loss   | -2.69    |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=3019.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -47.1    |
|    critic_loss     | 0.591    |
|    ent_coef        | 0.00782  |
|    ent_coef_loss   | 1.45     |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 40       |
|    time_elapsed    | 8964     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=3392.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.39e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 0.765    |
|    ent_coef        | 0.00854  |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
Eval num_timesteps=380000, episode_reward=3286.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00908  |
|    ent_coef_loss   | 0.0393   |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 40       |
|    time_elapsed    | 9463     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=3189.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 0.867    |
|    ent_coef        | 0.00863  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=3288.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 1.55     |
|    ent_coef        | 0.00777  |
|    ent_coef_loss   | -0.384   |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 40       |
|    time_elapsed    | 9960     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2432.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00813  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
Eval num_timesteps=420000, episode_reward=2364.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 0.467    |
|    ent_coef        | 0.00783  |
|    ent_coef_loss   | -4.08    |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 40       |
|    time_elapsed    | 10457    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=3452.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00827  |
|    ent_coef_loss   | -0.197   |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=2509.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 2.82     |
|    ent_coef        | 0.00761  |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 40       |
|    time_elapsed    | 10955    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=3468.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 1.62     |
|    ent_coef        | 0.00816  |
|    ent_coef_loss   | 1.16     |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=3468.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 1.62     |
|    ent_coef        | 0.00902  |
|    ent_coef_loss   | -5.11    |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 40       |
|    time_elapsed    | 11454    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=3311.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00952  |
|    ent_coef_loss   | 2.92     |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=3459.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 40       |
|    time_elapsed    | 11951    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=3464.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 2.87     |
|    ent_coef        | 0.0119   |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=3464.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 6.71     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 100      |
|    fps             | 40       |
|    time_elapsed    | 12450    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=3465.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.0143   |
|    ent_coef_loss   | -5.5     |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=3475.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -55.3    |
|    critic_loss     | 3.7      |
|    ent_coef        | 0.0172   |
|    ent_coef_loss   | 0.767    |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 40       |
|    time_elapsed    | 12968    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=3463.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 2.18     |
|    ent_coef        | 0.0172   |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
Eval num_timesteps=540000, episode_reward=3476.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -56.3    |
|    critic_loss     | 2.35     |
|    ent_coef        | 0.0192   |
|    ent_coef_loss   | -0.745   |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 40       |
|    time_elapsed    | 13465    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=3476.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 4.88     |
|    ent_coef        | 0.0187   |
|    ent_coef_loss   | -2.46    |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=3475.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 4.03     |
|    ent_coef        | 0.0215   |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 40       |
|    time_elapsed    | 13962    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=3468.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -59.8    |
|    critic_loss     | 3.72     |
|    ent_coef        | 0.0208   |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=3477.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 9.77     |
|    ent_coef        | 0.0184   |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 40       |
|    time_elapsed    | 14457    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=3468.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 22       |
|    ent_coef        | 0.0188   |
|    ent_coef_loss   | 1.5      |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=3468.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 9.27     |
|    ent_coef        | 0.0217   |
|    ent_coef_loss   | 0.701    |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 40       |
|    time_elapsed    | 14953    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=3475.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 9.38     |
|    ent_coef        | 0.0203   |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=2369.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -69.2    |
|    critic_loss     | 39.8     |
|    ent_coef        | 0.026    |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 40       |
|    time_elapsed    | 15449    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2485.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -68.6    |
|    critic_loss     | 34.2     |
|    ent_coef        | 0.0287   |
|    ent_coef_loss   | 5.34     |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=3471.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 32.4     |
|    ent_coef        | 0.0295   |
|    ent_coef_loss   | -2.78    |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 40       |
|    time_elapsed    | 15946    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2967.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 24.2     |
|    ent_coef        | 0.0287   |
|    ent_coef_loss   | 4.17     |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=3300.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 41.8     |
|    ent_coef        | 0.0325   |
|    ent_coef_loss   | 0.659    |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 40       |
|    time_elapsed    | 16443    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=3474.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 56.8     |
|    ent_coef        | 0.0288   |
|    ent_coef_loss   | 1.94     |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=2552.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -61.3    |
|    critic_loss     | 37.1     |
|    ent_coef        | 0.032    |
|    ent_coef_loss   | 0.715    |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 40       |
|    time_elapsed    | 16940    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2125.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 25.6     |
|    ent_coef        | 0.0342   |
|    ent_coef_loss   | -3.58    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=1886.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -84.2    |
|    critic_loss     | 43.7     |
|    ent_coef        | 0.0372   |
|    ent_coef_loss   | 0.563    |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.95e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 40       |
|    time_elapsed    | 17435    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=3471.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -85.4    |
|    critic_loss     | 50.6     |
|    ent_coef        | 0.0445   |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=3473.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -67.6    |
|    critic_loss     | 32.5     |
|    ent_coef        | 0.0602   |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.95e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 40       |
|    time_elapsed    | 17931    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=3472.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 168      |
|    ent_coef        | 0.0641   |
|    ent_coef_loss   | -0.547   |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=3470.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -74.3    |
|    critic_loss     | 58.6     |
|    ent_coef        | 0.0632   |
|    ent_coef_loss   | -0.78    |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 40       |
|    time_elapsed    | 18427    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=3303.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -41.2    |
|    critic_loss     | 48       |
|    ent_coef        | 0.0656   |
|    ent_coef_loss   | -0.0461  |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
Eval num_timesteps=760000, episode_reward=2127.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 51.4     |
|    ent_coef        | 0.0669   |
|    ent_coef_loss   | 1.82     |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 40       |
|    time_elapsed    | 18922    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2091.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -45      |
|    critic_loss     | 53.3     |
|    ent_coef        | 0.076    |
|    ent_coef_loss   | -0.579   |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=3322.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -21.9    |
|    critic_loss     | 66.4     |
|    ent_coef        | 0.0775   |
|    ent_coef_loss   | 0.478    |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 40       |
|    time_elapsed    | 19418    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=3473.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 166      |
|    ent_coef        | 0.0801   |
|    ent_coef_loss   | 0.616    |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=1870.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 25.8     |
|    ent_coef        | 0.0708   |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 40       |
|    time_elapsed    | 19844    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=1875.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -42.4    |
|    critic_loss     | 74.2     |
|    ent_coef        | 0.0687   |
|    ent_coef_loss   | -0.415   |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=1884.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 26.5     |
|    ent_coef        | 0.0668   |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 40       |
|    time_elapsed    | 20271    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=1932.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -57.6    |
|    critic_loss     | 73.5     |
|    ent_coef        | 0.0665   |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=1908.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -42.2    |
|    critic_loss     | 51.6     |
|    ent_coef        | 0.0619   |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 40       |
|    time_elapsed    | 20685    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=3474.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -63.9    |
|    critic_loss     | 32.9     |
|    ent_coef        | 0.0634   |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=1937.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -66.8    |
|    critic_loss     | 72.2     |
|    ent_coef        | 0.066    |
|    ent_coef_loss   | 0.803    |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 40       |
|    time_elapsed    | 21099    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=1940.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -83.2    |
|    critic_loss     | 449      |
|    ent_coef        | 0.0663   |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=3409.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.41e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -69.2    |
|    critic_loss     | 142      |
|    ent_coef        | 0.0538   |
|    ent_coef_loss   | -0.407   |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 40       |
|    time_elapsed    | 21512    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2010.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -85      |
|    critic_loss     | 103      |
|    ent_coef        | 0.0508   |
|    ent_coef_loss   | -2.61    |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=1917.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -71.3    |
|    critic_loss     | 278      |
|    ent_coef        | 0.0735   |
|    ent_coef_loss   | -2.23    |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 180      |
|    fps             | 41       |
|    time_elapsed    | 21927    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2644.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -74.2    |
|    critic_loss     | 167      |
|    ent_coef        | 0.0794   |
|    ent_coef_loss   | -2.41    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=2442.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -79.8    |
|    critic_loss     | 422      |
|    ent_coef        | 0.0893   |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 41       |
|    time_elapsed    | 22340    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2441.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -62.3    |
|    critic_loss     | 1.26e+03 |
|    ent_coef        | 0.0974   |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=1826.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -81.6    |
|    critic_loss     | 79.8     |
|    ent_coef        | 0.0848   |
|    ent_coef_loss   | -0.641   |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 188      |
|    fps             | 41       |
|    time_elapsed    | 22755    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=3460.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -68.1    |
|    critic_loss     | 355      |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | 0.617    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=3468.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 531      |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.619    |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 41       |
|    time_elapsed    | 23168    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=3474.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -48.4    |
|    critic_loss     | 100      |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=3474.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -39.8    |
|    critic_loss     | 179      |
|    ent_coef        | 0.126    |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 41       |
|    time_elapsed    | 23580    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=3473.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 139      |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.00933 |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=3473.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -58.9    |
|    critic_loss     | 195      |
|    ent_coef        | 0.0911   |
|    ent_coef_loss   | 1.76     |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 41       |
|    time_elapsed    | 23992    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=3473.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 398      |
|    ent_coef        | 0.105    |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=3473.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -41.2    |
|    critic_loss     | 210      |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 41       |
|    time_elapsed    | 24405    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=3471.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -68.2    |
|    critic_loss     | 226      |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=3474.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 215      |
|    ent_coef        | 0.0918   |
|    ent_coef_loss   | -0.722   |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 41       |
|    time_elapsed    | 24819    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=3475.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -62.2    |
|    critic_loss     | 802      |
|    ent_coef        | 0.0892   |
|    ent_coef_loss   | -0.0296  |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=3474.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 192      |
|    ent_coef        | 0.0768   |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 42       |
|    time_elapsed    | 25232    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=3475.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -84.8    |
|    critic_loss     | 65.4     |
|    ent_coef        | 0.0637   |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=3464.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -92.3    |
|    critic_loss     | 168      |
|    ent_coef        | 0.0571   |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 42       |
|    time_elapsed    | 25645    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=3475.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -69.3    |
|    critic_loss     | 82.7     |
|    ent_coef        | 0.0537   |
|    ent_coef_loss   | -0.1     |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=3476.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -72.1    |
|    critic_loss     | 30.1     |
|    ent_coef        | 0.0489   |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 42       |
|    time_elapsed    | 26057    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=3475.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 45.7     |
|    ent_coef        | 0.0401   |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=1905.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -43.4    |
|    critic_loss     | 25.4     |
|    ent_coef        | 0.0393   |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 42       |
|    time_elapsed    | 26471    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=3477.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 30       |
|    ent_coef        | 0.0364   |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=1864.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -67.8    |
|    critic_loss     | 28.5     |
|    ent_coef        | 0.0421   |
|    ent_coef_loss   | 0.939    |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 42       |
|    time_elapsed    | 26884    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=1889.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -123     |
|    critic_loss     | 71.7     |
|    ent_coef        | 0.0438   |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=3473.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -92.7    |
|    critic_loss     | 36.7     |
|    ent_coef        | 0.0523   |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 42       |
|    time_elapsed    | 27298    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=3475.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -78.6    |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.0414   |
|    ent_coef_loss   | -0.21    |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=3474.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -80.8    |
|    critic_loss     | 28.6     |
|    ent_coef        | 0.0371   |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 42       |
|    time_elapsed    | 27710    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=1995.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -73.4    |
|    critic_loss     | 20       |
|    ent_coef        | 0.0327   |
|    ent_coef_loss   | 0.672    |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=1887.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 6.23     |
|    ent_coef        | 0.0313   |
|    ent_coef_loss   | -2.67    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 42       |
|    time_elapsed    | 28124    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=1922.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 11.6     |
|    ent_coef        | 0.0249   |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=1955.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 16.5     |
|    ent_coef        | 0.0325   |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 42       |
|    time_elapsed    | 28539    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=3470.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -70.8    |
|    critic_loss     | 28.5     |
|    ent_coef        | 0.0361   |
|    ent_coef_loss   | -0.494   |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=3473.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -70.5    |
|    critic_loss     | 50.7     |
|    ent_coef        | 0.043    |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 42       |
|    time_elapsed    | 28953    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=3473.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 52.3     |
|    ent_coef        | 0.0474   |
|    ent_coef_loss   | -2.09    |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=3477.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -75.6    |
|    critic_loss     | 28.5     |
|    ent_coef        | 0.0315   |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 42       |
|    time_elapsed    | 29369    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=3477.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -67.3    |
|    critic_loss     | 9.63     |
|    ent_coef        | 0.0236   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=3477.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 15.7     |
|    ent_coef        | 0.0194   |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 42       |
|    time_elapsed    | 29783    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=3475.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -64.4    |
|    critic_loss     | 8.29     |
|    ent_coef        | 0.022    |
|    ent_coef_loss   | -3.92    |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2444.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -73      |
|    critic_loss     | 6.49     |
|    ent_coef        | 0.0218   |
|    ent_coef_loss   | -5.05    |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 43       |
|    time_elapsed    | 30197    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=3473.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -70.9    |
|    critic_loss     | 23.2     |
|    ent_coef        | 0.0211   |
|    ent_coef_loss   | -3.82    |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=3475.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -70.6    |
|    critic_loss     | 7.72     |
|    ent_coef        | 0.0253   |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 43       |
|    time_elapsed    | 30611    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=3475.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -80.6    |
|    critic_loss     | 40.7     |
|    ent_coef        | 0.0227   |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=3464.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -75.1    |
|    critic_loss     | 22.9     |
|    ent_coef        | 0.0192   |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 268      |
|    fps             | 43       |
|    time_elapsed    | 31024    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=3466.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 4.87     |
|    ent_coef        | 0.0178   |
|    ent_coef_loss   | 2.29     |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=3475.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 5.03     |
|    ent_coef        | 0.0253   |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.05e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 43       |
|    time_elapsed    | 31438    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2371.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -88.1    |
|    critic_loss     | 28.1     |
|    ent_coef        | 0.0276   |
|    ent_coef_loss   | 6.2      |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=1883.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -60      |
|    critic_loss     | 16       |
|    ent_coef        | 0.0267   |
|    ent_coef_loss   | -0.861   |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 43       |
|    time_elapsed    | 31854    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=3480.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -79      |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.0294   |
|    ent_coef_loss   | 1.74     |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1400000, episode_reward=3109.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -66.6    |
|    critic_loss     | 26.7     |
|    ent_coef        | 0.0322   |
|    ent_coef_loss   | 3.11     |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 43       |
|    time_elapsed    | 32267    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=3099.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.0293   |
|    ent_coef_loss   | 0.587    |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2114.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -77.8    |
|    critic_loss     | 13.3     |
|    ent_coef        | 0.0277   |
|    ent_coef_loss   | 1.77     |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.06e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 43       |
|    time_elapsed    | 32681    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=2034.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -88.3    |
|    critic_loss     | 8.67     |
|    ent_coef        | 0.0244   |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=3477.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -71      |
|    critic_loss     | 42.7     |
|    ent_coef        | 0.023    |
|    ent_coef_loss   | -1.76    |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 43       |
|    time_elapsed    | 33094    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=3475.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 20       |
|    ent_coef        | 0.0202   |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=3473.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -65.1    |
|    critic_loss     | 6.39     |
|    ent_coef        | 0.0207   |
|    ent_coef_loss   | -2.93    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    episodes        | 292      |
|    fps             | 43       |
|    time_elapsed    | 33509    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=3472.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -73.3    |
|    critic_loss     | 3.53     |
|    ent_coef        | 0.0159   |
|    ent_coef_loss   | 0.872    |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=3475.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -70.5    |
|    critic_loss     | 6.07     |
|    ent_coef        | 0.0183   |
|    ent_coef_loss   | 0.704    |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    episodes        | 296      |
|    fps             | 43       |
|    time_elapsed    | 33924    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=3475.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -62.3    |
|    critic_loss     | 6.73     |
|    ent_coef        | 0.0171   |
|    ent_coef_loss   | -0.458   |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=3474.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 7.46     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | -2.22    |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 43       |
|    time_elapsed    | 34339    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=3475.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 21.7     |
|    ent_coef        | 0.0155   |
|    ent_coef_loss   | 0.56     |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=3475.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -60.1    |
|    critic_loss     | 9.24     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 2.47     |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 43       |
|    time_elapsed    | 34754    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=3475.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -59.8    |
|    critic_loss     | 7.62     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 4.88     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=3475.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -56.6    |
|    critic_loss     | 2.66     |
|    ent_coef        | 0.0209   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 43       |
|    time_elapsed    | 35169    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=3474.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 7.69     |
|    ent_coef        | 0.0168   |
|    ent_coef_loss   | 4.96     |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=3475.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -58.4    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.0178   |
|    ent_coef_loss   | 0.286    |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 43       |
|    time_elapsed    | 35584    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=1980.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -56.7    |
|    critic_loss     | 2.28     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 0.404    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=3476.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 5.81     |
|    ent_coef        | 0.0168   |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 43       |
|    time_elapsed    | 36024    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=3473.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 6.52     |
|    ent_coef        | 0.0205   |
|    ent_coef_loss   | 2.6      |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=3470.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 4.94     |
|    ent_coef        | 0.02     |
|    ent_coef_loss   | 0.711    |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 43       |
|    time_elapsed    | 36520    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=3471.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -48.4    |
|    critic_loss     | 4.22     |
|    ent_coef        | 0.0217   |
|    ent_coef_loss   | -3.65    |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=3473.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 9.03     |
|    ent_coef        | 0.0215   |
|    ent_coef_loss   | 1.16     |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    episodes        | 324      |
|    fps             | 43       |
|    time_elapsed    | 36925    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=3474.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -57      |
|    critic_loss     | 3.27     |
|    ent_coef        | 0.0207   |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=3480.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -60.9    |
|    critic_loss     | 14.5     |
|    ent_coef        | 0.0195   |
|    ent_coef_loss   | -0.807   |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.11e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 43       |
|    time_elapsed    | 37330    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=3479.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -74.8    |
|    critic_loss     | 15.3     |
|    ent_coef        | 0.019    |
|    ent_coef_loss   | 4.85     |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2499.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -193     |
|    critic_loss     | 43.9     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | 0.91     |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 43       |
|    time_elapsed    | 37731    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2486.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -190     |
|    critic_loss     | 69.9     |
|    ent_coef        | 0.0914   |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2492.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -159     |
|    critic_loss     | 21.6     |
|    ent_coef        | 0.051    |
|    ent_coef_loss   | -1.13    |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.12e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 44       |
|    time_elapsed    | 38137    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2489.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -143     |
|    critic_loss     | 38.6     |
|    ent_coef        | 0.0579   |
|    ent_coef_loss   | 1.52     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=3478.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 30.2     |
|    ent_coef        | 0.0418   |
|    ent_coef_loss   | -0.0721  |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 44       |
|    time_elapsed    | 38547    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=1880.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 6.8      |
|    ent_coef        | 0.0317   |
|    ent_coef_loss   | -0.246   |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=3425.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.43e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 27.7     |
|    ent_coef        | 0.0238   |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 44       |
|    time_elapsed    | 39035    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=3466.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -97.1    |
|    critic_loss     | 11.9     |
|    ent_coef        | 0.0255   |
|    ent_coef_loss   | 0.346    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=3473.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -94.1    |
|    critic_loss     | 4.89     |
|    ent_coef        | 0.0241   |
|    ent_coef_loss   | 0.996    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 44       |
|    time_elapsed    | 39534    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=3477.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -83.6    |
|    critic_loss     | 9.38     |
|    ent_coef        | 0.0277   |
|    ent_coef_loss   | -0.623   |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=3476.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -92      |
|    critic_loss     | 8.92     |
|    ent_coef        | 0.0264   |
|    ent_coef_loss   | -0.996   |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 43       |
|    time_elapsed    | 40032    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=1986.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 65       |
|    ent_coef        | 0.0424   |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=3477.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -114     |
|    critic_loss     | 23       |
|    ent_coef        | 0.0444   |
|    ent_coef_loss   | -2.01    |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 43       |
|    time_elapsed    | 40526    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2010.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -98.8    |
|    critic_loss     | 15.9     |
|    ent_coef        | 0.0253   |
|    ent_coef_loss   | 3.56     |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2128.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -119     |
|    critic_loss     | 34.3     |
|    ent_coef        | 0.0245   |
|    ent_coef_loss   | -1.64    |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 43       |
|    time_elapsed    | 41021    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=3478.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -80.5    |
|    critic_loss     | 41.9     |
|    ent_coef        | 0.0299   |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2696.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -94.8    |
|    critic_loss     | 17.1     |
|    ent_coef        | 0.0297   |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 43       |
|    time_elapsed    | 41510    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=3473.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -75      |
|    critic_loss     | 6.41     |
|    ent_coef        | 0.0239   |
|    ent_coef_loss   | -2.26    |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=3477.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -59.7    |
|    critic_loss     | 38.6     |
|    ent_coef        | 0.0191   |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    episodes        | 368      |
|    fps             | 43       |
|    time_elapsed    | 41912    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=1902.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -73.7    |
|    critic_loss     | 8.17     |
|    ent_coef        | 0.017    |
|    ent_coef_loss   | -0.681   |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=2490.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -79.4    |
|    critic_loss     | 8.89     |
|    ent_coef        | 0.018    |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 43       |
|    time_elapsed    | 42314    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2224.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -76.9    |
|    critic_loss     | 5.27     |
|    ent_coef        | 0.0224   |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=3369.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.37e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -68.7    |
|    critic_loss     | 3.6      |
|    ent_coef        | 0.0233   |
|    ent_coef_loss   | -0.761   |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.06e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 44       |
|    time_elapsed    | 42717    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=3477.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 6.4      |
|    ent_coef        | 0.0232   |
|    ent_coef_loss   | 0.529    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=3481.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 4.31     |
|    ent_coef        | 0.0207   |
|    ent_coef_loss   | -0.603   |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 44       |
|    time_elapsed    | 43119    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=3478.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -60.2    |
|    critic_loss     | 5.66     |
|    ent_coef        | 0.0179   |
|    ent_coef_loss   | -0.1     |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=3477.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 3.62     |
|    ent_coef        | 0.0151   |
|    ent_coef_loss   | 0.398    |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.12e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 44       |
|    time_elapsed    | 43521    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2531.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -71.6    |
|    critic_loss     | 3.86     |
|    ent_coef        | 0.016    |
|    ent_coef_loss   | 0.0472   |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=3477.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -67      |
|    critic_loss     | 4.79     |
|    ent_coef        | 0.0172   |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.14e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 44       |
|    time_elapsed    | 43923    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=3479.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -60.1    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.0146   |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=3474.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 3.28     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 44       |
|    time_elapsed    | 44326    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=3476.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -66.3    |
|    critic_loss     | 2.64     |
|    ent_coef        | 0.0136   |
|    ent_coef_loss   | 0.542    |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=3478.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 3.09     |
|    ent_coef        | 0.00999  |
|    ent_coef_loss   | 4.27     |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 44       |
|    time_elapsed    | 44729    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=3481.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -64.7    |
|    critic_loss     | 2.9      |
|    ent_coef        | 0.00822  |
|    ent_coef_loss   | 0.722    |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
New best mean reward!
Eval num_timesteps=2000000, episode_reward=3485.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.49e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -60.8    |
|    critic_loss     | 12.5     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | -4.68    |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 44       |
|    time_elapsed    | 45131    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=3480.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 3.41     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=3476.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -63.2    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | 4.07     |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 44       |
|    time_elapsed    | 45536    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=3475.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=3479.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 9.61     |
|    ent_coef        | 0.0132   |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 44       |
|    time_elapsed    | 45942    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2017.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -68.7    |
|    critic_loss     | 5.84     |
|    ent_coef        | 0.0161   |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2586.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -66.3    |
|    critic_loss     | 3.33     |
|    ent_coef        | 0.0194   |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.11e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 44       |
|    time_elapsed    | 46342    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=3477.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -67.1    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 2.15     |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=3477.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -68.9    |
|    critic_loss     | 2.42     |
|    ent_coef        | 0.0158   |
|    ent_coef_loss   | 0.673    |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 44       |
|    time_elapsed    | 46751    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=3477.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -67.2    |
|    critic_loss     | 4.19     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | -0.383   |
|    learning_rate   | 0.000791 |
|    n_updates       | 3434324  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=2416.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.0118   |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 44       |
|    time_elapsed    | 47156    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2089.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -68      |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=3471.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 5.1      |
|    ent_coef        | 0.0137   |
|    ent_coef_loss   | 3        |
|    learning_rate   | 0.000788 |
|    n_updates       | 3464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 44       |
|    time_elapsed    | 47559    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=3474.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -60.9    |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.0138   |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=3475.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 1.71     |
|    ent_coef        | 0.0142   |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 44       |
|    time_elapsed    | 47963    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=3475.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -59.5    |
|    critic_loss     | 7.08     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | -2.83    |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=3474.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -61.7    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | -0.0852  |
|    learning_rate   | 0.000784 |
|    n_updates       | 3504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.1e+03  |
| time/              |          |
|    episodes        | 432      |
|    fps             | 44       |
|    time_elapsed    | 48367    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=3477.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.000783 |
|    n_updates       | 3514324  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=3473.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | -0.844   |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.14e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 44       |
|    time_elapsed    | 48772    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=3480.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -64.2    |
|    critic_loss     | 4.04     |
|    ent_coef        | 0.0122   |
|    ent_coef_loss   | 8.9      |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=3476.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -61.8    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | 0.053    |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 44       |
|    time_elapsed    | 49179    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=3481.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -63      |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | -2.97    |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=3476.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -64      |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 3.77     |
|    learning_rate   | 0.000778 |
|    n_updates       | 3564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 44       |
|    time_elapsed    | 49587    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=3478.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 1.69     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.000777 |
|    n_updates       | 3574324  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=3477.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -63      |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | -2.52    |
|    learning_rate   | 0.000776 |
|    n_updates       | 3584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 44       |
|    time_elapsed    | 49991    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=3477.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -67.2    |
|    critic_loss     | 1.96     |
|    ent_coef        | 0.00929  |
|    ent_coef_loss   | 0.716    |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=3478.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 0.454    |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -0.913   |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 44       |
|    time_elapsed    | 50395    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=3464.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 0.778    |
|    ent_coef        | 0.00777  |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=3479.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -68.7    |
|    critic_loss     | 2.62     |
|    ent_coef        | 0.00788  |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 44       |
|    time_elapsed    | 50799    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=3478.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -69.7    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.00679  |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=3477.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.531    |
|    ent_coef        | 0.00733  |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 44       |
|    time_elapsed    | 51204    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=3476.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.00768  |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=3474.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 0.342    |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | -0.176   |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.29e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 44       |
|    time_elapsed    | 51608    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=3474.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -62.8    |
|    critic_loss     | 1.35     |
|    ent_coef        | 0.00585  |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=3476.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -63.6    |
|    critic_loss     | 2.57     |
|    ent_coef        | 0.0046   |
|    ent_coef_loss   | 0.144    |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.3e+03  |
| time/              |          |
|    episodes        | 468      |
|    fps             | 44       |
|    time_elapsed    | 52013    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=3479.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 3.67     |
|    ent_coef        | 0.00448  |
|    ent_coef_loss   | 5.17     |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=3478.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -66.2    |
|    critic_loss     | 0.205    |
|    ent_coef        | 0.00461  |
|    ent_coef_loss   | -6.52    |
|    learning_rate   | 0.000764 |
|    n_updates       | 3704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.33e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 45       |
|    time_elapsed    | 52416    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=3098.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -66.6    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | -4.4     |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=3306.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -67      |
|    critic_loss     | 0.492    |
|    ent_coef        | 0.00574  |
|    ent_coef_loss   | 4.37     |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.35e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 45       |
|    time_elapsed    | 52821    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=3477.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -66      |
|    critic_loss     | 0.319    |
|    ent_coef        | 0.00556  |
|    ent_coef_loss   | 0.624    |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=3476.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 0.305    |
|    ent_coef        | 0.00562  |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.35e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 45       |
|    time_elapsed    | 53224    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=3477.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -70.3    |
|    critic_loss     | 0.635    |
|    ent_coef        | 0.005    |
|    ent_coef_loss   | -0.523   |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=3477.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -67.9    |
|    critic_loss     | 0.755    |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.35e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 45       |
|    time_elapsed    | 53631    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=3477.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -67.2    |
|    critic_loss     | 0.394    |
|    ent_coef        | 0.00346  |
|    ent_coef_loss   | -0.758   |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=3477.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -67.5    |
|    critic_loss     | 0.487    |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | 1.8      |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.36e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 45       |
|    time_elapsed    | 54035    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=3018.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -67.5    |
|    critic_loss     | 0.794    |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | -0.762   |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=1924.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -72      |
|    critic_loss     | 0.534    |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.37e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 45       |
|    time_elapsed    | 54440    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=1744.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -95.3    |
|    critic_loss     | 7.49     |
|    ent_coef        | 0.0204   |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=3477.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -98.3    |
|    critic_loss     | 10.5     |
|    ent_coef        | 0.0112   |
|    ent_coef_loss   | -0.259   |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.32e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 45       |
|    time_elapsed    | 54845    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=3477.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -85.6    |
|    critic_loss     | 0.806    |
|    ent_coef        | 0.00476  |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000751 |
|    n_updates       | 3834324  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=1912.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -79.9    |
|    critic_loss     | 2.55     |
|    ent_coef        | 0.00853  |
|    ent_coef_loss   | 4.42     |
|    learning_rate   | 0.00075  |
|    n_updates       | 3844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.3e+03  |
| time/              |          |
|    episodes        | 500      |
|    fps             | 45       |
|    time_elapsed    | 55250    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=2481.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -83.1    |
|    critic_loss     | 1.3      |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | -0.00859 |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=3471.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -84.6    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.0094   |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 45       |
|    time_elapsed    | 55655    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=3466.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -74.8    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=3456.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -71.4    |
|    critic_loss     | 0.577    |
|    ent_coef        | 0.00657  |
|    ent_coef_loss   | 0.403    |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 45       |
|    time_elapsed    | 56060    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=3322.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -68.2    |
|    critic_loss     | 0.601    |
|    ent_coef        | 0.00781  |
|    ent_coef_loss   | -0.972   |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=3470.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -67      |
|    critic_loss     | 0.454    |
|    ent_coef        | 0.0075   |
|    ent_coef_loss   | 1.55     |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.27e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 45       |
|    time_elapsed    | 56465    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=2497.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 0.677    |
|    ent_coef        | 0.0065   |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2488.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 0.479    |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 45       |
|    time_elapsed    | 56871    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=3297.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | -0.417   |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=3330.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -63.2    |
|    critic_loss     | 0.336    |
|    ent_coef        | 0.00601  |
|    ent_coef_loss   | -0.274   |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.22e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 45       |
|    time_elapsed    | 57276    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=3476.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -63      |
|    critic_loss     | 0.774    |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | -0.63    |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=2442.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -65.4    |
|    critic_loss     | 0.241    |
|    ent_coef        | 0.00534  |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 45       |
|    time_elapsed    | 57682    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=1959.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -62.6    |
|    critic_loss     | 0.244    |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | 3.01     |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=2168.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 0.647    |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | 0.723    |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 45       |
|    time_elapsed    | 58091    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=3478.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -63.5    |
|    critic_loss     | 0.784    |
|    ent_coef        | 0.01     |
|    ent_coef_loss   | -0.679   |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=3476.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -61.6    |
|    critic_loss     | 0.659    |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | -0.298   |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 45       |
|    time_elapsed    | 58497    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=3476.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -61.1    |
|    critic_loss     | 1.22     |
|    ent_coef        | 0.00732  |
|    ent_coef_loss   | 3.72     |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=3476.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 2.22     |
|    ent_coef        | 0.00655  |
|    ent_coef_loss   | -1.84    |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 45       |
|    time_elapsed    | 58902    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=3476.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -63.4    |
|    critic_loss     | 0.5      |
|    ent_coef        | 0.00676  |
|    ent_coef_loss   | -0.502   |
|    learning_rate   | 0.000731 |
|    n_updates       | 4034324  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=3477.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -63.2    |
|    critic_loss     | 0.609    |
|    ent_coef        | 0.00608  |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 540      |
|    fps             | 45       |
|    time_elapsed    | 59352    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=3477.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 0.367    |
|    ent_coef        | 0.00474  |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.000729 |
|    n_updates       | 4054324  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=3478.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -64.3    |
|    critic_loss     | 0.253    |
|    ent_coef        | 0.00424  |
|    ent_coef_loss   | 0.512    |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 45       |
|    time_elapsed    | 59876    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=2127.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 0.318    |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.000727 |
|    n_updates       | 4074324  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=3477.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 0.275    |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | 0.882    |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 45       |
|    time_elapsed    | 60398    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=3478.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | -5.53    |
|    learning_rate   | 0.000725 |
|    n_updates       | 4094324  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=3477.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 0.551    |
|    ent_coef        | 0.00365  |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 552      |
|    fps             | 45       |
|    time_elapsed    | 60897    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=3477.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00267  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2561.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.77     |
|    ent_coef        | 0.00378  |
|    ent_coef_loss   | -2.83    |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 556      |
|    fps             | 45       |
|    time_elapsed    | 61394    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=3477.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 0.489    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | 3.55     |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=3477.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 0.0906   |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 45       |
|    time_elapsed    | 61888    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=3477.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 0.563    |
|    ent_coef        | 0.00472  |
|    ent_coef_loss   | -0.516   |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=3478.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 0.338    |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | -0.199   |
|    learning_rate   | 0.000718 |
|    n_updates       | 4164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 45       |
|    time_elapsed    | 62384    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=3477.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -65.5    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=2345.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 0.224    |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | -2.8     |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 568      |
|    fps             | 45       |
|    time_elapsed    | 62825    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=3478.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -67.4    |
|    critic_loss     | 0.697    |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | -0.14    |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=3473.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -66.8    |
|    critic_loss     | 0.566    |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | 0.349    |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 572      |
|    fps             | 45       |
|    time_elapsed    | 63226    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=3477.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 0.253    |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=3477.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -66.9    |
|    critic_loss     | 0.119    |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 576      |
|    fps             | 45       |
|    time_elapsed    | 63628    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=3477.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -65.9    |
|    critic_loss     | 0.108    |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=3477.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.314    |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | -0.803   |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 580      |
|    fps             | 45       |
|    time_elapsed    | 64031    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=3477.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=3477.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -65.3    |
|    critic_loss     | 0.295    |
|    ent_coef        | 0.00471  |
|    ent_coef_loss   | 0.282    |
|    learning_rate   | 0.000708 |
|    n_updates       | 4264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 584      |
|    fps             | 45       |
|    time_elapsed    | 64434    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=3477.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -65      |
|    critic_loss     | 0.272    |
|    ent_coef        | 0.00433  |
|    ent_coef_loss   | -1.76    |
|    learning_rate   | 0.000707 |
|    n_updates       | 4274324  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=3477.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 0.445    |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.000706 |
|    n_updates       | 4284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 588      |
|    fps             | 45       |
|    time_elapsed    | 64837    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=3477.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | -0.617   |
|    learning_rate   | 0.000705 |
|    n_updates       | 4294324  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=3477.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -64.6    |
|    critic_loss     | 0.164    |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | 0.936    |
|    learning_rate   | 0.000704 |
|    n_updates       | 4304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 592      |
|    fps             | 45       |
|    time_elapsed    | 65260    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=3478.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 0.149    |
|    ent_coef        | 0.00498  |
|    ent_coef_loss   | -2.67    |
|    learning_rate   | 0.000703 |
|    n_updates       | 4314324  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=3477.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -63.3    |
|    critic_loss     | 0.328    |
|    ent_coef        | 0.00646  |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.000702 |
|    n_updates       | 4324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 596      |
|    fps             | 45       |
|    time_elapsed    | 65786    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=3477.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -62.7    |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.0062   |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.000701 |
|    n_updates       | 4334324  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=3476.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | 2.78     |
|    learning_rate   | 0.0007   |
|    n_updates       | 4344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 600      |
|    fps             | 45       |
|    time_elapsed    | 66287    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=2485.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -63.4    |
|    critic_loss     | 1.88     |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000699 |
|    n_updates       | 4354324  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=3477.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -62.8    |
|    critic_loss     | 4.22     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000698 |
|    n_updates       | 4364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.28e+03 |
| time/              |          |
|    episodes        | 604      |
|    fps             | 45       |
|    time_elapsed    | 66782    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=3240.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -104     |
|    critic_loss     | 29.5     |
|    ent_coef        | 0.0789   |
|    ent_coef_loss   | -2.4     |
|    learning_rate   | 0.000697 |
|    n_updates       | 4374324  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=2062.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -96.2    |
|    critic_loss     | 8.98     |
|    ent_coef        | 0.0498   |
|    ent_coef_loss   | -4.58    |
|    learning_rate   | 0.000696 |
|    n_updates       | 4384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 608      |
|    fps             | 45       |
|    time_elapsed    | 67276    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=3477.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -77.1    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.034    |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000695 |
|    n_updates       | 4394324  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=3452.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -67.4    |
|    critic_loss     | 1.35     |
|    ent_coef        | 0.0221   |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.000694 |
|    n_updates       | 4404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 612      |
|    fps             | 45       |
|    time_elapsed    | 67770    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=1881.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 1.1      |
|    ent_coef        | 0.0159   |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.000693 |
|    n_updates       | 4414324  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=1847.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -58.9    |
|    critic_loss     | 0.67     |
|    ent_coef        | 0.0128   |
|    ent_coef_loss   | 0.875    |
|    learning_rate   | 0.000692 |
|    n_updates       | 4424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 616      |
|    fps             | 45       |
|    time_elapsed    | 68266    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=3477.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -58.2    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | 0.726    |
|    learning_rate   | 0.000691 |
|    n_updates       | 4434324  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=3477.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -57.4    |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.00069  |
|    n_updates       | 4444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 620      |
|    fps             | 45       |
|    time_elapsed    | 68760    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=3479.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -58.7    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.00775  |
|    ent_coef_loss   | -0.0446  |
|    learning_rate   | 0.000689 |
|    n_updates       | 4454324  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=3477.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -61.1    |
|    critic_loss     | 0.143    |
|    ent_coef        | 0.0066   |
|    ent_coef_loss   | 0.462    |
|    learning_rate   | 0.000688 |
|    n_updates       | 4464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 624      |
|    fps             | 45       |
|    time_elapsed    | 69256    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=3475.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 0.779    |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | 3.66     |
|    learning_rate   | 0.000687 |
|    n_updates       | 4474324  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=3477.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -62.7    |
|    critic_loss     | 0.352    |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000686 |
|    n_updates       | 4484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 628      |
|    fps             | 45       |
|    time_elapsed    | 69749    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=2524.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 0.372    |
|    ent_coef        | 0.00402  |
|    ent_coef_loss   | -5.44    |
|    learning_rate   | 0.000685 |
|    n_updates       | 4494324  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=3462.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -63.6    |
|    critic_loss     | 0.255    |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | 0.784    |
|    learning_rate   | 0.000684 |
|    n_updates       | 4504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 632      |
|    fps             | 44       |
|    time_elapsed    | 70242    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=3470.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 0.94     |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | -4.06    |
|    learning_rate   | 0.000683 |
|    n_updates       | 4514324  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=3476.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 0.472    |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | 0.6      |
|    learning_rate   | 0.000682 |
|    n_updates       | 4524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 636      |
|    fps             | 44       |
|    time_elapsed    | 70736    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=3476.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -62.5    |
|    critic_loss     | 0.523    |
|    ent_coef        | 0.00755  |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000681 |
|    n_updates       | 4534324  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=3476.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -61.6    |
|    critic_loss     | 0.531    |
|    ent_coef        | 0.00795  |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.00068  |
|    n_updates       | 4544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 640      |
|    fps             | 44       |
|    time_elapsed    | 71230    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=3478.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 0.381    |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | -3.54    |
|    learning_rate   | 0.000679 |
|    n_updates       | 4554324  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=3474.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -59.5    |
|    critic_loss     | 0.628    |
|    ent_coef        | 0.00677  |
|    ent_coef_loss   | 0.434    |
|    learning_rate   | 0.000678 |
|    n_updates       | 4564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 644      |
|    fps             | 44       |
|    time_elapsed    | 71723    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=3470.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -60.5    |
|    critic_loss     | 0.663    |
|    ent_coef        | 0.00684  |
|    ent_coef_loss   | -2.72    |
|    learning_rate   | 0.000677 |
|    n_updates       | 4574324  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=3475.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -58.6    |
|    critic_loss     | 0.728    |
|    ent_coef        | 0.00729  |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000676 |
|    n_updates       | 4584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.22e+03 |
| time/              |          |
|    episodes        | 648      |
|    fps             | 44       |
|    time_elapsed    | 72218    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=3471.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -59.7    |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00736  |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000675 |
|    n_updates       | 4594324  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=3476.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -59.7    |
|    critic_loss     | 2.09     |
|    ent_coef        | 0.00729  |
|    ent_coef_loss   | 3.66     |
|    learning_rate   | 0.000674 |
|    n_updates       | 4604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.22e+03 |
| time/              |          |
|    episodes        | 652      |
|    fps             | 44       |
|    time_elapsed    | 72712    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=3476.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -60.4    |
|    critic_loss     | 0.8      |
|    ent_coef        | 0.00639  |
|    ent_coef_loss   | 1.78     |
|    learning_rate   | 0.000673 |
|    n_updates       | 4614324  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=3471.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 6.27     |
|    learning_rate   | 0.000672 |
|    n_updates       | 4624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 656      |
|    fps             | 44       |
|    time_elapsed    | 73206    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=3476.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 0.366    |
|    ent_coef        | 0.00438  |
|    ent_coef_loss   | -2.09    |
|    learning_rate   | 0.000671 |
|    n_updates       | 4634324  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=3477.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -65      |
|    critic_loss     | 0.733    |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.00067  |
|    n_updates       | 4644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.25e+03 |
| time/              |          |
|    episodes        | 660      |
|    fps             | 44       |
|    time_elapsed    | 73700    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=2450.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 0.49     |
|    ent_coef        | 0.00374  |
|    ent_coef_loss   | -1.37    |
|    learning_rate   | 0.000669 |
|    n_updates       | 4654324  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=2395.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -64.5    |
|    critic_loss     | 0.961    |
|    ent_coef        | 0.00454  |
|    ent_coef_loss   | 3.14     |
|    learning_rate   | 0.000668 |
|    n_updates       | 4664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 664      |
|    fps             | 44       |
|    time_elapsed    | 74194    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=3476.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 0.503    |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 0.478    |
|    learning_rate   | 0.000667 |
|    n_updates       | 4674324  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=3477.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 0.705    |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | -2.84    |
|    learning_rate   | 0.000666 |
|    n_updates       | 4684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 668      |
|    fps             | 44       |
|    time_elapsed    | 74705    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=3477.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -63.5    |
|    critic_loss     | 0.539    |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | 5.86     |
|    learning_rate   | 0.000665 |
|    n_updates       | 4694324  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=3478.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 0.442    |
|    ent_coef        | 0.00635  |
|    ent_coef_loss   | -3.76    |
|    learning_rate   | 0.000664 |
|    n_updates       | 4704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 672      |
|    fps             | 44       |
|    time_elapsed    | 75226    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=3477.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -64.7    |
|    critic_loss     | 0.752    |
|    ent_coef        | 0.0065   |
|    ent_coef_loss   | -0.907   |
|    learning_rate   | 0.000663 |
|    n_updates       | 4714324  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=3476.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -65.6    |
|    critic_loss     | 1.13     |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | 2.59     |
|    learning_rate   | 0.000662 |
|    n_updates       | 4724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 676      |
|    fps             | 44       |
|    time_elapsed    | 75741    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=3476.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 1.63     |
|    ent_coef        | 0.00641  |
|    ent_coef_loss   | -0.661   |
|    learning_rate   | 0.000661 |
|    n_updates       | 4734324  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=3477.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -66.2    |
|    critic_loss     | 2.82     |
|    ent_coef        | 0.00697  |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.00066  |
|    n_updates       | 4744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 680      |
|    fps             | 44       |
|    time_elapsed    | 76255    |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=3477.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -63.1    |
|    critic_loss     | 0.502    |
|    ent_coef        | 0.0081   |
|    ent_coef_loss   | 0.626    |
|    learning_rate   | 0.000659 |
|    n_updates       | 4754324  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=2448.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -4.9     |
|    learning_rate   | 0.000658 |
|    n_updates       | 4764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 684      |
|    fps             | 44       |
|    time_elapsed    | 76759    |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=3477.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -61.8    |
|    critic_loss     | 1.83     |
|    ent_coef        | 0.0133   |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.000657 |
|    n_updates       | 4774324  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=3477.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3440000  |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00955  |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.000656 |
|    n_updates       | 4784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 688      |
|    fps             | 44       |
|    time_elapsed    | 77250    |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=1926.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -60.7    |
|    critic_loss     | 0.746    |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000655 |
|    n_updates       | 4794324  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=1956.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 6.32     |
|    ent_coef        | 0.0132   |
|    ent_coef_loss   | 3.33     |
|    learning_rate   | 0.000654 |
|    n_updates       | 4804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 692      |
|    fps             | 44       |
|    time_elapsed    | 77740    |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=3477.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -73      |
|    critic_loss     | 11.4     |
|    ent_coef        | 0.0151   |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.000653 |
|    n_updates       | 4814324  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=3476.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -76.9    |
|    critic_loss     | 2.81     |
|    ent_coef        | 0.0143   |
|    ent_coef_loss   | -0.075   |
|    learning_rate   | 0.000652 |
|    n_updates       | 4824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 696      |
|    fps             | 44       |
|    time_elapsed    | 78233    |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=3472.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -62      |
|    critic_loss     | 0.959    |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.000651 |
|    n_updates       | 4834324  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3475.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3500000  |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | -0.821   |
|    learning_rate   | 0.00065  |
|    n_updates       | 4844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 700      |
|    fps             | 44       |
|    time_elapsed    | 78726    |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=3464.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3510000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 4.22     |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | -5.55    |
|    learning_rate   | 0.000649 |
|    n_updates       | 4854324  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=3472.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3520000  |
| train/             |          |
|    actor_loss      | -76      |
|    critic_loss     | 10.1     |
|    ent_coef        | 0.00977  |
|    ent_coef_loss   | 0.0192   |
|    learning_rate   | 0.000648 |
|    n_updates       | 4864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 704      |
|    fps             | 44       |
|    time_elapsed    | 79218    |
|    total_timesteps | 3520000  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=2357.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 3530000  |
| train/             |          |
|    actor_loss      | -88.6    |
|    critic_loss     | 18.7     |
|    ent_coef        | 0.00951  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000647 |
|    n_updates       | 4874324  |
---------------------------------
Eval num_timesteps=3540000, episode_reward=3476.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3540000  |
| train/             |          |
|    actor_loss      | -71      |
|    critic_loss     | 22.3     |
|    ent_coef        | 0.0141   |
|    ent_coef_loss   | -0.353   |
|    learning_rate   | 0.000646 |
|    n_updates       | 4884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 708      |
|    fps             | 44       |
|    time_elapsed    | 79712    |
|    total_timesteps | 3540000  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=3475.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | -73.7    |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.0122   |
|    ent_coef_loss   | -1.4     |
|    learning_rate   | 0.000645 |
|    n_updates       | 4894324  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=3477.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -69.2    |
|    critic_loss     | 3.28     |
|    ent_coef        | 0.00741  |
|    ent_coef_loss   | 3.35     |
|    learning_rate   | 0.000644 |
|    n_updates       | 4904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 712      |
|    fps             | 44       |
|    time_elapsed    | 80204    |
|    total_timesteps | 3560000  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=3475.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | -68.8    |
|    critic_loss     | 2.58     |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | 0.464    |
|    learning_rate   | 0.000643 |
|    n_updates       | 4914324  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=3476.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | -63.9    |
|    critic_loss     | 0.879    |
|    ent_coef        | 0.00545  |
|    ent_coef_loss   | 2.06     |
|    learning_rate   | 0.000642 |
|    n_updates       | 4924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 716      |
|    fps             | 44       |
|    time_elapsed    | 80697    |
|    total_timesteps | 3580000  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=2068.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 0.923    |
|    ent_coef        | 0.00656  |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000641 |
|    n_updates       | 4934324  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=3476.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3600000  |
| train/             |          |
|    actor_loss      | -73.6    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00759  |
|    ent_coef_loss   | 2.02     |
|    learning_rate   | 0.00064  |
|    n_updates       | 4944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 720      |
|    fps             | 44       |
|    time_elapsed    | 81189    |
|    total_timesteps | 3600000  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=3477.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3610000  |
| train/             |          |
|    actor_loss      | -64.8    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00862  |
|    ent_coef_loss   | 0.306    |
|    learning_rate   | 0.000639 |
|    n_updates       | 4954324  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=3477.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3620000  |
| train/             |          |
|    actor_loss      | -67.8    |
|    critic_loss     | 1.12     |
|    ent_coef        | 0.0075   |
|    ent_coef_loss   | 3.31     |
|    learning_rate   | 0.000638 |
|    n_updates       | 4964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 724      |
|    fps             | 44       |
|    time_elapsed    | 81681    |
|    total_timesteps | 3620000  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=3478.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 0.807    |
|    ent_coef        | 0.00868  |
|    ent_coef_loss   | -2.94    |
|    learning_rate   | 0.000637 |
|    n_updates       | 4974324  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=3478.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    actor_loss      | -71.3    |
|    critic_loss     | 1.67     |
|    ent_coef        | 0.00906  |
|    ent_coef_loss   | 0.0716   |
|    learning_rate   | 0.000636 |
|    n_updates       | 4984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 728      |
|    fps             | 44       |
|    time_elapsed    | 82174    |
|    total_timesteps | 3640000  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=3480.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | -64.1    |
|    critic_loss     | 0.926    |
|    ent_coef        | 0.00887  |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.000635 |
|    n_updates       | 4994324  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=3481.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | -64.3    |
|    critic_loss     | 2.15     |
|    ent_coef        | 0.00889  |
|    ent_coef_loss   | -0.128   |
|    learning_rate   | 0.000634 |
|    n_updates       | 5004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.27e+03 |
| time/              |          |
|    episodes        | 732      |
|    fps             | 44       |
|    time_elapsed    | 82670    |
|    total_timesteps | 3660000  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=3478.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3670000  |
| train/             |          |
|    actor_loss      | -70.3    |
|    critic_loss     | 0.708    |
|    ent_coef        | 0.00842  |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.000633 |
|    n_updates       | 5014324  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=3480.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3680000  |
| train/             |          |
|    actor_loss      | -70.5    |
|    critic_loss     | 3.71     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | -0.287   |
|    learning_rate   | 0.000632 |
|    n_updates       | 5024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.32e+03 |
| time/              |          |
|    episodes        | 736      |
|    fps             | 44       |
|    time_elapsed    | 83163    |
|    total_timesteps | 3680000  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=3479.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3690000  |
| train/             |          |
|    actor_loss      | -70      |
|    critic_loss     | 9.05     |
|    ent_coef        | 0.0158   |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.000631 |
|    n_updates       | 5034324  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=3476.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3700000  |
| train/             |          |
|    actor_loss      | -80.7    |
|    critic_loss     | 9.45     |
|    ent_coef        | 0.0136   |
|    ent_coef_loss   | -0.165   |
|    learning_rate   | 0.00063  |
|    n_updates       | 5044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.33e+03 |
| time/              |          |
|    episodes        | 740      |
|    fps             | 44       |
|    time_elapsed    | 83657    |
|    total_timesteps | 3700000  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=1878.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3710000  |
| train/             |          |
|    actor_loss      | -68.4    |
|    critic_loss     | 25       |
|    ent_coef        | 0.0138   |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000629 |
|    n_updates       | 5054324  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=3476.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3720000  |
| train/             |          |
|    actor_loss      | -75.3    |
|    critic_loss     | 8.42     |
|    ent_coef        | 0.0143   |
|    ent_coef_loss   | 1.79     |
|    learning_rate   | 0.000628 |
|    n_updates       | 5064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.32e+03 |
| time/              |          |
|    episodes        | 744      |
|    fps             | 44       |
|    time_elapsed    | 84151    |
|    total_timesteps | 3720000  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=3480.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | -102     |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.0175   |
|    ent_coef_loss   | -0.691   |
|    learning_rate   | 0.000627 |
|    n_updates       | 5074324  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=1945.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | -88      |
|    critic_loss     | 5.73     |
|    ent_coef        | 0.0173   |
|    ent_coef_loss   | -1.33    |
|    learning_rate   | 0.000626 |
|    n_updates       | 5084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.32e+03 |
| time/              |          |
|    episodes        | 748      |
|    fps             | 44       |
|    time_elapsed    | 84642    |
|    total_timesteps | 3740000  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=1749.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 46.7     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | 8.99     |
|    learning_rate   | 0.000625 |
|    n_updates       | 5094324  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=3462.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3760000  |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 98.1     |
|    ent_coef        | 0.097    |
|    ent_coef_loss   | 0.149    |
|    learning_rate   | 0.000624 |
|    n_updates       | 5104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 752      |
|    fps             | 44       |
|    time_elapsed    | 85133    |
|    total_timesteps | 3760000  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=3464.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    actor_loss      | -131     |
|    critic_loss     | 119      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | 0.289    |
|    learning_rate   | 0.000623 |
|    n_updates       | 5114324  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=3454.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 3780000  |
| train/             |          |
|    actor_loss      | -111     |
|    critic_loss     | 43.8     |
|    ent_coef        | 0.0534   |
|    ent_coef_loss   | -0.496   |
|    learning_rate   | 0.000622 |
|    n_updates       | 5124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 756      |
|    fps             | 44       |
|    time_elapsed    | 85624    |
|    total_timesteps | 3780000  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=3477.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3790000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 41.4     |
|    ent_coef        | 0.0323   |
|    ent_coef_loss   | 3.05     |
|    learning_rate   | 0.000621 |
|    n_updates       | 5134324  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=1921.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | -88      |
|    critic_loss     | 18.6     |
|    ent_coef        | 0.03     |
|    ent_coef_loss   | -0.437   |
|    learning_rate   | 0.00062  |
|    n_updates       | 5144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.24e+03 |
| time/              |          |
|    episodes        | 760      |
|    fps             | 44       |
|    time_elapsed    | 86114    |
|    total_timesteps | 3800000  |
---------------------------------
Eval num_timesteps=3810000, episode_reward=2398.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3810000  |
| train/             |          |
|    actor_loss      | -97.8    |
|    critic_loss     | 89.6     |
|    ent_coef        | 0.0299   |
|    ent_coef_loss   | -0.551   |
|    learning_rate   | 0.000619 |
|    n_updates       | 5154324  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=3465.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3820000  |
| train/             |          |
|    actor_loss      | -72.5    |
|    critic_loss     | 31.6     |
|    ent_coef        | 0.043    |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000618 |
|    n_updates       | 5164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 764      |
|    fps             | 44       |
|    time_elapsed    | 86606    |
|    total_timesteps | 3820000  |
---------------------------------
Eval num_timesteps=3830000, episode_reward=3465.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3830000  |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 46.9     |
|    ent_coef        | 0.0375   |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000617 |
|    n_updates       | 5174324  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=1966.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3840000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 53.7     |
|    ent_coef        | 0.0431   |
|    ent_coef_loss   | 0.611    |
|    learning_rate   | 0.000616 |
|    n_updates       | 5184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 768      |
|    fps             | 44       |
|    time_elapsed    | 87100    |
|    total_timesteps | 3840000  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=3467.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3850000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 12.3     |
|    ent_coef        | 0.0355   |
|    ent_coef_loss   | 0.879    |
|    learning_rate   | 0.000615 |
|    n_updates       | 5194324  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=3465.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3860000  |
| train/             |          |
|    actor_loss      | -65.2    |
|    critic_loss     | 22.6     |
|    ent_coef        | 0.0354   |
|    ent_coef_loss   | 0.947    |
|    learning_rate   | 0.000614 |
|    n_updates       | 5204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 772      |
|    fps             | 44       |
|    time_elapsed    | 87592    |
|    total_timesteps | 3860000  |
---------------------------------
Eval num_timesteps=3870000, episode_reward=3477.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3870000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 68.7     |
|    ent_coef        | 0.0302   |
|    ent_coef_loss   | -3.03    |
|    learning_rate   | 0.000613 |
|    n_updates       | 5214324  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=3329.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 3880000  |
| train/             |          |
|    actor_loss      | -58.4    |
|    critic_loss     | 39.1     |
|    ent_coef        | 0.0321   |
|    ent_coef_loss   | 4.6      |
|    learning_rate   | 0.000612 |
|    n_updates       | 5224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 776      |
|    fps             | 44       |
|    time_elapsed    | 88083    |
|    total_timesteps | 3880000  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=3477.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3890000  |
| train/             |          |
|    actor_loss      | -61.3    |
|    critic_loss     | 24.5     |
|    ent_coef        | 0.0384   |
|    ent_coef_loss   | 0.334    |
|    learning_rate   | 0.000611 |
|    n_updates       | 5234324  |
---------------------------------
Eval num_timesteps=3900000, episode_reward=3476.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3900000  |
| train/             |          |
|    actor_loss      | -60.3    |
|    critic_loss     | 40.8     |
|    ent_coef        | 0.0361   |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.00061  |
|    n_updates       | 5244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 780      |
|    fps             | 44       |
|    time_elapsed    | 88573    |
|    total_timesteps | 3900000  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=3475.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3910000  |
| train/             |          |
|    actor_loss      | -57.6    |
|    critic_loss     | 13.8     |
|    ent_coef        | 0.0272   |
|    ent_coef_loss   | 0.18     |
|    learning_rate   | 0.000609 |
|    n_updates       | 5254324  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=3475.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3920000  |
| train/             |          |
|    actor_loss      | -66.5    |
|    critic_loss     | 41.9     |
|    ent_coef        | 0.0259   |
|    ent_coef_loss   | 2.89     |
|    learning_rate   | 0.000608 |
|    n_updates       | 5264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 784      |
|    fps             | 44       |
|    time_elapsed    | 89066    |
|    total_timesteps | 3920000  |
---------------------------------
Eval num_timesteps=3930000, episode_reward=3472.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3930000  |
| train/             |          |
|    actor_loss      | -65.1    |
|    critic_loss     | 16.5     |
|    ent_coef        | 0.0257   |
|    ent_coef_loss   | -0.433   |
|    learning_rate   | 0.000607 |
|    n_updates       | 5274324  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=3472.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3940000  |
| train/             |          |
|    actor_loss      | -76.6    |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.028    |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000606 |
|    n_updates       | 5284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 788      |
|    fps             | 43       |
|    time_elapsed    | 89555    |
|    total_timesteps | 3940000  |
---------------------------------
Eval num_timesteps=3950000, episode_reward=3471.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3950000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 32.2     |
|    ent_coef        | 0.0269   |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.000605 |
|    n_updates       | 5294324  |
---------------------------------
Eval num_timesteps=3960000, episode_reward=3469.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3960000  |
| train/             |          |
|    actor_loss      | -64.9    |
|    critic_loss     | 23.3     |
|    ent_coef        | 0.0223   |
|    ent_coef_loss   | -4.76    |
|    learning_rate   | 0.000604 |
|    n_updates       | 5304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 792      |
|    fps             | 43       |
|    time_elapsed    | 90043    |
|    total_timesteps | 3960000  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=3478.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3970000  |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 18.8     |
|    ent_coef        | 0.0261   |
|    ent_coef_loss   | -4.27    |
|    learning_rate   | 0.000603 |
|    n_updates       | 5314324  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=3477.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 3980000  |
| train/             |          |
|    actor_loss      | -59.8    |
|    critic_loss     | 45.1     |
|    ent_coef        | 0.0342   |
|    ent_coef_loss   | -0.557   |
|    learning_rate   | 0.000602 |
|    n_updates       | 5324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 796      |
|    fps             | 43       |
|    time_elapsed    | 90534    |
|    total_timesteps | 3980000  |
---------------------------------
Eval num_timesteps=3990000, episode_reward=3465.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 3990000  |
| train/             |          |
|    actor_loss      | -57.9    |
|    critic_loss     | 71.8     |
|    ent_coef        | 0.034    |
|    ent_coef_loss   | -0.336   |
|    learning_rate   | 0.000601 |
|    n_updates       | 5334324  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=3465.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4000000  |
| train/             |          |
|    actor_loss      | -60.1    |
|    critic_loss     | 68.2     |
|    ent_coef        | 0.0353   |
|    ent_coef_loss   | -0.659   |
|    learning_rate   | 0.0006   |
|    n_updates       | 5344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 800      |
|    fps             | 43       |
|    time_elapsed    | 91024    |
|    total_timesteps | 4000000  |
---------------------------------
Eval num_timesteps=4010000, episode_reward=3465.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4010000  |
| train/             |          |
|    actor_loss      | -58.3    |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.0329   |
|    ent_coef_loss   | 0.472    |
|    learning_rate   | 0.000599 |
|    n_updates       | 5354324  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=3465.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4020000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 30.6     |
|    ent_coef        | 0.0337   |
|    ent_coef_loss   | -2.49    |
|    learning_rate   | 0.000598 |
|    n_updates       | 5364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 804      |
|    fps             | 43       |
|    time_elapsed    | 91513    |
|    total_timesteps | 4020000  |
---------------------------------
Eval num_timesteps=4030000, episode_reward=2160.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 4030000  |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 58.1     |
|    ent_coef        | 0.0289   |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.000597 |
|    n_updates       | 5374324  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=3465.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4040000  |
| train/             |          |
|    actor_loss      | -63.8    |
|    critic_loss     | 26.5     |
|    ent_coef        | 0.0323   |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000596 |
|    n_updates       | 5384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 808      |
|    fps             | 43       |
|    time_elapsed    | 92003    |
|    total_timesteps | 4040000  |
---------------------------------
Eval num_timesteps=4050000, episode_reward=3465.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4050000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.0339   |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000595 |
|    n_updates       | 5394324  |
---------------------------------
Eval num_timesteps=4060000, episode_reward=3466.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | -58.5    |
|    critic_loss     | 15.2     |
|    ent_coef        | 0.0253   |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000594 |
|    n_updates       | 5404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 812      |
|    fps             | 43       |
|    time_elapsed    | 92494    |
|    total_timesteps | 4060000  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=3465.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4070000  |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 15.2     |
|    ent_coef        | 0.0203   |
|    ent_coef_loss   | -1.91    |
|    learning_rate   | 0.000593 |
|    n_updates       | 5414324  |
---------------------------------
Eval num_timesteps=4080000, episode_reward=3465.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4080000  |
| train/             |          |
|    actor_loss      | -61.9    |
|    critic_loss     | 4.39     |
|    ent_coef        | 0.0227   |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.000592 |
|    n_updates       | 5424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.19e+03 |
| time/              |          |
|    episodes        | 816      |
|    fps             | 43       |
|    time_elapsed    | 92987    |
|    total_timesteps | 4080000  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=2434.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | -67.7    |
|    critic_loss     | 28.6     |
|    ent_coef        | 0.0252   |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000591 |
|    n_updates       | 5434324  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=2411.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4100000  |
| train/             |          |
|    actor_loss      | -70.8    |
|    critic_loss     | 163      |
|    ent_coef        | 0.02     |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.00059  |
|    n_updates       | 5444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.21e+03 |
| time/              |          |
|    episodes        | 820      |
|    fps             | 43       |
|    time_elapsed    | 93477    |
|    total_timesteps | 4100000  |
---------------------------------
Eval num_timesteps=4110000, episode_reward=2374.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 4110000  |
| train/             |          |
|    actor_loss      | -69.7    |
|    critic_loss     | 63       |
|    ent_coef        | 0.0267   |
|    ent_coef_loss   | 3.91     |
|    learning_rate   | 0.000589 |
|    n_updates       | 5454324  |
---------------------------------
Eval num_timesteps=4120000, episode_reward=2531.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 4120000  |
| train/             |          |
|    actor_loss      | -65.5    |
|    critic_loss     | 15.4     |
|    ent_coef        | 0.0225   |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.000588 |
|    n_updates       | 5464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 824      |
|    fps             | 43       |
|    time_elapsed    | 93968    |
|    total_timesteps | 4120000  |
---------------------------------
Eval num_timesteps=4130000, episode_reward=3464.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4130000  |
| train/             |          |
|    actor_loss      | -77.9    |
|    critic_loss     | 44.9     |
|    ent_coef        | 0.0241   |
|    ent_coef_loss   | 3.12     |
|    learning_rate   | 0.000587 |
|    n_updates       | 5474324  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=3466.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4140000  |
| train/             |          |
|    actor_loss      | -62.1    |
|    critic_loss     | 26.3     |
|    ent_coef        | 0.0314   |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000586 |
|    n_updates       | 5484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 828      |
|    fps             | 43       |
|    time_elapsed    | 94461    |
|    total_timesteps | 4140000  |
---------------------------------
Eval num_timesteps=4150000, episode_reward=3466.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4150000  |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 570      |
|    ent_coef        | 0.0284   |
|    ent_coef_loss   | 0.58     |
|    learning_rate   | 0.000585 |
|    n_updates       | 5494324  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=3467.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4160000  |
| train/             |          |
|    actor_loss      | -78.2    |
|    critic_loss     | 88.4     |
|    ent_coef        | 0.0236   |
|    ent_coef_loss   | -6.31    |
|    learning_rate   | 0.000584 |
|    n_updates       | 5504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 832      |
|    fps             | 43       |
|    time_elapsed    | 94963    |
|    total_timesteps | 4160000  |
---------------------------------
Eval num_timesteps=4170000, episode_reward=3467.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4170000  |
| train/             |          |
|    actor_loss      | -84.5    |
|    critic_loss     | 101      |
|    ent_coef        | 0.0188   |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.000583 |
|    n_updates       | 5514324  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=3466.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4180000  |
| train/             |          |
|    actor_loss      | -70.6    |
|    critic_loss     | 26.7     |
|    ent_coef        | 0.0277   |
|    ent_coef_loss   | 6.47     |
|    learning_rate   | 0.000582 |
|    n_updates       | 5524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 836      |
|    fps             | 43       |
|    time_elapsed    | 95453    |
|    total_timesteps | 4180000  |
---------------------------------
Eval num_timesteps=4190000, episode_reward=3465.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4190000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 42.9     |
|    ent_coef        | 0.033    |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000581 |
|    n_updates       | 5534324  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=3466.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4200000  |
| train/             |          |
|    actor_loss      | -66.1    |
|    critic_loss     | 45.4     |
|    ent_coef        | 0.0381   |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.00058  |
|    n_updates       | 5544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.11e+03 |
| time/              |          |
|    episodes        | 840      |
|    fps             | 43       |
|    time_elapsed    | 95945    |
|    total_timesteps | 4200000  |
---------------------------------
Eval num_timesteps=4210000, episode_reward=2489.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 4210000  |
| train/             |          |
|    actor_loss      | -83.9    |
|    critic_loss     | 79.2     |
|    ent_coef        | 0.0737   |
|    ent_coef_loss   | 0.0741   |
|    learning_rate   | 0.000579 |
|    n_updates       | 5554324  |
---------------------------------
Eval num_timesteps=4220000, episode_reward=3466.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4220000  |
| train/             |          |
|    actor_loss      | -40      |
|    critic_loss     | 86.5     |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -3.72    |
|    learning_rate   | 0.000578 |
|    n_updates       | 5564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 844      |
|    fps             | 43       |
|    time_elapsed    | 96437    |
|    total_timesteps | 4220000  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=3467.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4230000  |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 71.3     |
|    ent_coef        | 0.107    |
|    ent_coef_loss   | 4.42     |
|    learning_rate   | 0.000577 |
|    n_updates       | 5574324  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=3465.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4240000  |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 192      |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000576 |
|    n_updates       | 5584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 848      |
|    fps             | 43       |
|    time_elapsed    | 96927    |
|    total_timesteps | 4240000  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=3454.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 4250000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 233      |
|    ent_coef        | 0.0905   |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000575 |
|    n_updates       | 5594324  |
---------------------------------
Eval num_timesteps=4260000, episode_reward=3455.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4260000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 61.9     |
|    ent_coef        | 0.0776   |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000574 |
|    n_updates       | 5604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.14e+03 |
| time/              |          |
|    episodes        | 852      |
|    fps             | 43       |
|    time_elapsed    | 97416    |
|    total_timesteps | 4260000  |
---------------------------------
Eval num_timesteps=4270000, episode_reward=3455.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4270000  |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 52.3     |
|    ent_coef        | 0.0435   |
|    ent_coef_loss   | 2.69     |
|    learning_rate   | 0.000573 |
|    n_updates       | 5614324  |
---------------------------------
Eval num_timesteps=4280000, episode_reward=3455.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4280000  |
| train/             |          |
|    actor_loss      | -81.4    |
|    critic_loss     | 31.2     |
|    ent_coef        | 0.0605   |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000572 |
|    n_updates       | 5624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 856      |
|    fps             | 43       |
|    time_elapsed    | 97910    |
|    total_timesteps | 4280000  |
---------------------------------
Eval num_timesteps=4290000, episode_reward=3466.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4290000  |
| train/             |          |
|    actor_loss      | -68.3    |
|    critic_loss     | 64.8     |
|    ent_coef        | 0.0677   |
|    ent_coef_loss   | 6.73     |
|    learning_rate   | 0.000571 |
|    n_updates       | 5634324  |
---------------------------------
Eval num_timesteps=4300000, episode_reward=3465.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4300000  |
| train/             |          |
|    actor_loss      | -60.4    |
|    critic_loss     | 88.6     |
|    ent_coef        | 0.0749   |
|    ent_coef_loss   | 0.774    |
|    learning_rate   | 0.00057  |
|    n_updates       | 5644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.17e+03 |
| time/              |          |
|    episodes        | 860      |
|    fps             | 43       |
|    time_elapsed    | 98401    |
|    total_timesteps | 4300000  |
---------------------------------
Eval num_timesteps=4310000, episode_reward=3454.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 4310000  |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 22.9     |
|    ent_coef        | 0.0734   |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.000569 |
|    n_updates       | 5654324  |
---------------------------------
Eval num_timesteps=4320000, episode_reward=3477.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4320000  |
| train/             |          |
|    actor_loss      | -58.2    |
|    critic_loss     | 45.1     |
|    ent_coef        | 0.0677   |
|    ent_coef_loss   | 0.646    |
|    learning_rate   | 0.000568 |
|    n_updates       | 5664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 864      |
|    fps             | 43       |
|    time_elapsed    | 98892    |
|    total_timesteps | 4320000  |
---------------------------------
Eval num_timesteps=4330000, episode_reward=3466.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4330000  |
| train/             |          |
|    actor_loss      | -79.2    |
|    critic_loss     | 21.1     |
|    ent_coef        | 0.058    |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.000567 |
|    n_updates       | 5674324  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=3477.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4340000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 32.8     |
|    ent_coef        | 0.0514   |
|    ent_coef_loss   | -0.00315 |
|    learning_rate   | 0.000566 |
|    n_updates       | 5684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.22e+03 |
| time/              |          |
|    episodes        | 868      |
|    fps             | 43       |
|    time_elapsed    | 99380    |
|    total_timesteps | 4340000  |
---------------------------------
Eval num_timesteps=4350000, episode_reward=3466.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4350000  |
| train/             |          |
|    actor_loss      | -71.6    |
|    critic_loss     | 48.7     |
|    ent_coef        | 0.0484   |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.000565 |
|    n_updates       | 5694324  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=3465.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4360000  |
| train/             |          |
|    actor_loss      | -79.4    |
|    critic_loss     | 40.4     |
|    ent_coef        | 0.0624   |
|    ent_coef_loss   | 0.933    |
|    learning_rate   | 0.000564 |
|    n_updates       | 5704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.27e+03 |
| time/              |          |
|    episodes        | 872      |
|    fps             | 43       |
|    time_elapsed    | 99871    |
|    total_timesteps | 4360000  |
---------------------------------
Eval num_timesteps=4370000, episode_reward=3465.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4370000  |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 41.4     |
|    ent_coef        | 0.0514   |
|    ent_coef_loss   | 0.401    |
|    learning_rate   | 0.000563 |
|    n_updates       | 5714324  |
---------------------------------
Eval num_timesteps=4380000, episode_reward=3465.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4380000  |
| train/             |          |
|    actor_loss      | -159     |
|    critic_loss     | 111      |
|    ent_coef        | 0.0453   |
|    ent_coef_loss   | 6.59     |
|    learning_rate   | 0.000562 |
|    n_updates       | 5724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 876      |
|    fps             | 43       |
|    time_elapsed    | 100360   |
|    total_timesteps | 4380000  |
---------------------------------
Eval num_timesteps=4390000, episode_reward=3465.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4390000  |
| train/             |          |
|    actor_loss      | -83.9    |
|    critic_loss     | 20.8     |
|    ent_coef        | 0.053    |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.000561 |
|    n_updates       | 5734324  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=3465.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4400000  |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 149      |
|    ent_coef        | 0.0653   |
|    ent_coef_loss   | 3.55     |
|    learning_rate   | 0.00056  |
|    n_updates       | 5744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 880      |
|    fps             | 43       |
|    time_elapsed    | 100849   |
|    total_timesteps | 4400000  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=3465.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4410000  |
| train/             |          |
|    actor_loss      | -79.5    |
|    critic_loss     | 29.8     |
|    ent_coef        | 0.0645   |
|    ent_coef_loss   | -0.832   |
|    learning_rate   | 0.000559 |
|    n_updates       | 5754324  |
---------------------------------
Eval num_timesteps=4420000, episode_reward=3465.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4420000  |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 122      |
|    ent_coef        | 0.0672   |
|    ent_coef_loss   | 0.467    |
|    learning_rate   | 0.000558 |
|    n_updates       | 5764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 884      |
|    fps             | 43       |
|    time_elapsed    | 101338   |
|    total_timesteps | 4420000  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=3465.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4430000  |
| train/             |          |
|    actor_loss      | -43      |
|    critic_loss     | 25       |
|    ent_coef        | 0.0623   |
|    ent_coef_loss   | -0.968   |
|    learning_rate   | 0.000557 |
|    n_updates       | 5774324  |
---------------------------------
Eval num_timesteps=4440000, episode_reward=3465.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4440000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 13.1     |
|    ent_coef        | 0.0641   |
|    ent_coef_loss   | 0.76     |
|    learning_rate   | 0.000556 |
|    n_updates       | 5784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 888      |
|    fps             | 43       |
|    time_elapsed    | 101826   |
|    total_timesteps | 4440000  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=3435.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.44e+03 |
| time/              |          |
|    total_timesteps | 4450000  |
| train/             |          |
|    actor_loss      | -34.1    |
|    critic_loss     | 27.5     |
|    ent_coef        | 0.0547   |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.000555 |
|    n_updates       | 5794324  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=3399.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.4e+03  |
| time/              |          |
|    total_timesteps | 4460000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 28.4     |
|    ent_coef        | 0.0577   |
|    ent_coef_loss   | 0.408    |
|    learning_rate   | 0.000554 |
|    n_updates       | 5804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.27e+03 |
| time/              |          |
|    episodes        | 892      |
|    fps             | 43       |
|    time_elapsed    | 102316   |
|    total_timesteps | 4460000  |
---------------------------------
Eval num_timesteps=4470000, episode_reward=3454.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.45e+03 |
| time/              |          |
|    total_timesteps | 4470000  |
| train/             |          |
|    actor_loss      | -42.6    |
|    critic_loss     | 19.6     |
|    ent_coef        | 0.06     |
|    ent_coef_loss   | -0.847   |
|    learning_rate   | 0.000553 |
|    n_updates       | 5814324  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=3391.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.39e+03 |
| time/              |          |
|    total_timesteps | 4480000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 35.2     |
|    ent_coef        | 0.0567   |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.000552 |
|    n_updates       | 5824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.28e+03 |
| time/              |          |
|    episodes        | 896      |
|    fps             | 43       |
|    time_elapsed    | 102805   |
|    total_timesteps | 4480000  |
---------------------------------
Eval num_timesteps=4490000, episode_reward=2411.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4490000  |
| train/             |          |
|    actor_loss      | -40.2    |
|    critic_loss     | 18.1     |
|    ent_coef        | 0.0536   |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.000551 |
|    n_updates       | 5834324  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=3438.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.44e+03 |
| time/              |          |
|    total_timesteps | 4500000  |
| train/             |          |
|    actor_loss      | -61.3    |
|    critic_loss     | 79       |
|    ent_coef        | 0.0449   |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.00055  |
|    n_updates       | 5844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 900      |
|    fps             | 43       |
|    time_elapsed    | 103294   |
|    total_timesteps | 4500000  |
---------------------------------
Eval num_timesteps=4510000, episode_reward=3474.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4510000  |
| train/             |          |
|    actor_loss      | -55.8    |
|    critic_loss     | 71.3     |
|    ent_coef        | 0.0382   |
|    ent_coef_loss   | -0.984   |
|    learning_rate   | 0.000549 |
|    n_updates       | 5854324  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=2614.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 4520000  |
| train/             |          |
|    actor_loss      | -76.5    |
|    critic_loss     | 13.5     |
|    ent_coef        | 0.0331   |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000548 |
|    n_updates       | 5864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.26e+03 |
| time/              |          |
|    episodes        | 904      |
|    fps             | 43       |
|    time_elapsed    | 103784   |
|    total_timesteps | 4520000  |
---------------------------------
Eval num_timesteps=4530000, episode_reward=2485.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 4530000  |
| train/             |          |
|    actor_loss      | -65.5    |
|    critic_loss     | 86.6     |
|    ent_coef        | 0.0318   |
|    ent_coef_loss   | -6.5     |
|    learning_rate   | 0.000547 |
|    n_updates       | 5874324  |
---------------------------------
Eval num_timesteps=4540000, episode_reward=2578.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4540000  |
| train/             |          |
|    actor_loss      | -76      |
|    critic_loss     | 59.9     |
|    ent_coef        | 0.0278   |
|    ent_coef_loss   | -0.27    |
|    learning_rate   | 0.000546 |
|    n_updates       | 5884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.23e+03 |
| time/              |          |
|    episodes        | 908      |
|    fps             | 43       |
|    time_elapsed    | 104276   |
|    total_timesteps | 4540000  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=1980.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 4550000  |
| train/             |          |
|    actor_loss      | -72.4    |
|    critic_loss     | 63       |
|    ent_coef        | 0.0312   |
|    ent_coef_loss   | 10       |
|    learning_rate   | 0.000545 |
|    n_updates       | 5894324  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=1979.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 4560000  |
| train/             |          |
|    actor_loss      | -78.2    |
|    critic_loss     | 39.6     |
|    ent_coef        | 0.0321   |
|    ent_coef_loss   | 0.714    |
|    learning_rate   | 0.000544 |
|    n_updates       | 5904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 912      |
|    fps             | 43       |
|    time_elapsed    | 104768   |
|    total_timesteps | 4560000  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=2420.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 4570000  |
| train/             |          |
|    actor_loss      | -67.5    |
|    critic_loss     | 144      |
|    ent_coef        | 0.0401   |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000543 |
|    n_updates       | 5914324  |
---------------------------------
Eval num_timesteps=4580000, episode_reward=3477.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4580000  |
| train/             |          |
|    actor_loss      | -72      |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.0351   |
|    ent_coef_loss   | -0.615   |
|    learning_rate   | 0.000542 |
|    n_updates       | 5924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.14e+03 |
| time/              |          |
|    episodes        | 916      |
|    fps             | 43       |
|    time_elapsed    | 105260   |
|    total_timesteps | 4580000  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=3475.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4590000  |
| train/             |          |
|    actor_loss      | -80      |
|    critic_loss     | 31       |
|    ent_coef        | 0.0445   |
|    ent_coef_loss   | 2.62     |
|    learning_rate   | 0.000541 |
|    n_updates       | 5934324  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=3477.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4600000  |
| train/             |          |
|    actor_loss      | -87.2    |
|    critic_loss     | 24.1     |
|    ent_coef        | 0.0362   |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.00054  |
|    n_updates       | 5944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.14e+03 |
| time/              |          |
|    episodes        | 920      |
|    fps             | 43       |
|    time_elapsed    | 105750   |
|    total_timesteps | 4600000  |
---------------------------------
Eval num_timesteps=4610000, episode_reward=2140.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 4610000  |
| train/             |          |
|    actor_loss      | -109     |
|    critic_loss     | 69.7     |
|    ent_coef        | 0.0421   |
|    ent_coef_loss   | 5.61     |
|    learning_rate   | 0.000539 |
|    n_updates       | 5954324  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=2448.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 4620000  |
| train/             |          |
|    actor_loss      | -115     |
|    critic_loss     | 136      |
|    ent_coef        | 0.0494   |
|    ent_coef_loss   | 1.93     |
|    learning_rate   | 0.000538 |
|    n_updates       | 5964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.13e+03 |
| time/              |          |
|    episodes        | 924      |
|    fps             | 43       |
|    time_elapsed    | 106241   |
|    total_timesteps | 4620000  |
---------------------------------
Eval num_timesteps=4630000, episode_reward=2473.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4630000  |
| train/             |          |
|    actor_loss      | -146     |
|    critic_loss     | 1.14e+03 |
|    ent_coef        | 0.0484   |
|    ent_coef_loss   | 0.492    |
|    learning_rate   | 0.000537 |
|    n_updates       | 5974324  |
---------------------------------
Eval num_timesteps=4640000, episode_reward=2470.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4640000  |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 350      |
|    ent_coef        | 0.0929   |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.000536 |
|    n_updates       | 5984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.09e+03 |
| time/              |          |
|    episodes        | 928      |
|    fps             | 43       |
|    time_elapsed    | 106732   |
|    total_timesteps | 4640000  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2474.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4650000  |
| train/             |          |
|    actor_loss      | -190     |
|    critic_loss     | 79.2     |
|    ent_coef        | 0.0509   |
|    ent_coef_loss   | -2.45    |
|    learning_rate   | 0.000535 |
|    n_updates       | 5994324  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=2472.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4660000  |
| train/             |          |
|    actor_loss      | -242     |
|    critic_loss     | 76.1     |
|    ent_coef        | 0.0489   |
|    ent_coef_loss   | 3.34     |
|    learning_rate   | 0.000534 |
|    n_updates       | 6004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.05e+03 |
| time/              |          |
|    episodes        | 932      |
|    fps             | 43       |
|    time_elapsed    | 107224   |
|    total_timesteps | 4660000  |
---------------------------------
Eval num_timesteps=4670000, episode_reward=2469.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4670000  |
| train/             |          |
|    actor_loss      | -197     |
|    critic_loss     | 107      |
|    ent_coef        | 0.0643   |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.000533 |
|    n_updates       | 6014324  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=2468.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4680000  |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 165      |
|    ent_coef        | 0.0645   |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000532 |
|    n_updates       | 6024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.04e+03 |
| time/              |          |
|    episodes        | 936      |
|    fps             | 43       |
|    time_elapsed    | 107714   |
|    total_timesteps | 4680000  |
---------------------------------
Eval num_timesteps=4690000, episode_reward=2476.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 4690000  |
| train/             |          |
|    actor_loss      | -145     |
|    critic_loss     | 145      |
|    ent_coef        | 0.085    |
|    ent_coef_loss   | -2.51    |
|    learning_rate   | 0.000531 |
|    n_updates       | 6034324  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2470.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4700000  |
| train/             |          |
|    actor_loss      | -179     |
|    critic_loss     | 140      |
|    ent_coef        | 0.095    |
|    ent_coef_loss   | 0.499    |
|    learning_rate   | 0.00053  |
|    n_updates       | 6044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 940      |
|    fps             | 43       |
|    time_elapsed    | 108204   |
|    total_timesteps | 4700000  |
---------------------------------
Eval num_timesteps=4710000, episode_reward=2281.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4710000  |
| train/             |          |
|    actor_loss      | -160     |
|    critic_loss     | 52.1     |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -1.93    |
|    learning_rate   | 0.000529 |
|    n_updates       | 6054324  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=1949.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 4720000  |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 484      |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -0.865   |
|    learning_rate   | 0.000528 |
|    n_updates       | 6064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 944      |
|    fps             | 43       |
|    time_elapsed    | 108696   |
|    total_timesteps | 4720000  |
---------------------------------
Eval num_timesteps=4730000, episode_reward=2163.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 4730000  |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 110      |
|    ent_coef        | 0.125    |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000527 |
|    n_updates       | 6074324  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=3466.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4740000  |
| train/             |          |
|    actor_loss      | -94.1    |
|    critic_loss     | 110      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000526 |
|    n_updates       | 6084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 948      |
|    fps             | 43       |
|    time_elapsed    | 109185   |
|    total_timesteps | 4740000  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=3457.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4750000  |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 413      |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -0.498   |
|    learning_rate   | 0.000525 |
|    n_updates       | 6094324  |
---------------------------------
Eval num_timesteps=4760000, episode_reward=3472.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4760000  |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 104      |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | -0.867   |
|    learning_rate   | 0.000524 |
|    n_updates       | 6104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 952      |
|    fps             | 43       |
|    time_elapsed    | 109674   |
|    total_timesteps | 4760000  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=3418.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.42e+03 |
| time/              |          |
|    total_timesteps | 4770000  |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 223      |
|    ent_coef        | 0.166    |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.000523 |
|    n_updates       | 6114324  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=3433.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.43e+03 |
| time/              |          |
|    total_timesteps | 4780000  |
| train/             |          |
|    actor_loss      | -65.7    |
|    critic_loss     | 220      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.582   |
|    learning_rate   | 0.000522 |
|    n_updates       | 6124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 956      |
|    fps             | 43       |
|    time_elapsed    | 110164   |
|    total_timesteps | 4780000  |
---------------------------------
Eval num_timesteps=4790000, episode_reward=3310.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4790000  |
| train/             |          |
|    actor_loss      | -113     |
|    critic_loss     | 225      |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | 3.19     |
|    learning_rate   | 0.000521 |
|    n_updates       | 6134324  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=3045.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4800000  |
| train/             |          |
|    actor_loss      | -109     |
|    critic_loss     | 155      |
|    ent_coef        | 0.0893   |
|    ent_coef_loss   | 0.953    |
|    learning_rate   | 0.00052  |
|    n_updates       | 6144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 960      |
|    fps             | 43       |
|    time_elapsed    | 110656   |
|    total_timesteps | 4800000  |
---------------------------------
Eval num_timesteps=4810000, episode_reward=3459.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4810000  |
| train/             |          |
|    actor_loss      | -71.7    |
|    critic_loss     | 106      |
|    ent_coef        | 0.0684   |
|    ent_coef_loss   | -2.67    |
|    learning_rate   | 0.000519 |
|    n_updates       | 6154324  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=1886.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 4820000  |
| train/             |          |
|    actor_loss      | -129     |
|    critic_loss     | 96.7     |
|    ent_coef        | 0.0595   |
|    ent_coef_loss   | 1.16     |
|    learning_rate   | 0.000518 |
|    n_updates       | 6164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 964      |
|    fps             | 43       |
|    time_elapsed    | 111147   |
|    total_timesteps | 4820000  |
---------------------------------
Eval num_timesteps=4830000, episode_reward=1903.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 4830000  |
| train/             |          |
|    actor_loss      | -86.1    |
|    critic_loss     | 71.9     |
|    ent_coef        | 0.0644   |
|    ent_coef_loss   | -4.32    |
|    learning_rate   | 0.000517 |
|    n_updates       | 6174324  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=3459.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4840000  |
| train/             |          |
|    actor_loss      | -110     |
|    critic_loss     | 62.4     |
|    ent_coef        | 0.0644   |
|    ent_coef_loss   | 3.25     |
|    learning_rate   | 0.000516 |
|    n_updates       | 6184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 968      |
|    fps             | 43       |
|    time_elapsed    | 111640   |
|    total_timesteps | 4840000  |
---------------------------------
Eval num_timesteps=4850000, episode_reward=3462.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 4850000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 210      |
|    ent_coef        | 0.0717   |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000515 |
|    n_updates       | 6194324  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=3466.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4860000  |
| train/             |          |
|    actor_loss      | -121     |
|    critic_loss     | 46.8     |
|    ent_coef        | 0.0599   |
|    ent_coef_loss   | 0.988    |
|    learning_rate   | 0.000514 |
|    n_updates       | 6204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 972      |
|    fps             | 43       |
|    time_elapsed    | 112132   |
|    total_timesteps | 4860000  |
---------------------------------
Eval num_timesteps=4870000, episode_reward=3466.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4870000  |
| train/             |          |
|    actor_loss      | -99.1    |
|    critic_loss     | 47.6     |
|    ent_coef        | 0.0478   |
|    ent_coef_loss   | -0.193   |
|    learning_rate   | 0.000513 |
|    n_updates       | 6214324  |
---------------------------------
Eval num_timesteps=4880000, episode_reward=3334.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 4880000  |
| train/             |          |
|    actor_loss      | -91.9    |
|    critic_loss     | 41       |
|    ent_coef        | 0.0491   |
|    ent_coef_loss   | -2.48    |
|    learning_rate   | 0.000512 |
|    n_updates       | 6224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 976      |
|    fps             | 43       |
|    time_elapsed    | 112622   |
|    total_timesteps | 4880000  |
---------------------------------
Eval num_timesteps=4890000, episode_reward=2029.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 4890000  |
| train/             |          |
|    actor_loss      | -114     |
|    critic_loss     | 128      |
|    ent_coef        | 0.0561   |
|    ent_coef_loss   | 3.41     |
|    learning_rate   | 0.000511 |
|    n_updates       | 6234324  |
---------------------------------
Eval num_timesteps=4900000, episode_reward=3466.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4900000  |
| train/             |          |
|    actor_loss      | -97.3    |
|    critic_loss     | 33.5     |
|    ent_coef        | 0.0426   |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.00051  |
|    n_updates       | 6244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 980      |
|    fps             | 43       |
|    time_elapsed    | 113114   |
|    total_timesteps | 4900000  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=2063.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 4910000  |
| train/             |          |
|    actor_loss      | -95.9    |
|    critic_loss     | 108      |
|    ent_coef        | 0.0252   |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 0.000509 |
|    n_updates       | 6254324  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=2503.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 4920000  |
| train/             |          |
|    actor_loss      | -93      |
|    critic_loss     | 67.1     |
|    ent_coef        | 0.0299   |
|    ent_coef_loss   | -0.503   |
|    learning_rate   | 0.000508 |
|    n_updates       | 6264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 984      |
|    fps             | 43       |
|    time_elapsed    | 113606   |
|    total_timesteps | 4920000  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=2483.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 4930000  |
| train/             |          |
|    actor_loss      | -142     |
|    critic_loss     | 184      |
|    ent_coef        | 0.177    |
|    ent_coef_loss   | -0.919   |
|    learning_rate   | 0.000507 |
|    n_updates       | 6274324  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=3466.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    actor_loss      | -128     |
|    critic_loss     | 113      |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | 0.533    |
|    learning_rate   | 0.000506 |
|    n_updates       | 6284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 988      |
|    fps             | 43       |
|    time_elapsed    | 114101   |
|    total_timesteps | 4940000  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=3477.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4950000  |
| train/             |          |
|    actor_loss      | -107     |
|    critic_loss     | 41.5     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000505 |
|    n_updates       | 6294324  |
---------------------------------
Eval num_timesteps=4960000, episode_reward=2042.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 4960000  |
| train/             |          |
|    actor_loss      | -112     |
|    critic_loss     | 44.7     |
|    ent_coef        | 0.071    |
|    ent_coef_loss   | 0.639    |
|    learning_rate   | 0.000504 |
|    n_updates       | 6304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 992      |
|    fps             | 43       |
|    time_elapsed    | 114596   |
|    total_timesteps | 4960000  |
---------------------------------
Eval num_timesteps=4970000, episode_reward=3475.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 4970000  |
| train/             |          |
|    actor_loss      | -120     |
|    critic_loss     | 38.8     |
|    ent_coef        | 0.0557   |
|    ent_coef_loss   | -0.418   |
|    learning_rate   | 0.000503 |
|    n_updates       | 6314324  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=1994.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 4980000  |
| train/             |          |
|    actor_loss      | -86.2    |
|    critic_loss     | 60.2     |
|    ent_coef        | 0.0547   |
|    ent_coef_loss   | -0.701   |
|    learning_rate   | 0.000502 |
|    n_updates       | 6324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 996      |
|    fps             | 43       |
|    time_elapsed    | 115088   |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4990000, episode_reward=1964.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 4990000  |
| train/             |          |
|    actor_loss      | -86.7    |
|    critic_loss     | 32       |
|    ent_coef        | 0.0502   |
|    ent_coef_loss   | -0.953   |
|    learning_rate   | 0.000501 |
|    n_updates       | 6334324  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=3467.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 5000000  |
| train/             |          |
|    actor_loss      | -79.8    |
|    critic_loss     | 33.4     |
|    ent_coef        | 0.044    |
|    ent_coef_loss   | 0.0956   |
|    learning_rate   | 0.0005   |
|    n_updates       | 6344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 1000     |
|    fps             | 43       |
|    time_elapsed    | 115580   |
|    total_timesteps | 5000000  |
---------------------------------
Eval num_timesteps=5010000, episode_reward=3463.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.46e+03 |
| time/              |          |
|    total_timesteps | 5010000  |
| train/             |          |
|    actor_loss      | -88.5    |
|    critic_loss     | 34.1     |
|    ent_coef        | 0.0489   |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.000499 |
|    n_updates       | 6354324  |
---------------------------------
Eval num_timesteps=5020000, episode_reward=3474.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 5020000  |
| train/             |          |
|    actor_loss      | -87.7    |
|    critic_loss     | 26.7     |
|    ent_coef        | 0.0607   |
|    ent_coef_loss   | -1.6     |
|    learning_rate   | 0.000498 |
|    n_updates       | 6364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 1004     |
|    fps             | 43       |
|    time_elapsed    | 116072   |
|    total_timesteps | 5020000  |
---------------------------------
Eval num_timesteps=5030000, episode_reward=3476.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5030000  |
| train/             |          |
|    actor_loss      | -91.9    |
|    critic_loss     | 39.1     |
|    ent_coef        | 0.0631   |
|    ent_coef_loss   | 0.597    |
|    learning_rate   | 0.000497 |
|    n_updates       | 6374324  |
---------------------------------
Eval num_timesteps=5040000, episode_reward=2382.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 5040000  |
| train/             |          |
|    actor_loss      | -106     |
|    critic_loss     | 38       |
|    ent_coef        | 0.0512   |
|    ent_coef_loss   | 0.766    |
|    learning_rate   | 0.000496 |
|    n_updates       | 6384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 1008     |
|    fps             | 43       |
|    time_elapsed    | 116564   |
|    total_timesteps | 5040000  |
---------------------------------
Eval num_timesteps=5050000, episode_reward=1996.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 5050000  |
| train/             |          |
|    actor_loss      | -101     |
|    critic_loss     | 31.6     |
|    ent_coef        | 0.0581   |
|    ent_coef_loss   | 0.652    |
|    learning_rate   | 0.000495 |
|    n_updates       | 6394324  |
---------------------------------
Eval num_timesteps=5060000, episode_reward=3477.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5060000  |
| train/             |          |
|    actor_loss      | -73.1    |
|    critic_loss     | 24.6     |
|    ent_coef        | 0.0609   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000494 |
|    n_updates       | 6404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 1012     |
|    fps             | 43       |
|    time_elapsed    | 117055   |
|    total_timesteps | 5060000  |
---------------------------------
Eval num_timesteps=5070000, episode_reward=3477.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5070000  |
| train/             |          |
|    actor_loss      | -81.7    |
|    critic_loss     | 28.8     |
|    ent_coef        | 0.0536   |
|    ent_coef_loss   | 1.22     |
|    learning_rate   | 0.000493 |
|    n_updates       | 6414324  |
---------------------------------
Eval num_timesteps=5080000, episode_reward=3477.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5080000  |
| train/             |          |
|    actor_loss      | -80.1    |
|    critic_loss     | 41.5     |
|    ent_coef        | 0.0489   |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.000492 |
|    n_updates       | 6424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 1016     |
|    fps             | 43       |
|    time_elapsed    | 117547   |
|    total_timesteps | 5080000  |
---------------------------------
Eval num_timesteps=5090000, episode_reward=3475.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5090000  |
| train/             |          |
|    actor_loss      | -75.4    |
|    critic_loss     | 13.3     |
|    ent_coef        | 0.0476   |
|    ent_coef_loss   | -0.529   |
|    learning_rate   | 0.000491 |
|    n_updates       | 6434324  |
---------------------------------
Eval num_timesteps=5100000, episode_reward=3477.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5100000  |
| train/             |          |
|    actor_loss      | -75.2    |
|    critic_loss     | 17.2     |
|    ent_coef        | 0.0427   |
|    ent_coef_loss   | -2.59    |
|    learning_rate   | 0.00049  |
|    n_updates       | 6444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 1020     |
|    fps             | 43       |
|    time_elapsed    | 118042   |
|    total_timesteps | 5100000  |
---------------------------------
Eval num_timesteps=5110000, episode_reward=3473.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 5110000  |
| train/             |          |
|    actor_loss      | -82.6    |
|    critic_loss     | 7.6      |
|    ent_coef        | 0.0375   |
|    ent_coef_loss   | -0.294   |
|    learning_rate   | 0.000489 |
|    n_updates       | 6454324  |
---------------------------------
Eval num_timesteps=5120000, episode_reward=3476.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5120000  |
| train/             |          |
|    actor_loss      | -73      |
|    critic_loss     | 67.3     |
|    ent_coef        | 0.0316   |
|    ent_coef_loss   | -0.131   |
|    learning_rate   | 0.000488 |
|    n_updates       | 6464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    episodes        | 1024     |
|    fps             | 43       |
|    time_elapsed    | 118538   |
|    total_timesteps | 5120000  |
---------------------------------
Eval num_timesteps=5130000, episode_reward=3480.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5130000  |
| train/             |          |
|    actor_loss      | -82.8    |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.0309   |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.000487 |
|    n_updates       | 6474324  |
---------------------------------
Eval num_timesteps=5140000, episode_reward=3477.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5140000  |
| train/             |          |
|    actor_loss      | -84.8    |
|    critic_loss     | 22.5     |
|    ent_coef        | 0.0322   |
|    ent_coef_loss   | 3.62     |
|    learning_rate   | 0.000486 |
|    n_updates       | 6484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 1028     |
|    fps             | 43       |
|    time_elapsed    | 119033   |
|    total_timesteps | 5140000  |
---------------------------------
Eval num_timesteps=5150000, episode_reward=3480.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5150000  |
| train/             |          |
|    actor_loss      | -86.2    |
|    critic_loss     | 9.13     |
|    ent_coef        | 0.033    |
|    ent_coef_loss   | -0.761   |
|    learning_rate   | 0.000485 |
|    n_updates       | 6494324  |
---------------------------------
Eval num_timesteps=5160000, episode_reward=3478.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5160000  |
| train/             |          |
|    actor_loss      | -88      |
|    critic_loss     | 7.65     |
|    ent_coef        | 0.0312   |
|    ent_coef_loss   | 0.816    |
|    learning_rate   | 0.000484 |
|    n_updates       | 6504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 1032     |
|    fps             | 43       |
|    time_elapsed    | 119528   |
|    total_timesteps | 5160000  |
---------------------------------
Eval num_timesteps=5170000, episode_reward=3478.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5170000  |
| train/             |          |
|    actor_loss      | -92.3    |
|    critic_loss     | 4.3      |
|    ent_coef        | 0.028    |
|    ent_coef_loss   | -0.852   |
|    learning_rate   | 0.000483 |
|    n_updates       | 6514324  |
---------------------------------
Eval num_timesteps=5180000, episode_reward=3483.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5180000  |
| train/             |          |
|    actor_loss      | -85.1    |
|    critic_loss     | 6.68     |
|    ent_coef        | 0.0272   |
|    ent_coef_loss   | -0.389   |
|    learning_rate   | 0.000482 |
|    n_updates       | 6524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.03e+03 |
| time/              |          |
|    episodes        | 1036     |
|    fps             | 43       |
|    time_elapsed    | 120024   |
|    total_timesteps | 5180000  |
---------------------------------
Eval num_timesteps=5190000, episode_reward=3470.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 5190000  |
| train/             |          |
|    actor_loss      | -93.5    |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.0283   |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 0.000481 |
|    n_updates       | 6534324  |
---------------------------------
Eval num_timesteps=5200000, episode_reward=3480.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5200000  |
| train/             |          |
|    actor_loss      | -89.8    |
|    critic_loss     | 127      |
|    ent_coef        | 0.0274   |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.00048  |
|    n_updates       | 6544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.08e+03 |
| time/              |          |
|    episodes        | 1040     |
|    fps             | 43       |
|    time_elapsed    | 120519   |
|    total_timesteps | 5200000  |
---------------------------------
Eval num_timesteps=5210000, episode_reward=3479.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5210000  |
| train/             |          |
|    actor_loss      | -84.5    |
|    critic_loss     | 5.17     |
|    ent_coef        | 0.0278   |
|    ent_coef_loss   | -4.16    |
|    learning_rate   | 0.000479 |
|    n_updates       | 6554324  |
---------------------------------
Eval num_timesteps=5220000, episode_reward=3475.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5220000  |
| train/             |          |
|    actor_loss      | -85.9    |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.0293   |
|    ent_coef_loss   | -2.92    |
|    learning_rate   | 0.000478 |
|    n_updates       | 6564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.12e+03 |
| time/              |          |
|    episodes        | 1044     |
|    fps             | 43       |
|    time_elapsed    | 121014   |
|    total_timesteps | 5220000  |
---------------------------------
Eval num_timesteps=5230000, episode_reward=3481.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5230000  |
| train/             |          |
|    actor_loss      | -91.3    |
|    critic_loss     | 1.86     |
|    ent_coef        | 0.027    |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000477 |
|    n_updates       | 6574324  |
---------------------------------
Eval num_timesteps=5240000, episode_reward=3481.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5240000  |
| train/             |          |
|    actor_loss      | -83.6    |
|    critic_loss     | 3.11     |
|    ent_coef        | 0.0278   |
|    ent_coef_loss   | -0.37    |
|    learning_rate   | 0.000476 |
|    n_updates       | 6584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 1048     |
|    fps             | 43       |
|    time_elapsed    | 121507   |
|    total_timesteps | 5240000  |
---------------------------------
Eval num_timesteps=5250000, episode_reward=3477.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5250000  |
| train/             |          |
|    actor_loss      | -95.9    |
|    critic_loss     | 1.79     |
|    ent_coef        | 0.0268   |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.000475 |
|    n_updates       | 6594324  |
---------------------------------
Eval num_timesteps=5260000, episode_reward=3477.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5260000  |
| train/             |          |
|    actor_loss      | -76.8    |
|    critic_loss     | 3.72     |
|    ent_coef        | 0.0242   |
|    ent_coef_loss   | -5.42    |
|    learning_rate   | 0.000474 |
|    n_updates       | 6604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 1052     |
|    fps             | 43       |
|    time_elapsed    | 122000   |
|    total_timesteps | 5260000  |
---------------------------------
Eval num_timesteps=5270000, episode_reward=3479.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5270000  |
| train/             |          |
|    actor_loss      | -85.4    |
|    critic_loss     | 2.83     |
|    ent_coef        | 0.0238   |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000473 |
|    n_updates       | 6614324  |
---------------------------------
Eval num_timesteps=5280000, episode_reward=3478.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5280000  |
| train/             |          |
|    actor_loss      | -77.3    |
|    critic_loss     | 3.74     |
|    ent_coef        | 0.0214   |
|    ent_coef_loss   | -2.53    |
|    learning_rate   | 0.000472 |
|    n_updates       | 6624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.16e+03 |
| time/              |          |
|    episodes        | 1056     |
|    fps             | 43       |
|    time_elapsed    | 122497   |
|    total_timesteps | 5280000  |
---------------------------------
Eval num_timesteps=5290000, episode_reward=2503.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 5290000  |
| train/             |          |
|    actor_loss      | -182     |
|    critic_loss     | 54.1     |
|    ent_coef        | 0.112    |
|    ent_coef_loss   | 0.429    |
|    learning_rate   | 0.000471 |
|    n_updates       | 6634324  |
---------------------------------
Eval num_timesteps=5300000, episode_reward=3483.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5300000  |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 17       |
|    ent_coef        | 0.0733   |
|    ent_coef_loss   | -0.656   |
|    learning_rate   | 0.00047  |
|    n_updates       | 6644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.15e+03 |
| time/              |          |
|    episodes        | 1060     |
|    fps             | 43       |
|    time_elapsed    | 122991   |
|    total_timesteps | 5300000  |
---------------------------------
Eval num_timesteps=5310000, episode_reward=3474.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.47e+03 |
| time/              |          |
|    total_timesteps | 5310000  |
| train/             |          |
|    actor_loss      | -130     |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.0555   |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000469 |
|    n_updates       | 6654324  |
---------------------------------
Eval num_timesteps=5320000, episode_reward=3475.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5320000  |
| train/             |          |
|    actor_loss      | -108     |
|    critic_loss     | 6.99     |
|    ent_coef        | 0.0448   |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000468 |
|    n_updates       | 6664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.18e+03 |
| time/              |          |
|    episodes        | 1064     |
|    fps             | 43       |
|    time_elapsed    | 123481   |
|    total_timesteps | 5320000  |
---------------------------------
Eval num_timesteps=5330000, episode_reward=3475.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.48e+03 |
| time/              |          |
|    total_timesteps | 5330000  |
| train/             |          |
|    actor_loss      | -95.3    |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.0326   |
|    ent_coef_loss   | -0.647   |
|    learning_rate   | 0.000467 |
|    n_updates       | 6674324  |
---------------------------------
Eval num_timesteps=5340000, episode_reward=2411.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 5340000  |
| train/             |          |
|    actor_loss      | -86      |
|    critic_loss     | 4.89     |
|    ent_coef        | 0.0281   |
|    ent_coef_loss   | -0.643   |
|    learning_rate   | 0.000466 |
|    n_updates       | 6684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.2e+03  |
| time/              |          |
|    episodes        | 1068     |
|    fps             | 43       |
|    time_elapsed    | 123974   |
|    total_timesteps | 5340000  |
