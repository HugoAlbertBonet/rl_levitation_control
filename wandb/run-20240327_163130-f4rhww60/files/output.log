Using cuda device
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_38
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=45.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 45.5     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 6.4      |
|    critic_loss     | 0.0753   |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -2.21    |
|    learning_rate   | 0.000999 |
|    n_updates       | 9899     |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=944.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.0461   |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -4.48    |
|    learning_rate   | 0.000998 |
|    n_updates       | 19899    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 730      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 59       |
|    time_elapsed    | 337      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1169.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 0.0764   |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | -2.99    |
|    learning_rate   | 0.000997 |
|    n_updates       | 29899    |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=1202.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000996 |
|    n_updates       | 39899    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 784      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 59       |
|    time_elapsed    | 676      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1170.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.0852   |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000995 |
|    n_updates       | 49899    |
---------------------------------
Eval num_timesteps=60000, episode_reward=1190.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.161    |
|    ent_coef        | 0.00292  |
|    ent_coef_loss   | 8.39     |
|    learning_rate   | 0.000994 |
|    n_updates       | 59899    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 911      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 60       |
|    time_elapsed    | 999      |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1172.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 2.4      |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.000993 |
|    n_updates       | 69899    |
---------------------------------
Eval num_timesteps=80000, episode_reward=1041.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00545  |
|    ent_coef_loss   | -2.98    |
|    learning_rate   | 0.000992 |
|    n_updates       | 79899    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 925      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 60       |
|    time_elapsed    | 1323     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1020.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -41.2    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | 3.83     |
|    learning_rate   | 0.000991 |
|    n_updates       | 89899    |
---------------------------------
Eval num_timesteps=100000, episode_reward=1014.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -38.6    |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.00422  |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.00099  |
|    n_updates       | 99899    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 889      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 60       |
|    time_elapsed    | 1648     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1176.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -37.1    |
|    critic_loss     | 0.109    |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | -0.509   |
|    learning_rate   | 0.000989 |
|    n_updates       | 109899   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1178.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 0.138    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.000988 |
|    n_updates       | 119899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 917      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 60       |
|    time_elapsed    | 1970     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1201.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -0.434   |
|    learning_rate   | 0.000987 |
|    n_updates       | 129899   |
---------------------------------
Eval num_timesteps=140000, episode_reward=1246.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.0918   |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | 3.71     |
|    learning_rate   | 0.000986 |
|    n_updates       | 139899   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 955      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 61       |
|    time_elapsed    | 2293     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=1001.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 0.09     |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -3.07    |
|    learning_rate   | 0.000985 |
|    n_updates       | 149899   |
---------------------------------
Eval num_timesteps=160000, episode_reward=119.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 120      |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 0.218    |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.000984 |
|    n_updates       | 159899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 936      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 61       |
|    time_elapsed    | 2615     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1049.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.519    |
|    ent_coef        | 0.00512  |
|    ent_coef_loss   | -6.4     |
|    learning_rate   | 0.000983 |
|    n_updates       | 169899   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1169.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 0.497    |
|    ent_coef        | 0.00807  |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000982 |
|    n_updates       | 179899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 61       |
|    time_elapsed    | 2936     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1196.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 0.242    |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | -0.129   |
|    learning_rate   | 0.000981 |
|    n_updates       | 189899   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1180.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -42      |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 4.46     |
|    learning_rate   | 0.00098  |
|    n_updates       | 199899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 893      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 61       |
|    time_elapsed    | 3262     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=156.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -41.6    |
|    critic_loss     | 0.293    |
|    ent_coef        | 0.00464  |
|    ent_coef_loss   | -0.162   |
|    learning_rate   | 0.000979 |
|    n_updates       | 209899   |
---------------------------------
Eval num_timesteps=220000, episode_reward=1183.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 0.199    |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.000978 |
|    n_updates       | 219899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 896      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 61       |
|    time_elapsed    | 3582     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=1258.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 0.3      |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | 0.609    |
|    learning_rate   | 0.000977 |
|    n_updates       | 229899   |
---------------------------------
New best mean reward!
Eval num_timesteps=240000, episode_reward=1181.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 0.656    |
|    ent_coef        | 0.00438  |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.000976 |
|    n_updates       | 239899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 923      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 61       |
|    time_elapsed    | 3904     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=433.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.006    |
|    ent_coef_loss   | -4.02    |
|    learning_rate   | 0.000975 |
|    n_updates       | 249899   |
---------------------------------
Eval num_timesteps=260000, episode_reward=172.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -36.6    |
|    critic_loss     | 4.65     |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | 6.02     |
|    learning_rate   | 0.000974 |
|    n_updates       | 259899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 939      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 61       |
|    time_elapsed    | 4224     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=114.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 5.15     |
|    ent_coef        | 0.00895  |
|    ent_coef_loss   | 0.565    |
|    learning_rate   | 0.000973 |
|    n_updates       | 269899   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1251.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 2.08     |
|    ent_coef        | 0.00827  |
|    ent_coef_loss   | 4.06     |
|    learning_rate   | 0.000972 |
|    n_updates       | 279899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 920      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 61       |
|    time_elapsed    | 4543     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1192.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -39.3    |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.00834  |
|    ent_coef_loss   | 0.567    |
|    learning_rate   | 0.000971 |
|    n_updates       | 289899   |
---------------------------------
Eval num_timesteps=300000, episode_reward=486.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 2.74     |
|    ent_coef        | 0.0084   |
|    ent_coef_loss   | 0.48     |
|    learning_rate   | 0.00097  |
|    n_updates       | 299899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 940      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 61       |
|    time_elapsed    | 4864     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=1165.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -32.8    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00859  |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000969 |
|    n_updates       | 309899   |
---------------------------------
Eval num_timesteps=320000, episode_reward=145.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 145      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 16       |
|    ent_coef        | 0.00897  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000968 |
|    n_updates       | 319899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 932      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 61       |
|    time_elapsed    | 5186     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=1214.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 2.17     |
|    ent_coef        | 0.00902  |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.000967 |
|    n_updates       | 329899   |
---------------------------------
Eval num_timesteps=340000, episode_reward=350.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 9.74     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000966 |
|    n_updates       | 339899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 928      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 61       |
|    time_elapsed    | 5508     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=174.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 2.91     |
|    ent_coef        | 0.00858  |
|    ent_coef_loss   | 2.69     |
|    learning_rate   | 0.000965 |
|    n_updates       | 349899   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1206.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 2.96     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | -0.42    |
|    learning_rate   | 0.000964 |
|    n_updates       | 359899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 923      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 61       |
|    time_elapsed    | 5831     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=1035.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -33.5    |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -0.751   |
|    learning_rate   | 0.000963 |
|    n_updates       | 369899   |
---------------------------------
Eval num_timesteps=380000, episode_reward=1209.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -36.3    |
|    critic_loss     | 8.84     |
|    ent_coef        | 0.0119   |
|    ent_coef_loss   | -6.11    |
|    learning_rate   | 0.000962 |
|    n_updates       | 379899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 928      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 61       |
|    time_elapsed    | 6154     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=309.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 5.7      |
|    ent_coef        | 0.0152   |
|    ent_coef_loss   | 5.52     |
|    learning_rate   | 0.000961 |
|    n_updates       | 389899   |
---------------------------------
Eval num_timesteps=400000, episode_reward=129.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -59      |
|    critic_loss     | 4.04     |
|    ent_coef        | 0.0216   |
|    ent_coef_loss   | -0.717   |
|    learning_rate   | 0.00096  |
|    n_updates       | 399899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 898      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 61       |
|    time_elapsed    | 6484     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=745.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -60.9    |
|    critic_loss     | 4.24     |
|    ent_coef        | 0.0173   |
|    ent_coef_loss   | -0.0505  |
|    learning_rate   | 0.000959 |
|    n_updates       | 409899   |
---------------------------------
Eval num_timesteps=420000, episode_reward=1222.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -55.5    |
|    critic_loss     | 5.74     |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | 0.41     |
|    learning_rate   | 0.000958 |
|    n_updates       | 419899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 61       |
|    time_elapsed    | 6802     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=1213.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 5.54     |
|    ent_coef        | 0.014    |
|    ent_coef_loss   | -2.89    |
|    learning_rate   | 0.000957 |
|    n_updates       | 429899   |
---------------------------------
Eval num_timesteps=440000, episode_reward=1197.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -42.1    |
|    critic_loss     | 6.17     |
|    ent_coef        | 0.0159   |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.000956 |
|    n_updates       | 439899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 897      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 61       |
|    time_elapsed    | 7110     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=1220.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 3.16     |
|    ent_coef        | 0.0163   |
|    ent_coef_loss   | -10.5    |
|    learning_rate   | 0.000955 |
|    n_updates       | 449899   |
---------------------------------
Eval num_timesteps=460000, episode_reward=1290.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 31.9     |
|    ent_coef        | 0.0176   |
|    ent_coef_loss   | 5.09     |
|    learning_rate   | 0.000954 |
|    n_updates       | 459899   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 891      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 61       |
|    time_elapsed    | 7420     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=124.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -40.4    |
|    critic_loss     | 26.9     |
|    ent_coef        | 0.0218   |
|    ent_coef_loss   | 4.05     |
|    learning_rate   | 0.000953 |
|    n_updates       | 469899   |
---------------------------------
Eval num_timesteps=480000, episode_reward=251.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 8.14     |
|    ent_coef        | 0.0161   |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000952 |
|    n_updates       | 479899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 875      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 62       |
|    time_elapsed    | 7732     |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=1221.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -34.5    |
|    critic_loss     | 4.41     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | 3.16     |
|    learning_rate   | 0.000951 |
|    n_updates       | 489899   |
---------------------------------
Eval num_timesteps=500000, episode_reward=1230.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -36      |
|    critic_loss     | 3.78     |
|    ent_coef        | 0.0131   |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.00095  |
|    n_updates       | 499899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 889      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 62       |
|    time_elapsed    | 8043     |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=1239.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -42.9    |
|    critic_loss     | 4.53     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.000949 |
|    n_updates       | 509899   |
---------------------------------
Eval num_timesteps=520000, episode_reward=81.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 81.4     |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -42.3    |
|    critic_loss     | 4.48     |
|    ent_coef        | 0.0134   |
|    ent_coef_loss   | -5.17    |
|    learning_rate   | 0.000948 |
|    n_updates       | 519899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 898      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 62       |
|    time_elapsed    | 8356     |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=1761.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -62.2    |
|    critic_loss     | 13.2     |
|    ent_coef        | 0.0164   |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.000947 |
|    n_updates       | 529899   |
---------------------------------
New best mean reward!
Eval num_timesteps=540000, episode_reward=1976.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 4.93     |
|    ent_coef        | 0.0162   |
|    ent_coef_loss   | -4.73    |
|    learning_rate   | 0.000946 |
|    n_updates       | 539899   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 882      |
| time/              |          |
|    episodes        | 108      |
|    fps             | 62       |
|    time_elapsed    | 8668     |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=126.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 127      |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.000945 |
|    n_updates       | 549899   |
---------------------------------
Eval num_timesteps=560000, episode_reward=105.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -43.1    |
|    critic_loss     | 1.97     |
|    ent_coef        | 0.00913  |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 0.000944 |
|    n_updates       | 559899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 881      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 62       |
|    time_elapsed    | 8980     |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=936.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 0.974    |
|    ent_coef        | 0.00788  |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.000943 |
|    n_updates       | 569899   |
---------------------------------
Eval num_timesteps=580000, episode_reward=1195.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00779  |
|    ent_coef_loss   | -0.478   |
|    learning_rate   | 0.000942 |
|    n_updates       | 579899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 874      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 62       |
|    time_elapsed    | 9292     |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=1229.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.885    |
|    ent_coef        | 0.00772  |
|    ent_coef_loss   | -4.9     |
|    learning_rate   | 0.000941 |
|    n_updates       | 589899   |
---------------------------------
Eval num_timesteps=600000, episode_reward=230.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00787  |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 0.00094  |
|    n_updates       | 599899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 880      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 62       |
|    time_elapsed    | 9601     |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=1237.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.00902  |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000939 |
|    n_updates       | 609899   |
---------------------------------
Eval num_timesteps=620000, episode_reward=1217.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -35.8    |
|    critic_loss     | 3.24     |
|    ent_coef        | 0.0104   |
|    ent_coef_loss   | 8.99     |
|    learning_rate   | 0.000938 |
|    n_updates       | 619899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 864      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 62       |
|    time_elapsed    | 9909     |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=1213.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 0.969    |
|    learning_rate   | 0.000937 |
|    n_updates       | 629899   |
---------------------------------
Eval num_timesteps=640000, episode_reward=1027.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 5.99     |
|    ent_coef        | 0.0128   |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.000936 |
|    n_updates       | 639899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 857      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 62       |
|    time_elapsed    | 10217    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=105.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -42.3    |
|    critic_loss     | 27.5     |
|    ent_coef        | 0.0158   |
|    ent_coef_loss   | 3.12     |
|    learning_rate   | 0.000935 |
|    n_updates       | 649899   |
---------------------------------
Eval num_timesteps=660000, episode_reward=379.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 6.5      |
|    ent_coef        | 0.0209   |
|    ent_coef_loss   | -3.17    |
|    learning_rate   | 0.000934 |
|    n_updates       | 659899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 862      |
| time/              |          |
|    episodes        | 132      |
|    fps             | 62       |
|    time_elapsed    | 10527    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=1207.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -57      |
|    critic_loss     | 20.6     |
|    ent_coef        | 0.0239   |
|    ent_coef_loss   | -2.52    |
|    learning_rate   | 0.000933 |
|    n_updates       | 669899   |
---------------------------------
Eval num_timesteps=680000, episode_reward=1314.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -58.1    |
|    critic_loss     | 8.09     |
|    ent_coef        | 0.0256   |
|    ent_coef_loss   | -3.64    |
|    learning_rate   | 0.000932 |
|    n_updates       | 679899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 872      |
| time/              |          |
|    episodes        | 136      |
|    fps             | 62       |
|    time_elapsed    | 10880    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=1040.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 11.1     |
|    ent_coef        | 0.0249   |
|    ent_coef_loss   | -4.85    |
|    learning_rate   | 0.000931 |
|    n_updates       | 689899   |
---------------------------------
Eval num_timesteps=700000, episode_reward=1182.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -60.3    |
|    critic_loss     | 16.4     |
|    ent_coef        | 0.0263   |
|    ent_coef_loss   | 0.327    |
|    learning_rate   | 0.00093  |
|    n_updates       | 699899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 879      |
| time/              |          |
|    episodes        | 140      |
|    fps             | 62       |
|    time_elapsed    | 11247    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=1154.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -66      |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.024    |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000929 |
|    n_updates       | 709899   |
---------------------------------
Eval num_timesteps=720000, episode_reward=1048.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -82.8    |
|    critic_loss     | 33.2     |
|    ent_coef        | 0.0258   |
|    ent_coef_loss   | 8.75     |
|    learning_rate   | 0.000928 |
|    n_updates       | 719899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 891      |
| time/              |          |
|    episodes        | 144      |
|    fps             | 62       |
|    time_elapsed    | 11599    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=83.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 83.1     |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -78      |
|    critic_loss     | 15.2     |
|    ent_coef        | 0.0294   |
|    ent_coef_loss   | 2.08     |
|    learning_rate   | 0.000927 |
|    n_updates       | 729899   |
---------------------------------
Eval num_timesteps=740000, episode_reward=57.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57.8     |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -119     |
|    critic_loss     | 258      |
|    ent_coef        | 0.043    |
|    ent_coef_loss   | 0.353    |
|    learning_rate   | 0.000926 |
|    n_updates       | 739899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 864      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 61       |
|    time_elapsed    | 11948    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=69.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 69.6     |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -150     |
|    critic_loss     | 83.6     |
|    ent_coef        | 0.0409   |
|    ent_coef_loss   | 2.96     |
|    learning_rate   | 0.000925 |
|    n_updates       | 749899   |
---------------------------------
Eval num_timesteps=760000, episode_reward=1008.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -130     |
|    critic_loss     | 33.4     |
|    ent_coef        | 0.0435   |
|    ent_coef_loss   | 4.57     |
|    learning_rate   | 0.000924 |
|    n_updates       | 759899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 822      |
| time/              |          |
|    episodes        | 152      |
|    fps             | 61       |
|    time_elapsed    | 12298    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=258.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -82.8    |
|    critic_loss     | 17.9     |
|    ent_coef        | 0.0378   |
|    ent_coef_loss   | -2.21    |
|    learning_rate   | 0.000923 |
|    n_updates       | 769899   |
---------------------------------
Eval num_timesteps=780000, episode_reward=156.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -68.2    |
|    critic_loss     | 12       |
|    ent_coef        | 0.0332   |
|    ent_coef_loss   | -4.16    |
|    learning_rate   | 0.000922 |
|    n_updates       | 779899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 802      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 61       |
|    time_elapsed    | 12649    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=154.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 154      |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -59.4    |
|    critic_loss     | 17.7     |
|    ent_coef        | 0.0316   |
|    ent_coef_loss   | -0.875   |
|    learning_rate   | 0.000921 |
|    n_updates       | 789899   |
---------------------------------
Eval num_timesteps=800000, episode_reward=1192.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -61.5    |
|    critic_loss     | 16.3     |
|    ent_coef        | 0.0336   |
|    ent_coef_loss   | 2.61     |
|    learning_rate   | 0.00092  |
|    n_updates       | 799899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 768      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 61       |
|    time_elapsed    | 12999    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=288.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -44.4    |
|    critic_loss     | 21.6     |
|    ent_coef        | 0.036    |
|    ent_coef_loss   | -4       |
|    learning_rate   | 0.000919 |
|    n_updates       | 809899   |
---------------------------------
Eval num_timesteps=820000, episode_reward=1203.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -76.5    |
|    critic_loss     | 30.7     |
|    ent_coef        | 0.0376   |
|    ent_coef_loss   | 1.49     |
|    learning_rate   | 0.000918 |
|    n_updates       | 819899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 774      |
| time/              |          |
|    episodes        | 164      |
|    fps             | 61       |
|    time_elapsed    | 13350    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=1190.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 33.8     |
|    ent_coef        | 0.0334   |
|    ent_coef_loss   | 0.188    |
|    learning_rate   | 0.000917 |
|    n_updates       | 829899   |
---------------------------------
Eval num_timesteps=840000, episode_reward=1194.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 25.9     |
|    ent_coef        | 0.0332   |
|    ent_coef_loss   | 2.7      |
|    learning_rate   | 0.000916 |
|    n_updates       | 839899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 787      |
| time/              |          |
|    episodes        | 168      |
|    fps             | 61       |
|    time_elapsed    | 13702    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=1187.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -80.2    |
|    critic_loss     | 17.1     |
|    ent_coef        | 0.0306   |
|    ent_coef_loss   | 0.752    |
|    learning_rate   | 0.000915 |
|    n_updates       | 849899   |
---------------------------------
Eval num_timesteps=860000, episode_reward=1181.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 35.6     |
|    ent_coef        | 0.0315   |
|    ent_coef_loss   | -3.49    |
|    learning_rate   | 0.000914 |
|    n_updates       | 859899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 801      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 61       |
|    time_elapsed    | 14053    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=231.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 27.6     |
|    ent_coef        | 0.0303   |
|    ent_coef_loss   | -0.931   |
|    learning_rate   | 0.000913 |
|    n_updates       | 869899   |
---------------------------------
Eval num_timesteps=880000, episode_reward=275.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -67.2    |
|    critic_loss     | 17       |
|    ent_coef        | 0.028    |
|    ent_coef_loss   | 2.04     |
|    learning_rate   | 0.000912 |
|    n_updates       | 879899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 779      |
| time/              |          |
|    episodes        | 176      |
|    fps             | 61       |
|    time_elapsed    | 14401    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=1190.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -62.4    |
|    critic_loss     | 22.9     |
|    ent_coef        | 0.0291   |
|    ent_coef_loss   | -2.63    |
|    learning_rate   | 0.000911 |
|    n_updates       | 889899   |
---------------------------------
Eval num_timesteps=900000, episode_reward=296.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -68.2    |
|    critic_loss     | 20.6     |
|    ent_coef        | 0.0295   |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.00091  |
|    n_updates       | 899899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 796      |
| time/              |          |
|    episodes        | 180      |
|    fps             | 61       |
|    time_elapsed    | 14751    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=176.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -61.2    |
|    critic_loss     | 21.6     |
|    ent_coef        | 0.0286   |
|    ent_coef_loss   | 3.96     |
|    learning_rate   | 0.000909 |
|    n_updates       | 909899   |
---------------------------------
Eval num_timesteps=920000, episode_reward=2489.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 26       |
|    ent_coef        | 0.028    |
|    ent_coef_loss   | 0.433    |
|    learning_rate   | 0.000908 |
|    n_updates       | 919899   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 786      |
| time/              |          |
|    episodes        | 184      |
|    fps             | 60       |
|    time_elapsed    | 15101    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=1440.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 42.7     |
|    ent_coef        | 0.0314   |
|    ent_coef_loss   | 3.54     |
|    learning_rate   | 0.000907 |
|    n_updates       | 929899   |
---------------------------------
Eval num_timesteps=940000, episode_reward=2534.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -71.9    |
|    critic_loss     | 44.3     |
|    ent_coef        | 0.0577   |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000906 |
|    n_updates       | 939899   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 827      |
| time/              |          |
|    episodes        | 188      |
|    fps             | 60       |
|    time_elapsed    | 15450    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2245.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -152     |
|    critic_loss     | 72.8     |
|    ent_coef        | 0.108    |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000905 |
|    n_updates       | 949899   |
---------------------------------
Eval num_timesteps=960000, episode_reward=1191.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -257     |
|    critic_loss     | 466      |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000904 |
|    n_updates       | 959899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 871      |
| time/              |          |
|    episodes        | 192      |
|    fps             | 60       |
|    time_elapsed    | 15801    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=327.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -200     |
|    critic_loss     | 51.3     |
|    ent_coef        | 0.0704   |
|    ent_coef_loss   | 0.431    |
|    learning_rate   | 0.000903 |
|    n_updates       | 969899   |
---------------------------------
Eval num_timesteps=980000, episode_reward=1208.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -162     |
|    critic_loss     | 35.6     |
|    ent_coef        | 0.0566   |
|    ent_coef_loss   | -0.903   |
|    learning_rate   | 0.000902 |
|    n_updates       | 979899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 867      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 60       |
|    time_elapsed    | 16152    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=1196.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -123     |
|    critic_loss     | 43.6     |
|    ent_coef        | 0.0487   |
|    ent_coef_loss   | -0.929   |
|    learning_rate   | 0.000901 |
|    n_updates       | 989899   |
---------------------------------
Eval num_timesteps=1000000, episode_reward=1226.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -108     |
|    critic_loss     | 14.7     |
|    ent_coef        | 0.0475   |
|    ent_coef_loss   | -0.563   |
|    learning_rate   | 0.0009   |
|    n_updates       | 999899   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 873      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 60       |
|    time_elapsed    | 16501    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=1222.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -76.3    |
|    critic_loss     | 11.5     |
|    ent_coef        | 0.0319   |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.000899 |
|    n_updates       | 1009899  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=2512.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -58.7    |
|    critic_loss     | 13.6     |
|    ent_coef        | 0.027    |
|    ent_coef_loss   | -1.9     |
|    learning_rate   | 0.000898 |
|    n_updates       | 1019899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 890      |
| time/              |          |
|    episodes        | 204      |
|    fps             | 60       |
|    time_elapsed    | 16851    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=1699.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 3.86     |
|    ent_coef        | 0.0204   |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000897 |
|    n_updates       | 1029899  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=1217.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -41.7    |
|    critic_loss     | 5.17     |
|    ent_coef        | 0.0179   |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000896 |
|    n_updates       | 1039899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 928      |
| time/              |          |
|    episodes        | 208      |
|    fps             | 60       |
|    time_elapsed    | 17202    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=1197.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -35.3    |
|    critic_loss     | 5.94     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000895 |
|    n_updates       | 1049899  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=1830.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -34      |
|    critic_loss     | 6.05     |
|    ent_coef        | 0.017    |
|    ent_coef_loss   | -0.236   |
|    learning_rate   | 0.000894 |
|    n_updates       | 1059899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 931      |
| time/              |          |
|    episodes        | 212      |
|    fps             | 60       |
|    time_elapsed    | 17554    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=1192.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 29.3     |
|    ent_coef        | 0.0167   |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.000893 |
|    n_updates       | 1069899  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=1199.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -42.2    |
|    critic_loss     | 13.9     |
|    ent_coef        | 0.0188   |
|    ent_coef_loss   | 4.42     |
|    learning_rate   | 0.000892 |
|    n_updates       | 1079899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 944      |
| time/              |          |
|    episodes        | 216      |
|    fps             | 60       |
|    time_elapsed    | 17905    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=1209.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -48.6    |
|    critic_loss     | 8.88     |
|    ent_coef        | 0.0254   |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.000891 |
|    n_updates       | 1089899  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=263.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -85.5    |
|    critic_loss     | 22.7     |
|    ent_coef        | 0.037    |
|    ent_coef_loss   | 3.02     |
|    learning_rate   | 0.00089  |
|    n_updates       | 1099899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 947      |
| time/              |          |
|    episodes        | 220      |
|    fps             | 60       |
|    time_elapsed    | 18255    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=56.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57       |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -108     |
|    critic_loss     | 44       |
|    ent_coef        | 0.046    |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000889 |
|    n_updates       | 1109899  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=85.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 85.2     |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -105     |
|    critic_loss     | 36.2     |
|    ent_coef        | 0.0506   |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000888 |
|    n_updates       | 1119899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 927      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 60       |
|    time_elapsed    | 18606    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2539.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -124     |
|    critic_loss     | 57.6     |
|    ent_coef        | 0.0546   |
|    ent_coef_loss   | -0.343   |
|    learning_rate   | 0.000887 |
|    n_updates       | 1129899  |
---------------------------------
New best mean reward!
Eval num_timesteps=1140000, episode_reward=911.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -71.3    |
|    critic_loss     | 20.8     |
|    ent_coef        | 0.0512   |
|    ent_coef_loss   | -0.622   |
|    learning_rate   | 0.000886 |
|    n_updates       | 1139899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 944      |
| time/              |          |
|    episodes        | 228      |
|    fps             | 60       |
|    time_elapsed    | 18957    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=205.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -62.6    |
|    critic_loss     | 57.3     |
|    ent_coef        | 0.0407   |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000885 |
|    n_updates       | 1149899  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=241.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -61.6    |
|    critic_loss     | 13.4     |
|    ent_coef        | 0.0417   |
|    ent_coef_loss   | -0.814   |
|    learning_rate   | 0.000884 |
|    n_updates       | 1159899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 963      |
| time/              |          |
|    episodes        | 232      |
|    fps             | 60       |
|    time_elapsed    | 19307    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=662.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -79.4    |
|    critic_loss     | 426      |
|    ent_coef        | 0.0458   |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000883 |
|    n_updates       | 1169899  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=419.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -84.8    |
|    critic_loss     | 29.2     |
|    ent_coef        | 0.0466   |
|    ent_coef_loss   | 0.704    |
|    learning_rate   | 0.000882 |
|    n_updates       | 1179899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 960      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 60       |
|    time_elapsed    | 19657    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=1188.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 36       |
|    ent_coef        | 0.0548   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000881 |
|    n_updates       | 1189899  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=253.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 253      |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 48       |
|    ent_coef        | 0.067    |
|    ent_coef_loss   | -0.776   |
|    learning_rate   | 0.00088  |
|    n_updates       | 1199899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 944      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 59       |
|    time_elapsed    | 20006    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=1181.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 84.9     |
|    ent_coef        | 0.0705   |
|    ent_coef_loss   | 3.21     |
|    learning_rate   | 0.000879 |
|    n_updates       | 1209899  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=1201.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -37.9    |
|    critic_loss     | 250      |
|    ent_coef        | 0.0745   |
|    ent_coef_loss   | 4.7      |
|    learning_rate   | 0.000878 |
|    n_updates       | 1219899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 941      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 59       |
|    time_elapsed    | 20356    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=277.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 126      |
|    ent_coef        | 0.0802   |
|    ent_coef_loss   | -2.96    |
|    learning_rate   | 0.000877 |
|    n_updates       | 1229899  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=294.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -187     |
|    critic_loss     | 176      |
|    ent_coef        | 0.197    |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.000876 |
|    n_updates       | 1239899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 956      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 59       |
|    time_elapsed    | 20705    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=1178.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -263     |
|    critic_loss     | 269      |
|    ent_coef        | 0.211    |
|    ent_coef_loss   | 0.292    |
|    learning_rate   | 0.000875 |
|    n_updates       | 1249899  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=1169.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -192     |
|    critic_loss     | 216      |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | -0.33    |
|    learning_rate   | 0.000874 |
|    n_updates       | 1259899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 981      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 59       |
|    time_elapsed    | 21054    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=1174.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -147     |
|    critic_loss     | 271      |
|    ent_coef        | 0.138    |
|    ent_coef_loss   | 0.936    |
|    learning_rate   | 0.000873 |
|    n_updates       | 1269899  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=1182.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 146      |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.000872 |
|    n_updates       | 1279899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 59       |
|    time_elapsed    | 21404    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=1177.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -144     |
|    critic_loss     | 120      |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.000871 |
|    n_updates       | 1289899  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=1204.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -157     |
|    critic_loss     | 122      |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 0.339    |
|    learning_rate   | 0.00087  |
|    n_updates       | 1299899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 59       |
|    time_elapsed    | 21753    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=1184.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -118     |
|    critic_loss     | 169      |
|    ent_coef        | 0.116    |
|    ent_coef_loss   | 0.935    |
|    learning_rate   | 0.000869 |
|    n_updates       | 1309899  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=1189.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -108     |
|    critic_loss     | 285      |
|    ent_coef        | 0.114    |
|    ent_coef_loss   | 0.636    |
|    learning_rate   | 0.000868 |
|    n_updates       | 1319899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 59       |
|    time_elapsed    | 22103    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=1187.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -123     |
|    critic_loss     | 92       |
|    ent_coef        | 0.104    |
|    ent_coef_loss   | -0.0466  |
|    learning_rate   | 0.000867 |
|    n_updates       | 1329899  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=1057.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -102     |
|    critic_loss     | 141      |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -0.229   |
|    learning_rate   | 0.000866 |
|    n_updates       | 1339899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 59       |
|    time_elapsed    | 22453    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=906.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -116     |
|    critic_loss     | 167      |
|    ent_coef        | 0.103    |
|    ent_coef_loss   | -1.96    |
|    learning_rate   | 0.000865 |
|    n_updates       | 1349899  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2006.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -130     |
|    critic_loss     | 131      |
|    ent_coef        | 0.0912   |
|    ent_coef_loss   | -0.387   |
|    learning_rate   | 0.000864 |
|    n_updates       | 1359899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 59       |
|    time_elapsed    | 22808    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=1990.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -133     |
|    critic_loss     | 152      |
|    ent_coef        | 0.0998   |
|    ent_coef_loss   | 0.302    |
|    learning_rate   | 0.000863 |
|    n_updates       | 1369899  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=250.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -165     |
|    critic_loss     | 157      |
|    ent_coef        | 0.102    |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.000862 |
|    n_updates       | 1379899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 59       |
|    time_elapsed    | 23157    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=254.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -242     |
|    critic_loss     | 463      |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 1.14     |
|    learning_rate   | 0.000861 |
|    n_updates       | 1389899  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=497.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -265     |
|    critic_loss     | 760      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 0.03     |
|    learning_rate   | 0.00086  |
|    n_updates       | 1399899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 59       |
|    time_elapsed    | 23507    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=233.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -277     |
|    critic_loss     | 349      |
|    ent_coef        | 0.147    |
|    ent_coef_loss   | -1.37    |
|    learning_rate   | 0.000859 |
|    n_updates       | 1409899  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2473.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -305     |
|    critic_loss     | 789      |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.000858 |
|    n_updates       | 1419899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    episodes        | 284      |
|    fps             | 59       |
|    time_elapsed    | 23855    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=2528.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -288     |
|    critic_loss     | 159      |
|    ent_coef        | 0.13     |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.000857 |
|    n_updates       | 1429899  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=159.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -313     |
|    critic_loss     | 120      |
|    ent_coef        | 0.118    |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.000856 |
|    n_updates       | 1439899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 59       |
|    time_elapsed    | 24204    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=430.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 355      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | 0.916    |
|    learning_rate   | 0.000855 |
|    n_updates       | 1449899  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=452.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -190     |
|    critic_loss     | 359      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000854 |
|    n_updates       | 1459899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 59       |
|    time_elapsed    | 24552    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=1024.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -259     |
|    critic_loss     | 638      |
|    ent_coef        | 0.137    |
|    ent_coef_loss   | 0.722    |
|    learning_rate   | 0.000853 |
|    n_updates       | 1469899  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2520.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -202     |
|    critic_loss     | 605      |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.000852 |
|    n_updates       | 1479899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 59       |
|    time_elapsed    | 24902    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2519.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -176     |
|    critic_loss     | 556      |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.106   |
|    learning_rate   | 0.000851 |
|    n_updates       | 1489899  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2524.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -204     |
|    critic_loss     | 673      |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | 0.719    |
|    learning_rate   | 0.00085  |
|    n_updates       | 1499899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 59       |
|    time_elapsed    | 25251    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2522.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -171     |
|    critic_loss     | 252      |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.000849 |
|    n_updates       | 1509899  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=2268.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -142     |
|    critic_loss     | 445      |
|    ent_coef        | 0.144    |
|    ent_coef_loss   | -2.85    |
|    learning_rate   | 0.000848 |
|    n_updates       | 1519899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 59       |
|    time_elapsed    | 25599    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=1115.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -211     |
|    critic_loss     | 989      |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.000847 |
|    n_updates       | 1529899  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=1766.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -161     |
|    critic_loss     | 400      |
|    ent_coef        | 0.178    |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.000846 |
|    n_updates       | 1539899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 59       |
|    time_elapsed    | 25948    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2482.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -361     |
|    critic_loss     | 485      |
|    ent_coef        | 0.215    |
|    ent_coef_loss   | 0.114    |
|    learning_rate   | 0.000845 |
|    n_updates       | 1549899  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=873.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -445     |
|    critic_loss     | 1.86e+03 |
|    ent_coef        | 0.244    |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000844 |
|    n_updates       | 1559899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 59       |
|    time_elapsed    | 26297    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=273.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -492     |
|    critic_loss     | 2.23e+03 |
|    ent_coef        | 0.3      |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.000843 |
|    n_updates       | 1569899  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=1046.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -550     |
|    critic_loss     | 1.91e+03 |
|    ent_coef        | 0.305    |
|    ent_coef_loss   | 1.22     |
|    learning_rate   | 0.000842 |
|    n_updates       | 1579899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 59       |
|    time_elapsed    | 26646    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=2499.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -402     |
|    critic_loss     | 680      |
|    ent_coef        | 0.278    |
|    ent_coef_loss   | -0.233   |
|    learning_rate   | 0.000841 |
|    n_updates       | 1589899  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=1022.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -454     |
|    critic_loss     | 505      |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | 0.841    |
|    learning_rate   | 0.00084  |
|    n_updates       | 1599899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 59       |
|    time_elapsed    | 26994    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=1218.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -286     |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.274    |
|    ent_coef_loss   | -0.586   |
|    learning_rate   | 0.000839 |
|    n_updates       | 1609899  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=1167.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -292     |
|    critic_loss     | 2.35e+03 |
|    ent_coef        | 0.291    |
|    ent_coef_loss   | -0.478   |
|    learning_rate   | 0.000838 |
|    n_updates       | 1619899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 59       |
|    time_elapsed    | 27344    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=1290.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -420     |
|    critic_loss     | 1.72e+03 |
|    ent_coef        | 0.399    |
|    ent_coef_loss   | 0.256    |
|    learning_rate   | 0.000837 |
|    n_updates       | 1629899  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=1064.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -528     |
|    critic_loss     | 1.32e+04 |
|    ent_coef        | 0.406    |
|    ent_coef_loss   | 0.0244   |
|    learning_rate   | 0.000836 |
|    n_updates       | 1639899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 328      |
|    fps             | 59       |
|    time_elapsed    | 27693    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=131.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -583     |
|    critic_loss     | 1.13e+03 |
|    ent_coef        | 0.433    |
|    ent_coef_loss   | 0.352    |
|    learning_rate   | 0.000835 |
|    n_updates       | 1649899  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=310.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -541     |
|    critic_loss     | 1.42e+03 |
|    ent_coef        | 0.439    |
|    ent_coef_loss   | 0.772    |
|    learning_rate   | 0.000834 |
|    n_updates       | 1659899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 59       |
|    time_elapsed    | 28041    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2520.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -426     |
|    critic_loss     | 974      |
|    ent_coef        | 0.443    |
|    ent_coef_loss   | -0.29    |
|    learning_rate   | 0.000833 |
|    n_updates       | 1669899  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2522.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -453     |
|    critic_loss     | 1.39e+03 |
|    ent_coef        | 0.475    |
|    ent_coef_loss   | -0.671   |
|    learning_rate   | 0.000832 |
|    n_updates       | 1679899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 59       |
|    time_elapsed    | 28389    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=205.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -518     |
|    critic_loss     | 1.8e+03  |
|    ent_coef        | 0.486    |
|    ent_coef_loss   | -0.0484  |
|    learning_rate   | 0.000831 |
|    n_updates       | 1689899  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2522.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -333     |
|    critic_loss     | 3.7e+03  |
|    ent_coef        | 0.485    |
|    ent_coef_loss   | -0.167   |
|    learning_rate   | 0.00083  |
|    n_updates       | 1699899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 59       |
|    time_elapsed    | 28738    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2523.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -460     |
|    critic_loss     | 2.76e+03 |
|    ent_coef        | 0.503    |
|    ent_coef_loss   | -0.43    |
|    learning_rate   | 0.000829 |
|    n_updates       | 1709899  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2525.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -853     |
|    critic_loss     | 2.61e+03 |
|    ent_coef        | 0.526    |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.000828 |
|    n_updates       | 1719899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 59       |
|    time_elapsed    | 29087    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2524.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -951     |
|    critic_loss     | 6.25e+03 |
|    ent_coef        | 0.554    |
|    ent_coef_loss   | 0.944    |
|    learning_rate   | 0.000827 |
|    n_updates       | 1729899  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=2522.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -947     |
|    critic_loss     | 3.03e+03 |
|    ent_coef        | 0.564    |
|    ent_coef_loss   | -0.0575  |
|    learning_rate   | 0.000826 |
|    n_updates       | 1739899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 59       |
|    time_elapsed    | 29435    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2523.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -763     |
|    critic_loss     | 6.21e+03 |
|    ent_coef        | 0.548    |
|    ent_coef_loss   | -0.549   |
|    learning_rate   | 0.000825 |
|    n_updates       | 1749899  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=2525.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -942     |
|    critic_loss     | 5.3e+03  |
|    ent_coef        | 0.577    |
|    ent_coef_loss   | 0.0206   |
|    learning_rate   | 0.000824 |
|    n_updates       | 1759899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    episodes        | 352      |
|    fps             | 59       |
|    time_elapsed    | 29784    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2152.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -995     |
|    critic_loss     | 3.89e+03 |
|    ent_coef        | 0.535    |
|    ent_coef_loss   | -0.164   |
|    learning_rate   | 0.000823 |
|    n_updates       | 1769899  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=1061.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -992     |
|    critic_loss     | 6.34e+03 |
|    ent_coef        | 0.565    |
|    ent_coef_loss   | 0.148    |
|    learning_rate   | 0.000822 |
|    n_updates       | 1779899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 59       |
|    time_elapsed    | 30134    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=153.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | 153       |
| time/              |           |
|    total_timesteps | 1790000   |
| train/             |           |
|    actor_loss      | -1.05e+03 |
|    critic_loss     | 6.07e+03  |
|    ent_coef        | 0.529     |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 0.000821  |
|    n_updates       | 1789899   |
----------------------------------
Eval num_timesteps=1800000, episode_reward=1323.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -941     |
|    critic_loss     | 2.99e+03 |
|    ent_coef        | 0.534    |
|    ent_coef_loss   | 0.0605   |
|    learning_rate   | 0.00082  |
|    n_updates       | 1799899  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 59       |
|    time_elapsed    | 30494    |
|    total_timesteps | 1800000  |
---------------------------------