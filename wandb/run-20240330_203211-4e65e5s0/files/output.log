Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_53
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=1717.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=2122.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -39.8    |
|    critic_loss     | 0.149    |
|    ent_coef        | 0.00578  |
|    ent_coef_loss   | 3.62     |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 31       |
|    time_elapsed    | 638      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2181.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -38.1    |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | 4.42     |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=2269.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -39.9    |
|    critic_loss     | 0.211    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | 0.413    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 8        |
|    fps             | 31       |
|    time_elapsed    | 1268     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=2269.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -41.9    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | -4.55    |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=2286.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -41.2    |
|    critic_loss     | 0.0708   |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 32       |
|    time_elapsed    | 1872     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=2274.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -41      |
|    critic_loss     | 0.277    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -4.53    |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=2228.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -41.6    |
|    critic_loss     | 0.0911   |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 32       |
|    time_elapsed    | 2475     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1588.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -41.1    |
|    critic_loss     | 0.168    |
|    ent_coef        | 0.00228  |
|    ent_coef_loss   | -2.46    |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=2238.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -40.5    |
|    critic_loss     | 0.0929   |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 3.74     |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    episodes        | 20       |
|    fps             | 32       |
|    time_elapsed    | 3076     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=3077.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -40.5    |
|    critic_loss     | 0.685    |
|    ent_coef        | 0.00222  |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
New best mean reward!