Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_58
Eval num_timesteps=10000, episode_reward=-1440.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+03 |
| time/              |           |
|    total_timesteps | 10000     |
| train/             |           |
|    actor_loss      | -3.95     |
|    critic_loss     | 0.314     |
|    ent_coef        | 0.00442   |
|    ent_coef_loss   | -10.9     |
|    learning_rate   | 0.000999  |
|    n_updates       | 1354324   |
----------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-1463.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.46e+03 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | -26.7     |
|    critic_loss     | 0.396     |
|    ent_coef        | 0.005     |
|    ent_coef_loss   | 1.08      |
|    learning_rate   | 0.000998  |
|    n_updates       | 1364324   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.41e+03 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 33        |
|    time_elapsed    | 598       |
|    total_timesteps | 20000     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-1442.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+03 |
| time/              |           |
|    total_timesteps | 30000     |
| train/             |           |
|    actor_loss      | -22.7     |
|    critic_loss     | 0.544     |
|    ent_coef        | 0.0107    |
|    ent_coef_loss   | -6.97     |
|    learning_rate   | 0.000997  |
|    n_updates       | 1374324   |
----------------------------------
Eval num_timesteps=40000, episode_reward=-1485.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+03 |
| time/              |           |
|    total_timesteps | 40000     |
| train/             |           |
|    actor_loss      | -21.7     |
|    critic_loss     | 0.474     |
|    ent_coef        | 0.00666   |
|    ent_coef_loss   | -1.17     |
|    learning_rate   | 0.000996  |
|    n_updates       | 1384324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.2e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 33       |
|    time_elapsed    | 1188     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-1349.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.35e+03 |
| time/              |           |
|    total_timesteps | 50000     |
| train/             |           |
|    actor_loss      | -10.6     |
|    critic_loss     | 0.459     |
|    ent_coef        | 0.00685   |
|    ent_coef_loss   | -0.412    |
|    learning_rate   | 0.000995  |
|    n_updates       | 1394324   |
----------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=-1441.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+03 |
| time/              |           |
|    total_timesteps | 60000     |
| train/             |           |
|    actor_loss      | 1.57      |
|    critic_loss     | 0.349     |
|    ent_coef        | 0.00741   |
|    ent_coef_loss   | -0.458    |
|    learning_rate   | 0.000994  |
|    n_updates       | 1404324   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.17e+03 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 33        |
|    time_elapsed    | 1771      |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-588.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -589     |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 5.05     |
|    critic_loss     | 0.464    |
|    ent_coef        | 0.00516  |
|    ent_coef_loss   | -0.58    |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=-1770.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+03 |
| time/              |           |
|    total_timesteps | 80000     |
| train/             |           |
|    actor_loss      | 10.5      |
|    critic_loss     | 0.459     |
|    ent_coef        | 0.00446   |
|    ent_coef_loss   | 4.17      |
|    learning_rate   | 0.000992  |
|    n_updates       | 1424324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.1e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 33       |
|    time_elapsed    | 2374     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-620.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -621     |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 13.9     |
|    critic_loss     | 0.237    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=-1435.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+03 |
| time/              |           |
|    total_timesteps | 100000    |
| train/             |           |
|    actor_loss      | 16.7      |
|    critic_loss     | 0.45      |
|    ent_coef        | 0.00362   |
|    ent_coef_loss   | 4.12      |
|    learning_rate   | 0.00099   |
|    n_updates       | 1444324   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 33        |
|    time_elapsed    | 2976      |
|    total_timesteps | 100000    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-613.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -614     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 15.8     |
|    critic_loss     | 0.165    |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | 2.04     |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=178.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 17       |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.0033   |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -974     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 33       |
|    time_elapsed    | 3581     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-609.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -609     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 16.2     |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00352  |
|    ent_coef_loss   | 0.918    |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
Eval num_timesteps=140000, episode_reward=155.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 15.9     |
|    critic_loss     | 0.262    |
|    ent_coef        | 0.00377  |
|    ent_coef_loss   | 3.94     |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -884     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 33       |
|    time_elapsed    | 4183     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-603.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -603     |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 15.8     |
|    critic_loss     | 0.318    |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | -0.565   |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
Eval num_timesteps=160000, episode_reward=180.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 14.6     |
|    critic_loss     | 0.211    |
|    ent_coef        | 0.00346  |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -824     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 33       |
|    time_elapsed    | 4786     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=157.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 14.9     |
|    critic_loss     | 0.228    |
|    ent_coef        | 0.00366  |
|    ent_coef_loss   | 1.89     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=159.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 14       |
|    critic_loss     | 0.123    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | 3.22     |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -744     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 33       |
|    time_elapsed    | 5389     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-854.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -854     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 14.3     |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.00426  |
|    ent_coef_loss   | -0.808   |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=-826.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -826     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 15       |
|    critic_loss     | 0.104    |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -678     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 33       |
|    time_elapsed    | 5993     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=174.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 15.7     |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00437  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
Eval num_timesteps=220000, episode_reward=160.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 16.8     |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | -3.43    |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -650     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 33       |
|    time_elapsed    | 6596     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-187.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 16       |
|    critic_loss     | 0.255    |
|    ent_coef        | 0.00527  |
|    ent_coef_loss   | -2.85    |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=-211.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 15.6     |
|    critic_loss     | 0.906    |
|    ent_coef        | 0.00596  |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -594     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 33       |
|    time_elapsed    | 7202     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-1211.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.21e+03 |
| time/              |           |
|    total_timesteps | 250000    |
| train/             |           |
|    actor_loss      | 17.1      |
|    critic_loss     | 0.232     |
|    ent_coef        | 0.00585   |
|    ent_coef_loss   | -6.4      |
|    learning_rate   | 0.000975  |
|    n_updates       | 1594324   |
----------------------------------
Eval num_timesteps=260000, episode_reward=-647.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -647     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 15.9     |
|    critic_loss     | 0.254    |
|    ent_coef        | 0.00552  |
|    ent_coef_loss   | 1.83     |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -608     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 33       |
|    time_elapsed    | 7803     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-1534.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.53e+03 |
| time/              |           |
|    total_timesteps | 270000    |
| train/             |           |
|    actor_loss      | 16.1      |
|    critic_loss     | 0.313     |
|    ent_coef        | 0.00518   |
|    ent_coef_loss   | 3         |
|    learning_rate   | 0.000973  |
|    n_updates       | 1614324   |
----------------------------------
Eval num_timesteps=280000, episode_reward=-249.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 18.2     |
|    critic_loss     | 1.78     |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | -4.94    |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -660     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 33       |
|    time_elapsed    | 8406     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=335.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 18.4     |
|    critic_loss     | 0.288    |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
New best mean reward!
Eval num_timesteps=300000, episode_reward=-521.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 15.1     |
|    critic_loss     | 0.178    |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -621     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 33       |
|    time_elapsed    | 9009     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-781.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -781     |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 16.9     |
|    critic_loss     | 0.357    |
|    ent_coef        | 0.0053   |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
Eval num_timesteps=320000, episode_reward=-304.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | 18.2     |
|    critic_loss     | 0.362    |
|    ent_coef        | 0.0048   |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -608     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 33       |
|    time_elapsed    | 9610     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-304.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | 18.6     |
|    critic_loss     | 0.79     |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | -4.22    |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=-1116.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.12e+03 |
| time/              |           |
|    total_timesteps | 340000    |
| train/             |           |
|    actor_loss      | 16.9      |
|    critic_loss     | 0.657     |
|    ent_coef        | 0.00573   |
|    ent_coef_loss   | -1.44     |
|    learning_rate   | 0.000966  |
|    n_updates       | 1684324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -606     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 33       |
|    time_elapsed    | 10210    |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-595.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -596     |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | 17.2     |
|    critic_loss     | 1.93     |
|    ent_coef        | 0.00573  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=-1029.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 360000    |
| train/             |           |
|    actor_loss      | 19.2      |
|    critic_loss     | 1.46      |
|    ent_coef        | 0.0048    |
|    ent_coef_loss   | 1.85      |
|    learning_rate   | 0.000964  |
|    n_updates       | 1704324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -631     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 33       |
|    time_elapsed    | 10804    |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=464.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | 15.6     |
|    critic_loss     | 0.543    |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | -0.457   |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
New best mean reward!
Eval num_timesteps=380000, episode_reward=-1130.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.13e+03 |
| time/              |           |
|    total_timesteps | 380000    |
| train/             |           |
|    actor_loss      | 17.7      |
|    critic_loss     | 0.519     |
|    ent_coef        | 0.00588   |
|    ent_coef_loss   | -1.23     |
|    learning_rate   | 0.000962  |
|    n_updates       | 1724324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -614     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 33       |
|    time_elapsed    | 11407    |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-1014.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 390000    |
| train/             |           |
|    actor_loss      | 18.6      |
|    critic_loss     | 0.512     |
|    ent_coef        | 0.00565   |
|    ent_coef_loss   | 1.66      |
|    learning_rate   | 0.000961  |
|    n_updates       | 1734324   |
----------------------------------
Eval num_timesteps=400000, episode_reward=149.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | 19.3     |
|    critic_loss     | 0.354    |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -636     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 33       |
|    time_elapsed    | 12012    |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-1302.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.3e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | 17.6     |
|    critic_loss     | 0.432    |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | 1        |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
Eval num_timesteps=420000, episode_reward=168.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | 16.9     |
|    critic_loss     | 0.482    |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -656     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 33       |
|    time_elapsed    | 12616    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-303.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | 18.7     |
|    critic_loss     | 0.612    |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=-308.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | 17.8     |
|    critic_loss     | 0.219    |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 0.65     |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -629     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 33       |
|    time_elapsed    | 13219    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-306.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 17.5     |
|    critic_loss     | 0.345    |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | 1.96     |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=-846.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -847     |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 17.6     |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | 0.201    |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -614     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 33       |
|    time_elapsed    | 13822    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-233.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | 16.9     |
|    critic_loss     | 0.407    |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=-302.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | 16.3     |
|    critic_loss     | 0.269    |
|    ent_coef        | 0.00531  |
|    ent_coef_loss   | -4.11    |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -597     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 33       |
|    time_elapsed    | 14424    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-1107.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+03 |
| time/              |           |
|    total_timesteps | 490000    |
| train/             |           |
|    actor_loss      | 15.8      |
|    critic_loss     | 0.415     |
|    ent_coef        | 0.0051    |
|    ent_coef_loss   | -1.28     |
|    learning_rate   | 0.000951  |
|    n_updates       | 1834324   |
----------------------------------
Eval num_timesteps=500000, episode_reward=-1169.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.17e+03 |
| time/              |           |
|    total_timesteps | 500000    |
| train/             |           |
|    actor_loss      | 15.5      |
|    critic_loss     | 0.312     |
|    ent_coef        | 0.00456   |
|    ent_coef_loss   | 3.16      |
|    learning_rate   | 0.00095   |
|    n_updates       | 1844324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -603     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 33       |
|    time_elapsed    | 15035    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-133.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | 15.2     |
|    critic_loss     | 0.23     |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | 7.37     |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=-184.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | 15.5     |
|    critic_loss     | 0.598    |
|    ent_coef        | 0.00421  |
|    ent_coef_loss   | 2.74     |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -574     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 33       |
|    time_elapsed    | 15626    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2376.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | 14.8     |
|    critic_loss     | 0.296    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
New best mean reward!
Eval num_timesteps=540000, episode_reward=-225.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | 15.7     |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | -3.24    |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -557     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 33       |
|    time_elapsed    | 16203    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-926.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | 14.4     |
|    critic_loss     | 0.197    |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=-1019.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 560000    |
| train/             |           |
|    actor_loss      | 16.2      |
|    critic_loss     | 1.85      |
|    ent_coef        | 0.00371   |
|    ent_coef_loss   | -3.57     |
|    learning_rate   | 0.000944  |
|    n_updates       | 1904324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -543     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 33       |
|    time_elapsed    | 16781    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-198.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | 16.3     |
|    critic_loss     | 0.402    |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | 3.37     |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=-220.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | 15.9     |
|    critic_loss     | 0.559    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | 0.666    |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -530     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 33       |
|    time_elapsed    | 17354    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-201.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | 15.4     |
|    critic_loss     | 1.76     |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | -2.85    |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=-865.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -865     |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | 15.6     |
|    critic_loss     | 0.768    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | 16.7     |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -521     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 33       |
|    time_elapsed    | 17923    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-835.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -835     |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | 13.6     |
|    critic_loss     | 0.364    |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | -8.05    |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=-1083.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.08e+03 |
| time/              |           |
|    total_timesteps | 620000    |
| train/             |           |
|    actor_loss      | 13.2      |
|    critic_loss     | 0.292     |
|    ent_coef        | 0.00397   |
|    ent_coef_loss   | -4.73     |
|    learning_rate   | 0.000938  |
|    n_updates       | 1964324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -521     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 33       |
|    time_elapsed    | 18493    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-1106.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+03 |
| time/              |           |
|    total_timesteps | 630000    |
| train/             |           |
|    actor_loss      | 16.4      |
|    critic_loss     | 1.43      |
|    ent_coef        | 0.00422   |
|    ent_coef_loss   | 0.198     |
|    learning_rate   | 0.000937  |
|    n_updates       | 1974324   |
----------------------------------
Eval num_timesteps=640000, episode_reward=-1280.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.28e+03 |
| time/              |           |
|    total_timesteps | 640000    |
| train/             |           |
|    actor_loss      | 15        |
|    critic_loss     | 0.507     |
|    ent_coef        | 0.00402   |
|    ent_coef_loss   | -0.902    |
|    learning_rate   | 0.000936  |
|    n_updates       | 1984324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -543     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 33       |
|    time_elapsed    | 19060    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=-207.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | 16.3     |
|    critic_loss     | 0.444    |
|    ent_coef        | 0.00354  |
|    ent_coef_loss   | -4.88    |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=-214.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | 15.2     |
|    critic_loss     | 0.688    |
|    ent_coef        | 0.00356  |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -546     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 33       |
|    time_elapsed    | 19638    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=-1104.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.1e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | 13.6     |
|    critic_loss     | 0.648    |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=-915.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | 12.8     |
|    critic_loss     | 0.284    |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -561     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 33       |
|    time_elapsed    | 20221    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-892.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | 14.5     |
|    critic_loss     | 0.329    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=-202.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | 13       |
|    critic_loss     | 0.689    |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | 0.672    |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -589     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 33       |
|    time_elapsed    | 20778    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-198.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | 13.8     |
|    critic_loss     | 0.258    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 3.5      |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=-558.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 13.7     |
|    critic_loss     | 0.287    |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -576     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 33       |
|    time_elapsed    | 21327    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=-879.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -880     |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | 13.4     |
|    critic_loss     | 0.497    |
|    ent_coef        | 0.00383  |
|    ent_coef_loss   | 0.494    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=-859.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -860     |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | 12.8     |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | -3.64    |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -580     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 33       |
|    time_elapsed    | 21880    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2464.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | 12.7     |
|    critic_loss     | 0.359    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | -2.02    |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
New best mean reward!
Eval num_timesteps=760000, episode_reward=-915.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | 12.5     |
|    critic_loss     | 0.684    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -564     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 33       |
|    time_elapsed    | 22453    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2247.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | 13.3     |
|    critic_loss     | 0.806    |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | 0.576    |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=2416.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | 14.6     |
|    critic_loss     | 3.11     |
|    ent_coef        | 0.00414  |
|    ent_coef_loss   | 4.1      |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -518     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 33       |
|    time_elapsed    | 23031    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=-227.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | 13.1     |
|    critic_loss     | 0.495    |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | 8.71     |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=-222.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | 13.1     |
|    critic_loss     | 0.247    |
|    ent_coef        | 0.00323  |
|    ent_coef_loss   | -1.61    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -520     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 33       |
|    time_elapsed    | 23618    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-731.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -731     |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | 14.5     |
|    critic_loss     | 0.548    |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=-212.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | 12.8     |
|    critic_loss     | 0.304    |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | 0.908    |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -518     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 33       |
|    time_elapsed    | 24196    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=-877.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -877     |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | 12.8     |
|    critic_loss     | 0.367    |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | 2.52     |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=-1411.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.41e+03 |
| time/              |           |
|    total_timesteps | 840000    |
| train/             |           |
|    actor_loss      | 13        |
|    critic_loss     | 0.336     |
|    ent_coef        | 0.00375   |
|    ent_coef_loss   | 0.474     |
|    learning_rate   | 0.000916  |
|    n_updates       | 2184324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -523     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 33       |
|    time_elapsed    | 24769    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2524.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | 13.3     |
|    critic_loss     | 0.585    |
|    ent_coef        | 0.00351  |
|    ent_coef_loss   | -2.88    |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
New best mean reward!
Eval num_timesteps=860000, episode_reward=-895.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | 13.1     |
|    critic_loss     | 0.277    |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -4.95    |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -511     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 33       |
|    time_elapsed    | 25354    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2574.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | 12.2     |
|    critic_loss     | 0.247    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -0.231   |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
New best mean reward!
Eval num_timesteps=880000, episode_reward=2584.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | 13.4     |
|    critic_loss     | 0.207    |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -482     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 33       |
|    time_elapsed    | 25923    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=-907.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | 14.5     |
|    critic_loss     | 0.497    |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | 7.87     |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2448.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | 15.7     |
|    critic_loss     | 0.596    |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -455     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 33       |
|    time_elapsed    | 26505    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2484.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | 13.8     |
|    critic_loss     | 0.6      |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=1958.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | 13.4     |
|    critic_loss     | 0.507    |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | 0.152    |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -385     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 33       |
|    time_elapsed    | 27081    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2843.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | 12.8     |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | 5.28     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
New best mean reward!
Eval num_timesteps=940000, episode_reward=2804.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | 11       |
|    critic_loss     | 0.358    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -4.24    |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -359     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 33       |
|    time_elapsed    | 27663    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-161.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | 12.1     |
|    critic_loss     | 0.532    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=-217.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | 11.1     |
|    critic_loss     | 0.539    |
|    ent_coef        | 0.00431  |
|    ent_coef_loss   | 8        |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -321     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 33       |
|    time_elapsed    | 28250    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=-137.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | 12.4     |
|    critic_loss     | 0.25     |
|    ent_coef        | 0.00441  |
|    ent_coef_loss   | -6.29    |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=-210.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | 11.6     |
|    critic_loss     | 0.296    |
|    ent_coef        | 0.00526  |
|    ent_coef_loss   | -3.57    |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -317     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 33       |
|    time_elapsed    | 28836    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=-206.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | 9.75     |
|    critic_loss     | 0.579    |
|    ent_coef        | 0.00541  |
|    ent_coef_loss   | 1.58     |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=-772.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -772     |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | 11.7     |
|    critic_loss     | 0.91     |
|    ent_coef        | 0.00516  |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -271     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 33       |
|    time_elapsed    | 29418    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=2811.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | 10.2     |
|    critic_loss     | 0.48     |
|    ent_coef        | 0.00552  |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=-740.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -741     |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | 9.53     |
|    critic_loss     | 1.24     |
|    ent_coef        | 0.00544  |
|    ent_coef_loss   | 0.89     |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -184     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 34       |
|    time_elapsed    | 29980    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=2820.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | 8.08     |
|    critic_loss     | 0.258    |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | 0.491    |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=2591.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | 9.01     |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | -5.51    |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -138     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 34       |
|    time_elapsed    | 30460    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=2484.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | 8.13     |
|    critic_loss     | 0.446    |
|    ent_coef        | 0.00519  |
|    ent_coef_loss   | -4.47    |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=2583.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | 6.17     |
|    critic_loss     | 0.414    |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -40.1    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 34       |
|    time_elapsed    | 30941    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2459.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | 5.67     |
|    critic_loss     | 0.251    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=2310.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | 4.68     |
|    critic_loss     | 0.506    |
|    ent_coef        | 0.00475  |
|    ent_coef_loss   | 0.518    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 79.8     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 34       |
|    time_elapsed    | 31424    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2408.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | 5.24     |
|    critic_loss     | 0.289    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -2.77    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=2404.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | 2.82     |
|    critic_loss     | 0.469    |
|    ent_coef        | 0.00457  |
|    ent_coef_loss   | 4.61     |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 193      |
| time/              |          |
|    episodes        | 220      |
|    fps             | 34       |
|    time_elapsed    | 31911    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=2385.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | 1.79     |
|    critic_loss     | 0.92     |
|    ent_coef        | 0.00386  |
|    ent_coef_loss   | -2.83    |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=393.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | 0.867    |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00357  |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 292      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 34       |
|    time_elapsed    | 32408    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2417.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | 1.58     |
|    critic_loss     | 0.17     |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=2417.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | 0.609    |
|    critic_loss     | 0.279    |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 409      |
| time/              |          |
|    episodes        | 228      |
|    fps             | 34       |
|    time_elapsed    | 32902    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=2391.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -0.649   |
|    critic_loss     | 0.321    |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | 3.95     |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2450.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.345    |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | 4.06     |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 525      |
| time/              |          |
|    episodes        | 232      |
|    fps             | 34       |
|    time_elapsed    | 33389    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2476.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | 0.484    |
|    critic_loss     | 0.221    |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | -0.914   |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2528.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | 1.72     |
|    critic_loss     | 0.772    |
|    ent_coef        | 0.0033   |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 581      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 34       |
|    time_elapsed    | 33879    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2521.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | 3.1      |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | -1.1     |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=-309.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -0.698   |
|    critic_loss     | 0.509    |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 691      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 34       |
|    time_elapsed    | 34369    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=-123.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | 2.73     |
|    critic_loss     | 2.47     |
|    ent_coef        | 0.00398  |
|    ent_coef_loss   | 4.5      |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=1789.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | 1.18     |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | -0.364   |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 687      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 34       |
|    time_elapsed    | 34860    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2383.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | 1.13     |
|    critic_loss     | 0.249    |
|    ent_coef        | 0.00353  |
|    ent_coef_loss   | -5.25    |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=2443.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 0.294    |
|    ent_coef        | 0.00332  |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 790      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 35       |
|    time_elapsed    | 35348    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=2445.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | 1.54     |
|    critic_loss     | 0.168    |
|    ent_coef        | 0.0033   |
|    ent_coef_loss   | -6.28    |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=2607.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -1.84    |
|    critic_loss     | 0.121    |
|    ent_coef        | 0.00356  |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 905      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 35       |
|    time_elapsed    | 35831    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2798.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -0.0181  |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00334  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=-176.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -4.25    |
|    critic_loss     | 0.948    |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 35       |
|    time_elapsed    | 36312    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=2659.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | 0.206    |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2546.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -1.83    |
|    critic_loss     | 0.272    |
|    ent_coef        | 0.00383  |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 35       |
|    time_elapsed    | 36799    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=-1054.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.05e+03 |
| time/              |           |
|    total_timesteps | 1310000   |
| train/             |           |
|    actor_loss      | -4.17     |
|    critic_loss     | 0.244     |
|    ent_coef        | 0.0041    |
|    ent_coef_loss   | 1.33      |
|    learning_rate   | 0.000869  |
|    n_updates       | 2654324   |
----------------------------------
Eval num_timesteps=1320000, episode_reward=2337.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.282    |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 35       |
|    time_elapsed    | 37282    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2305.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 5.25     |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2476.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -7.2     |
|    critic_loss     | 0.324    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 35       |
|    time_elapsed    | 37760    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2375.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | 0.988    |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2399.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -7.02    |
|    critic_loss     | 0.18     |
|    ent_coef        | 0.00435  |
|    ent_coef_loss   | -2.39    |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    episodes        | 272      |
|    fps             | 35       |
|    time_elapsed    | 38252    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2375.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -8.91    |
|    critic_loss     | 0.19     |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2457.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -6.79    |
|    critic_loss     | 0.0939   |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | -6.72    |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 35       |
|    time_elapsed    | 38739    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=-411.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -9.74    |
|    critic_loss     | 0.135    |
|    ent_coef        | 0.00338  |
|    ent_coef_loss   | 2.86     |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2642.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.262    |
|    ent_coef        | 0.00353  |
|    ent_coef_loss   | -1.91    |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 35       |
|    time_elapsed    | 39227    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=886.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -9.18    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | -3.97    |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=1231.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | 8.35     |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 35       |
|    time_elapsed    | 39715    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=2553.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -6.65    |
|    critic_loss     | 0.413    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | -2.01    |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=1699.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.412    |
|    ent_coef        | 0.0041   |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 35       |
|    time_elapsed    | 40207    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2605.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -8.09    |
|    critic_loss     | 0.646    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -3.11    |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=1650.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.237    |
|    ent_coef        | 0.00488  |
|    ent_coef_loss   | 0.913    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 35       |
|    time_elapsed    | 40695    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2515.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -9.88    |
|    critic_loss     | 0.519    |
|    ent_coef        | 0.00516  |
|    ent_coef_loss   | 5.74     |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=-639.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -639     |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -9.88    |
|    critic_loss     | 0.314    |
|    ent_coef        | 0.00502  |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 35       |
|    time_elapsed    | 41186    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2367.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.37e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.252    |
|    ent_coef        | 0.00495  |
|    ent_coef_loss   | -3.96    |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1682.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.244    |
|    ent_coef        | 0.00485  |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 35       |
|    time_elapsed    | 41679    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2861.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.0051   |
|    ent_coef_loss   | -0.462   |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1520000, episode_reward=2776.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -9.1     |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | -6.92    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 36       |
|    time_elapsed    | 42165    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=2072.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -7.84    |
|    critic_loss     | 0.383    |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | 9.16     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2481.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -7.51    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00575  |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 36       |
|    time_elapsed    | 42655    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=1825.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -9.54    |
|    critic_loss     | 0.372    |
|    ent_coef        | 0.00569  |
|    ent_coef_loss   | -3.66    |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2057.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.395    |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | 4.7      |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 36       |
|    time_elapsed    | 43145    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=-270.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.215    |
|    ent_coef        | 0.00536  |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2462.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.6      |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 0.169    |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 36       |
|    time_elapsed    | 43631    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=-1559.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+03 |
| time/              |           |
|    total_timesteps | 1590000   |
| train/             |           |
|    actor_loss      | -10       |
|    critic_loss     | 1.93      |
|    ent_coef        | 0.00551   |
|    ent_coef_loss   | 0.939     |
|    learning_rate   | 0.000841  |
|    n_updates       | 2934324   |
----------------------------------
Eval num_timesteps=1600000, episode_reward=2550.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.8      |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | 0.702    |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 36       |
|    time_elapsed    | 44120    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=-621.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -621     |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=-614.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -615     |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 0.282    |
|    ent_coef        | 0.00526  |
|    ent_coef_loss   | -5.06    |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 36       |
|    time_elapsed    | 44599    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2667.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 3.02     |
|    ent_coef        | 0.00486  |
|    ent_coef_loss   | 1.93     |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2596.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.393    |
|    ent_coef        | 0.00601  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 36       |
|    time_elapsed    | 45081    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=185.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00624  |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=-484.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -484     |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -17.5    |
|    critic_loss     | 1.13     |
|    ent_coef        | 0.00687  |
|    ent_coef_loss   | 0.728    |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 36       |
|    time_elapsed    | 45566    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2496.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -17.1    |
|    critic_loss     | 0.378    |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | -2.08    |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2486.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -17.6    |
|    critic_loss     | 4.3      |
|    ent_coef        | 0.00726  |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 36       |
|    time_elapsed    | 46049    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=740.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -16.9    |
|    critic_loss     | 1.14     |
|    ent_coef        | 0.00671  |
|    ent_coef_loss   | 3.87     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=-1779.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.78e+03 |
| time/              |           |
|    total_timesteps | 1700000   |
| train/             |           |
|    actor_loss      | -13.8     |
|    critic_loss     | 1.01      |
|    ent_coef        | 0.00685   |
|    ent_coef_loss   | -2.47     |
|    learning_rate   | 0.00083   |
|    n_updates       | 3044324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 36       |
|    time_elapsed    | 46539    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=-954.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00592  |
|    ent_coef_loss   | -4.62    |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=-1228.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+03 |
| time/              |           |
|    total_timesteps | 1720000   |
| train/             |           |
|    actor_loss      | -14.3     |
|    critic_loss     | 0.358     |
|    ent_coef        | 0.00596   |
|    ent_coef_loss   | 1.47      |
|    learning_rate   | 0.000828  |
|    n_updates       | 3064324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 36       |
|    time_elapsed    | 47029    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=-598.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -598     |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.718    |
|    ent_coef        | 0.00584  |
|    ent_coef_loss   | -7.88    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=-610.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -610     |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 1.42     |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | 0.166    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 36       |
|    time_elapsed    | 47523    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=-1766.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+03 |
| time/              |           |
|    total_timesteps | 1750000   |
| train/             |           |
|    actor_loss      | -11       |
|    critic_loss     | 1.19      |
|    ent_coef        | 0.00666   |
|    ent_coef_loss   | -4.08     |
|    learning_rate   | 0.000825  |
|    n_updates       | 3094324   |
----------------------------------
Eval num_timesteps=1760000, episode_reward=2433.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.322    |
|    ent_coef        | 0.00731  |
|    ent_coef_loss   | -0.575   |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 36       |
|    time_elapsed    | 48011    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2492.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.474    |
|    ent_coef        | 0.00761  |
|    ent_coef_loss   | -2.69    |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2540.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -9.73    |
|    critic_loss     | 6.3      |
|    ent_coef        | 0.00723  |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    episodes        | 356      |
|    fps             | 36       |
|    time_elapsed    | 48509    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=-119.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.833    |
|    ent_coef        | 0.00781  |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=-1842.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.84e+03 |
| time/              |           |
|    total_timesteps | 1800000   |
| train/             |           |
|    actor_loss      | -14.5     |
|    critic_loss     | 0.756     |
|    ent_coef        | 0.00893   |
|    ent_coef_loss   | 3.52      |
|    learning_rate   | 0.00082   |
|    n_updates       | 3144324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 36       |
|    time_elapsed    | 49000    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2504.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -16      |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.0113   |
|    ent_coef_loss   | -2.15    |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2500.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -22.2    |
|    critic_loss     | 1.63     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 36       |
|    time_elapsed    | 49502    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=-1473.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 1830000   |
| train/             |           |
|    actor_loss      | -24.8     |
|    critic_loss     | 4.69      |
|    ent_coef        | 0.0101    |
|    ent_coef_loss   | -4        |
|    learning_rate   | 0.000817  |
|    n_updates       | 3174324   |
----------------------------------
Eval num_timesteps=1840000, episode_reward=-452.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 3.27     |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | -4       |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 36       |
|    time_elapsed    | 50074    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=-1028.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 1850000   |
| train/             |           |
|    actor_loss      | -21       |
|    critic_loss     | 1.31      |
|    ent_coef        | 0.0115    |
|    ent_coef_loss   | -2.33     |
|    learning_rate   | 0.000815  |
|    n_updates       | 3194324   |
----------------------------------
Eval num_timesteps=1860000, episode_reward=2311.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 1.61     |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | -3.2     |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 955      |
| time/              |          |
|    episodes        | 372      |
|    fps             | 36       |
|    time_elapsed    | 50640    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2360.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 2.33     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 8.06     |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=323.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -18.1    |
|    critic_loss     | 0.69     |
|    ent_coef        | 0.00992  |
|    ent_coef_loss   | 1.55     |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 922      |
| time/              |          |
|    episodes        | 376      |
|    fps             | 36       |
|    time_elapsed    | 51217    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2412.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -17.8    |
|    critic_loss     | 1.22     |
|    ent_coef        | 0.00957  |
|    ent_coef_loss   | 2.74     |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2430.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -17.4    |
|    critic_loss     | 0.721    |
|    ent_coef        | 0.00915  |
|    ent_coef_loss   | 0.528    |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 884      |
| time/              |          |
|    episodes        | 380      |
|    fps             | 36       |
|    time_elapsed    | 51799    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=2473.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 0.95     |
|    ent_coef        | 0.00819  |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=-1074.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 1920000   |
| train/             |           |
|    actor_loss      | -17.4     |
|    critic_loss     | 0.868     |
|    ent_coef        | 0.00675   |
|    ent_coef_loss   | -3.55     |
|    learning_rate   | 0.000808  |
|    n_updates       | 3264324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 875      |
| time/              |          |
|    episodes        | 384      |
|    fps             | 36       |
|    time_elapsed    | 52387    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2498.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 3.68     |
|    ent_coef        | 0.00782  |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2492.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -17.8    |
|    critic_loss     | 0.798    |
|    ent_coef        | 0.00801  |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 796      |
| time/              |          |
|    episodes        | 388      |
|    fps             | 36       |
|    time_elapsed    | 52952    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2499.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.00826  |
|    ent_coef_loss   | -5.14    |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2488.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.546    |
|    ent_coef        | 0.00739  |
|    ent_coef_loss   | 3.89     |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 768      |
| time/              |          |
|    episodes        | 392      |
|    fps             | 36       |
|    time_elapsed    | 53456    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=-1270.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.27e+03 |
| time/              |           |
|    total_timesteps | 1970000   |
| train/             |           |
|    actor_loss      | -17.1     |
|    critic_loss     | 0.498     |
|    ent_coef        | 0.00725   |
|    ent_coef_loss   | -0.897    |
|    learning_rate   | 0.000803  |
|    n_updates       | 3314324   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=2432.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -15.3    |
|    critic_loss     | 6.79     |
|    ent_coef        | 0.00669  |
|    ent_coef_loss   | -0.332   |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 805      |
| time/              |          |
|    episodes        | 396      |
|    fps             | 36       |
|    time_elapsed    | 53960    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=2502.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 0.793    |
|    ent_coef        | 0.00685  |
|    ent_coef_loss   | 6.72     |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2499.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 0.782    |
|    ent_coef        | 0.00645  |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 797      |
| time/              |          |
|    episodes        | 400      |
|    fps             | 36       |
|    time_elapsed    | 54459    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2546.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -18.6    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00765  |
|    ent_coef_loss   | 3.49     |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=-213.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 0.794    |
|    ent_coef        | 0.00723  |
|    ent_coef_loss   | -4.33    |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 829      |
| time/              |          |
|    episodes        | 404      |
|    fps             | 36       |
|    time_elapsed    | 54959    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2485.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | 0.57     |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=2485.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -18.3    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.00766  |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 847      |
| time/              |          |
|    episodes        | 408      |
|    fps             | 36       |
|    time_elapsed    | 55459    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2561.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.51     |
|    ent_coef        | 0.0073   |
|    ent_coef_loss   | -3       |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2500.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -19.8    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00846  |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 802      |
| time/              |          |
|    episodes        | 412      |
|    fps             | 36       |
|    time_elapsed    | 55964    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2506.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -17.7    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.008    |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=-213.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -19.4    |
|    critic_loss     | 0.59     |
|    ent_coef        | 0.00787  |
|    ent_coef_loss   | -2.7     |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 772      |
| time/              |          |
|    episodes        | 416      |
|    fps             | 36       |
|    time_elapsed    | 56473    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=-1305.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.31e+03 |
| time/              |           |
|    total_timesteps | 2090000   |
| train/             |           |
|    actor_loss      | -23.4     |
|    critic_loss     | 0.614     |
|    ent_coef        | 0.0069    |
|    ent_coef_loss   | -1.68     |
|    learning_rate   | 0.000791  |
|    n_updates       | 3434324   |
----------------------------------
Eval num_timesteps=2100000, episode_reward=2557.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -20.2    |
|    critic_loss     | 0.506    |
|    ent_coef        | 0.00789  |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 680      |
| time/              |          |
|    episodes        | 420      |
|    fps             | 36       |
|    time_elapsed    | 56982    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2523.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -20.7    |
|    critic_loss     | 0.93     |
|    ent_coef        | 0.00781  |
|    ent_coef_loss   | 0.844    |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=-1483.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+03 |
| time/              |           |
|    total_timesteps | 2120000   |
| train/             |           |
|    actor_loss      | -20.1     |
|    critic_loss     | 0.538     |
|    ent_coef        | 0.00742   |
|    ent_coef_loss   | -2.9      |
|    learning_rate   | 0.000788  |
|    n_updates       | 3464324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 647      |
| time/              |          |
|    episodes        | 424      |
|    fps             | 36       |
|    time_elapsed    | 57490    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2536.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -16.6    |
|    critic_loss     | 0.716    |
|    ent_coef        | 0.0082   |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=2426.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00757  |
|    ent_coef_loss   | -0.0105  |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 648      |
| time/              |          |
|    episodes        | 428      |
|    fps             | 36       |
|    time_elapsed    | 57997    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=-314.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -17.3    |
|    critic_loss     | 0.581    |
|    ent_coef        | 0.00684  |
|    ent_coef_loss   | 2.77     |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=-1792.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.79e+03 |
| time/              |           |
|    total_timesteps | 2160000   |
| train/             |           |
|    actor_loss      | -14.7     |
|    critic_loss     | 1.54      |
|    ent_coef        | 0.00751   |
|    ent_coef_loss   | 1.24      |
|    learning_rate   | 0.000784  |
|    n_updates       | 3504324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 660      |
| time/              |          |
|    episodes        | 432      |
|    fps             | 36       |
|    time_elapsed    | 58527    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=-1376.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.38e+03 |
| time/              |           |
|    total_timesteps | 2170000   |
| train/             |           |
|    actor_loss      | -12.3     |
|    critic_loss     | 0.433     |
|    ent_coef        | 0.00686   |
|    ent_coef_loss   | 1.27      |
|    learning_rate   | 0.000783  |
|    n_updates       | 3514324   |
----------------------------------
Eval num_timesteps=2180000, episode_reward=-195.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -14.9    |
|    critic_loss     | 0.466    |
|    ent_coef        | 0.00735  |
|    ent_coef_loss   | -1.1     |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 539      |
| time/              |          |
|    episodes        | 436      |
|    fps             | 36       |
|    time_elapsed    | 59071    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=-262.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.316    |
|    ent_coef        | 0.00782  |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=1819.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -16.1    |
|    critic_loss     | 2.16     |
|    ent_coef        | 0.00787  |
|    ent_coef_loss   | 0.0405   |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 502      |
| time/              |          |
|    episodes        | 440      |
|    fps             | 36       |
|    time_elapsed    | 59580    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=-465.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -465     |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.525    |
|    ent_coef        | 0.00732  |
|    ent_coef_loss   | 0.879    |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=-1348.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.35e+03 |
| time/              |           |
|    total_timesteps | 2220000   |
| train/             |           |
|    actor_loss      | -11.5     |
|    critic_loss     | 3.38      |
|    ent_coef        | 0.00786   |
|    ent_coef_loss   | -1.88     |
|    learning_rate   | 0.000778  |
|    n_updates       | 3564324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 533      |
| time/              |          |
|    episodes        | 444      |
|    fps             | 36       |
|    time_elapsed    | 60089    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=-1463.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.46e+03 |
| time/              |           |
|    total_timesteps | 2230000   |
| train/             |           |
|    actor_loss      | -7.23     |
|    critic_loss     | 1.47      |
|    ent_coef        | 0.00987   |
|    ent_coef_loss   | -2.19     |
|    learning_rate   | 0.000777  |
|    n_updates       | 3574324   |
----------------------------------
Eval num_timesteps=2240000, episode_reward=-1785.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.79e+03 |
| time/              |           |
|    total_timesteps | 2240000   |
| train/             |           |
|    actor_loss      | -10.3     |
|    critic_loss     | 1.21      |
|    ent_coef        | 0.00957   |
|    ent_coef_loss   | -1.41     |
|    learning_rate   | 0.000776  |
|    n_updates       | 3584324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 541      |
| time/              |          |
|    episodes        | 448      |
|    fps             | 36       |
|    time_elapsed    | 60598    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=-320.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -320     |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -6.4     |
|    critic_loss     | 1.08     |
|    ent_coef        | 0.0097   |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2656.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -3.92    |
|    critic_loss     | 2.44     |
|    ent_coef        | 0.00979  |
|    ent_coef_loss   | -0.463   |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 579      |
| time/              |          |
|    episodes        | 452      |
|    fps             | 36       |
|    time_elapsed    | 61109    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2623.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -6.31    |
|    critic_loss     | 3.32     |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | -0.108   |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=2658.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -4.33    |
|    critic_loss     | 2.92     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | 3.81     |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 585      |
| time/              |          |
|    episodes        | 456      |
|    fps             | 37       |
|    time_elapsed    | 61618    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2686.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -6.76    |
|    critic_loss     | 2.99     |
|    ent_coef        | 0.00916  |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=2601.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -7.12    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | 4.25     |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 628      |
| time/              |          |
|    episodes        | 460      |
|    fps             | 37       |
|    time_elapsed    | 62128    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=-996.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -996     |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -7.58    |
|    critic_loss     | 2.88     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 4.57     |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2693.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -6.63    |
|    critic_loss     | 4.41     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 615      |
| time/              |          |
|    episodes        | 464      |
|    fps             | 37       |
|    time_elapsed    | 62639    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=2428.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 4.13     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=1846.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -7.24    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 602      |
| time/              |          |
|    episodes        | 468      |
|    fps             | 37       |
|    time_elapsed    | 63149    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=2747.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -5.83    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | 4.25     |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=-1067.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 2360000   |
| train/             |           |
|    actor_loss      | -6.24     |
|    critic_loss     | 7.11      |
|    ent_coef        | 0.0127    |
|    ent_coef_loss   | -1.94     |
|    learning_rate   | 0.000764  |
|    n_updates       | 3704324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 715      |
| time/              |          |
|    episodes        | 472      |
|    fps             | 37       |
|    time_elapsed    | 63657    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=2450.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -5.09    |
|    critic_loss     | 0.968    |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=-308.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -1.86    |
|    critic_loss     | 2.5      |
|    ent_coef        | 0.0137   |
|    ent_coef_loss   | -4.3     |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 699      |
| time/              |          |
|    episodes        | 476      |
|    fps             | 37       |
|    time_elapsed    | 64166    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=-306.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -4.5     |
|    critic_loss     | 5.84     |
|    ent_coef        | 0.0135   |
|    ent_coef_loss   | -0.0704  |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2792.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -3.34    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 735      |
| time/              |          |
|    episodes        | 480      |
|    fps             | 37       |
|    time_elapsed    | 64673    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=-279.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -3.65    |
|    critic_loss     | 3.21     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | -3.59    |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=-296.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | 0.378    |
|    critic_loss     | 2.2      |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 690      |
| time/              |          |
|    episodes        | 484      |
|    fps             | 37       |
|    time_elapsed    | 65179    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=-589.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -4.14    |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | 0.15     |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=2648.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 3.24     |
|    ent_coef        | 0.0118   |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 488      |
|    fps             | 37       |
|    time_elapsed    | 65688    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2618.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | 1.74     |
|    critic_loss     | 3.91     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=165.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | -2.58    |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 715      |
| time/              |          |
|    episodes        | 492      |
|    fps             | 37       |
|    time_elapsed    | 66196    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=2591.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 3.76     |
|    ent_coef        | 0.0112   |
|    ent_coef_loss   | 1.39     |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=2691.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | 0.27     |
|    critic_loss     | 2.25     |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 718      |
| time/              |          |
|    episodes        | 496      |
|    fps             | 37       |
|    time_elapsed    | 66704    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=-1029.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 2490000   |
| train/             |           |
|    actor_loss      | 0.776     |
|    critic_loss     | 1.07      |
|    ent_coef        | 0.0115    |
|    ent_coef_loss   | -0.61     |
|    learning_rate   | 0.000751  |
|    n_updates       | 3834324   |
----------------------------------
Eval num_timesteps=2500000, episode_reward=-1069.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 2500000   |
| train/             |           |
|    actor_loss      | -1.6      |
|    critic_loss     | 1.07      |
|    ent_coef        | 0.0113    |
|    ent_coef_loss   | 0.54      |
|    learning_rate   | 0.00075   |
|    n_updates       | 3844324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 780      |
| time/              |          |
|    episodes        | 500      |
|    fps             | 37       |
|    time_elapsed    | 67212    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=-304.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 1.19     |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | -0.255   |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=-307.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 4.45     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | 1.89     |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 707      |
| time/              |          |
|    episodes        | 504      |
|    fps             | 37       |
|    time_elapsed    | 67722    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=-307.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 5.15     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=-308.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -4.13    |
|    critic_loss     | 3.64     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | -4.22    |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 637      |
| time/              |          |
|    episodes        | 508      |
|    fps             | 37       |
|    time_elapsed    | 68232    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2753.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -5.95    |
|    critic_loss     | 3.6      |
|    ent_coef        | 0.0119   |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=-311.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | 0.000202 |
|    critic_loss     | 1.18     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 637      |
| time/              |          |
|    episodes        | 512      |
|    fps             | 37       |
|    time_elapsed    | 68739    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=2768.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -0.861   |
|    critic_loss     | 2.64     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | -0.199   |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2814.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -3.77    |
|    critic_loss     | 1        |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 0.492    |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 732      |
| time/              |          |
|    episodes        | 516      |
|    fps             | 37       |
|    time_elapsed    | 69242    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=2806.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.88     |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | 0.557    |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2814.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 1.19     |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | 3.46     |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 837      |
| time/              |          |
|    episodes        | 520      |
|    fps             | 37       |
|    time_elapsed    | 69745    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=2857.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.646    |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | -0.279   |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=2824.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.0112   |
|    ent_coef_loss   | 1.78     |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 946      |
| time/              |          |
|    episodes        | 524      |
|    fps             | 37       |
|    time_elapsed    | 70253    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=2855.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -14.5    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | -0.115   |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=1132.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 37       |
|    time_elapsed    | 70761    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2847.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -15      |
|    critic_loss     | 1.62     |
|    ent_coef        | 0.00889  |
|    ent_coef_loss   | 2.87     |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2856.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 2.51     |
|    ent_coef        | 0.00891  |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 37       |
|    time_elapsed    | 71272    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=2819.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -13.6    |
|    critic_loss     | 3.05     |
|    ent_coef        | 0.00942  |
|    ent_coef_loss   | -0.958   |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=2660.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -17.5    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.00973  |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 37       |
|    time_elapsed    | 71780    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=-1242.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.24e+03 |
| time/              |           |
|    total_timesteps | 2690000   |
| train/             |           |
|    actor_loss      | -16.8     |
|    critic_loss     | 2.37      |
|    ent_coef        | 0.0106    |
|    ent_coef_loss   | 0.739     |
|    learning_rate   | 0.000731  |
|    n_updates       | 4034324   |
----------------------------------
Eval num_timesteps=2700000, episode_reward=-1795.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.8e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -17.7    |
|    critic_loss     | 2.12     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | 4.55     |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    episodes        | 540      |
|    fps             | 37       |
|    time_elapsed    | 72290    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=-1614.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.61e+03 |
| time/              |           |
|    total_timesteps | 2710000   |
| train/             |           |
|    actor_loss      | -13.8     |
|    critic_loss     | 1.94      |
|    ent_coef        | 0.0115    |
|    ent_coef_loss   | -1.07     |
|    learning_rate   | 0.000729  |
|    n_updates       | 4054324   |
----------------------------------
Eval num_timesteps=2720000, episode_reward=-307.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 1.46     |
|    ent_coef        | 0.0112   |
|    ent_coef_loss   | 0.612    |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 37       |
|    time_elapsed    | 72799    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=-1471.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 2730000   |
| train/             |           |
|    actor_loss      | -14.5     |
|    critic_loss     | 1.51      |
|    ent_coef        | 0.0107    |
|    ent_coef_loss   | -1.06     |
|    learning_rate   | 0.000727  |
|    n_updates       | 4074324   |
----------------------------------
Eval num_timesteps=2740000, episode_reward=-306.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 1.2      |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 37       |
|    time_elapsed    | 73308    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=-1735.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.74e+03 |
| time/              |           |
|    total_timesteps | 2750000   |
| train/             |           |
|    actor_loss      | -12.3     |
|    critic_loss     | 4.39      |
|    ent_coef        | 0.0103    |
|    ent_coef_loss   | 4.99      |
|    learning_rate   | 0.000725  |
|    n_updates       | 4094324   |
----------------------------------
Eval num_timesteps=2760000, episode_reward=2814.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -2.81    |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 552      |
|    fps             | 37       |
|    time_elapsed    | 73813    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=178.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -9.74    |
|    critic_loss     | 4.52     |
|    ent_coef        | 0.0108   |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=-578.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 1.79     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 7.33     |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    episodes        | 556      |
|    fps             | 37       |
|    time_elapsed    | 74326    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=182.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.876    |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | -0.711   |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2731.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.0109   |
|    ent_coef_loss   | 0.964    |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 37       |
|    time_elapsed    | 74836    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=-309.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 1.14     |
|    ent_coef        | 0.0109   |
|    ent_coef_loss   | -5.96    |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=-1281.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.28e+03 |
| time/              |           |
|    total_timesteps | 2820000   |
| train/             |           |
|    actor_loss      | -8.4      |
|    critic_loss     | 7.12      |
|    ent_coef        | 0.00942   |
|    ent_coef_loss   | 6.65      |
|    learning_rate   | 0.000718  |
|    n_updates       | 4164324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 37       |
|    time_elapsed    | 75346    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=-310.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -8.04    |
|    critic_loss     | 1.7      |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | -1.39    |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=-309.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -8.71    |
|    critic_loss     | 3.43     |
|    ent_coef        | 0.0109   |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 960      |
| time/              |          |
|    episodes        | 568      |
|    fps             | 37       |
|    time_elapsed    | 75859    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=-309.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -9.47    |
|    critic_loss     | 1.62     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | -5.41    |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=-331.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -5.85    |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | 0.929    |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 851      |
| time/              |          |
|    episodes        | 572      |
|    fps             | 37       |
|    time_elapsed    | 76371    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=-311.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -8.76    |
|    critic_loss     | 2.25     |
|    ent_coef        | 0.00971  |
|    ent_coef_loss   | -2.93    |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=-310.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -8.47    |
|    critic_loss     | 0.785    |
|    ent_coef        | 0.00846  |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 816      |
| time/              |          |
|    episodes        | 576      |
|    fps             | 37       |
|    time_elapsed    | 76883    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=-977.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -978     |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -9.85    |
|    critic_loss     | 0.897    |
|    ent_coef        | 0.00882  |
|    ent_coef_loss   | -0.386   |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=-302.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.652    |
|    ent_coef        | 0.00804  |
|    ent_coef_loss   | -0.657   |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 768      |
| time/              |          |
|    episodes        | 580      |
|    fps             | 37       |
|    time_elapsed    | 77393    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=-309.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -6.85    |
|    critic_loss     | 1.1      |
|    ent_coef        | 0.00772  |
|    ent_coef_loss   | 2.62     |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=-250.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -4.75    |
|    critic_loss     | 0.643    |
|    ent_coef        | 0.00818  |
|    ent_coef_loss   | -7.11    |
|    learning_rate   | 0.000708 |
|    n_updates       | 4264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 761      |
| time/              |          |
|    episodes        | 584      |
|    fps             | 37       |
|    time_elapsed    | 77906    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=-304.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 2.66     |
|    ent_coef        | 0.00801  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.000707 |
|    n_updates       | 4274324  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=-299.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -5.68    |
|    critic_loss     | 0.915    |
|    ent_coef        | 0.00788  |
|    ent_coef_loss   | -2.41    |
|    learning_rate   | 0.000706 |
|    n_updates       | 4284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 739      |
| time/              |          |
|    episodes        | 588      |
|    fps             | 37       |
|    time_elapsed    | 78415    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=-306.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -4.38    |
|    critic_loss     | 0.829    |
|    ent_coef        | 0.00735  |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000705 |
|    n_updates       | 4294324  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=-308.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.00899  |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000704 |
|    n_updates       | 4304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 675      |
| time/              |          |
|    episodes        | 592      |
|    fps             | 37       |
|    time_elapsed    | 78926    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=-308.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -7.48    |
|    critic_loss     | 0.668    |
|    ent_coef        | 0.00819  |
|    ent_coef_loss   | -8.52    |
|    learning_rate   | 0.000703 |
|    n_updates       | 4314324  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=-301.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 1.03     |
|    ent_coef        | 0.00692  |
|    ent_coef_loss   | 0.15     |
|    learning_rate   | 0.000702 |
|    n_updates       | 4324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 611      |
| time/              |          |
|    episodes        | 596      |
|    fps             | 37       |
|    time_elapsed    | 79437    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=-1021.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 2990000   |
| train/             |           |
|    actor_loss      | -7.43     |
|    critic_loss     | 1.19      |
|    ent_coef        | 0.00795   |
|    ent_coef_loss   | -2.56     |
|    learning_rate   | 0.000701  |
|    n_updates       | 4334324   |
----------------------------------
Eval num_timesteps=3000000, episode_reward=-309.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 3.63     |
|    ent_coef        | 0.00837  |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.0007   |
|    n_updates       | 4344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 518      |
| time/              |          |
|    episodes        | 600      |
|    fps             | 37       |
|    time_elapsed    | 79950    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=-302.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -15.3    |
|    critic_loss     | 0.609    |
|    ent_coef        | 0.00864  |
|    ent_coef_loss   | -0.519   |
|    learning_rate   | 0.000699 |
|    n_updates       | 4354324  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=-298.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 2.91     |
|    ent_coef        | 0.00906  |
|    ent_coef_loss   | 0.474    |
|    learning_rate   | 0.000698 |
|    n_updates       | 4364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 537      |
| time/              |          |
|    episodes        | 604      |
|    fps             | 37       |
|    time_elapsed    | 80460    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=-1008.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 3030000   |
| train/             |           |
|    actor_loss      | -10.1     |
|    critic_loss     | 1.64      |
|    ent_coef        | 0.00776   |
|    ent_coef_loss   | -0.0658   |
|    learning_rate   | 0.000697  |
|    n_updates       | 4374324   |
----------------------------------
Eval num_timesteps=3040000, episode_reward=-1023.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 3040000   |
| train/             |           |
|    actor_loss      | -12.3     |
|    critic_loss     | 4.2       |
|    ent_coef        | 0.00812   |
|    ent_coef_loss   | 2.39      |
|    learning_rate   | 0.000696  |
|    n_updates       | 4384324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 538      |
| time/              |          |
|    episodes        | 608      |
|    fps             | 37       |
|    time_elapsed    | 80970    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=-1057.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.06e+03 |
| time/              |           |
|    total_timesteps | 3050000   |
| train/             |           |
|    actor_loss      | -10.2     |
|    critic_loss     | 0.744     |
|    ent_coef        | 0.00851   |
|    ent_coef_loss   | 0.838     |
|    learning_rate   | 0.000695  |
|    n_updates       | 4394324   |
----------------------------------
Eval num_timesteps=3060000, episode_reward=-1773.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.77e+03 |
| time/              |           |
|    total_timesteps | 3060000   |
| train/             |           |
|    actor_loss      | -14.5     |
|    critic_loss     | 0.482     |
|    ent_coef        | 0.00827   |
|    ent_coef_loss   | -0.0874   |
|    learning_rate   | 0.000694  |
|    n_updates       | 4404324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 479      |
| time/              |          |
|    episodes        | 612      |
|    fps             | 37       |
|    time_elapsed    | 81500    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=-1832.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.83e+03 |
| time/              |           |
|    total_timesteps | 3070000   |
| train/             |           |
|    actor_loss      | -16.2     |
|    critic_loss     | 0.493     |
|    ent_coef        | 0.00882   |
|    ent_coef_loss   | 1.93      |
|    learning_rate   | 0.000693  |
|    n_updates       | 4414324   |
----------------------------------
Eval num_timesteps=3080000, episode_reward=-1578.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.58e+03 |
| time/              |           |
|    total_timesteps | 3080000   |
| train/             |           |
|    actor_loss      | -9.41     |
|    critic_loss     | 1.12      |
|    ent_coef        | 0.00878   |
|    ent_coef_loss   | 3.12      |
|    learning_rate   | 0.000692  |
|    n_updates       | 4424324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 312      |
| time/              |          |
|    episodes        | 616      |
|    fps             | 37       |
|    time_elapsed    | 82110    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=-282.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.00778  |
|    ent_coef_loss   | -0.341   |
|    learning_rate   | 0.000691 |
|    n_updates       | 4434324  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=-329.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -8.59    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00841  |
|    ent_coef_loss   | -5.54    |
|    learning_rate   | 0.00069  |
|    n_updates       | 4444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 181      |
| time/              |          |
|    episodes        | 620      |
|    fps             | 37       |
|    time_elapsed    | 82695    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=-423.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.624    |
|    ent_coef        | 0.00808  |
|    ent_coef_loss   | -4.38    |
|    learning_rate   | 0.000689 |
|    n_updates       | 4454324  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=-295.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 1.08     |
|    ent_coef        | 0.00792  |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000688 |
|    n_updates       | 4464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 41.7     |
| time/              |          |
|    episodes        | 624      |
|    fps             | 37       |
|    time_elapsed    | 83279    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=-925.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.511    |
|    ent_coef        | 0.00782  |
|    ent_coef_loss   | -4.48    |
|    learning_rate   | 0.000687 |
|    n_updates       | 4474324  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=-205.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -17.4    |
|    critic_loss     | 1.55     |
|    ent_coef        | 0.00762  |
|    ent_coef_loss   | 5.06     |
|    learning_rate   | 0.000686 |
|    n_updates       | 4484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -97.2    |
| time/              |          |
|    episodes        | 628      |
|    fps             | 37       |
|    time_elapsed    | 83884    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=-1111.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+03 |
| time/              |           |
|    total_timesteps | 3150000   |
| train/             |           |
|    actor_loss      | -10.1     |
|    critic_loss     | 0.96      |
|    ent_coef        | 0.00916   |
|    ent_coef_loss   | 2.28      |
|    learning_rate   | 0.000685  |
|    n_updates       | 4494324   |
----------------------------------
Eval num_timesteps=3160000, episode_reward=-987.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -987     |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -16      |
|    critic_loss     | 1.64     |
|    ent_coef        | 0.00931  |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000684 |
|    n_updates       | 4504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -238     |
| time/              |          |
|    episodes        | 632      |
|    fps             | 37       |
|    time_elapsed    | 84496    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=-302.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -18.4    |
|    critic_loss     | 0.771    |
|    ent_coef        | 0.00762  |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.000683 |
|    n_updates       | 4514324  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=-1263.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.26e+03 |
| time/              |           |
|    total_timesteps | 3180000   |
| train/             |           |
|    actor_loss      | -7.3      |
|    critic_loss     | 4.52      |
|    ent_coef        | 0.0101    |
|    ent_coef_loss   | -2.22     |
|    learning_rate   | 0.000682  |
|    n_updates       | 4524324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -352     |
| time/              |          |
|    episodes        | 636      |
|    fps             | 37       |
|    time_elapsed    | 85084    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=-303.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 5.62     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | 36.4     |
|    learning_rate   | 0.000681 |
|    n_updates       | 4534324  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=-290.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 6.53     |
|    ent_coef        | 0.0164   |
|    ent_coef_loss   | -0.969   |
|    learning_rate   | 0.00068  |
|    n_updates       | 4544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -372     |
| time/              |          |
|    episodes        | 640      |
|    fps             | 37       |
|    time_elapsed    | 85665    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=-289.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -4.7     |
|    critic_loss     | 28.2     |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | 0.399    |
|    learning_rate   | 0.000679 |
|    n_updates       | 4554324  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=-310.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 1.27     |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | -3.19    |
|    learning_rate   | 0.000678 |
|    n_updates       | 4564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -337     |
| time/              |          |
|    episodes        | 644      |
|    fps             | 37       |
|    time_elapsed    | 86285    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=-308.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 3.64     |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | -0.759   |
|    learning_rate   | 0.000677 |
|    n_updates       | 4574324  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=-310.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -15.9    |
|    critic_loss     | 1.02     |
|    ent_coef        | 0.00925  |
|    ent_coef_loss   | 0.304    |
|    learning_rate   | 0.000676 |
|    n_updates       | 4584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -337     |
| time/              |          |
|    episodes        | 648      |
|    fps             | 37       |
|    time_elapsed    | 86904    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=-309.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.0076   |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 0.000675 |
|    n_updates       | 4594324  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=-308.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 1.69     |
|    ent_coef        | 0.00718  |
|    ent_coef_loss   | -2.4     |
|    learning_rate   | 0.000674 |
|    n_updates       | 4604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -370     |
| time/              |          |
|    episodes        | 652      |
|    fps             | 37       |
|    time_elapsed    | 87505    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=-307.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.48     |
|    ent_coef        | 0.00696  |
|    ent_coef_loss   | -4.22    |
|    learning_rate   | 0.000673 |
|    n_updates       | 4614324  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=-307.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -7.16    |
|    critic_loss     | 0.327    |
|    ent_coef        | 0.0069   |
|    ent_coef_loss   | -3.69    |
|    learning_rate   | 0.000672 |
|    n_updates       | 4624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -434     |
| time/              |          |
|    episodes        | 656      |
|    fps             | 37       |
|    time_elapsed    | 88088    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=-308.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.721    |
|    ent_coef        | 0.00783  |
|    ent_coef_loss   | -2.74    |
|    learning_rate   | 0.000671 |
|    n_updates       | 4634324  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=-301.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -8.83    |
|    critic_loss     | 0.222    |
|    ent_coef        | 0.00865  |
|    ent_coef_loss   | 0.643    |
|    learning_rate   | 0.00067  |
|    n_updates       | 4644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -498     |
| time/              |          |
|    episodes        | 660      |
|    fps             | 37       |
|    time_elapsed    | 88706    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=-305.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -5.89    |
|    critic_loss     | 0.276    |
|    ent_coef        | 0.00798  |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.000669 |
|    n_updates       | 4654324  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=-305.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -6.38    |
|    critic_loss     | 0.457    |
|    ent_coef        | 0.0076   |
|    ent_coef_loss   | -1.78    |
|    learning_rate   | 0.000668 |
|    n_updates       | 4664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -531     |
| time/              |          |
|    episodes        | 664      |
|    fps             | 37       |
|    time_elapsed    | 89313    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=-307.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 0.382    |
|    ent_coef        | 0.00675  |
|    ent_coef_loss   | -1.5     |
|    learning_rate   | 0.000667 |
|    n_updates       | 4674324  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=-308.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -3.35    |
|    critic_loss     | 0.461    |
|    ent_coef        | 0.00578  |
|    ent_coef_loss   | -3.7     |
|    learning_rate   | 0.000666 |
|    n_updates       | 4684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -508     |
| time/              |          |
|    episodes        | 668      |
|    fps             | 37       |
|    time_elapsed    | 89890    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=-306.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.239    |
|    ent_coef        | 0.00609  |
|    ent_coef_loss   | -6.5     |
|    learning_rate   | 0.000665 |
|    n_updates       | 4694324  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=-702.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -702     |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.264    |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | -5.84    |
|    learning_rate   | 0.000664 |
|    n_updates       | 4704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -513     |
| time/              |          |
|    episodes        | 672      |
|    fps             | 37       |
|    time_elapsed    | 90459    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=-473.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -473     |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -1.71    |
|    critic_loss     | 0.366    |
|    ent_coef        | 0.00332  |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.000663 |
|    n_updates       | 4714324  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=-1756.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.76e+03 |
| time/              |           |
|    total_timesteps | 3380000   |
| train/             |           |
|    actor_loss      | -1.59     |
|    critic_loss     | 1.87      |
|    ent_coef        | 0.00478   |
|    ent_coef_loss   | -6.58     |
|    learning_rate   | 0.000662  |
|    n_updates       | 4724324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -555     |
| time/              |          |
|    episodes        | 676      |
|    fps             | 37       |
|    time_elapsed    | 91034    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=-1630.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+03 |
| time/              |           |
|    total_timesteps | 3390000   |
| train/             |           |
|    actor_loss      | -6.39     |
|    critic_loss     | 0.571     |
|    ent_coef        | 0.00542   |
|    ent_coef_loss   | 2.06      |
|    learning_rate   | 0.000661  |
|    n_updates       | 4734324   |
----------------------------------
Eval num_timesteps=3400000, episode_reward=-1202.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+03 |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | 0.365    |
|    critic_loss     | 0.398    |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | -7.42    |
|    learning_rate   | 0.00066  |
|    n_updates       | 4744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -603     |
| time/              |          |
|    episodes        | 680      |
|    fps             | 37       |
|    time_elapsed    | 91609    |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=-1045.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.05e+03 |
| time/              |           |
|    total_timesteps | 3410000   |
| train/             |           |
|    actor_loss      | -5.31     |
|    critic_loss     | 0.599     |
|    ent_coef        | 0.00525   |
|    ent_coef_loss   | -6.08     |
|    learning_rate   | 0.000659  |
|    n_updates       | 4754324   |
----------------------------------
Eval num_timesteps=3420000, episode_reward=-308.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.655    |
|    ent_coef        | 0.00521  |
|    ent_coef_loss   | 5.81     |
|    learning_rate   | 0.000658 |
|    n_updates       | 4764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -631     |
| time/              |          |
|    episodes        | 684      |
|    fps             | 37       |
|    time_elapsed    | 92187    |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=-307.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -0.837   |
|    critic_loss     | 0.355    |
|    ent_coef        | 0.00483  |
|    ent_coef_loss   | -4.17    |
|    learning_rate   | 0.000657 |
|    n_updates       | 4774324  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=-1072.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 3440000   |
| train/             |           |
|    actor_loss      | 1.37      |
|    critic_loss     | 2.32      |
|    ent_coef        | 0.00436   |
|    ent_coef_loss   | -0.271    |
|    learning_rate   | 0.000656  |
|    n_updates       | 4784324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -640     |
| time/              |          |
|    episodes        | 688      |
|    fps             | 37       |
|    time_elapsed    | 92765    |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=-306.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -0.131   |
|    critic_loss     | 1.42     |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 5.37     |
|    learning_rate   | 0.000655 |
|    n_updates       | 4794324  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=1198.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -0.344   |
|    critic_loss     | 9.55     |
|    ent_coef        | 0.00357  |
|    ent_coef_loss   | 7.51     |
|    learning_rate   | 0.000654 |
|    n_updates       | 4804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -684     |
| time/              |          |
|    episodes        | 692      |
|    fps             | 37       |
|    time_elapsed    | 93343    |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=-858.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -858     |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -1.9     |
|    critic_loss     | 5.14     |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.000653 |
|    n_updates       | 4814324  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=-1657.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.66e+03 |
| time/              |           |
|    total_timesteps | 3480000   |
| train/             |           |
|    actor_loss      | -0.135    |
|    critic_loss     | 0.601     |
|    ent_coef        | 0.00431   |
|    ent_coef_loss   | -10.6     |
|    learning_rate   | 0.000652  |
|    n_updates       | 4824324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -708     |
| time/              |          |
|    episodes        | 696      |
|    fps             | 37       |
|    time_elapsed    | 93924    |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=-1010.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 3490000   |
| train/             |           |
|    actor_loss      | 1.46      |
|    critic_loss     | 0.62      |
|    ent_coef        | 0.0037    |
|    ent_coef_loss   | 4.36      |
|    learning_rate   | 0.000651  |
|    n_updates       | 4834324   |
----------------------------------
Eval num_timesteps=3500000, episode_reward=470.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 3500000  |
| train/             |          |
|    actor_loss      | 2.31     |
|    critic_loss     | 0.83     |
|    ent_coef        | 0.00402  |
|    ent_coef_loss   | -6.84    |
|    learning_rate   | 0.00065  |
|    n_updates       | 4844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -734     |
| time/              |          |
|    episodes        | 700      |
|    fps             | 37       |
|    time_elapsed    | 94506    |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=-1846.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.85e+03 |
| time/              |           |
|    total_timesteps | 3510000   |
| train/             |           |
|    actor_loss      | 2.83      |
|    critic_loss     | 2.52      |
|    ent_coef        | 0.00426   |
|    ent_coef_loss   | -4.79     |
|    learning_rate   | 0.000649  |
|    n_updates       | 4854324   |
----------------------------------
Eval num_timesteps=3520000, episode_reward=-1937.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.94e+03 |
| time/              |           |
|    total_timesteps | 3520000   |
| train/             |           |
|    actor_loss      | 4.89      |
|    critic_loss     | 2.03      |
|    ent_coef        | 0.00439   |
|    ent_coef_loss   | -4.05     |
|    learning_rate   | 0.000648  |
|    n_updates       | 4864324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -753     |
| time/              |          |
|    episodes        | 704      |
|    fps             | 37       |
|    time_elapsed    | 95122    |
|    total_timesteps | 3520000  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=-1729.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.73e+03 |
| time/              |           |
|    total_timesteps | 3530000   |
| train/             |           |
|    actor_loss      | 1.61      |
|    critic_loss     | 2.51      |
|    ent_coef        | 0.00533   |
|    ent_coef_loss   | -2.79     |
|    learning_rate   | 0.000647  |
|    n_updates       | 4874324   |
----------------------------------
Eval num_timesteps=3540000, episode_reward=-1742.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.74e+03 |
| time/              |           |
|    total_timesteps | 3540000   |
| train/             |           |
|    actor_loss      | 3.64      |
|    critic_loss     | 0.686     |
|    ent_coef        | 0.00553   |
|    ent_coef_loss   | -1.29     |
|    learning_rate   | 0.000646  |
|    n_updates       | 4884324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -783     |
| time/              |          |
|    episodes        | 708      |
|    fps             | 36       |
|    time_elapsed    | 95763    |
|    total_timesteps | 3540000  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=2864.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | 2.02     |
|    critic_loss     | 4.49     |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.000645 |
|    n_updates       | 4894324  |
---------------------------------
New best mean reward!
Eval num_timesteps=3560000, episode_reward=2856.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.583    |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | 4.68     |
|    learning_rate   | 0.000644 |
|    n_updates       | 4904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -732     |
| time/              |          |
|    episodes        | 712      |
|    fps             | 36       |
|    time_elapsed    | 96361    |
|    total_timesteps | 3560000  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=-260.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | 1.05     |
|    critic_loss     | 0.738    |
|    ent_coef        | 0.00464  |
|    ent_coef_loss   | -6.74    |
|    learning_rate   | 0.000643 |
|    n_updates       | 4914324  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=2657.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | 3.45     |
|    critic_loss     | 12.3     |
|    ent_coef        | 0.00503  |
|    ent_coef_loss   | -2.09    |
|    learning_rate   | 0.000642 |
|    n_updates       | 4924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -695     |
| time/              |          |
|    episodes        | 716      |
|    fps             | 36       |
|    time_elapsed    | 96954    |
|    total_timesteps | 3580000  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=2777.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | 6.06     |
|    critic_loss     | 2.26     |
|    ent_coef        | 0.00428  |
|    ent_coef_loss   | -6.28    |
|    learning_rate   | 0.000641 |
|    n_updates       | 4934324  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=-1206.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.21e+03 |
| time/              |           |
|    total_timesteps | 3600000   |
| train/             |           |
|    actor_loss      | 3.12      |
|    critic_loss     | 3.84      |
|    ent_coef        | 0.00445   |
|    ent_coef_loss   | -1.56     |
|    learning_rate   | 0.00064   |
|    n_updates       | 4944324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -646     |
| time/              |          |
|    episodes        | 720      |
|    fps             | 36       |
|    time_elapsed    | 97540    |
|    total_timesteps | 3600000  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=-1713.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.71e+03 |
| time/              |           |
|    total_timesteps | 3610000   |
| train/             |           |
|    actor_loss      | 1.25      |
|    critic_loss     | 0.418     |
|    ent_coef        | 0.00462   |
|    ent_coef_loss   | 1.65      |
|    learning_rate   | 0.000639  |
|    n_updates       | 4954324   |
----------------------------------
Eval num_timesteps=3620000, episode_reward=3086.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 3620000  |
| train/             |          |
|    actor_loss      | 8.68     |
|    critic_loss     | 0.637    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.000638 |
|    n_updates       | 4964324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -629     |
| time/              |          |
|    episodes        | 724      |
|    fps             | 36       |
|    time_elapsed    | 98131    |
|    total_timesteps | 3620000  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=3249.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | 5.44     |
|    critic_loss     | 0.525    |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | 2.89     |
|    learning_rate   | 0.000637 |
|    n_updates       | 4974324  |
---------------------------------
New best mean reward!
Eval num_timesteps=3640000, episode_reward=-1492.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+03 |
| time/              |           |
|    total_timesteps | 3640000   |
| train/             |           |
|    actor_loss      | 5.21      |
|    critic_loss     | 0.556     |
|    ent_coef        | 0.00311   |
|    ent_coef_loss   | 2.83      |
|    learning_rate   | 0.000636  |
|    n_updates       | 4984324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -600     |
| time/              |          |
|    episodes        | 728      |
|    fps             | 36       |
|    time_elapsed    | 98717    |
|    total_timesteps | 3640000  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=-116.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | 8.63     |
|    critic_loss     | 0.536    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | -2.53    |
|    learning_rate   | 0.000635 |
|    n_updates       | 4994324  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=-593.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -594     |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | 8.09     |
|    critic_loss     | 2.59     |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | 5.2      |
|    learning_rate   | 0.000634 |
|    n_updates       | 5004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -600     |
| time/              |          |
|    episodes        | 732      |
|    fps             | 36       |
|    time_elapsed    | 99306    |
|    total_timesteps | 3660000  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=3123.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 3670000  |
| train/             |          |
|    actor_loss      | 8.58     |
|    critic_loss     | 0.474    |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.000633 |
|    n_updates       | 5014324  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=166.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 3680000  |
| train/             |          |
|    actor_loss      | 7.76     |
|    critic_loss     | 0.584    |
|    ent_coef        | 0.00303  |
|    ent_coef_loss   | 6.75     |
|    learning_rate   | 0.000632 |
|    n_updates       | 5024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -591     |
| time/              |          |
|    episodes        | 736      |
|    fps             | 36       |
|    time_elapsed    | 99894    |
|    total_timesteps | 3680000  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=2926.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 3690000  |
| train/             |          |
|    actor_loss      | 9.22     |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.00319  |
|    ent_coef_loss   | -0.0133  |
|    learning_rate   | 0.000631 |
|    n_updates       | 5034324  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=-1298.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.3e+03 |
| time/              |          |
|    total_timesteps | 3700000  |
| train/             |          |
|    actor_loss      | 8.37     |
|    critic_loss     | 4.11     |
|    ent_coef        | 0.00294  |
|    ent_coef_loss   | 2.37     |
|    learning_rate   | 0.00063  |
|    n_updates       | 5044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -582     |
| time/              |          |
|    episodes        | 740      |
|    fps             | 36       |
|    time_elapsed    | 100482   |
|    total_timesteps | 3700000  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=-1566.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+03 |
| time/              |           |
|    total_timesteps | 3710000   |
| train/             |           |
|    actor_loss      | 12        |
|    critic_loss     | 2.01      |
|    ent_coef        | 0.00237   |
|    ent_coef_loss   | -4.56     |
|    learning_rate   | 0.000629  |
|    n_updates       | 5054324   |
----------------------------------
Eval num_timesteps=3720000, episode_reward=-1618.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+03 |
| time/              |           |
|    total_timesteps | 3720000   |
| train/             |           |
|    actor_loss      | 11.8      |
|    critic_loss     | 0.483     |
|    ent_coef        | 0.00274   |
|    ent_coef_loss   | 0.961     |
|    learning_rate   | 0.000628  |
|    n_updates       | 5064324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -642     |
| time/              |          |
|    episodes        | 744      |
|    fps             | 36       |
|    time_elapsed    | 101071   |
|    total_timesteps | 3720000  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=-957.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -958     |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | 12.3     |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -4.04    |
|    learning_rate   | 0.000627 |
|    n_updates       | 5074324  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=1192.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | 11.4     |
|    critic_loss     | 0.988    |
|    ent_coef        | 0.00226  |
|    ent_coef_loss   | -6.52    |
|    learning_rate   | 0.000626 |
|    n_updates       | 5084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -661     |
| time/              |          |
|    episodes        | 748      |
|    fps             | 36       |
|    time_elapsed    | 101658   |
|    total_timesteps | 3740000  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=2343.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | 11.5     |
|    critic_loss     | 0.336    |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000625 |
|    n_updates       | 5094324  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=2517.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3760000  |
| train/             |          |
|    actor_loss      | 9.59     |
|    critic_loss     | 0.253    |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -0.503   |
|    learning_rate   | 0.000624 |
|    n_updates       | 5104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -606     |
| time/              |          |
|    episodes        | 752      |
|    fps             | 36       |
|    time_elapsed    | 102245   |
|    total_timesteps | 3760000  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=2392.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    actor_loss      | 10.8     |
|    critic_loss     | 0.735    |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | 5.48     |
|    learning_rate   | 0.000623 |
|    n_updates       | 5114324  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=2772.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3780000  |
| train/             |          |
|    actor_loss      | 11.3     |
|    critic_loss     | 1.15     |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | 6.07     |
|    learning_rate   | 0.000622 |
|    n_updates       | 5124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -497     |
| time/              |          |
|    episodes        | 756      |
|    fps             | 36       |
|    time_elapsed    | 102833   |
|    total_timesteps | 3780000  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=-595.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -596     |
| time/              |          |
|    total_timesteps | 3790000  |
| train/             |          |
|    actor_loss      | 8.63     |
|    critic_loss     | 6.44     |
|    ent_coef        | 0.00303  |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.000621 |
|    n_updates       | 5134324  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=-231.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | 8.91     |
|    critic_loss     | 0.918    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -6.73    |
|    learning_rate   | 0.00062  |
|    n_updates       | 5144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -478     |
| time/              |          |
|    episodes        | 760      |
|    fps             | 36       |
|    time_elapsed    | 103413   |
|    total_timesteps | 3800000  |
---------------------------------
Eval num_timesteps=3810000, episode_reward=-320.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -320     |
| time/              |          |
|    total_timesteps | 3810000  |
| train/             |          |
|    actor_loss      | 10.2     |
|    critic_loss     | 0.27     |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -1.54    |
|    learning_rate   | 0.000619 |
|    n_updates       | 5154324  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=2653.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3820000  |
| train/             |          |
|    actor_loss      | 9.37     |
|    critic_loss     | 0.29     |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.000618 |
|    n_updates       | 5164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -466     |
| time/              |          |
|    episodes        | 764      |
|    fps             | 36       |
|    time_elapsed    | 103991   |
|    total_timesteps | 3820000  |
---------------------------------
Eval num_timesteps=3830000, episode_reward=2951.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 3830000  |
| train/             |          |
|    actor_loss      | 9.53     |
|    critic_loss     | 0.203    |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000617 |
|    n_updates       | 5174324  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=-970.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -970     |
| time/              |          |
|    total_timesteps | 3840000  |
| train/             |          |
|    actor_loss      | 11.6     |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000616 |
|    n_updates       | 5184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -481     |
| time/              |          |
|    episodes        | 768      |
|    fps             | 36       |
|    time_elapsed    | 104562   |
|    total_timesteps | 3840000  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=-232.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 3850000  |
| train/             |          |
|    actor_loss      | 10.4     |
|    critic_loss     | 0.346    |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.000615 |
|    n_updates       | 5194324  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=-240.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 3860000  |
| train/             |          |
|    actor_loss      | 8.09     |
|    critic_loss     | 0.748    |
|    ent_coef        | 0.00353  |
|    ent_coef_loss   | 2.3      |
|    learning_rate   | 0.000614 |
|    n_updates       | 5204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -493     |
| time/              |          |
|    episodes        | 772      |
|    fps             | 36       |
|    time_elapsed    | 105143   |
|    total_timesteps | 3860000  |
---------------------------------
Eval num_timesteps=3870000, episode_reward=-216.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 3870000  |
| train/             |          |
|    actor_loss      | 11.1     |
|    critic_loss     | 0.358    |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | -7.64    |
|    learning_rate   | 0.000613 |
|    n_updates       | 5214324  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=-214.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 3880000  |
| train/             |          |
|    actor_loss      | 11.2     |
|    critic_loss     | 1.86     |
|    ent_coef        | 0.00334  |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000612 |
|    n_updates       | 5224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -445     |
| time/              |          |
|    episodes        | 776      |
|    fps             | 36       |
|    time_elapsed    | 105727   |
|    total_timesteps | 3880000  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=-199.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 3890000  |
| train/             |          |
|    actor_loss      | 11.2     |
|    critic_loss     | 0.486    |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | 5.75     |
|    learning_rate   | 0.000611 |
|    n_updates       | 5234324  |
---------------------------------
Eval num_timesteps=3900000, episode_reward=-1423.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+03 |
| time/              |           |
|    total_timesteps | 3900000   |
| train/             |           |
|    actor_loss      | 9.94      |
|    critic_loss     | 0.215     |
|    ent_coef        | 0.0039    |
|    ent_coef_loss   | 0.914     |
|    learning_rate   | 0.00061   |
|    n_updates       | 5244324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -402     |
| time/              |          |
|    episodes        | 780      |
|    fps             | 36       |
|    time_elapsed    | 106317   |
|    total_timesteps | 3900000  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=-193.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 3910000  |
| train/             |          |
|    actor_loss      | 10.5     |
|    critic_loss     | 2.49     |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | 4.45     |
|    learning_rate   | 0.000609 |
|    n_updates       | 5254324  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=-191.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 3920000  |
| train/             |          |
|    actor_loss      | 13.1     |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | -1.26    |
|    learning_rate   | 0.000608 |
|    n_updates       | 5264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -364     |
| time/              |          |
|    episodes        | 784      |
|    fps             | 36       |
|    time_elapsed    | 106904   |
|    total_timesteps | 3920000  |
---------------------------------
Eval num_timesteps=3930000, episode_reward=2925.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 3930000  |
| train/             |          |
|    actor_loss      | 12.7     |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | -6.33    |
|    learning_rate   | 0.000607 |
|    n_updates       | 5274324  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=-131.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 3940000  |
| train/             |          |
|    actor_loss      | 11.7     |
|    critic_loss     | 0.806    |
|    ent_coef        | 0.00355  |
|    ent_coef_loss   | 4.45     |
|    learning_rate   | 0.000606 |
|    n_updates       | 5284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -338     |
| time/              |          |
|    episodes        | 788      |
|    fps             | 36       |
|    time_elapsed    | 107497   |
|    total_timesteps | 3940000  |
---------------------------------
Eval num_timesteps=3950000, episode_reward=-89.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -89.6    |
| time/              |          |
|    total_timesteps | 3950000  |
| train/             |          |
|    actor_loss      | 11.7     |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.000605 |
|    n_updates       | 5294324  |
---------------------------------
Eval num_timesteps=3960000, episode_reward=2733.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3960000  |
| train/             |          |
|    actor_loss      | 10.8     |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00367  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000604 |
|    n_updates       | 5304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -289     |
| time/              |          |
|    episodes        | 792      |
|    fps             | 36       |
|    time_elapsed    | 108088   |
|    total_timesteps | 3960000  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=2686.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3970000  |
| train/             |          |
|    actor_loss      | 10.4     |
|    critic_loss     | 0.836    |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -0.78    |
|    learning_rate   | 0.000603 |
|    n_updates       | 5314324  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=-1218.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+03 |
| time/              |           |
|    total_timesteps | 3980000   |
| train/             |           |
|    actor_loss      | 8.69      |
|    critic_loss     | 0.486     |
|    ent_coef        | 0.00342   |
|    ent_coef_loss   | -4.56     |
|    learning_rate   | 0.000602  |
|    n_updates       | 5324324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 796      |
|    fps             | 36       |
|    time_elapsed    | 108682   |
|    total_timesteps | 3980000  |
---------------------------------
Eval num_timesteps=3990000, episode_reward=-200.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 3990000  |
| train/             |          |
|    actor_loss      | 11.8     |
|    critic_loss     | 0.239    |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | -1.76    |
|    learning_rate   | 0.000601 |
|    n_updates       | 5334324  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=2625.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 4000000  |
| train/             |          |
|    actor_loss      | 8.13     |
|    critic_loss     | 0.271    |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | 6.54     |
|    learning_rate   | 0.0006   |
|    n_updates       | 5344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -239     |
| time/              |          |
|    episodes        | 800      |
|    fps             | 36       |
|    time_elapsed    | 109274   |
|    total_timesteps | 4000000  |
---------------------------------
Eval num_timesteps=4010000, episode_reward=2773.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4010000  |
| train/             |          |
|    actor_loss      | 8.74     |
|    critic_loss     | 0.213    |
|    ent_coef        | 0.00374  |
|    ent_coef_loss   | -7.18    |
|    learning_rate   | 0.000599 |
|    n_updates       | 5354324  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=-205.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 4020000  |
| train/             |          |
|    actor_loss      | 5.31     |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | 3.33     |
|    learning_rate   | 0.000598 |
|    n_updates       | 5364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -175     |
| time/              |          |
|    episodes        | 804      |
|    fps             | 36       |
|    time_elapsed    | 109874   |
|    total_timesteps | 4020000  |
---------------------------------
Eval num_timesteps=4030000, episode_reward=-213.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 4030000  |
| train/             |          |
|    actor_loss      | 9.47     |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | 5.73     |
|    learning_rate   | 0.000597 |
|    n_updates       | 5374324  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=-40.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -40.5    |
| time/              |          |
|    total_timesteps | 4040000  |
| train/             |          |
|    actor_loss      | 11.4     |
|    critic_loss     | 0.639    |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000596 |
|    n_updates       | 5384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -117     |
| time/              |          |
|    episodes        | 808      |
|    fps             | 36       |
|    time_elapsed    | 110464   |
|    total_timesteps | 4040000  |
---------------------------------
Eval num_timesteps=4050000, episode_reward=-133.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 4050000  |
| train/             |          |
|    actor_loss      | 9.03     |
|    critic_loss     | 0.578    |
|    ent_coef        | 0.00425  |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.000595 |
|    n_updates       | 5394324  |
---------------------------------
Eval num_timesteps=4060000, episode_reward=-153.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | 9.86     |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000594 |
|    n_updates       | 5404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 812      |
|    fps             | 36       |
|    time_elapsed    | 111050   |
|    total_timesteps | 4060000  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=-1808.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.81e+03 |
| time/              |           |
|    total_timesteps | 4070000   |
| train/             |           |
|    actor_loss      | 7.69      |
|    critic_loss     | 0.463     |
|    ent_coef        | 0.00438   |
|    ent_coef_loss   | 0.109     |
|    learning_rate   | 0.000593  |
|    n_updates       | 5414324   |
----------------------------------
Eval num_timesteps=4080000, episode_reward=-641.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -642     |
| time/              |          |
|    total_timesteps | 4080000  |
| train/             |          |
|    actor_loss      | 8.18     |
|    critic_loss     | 5.8      |
|    ent_coef        | 0.00537  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000592 |
|    n_updates       | 5424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -151     |
| time/              |          |
|    episodes        | 816      |
|    fps             | 36       |
|    time_elapsed    | 111660   |
|    total_timesteps | 4080000  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=2406.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | 10       |
|    critic_loss     | 0.322    |
|    ent_coef        | 0.00442  |
|    ent_coef_loss   | -3.92    |
|    learning_rate   | 0.000591 |
|    n_updates       | 5434324  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=-1209.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.21e+03 |
| time/              |           |
|    total_timesteps | 4100000   |
| train/             |           |
|    actor_loss      | 9.85      |
|    critic_loss     | 0.888     |
|    ent_coef        | 0.00506   |
|    ent_coef_loss   | 0.479     |
|    learning_rate   | 0.00059   |
|    n_updates       | 5444324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 820      |
|    fps             | 36       |
|    time_elapsed    | 112261   |
|    total_timesteps | 4100000  |
---------------------------------
Eval num_timesteps=4110000, episode_reward=-441.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 4110000  |
| train/             |          |
|    actor_loss      | 12.4     |
|    critic_loss     | 5.15     |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | 0.646    |
|    learning_rate   | 0.000589 |
|    n_updates       | 5454324  |
---------------------------------
Eval num_timesteps=4120000, episode_reward=-244.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 4120000  |
| train/             |          |
|    actor_loss      | 11.1     |
|    critic_loss     | 1.12     |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | 2.23     |
|    learning_rate   | 0.000588 |
|    n_updates       | 5464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -252     |
| time/              |          |
|    episodes        | 824      |
|    fps             | 36       |
|    time_elapsed    | 112866   |
|    total_timesteps | 4120000  |
---------------------------------
Eval num_timesteps=4130000, episode_reward=-202.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 4130000  |
| train/             |          |
|    actor_loss      | 9.87     |
|    critic_loss     | 0.597    |
|    ent_coef        | 0.00539  |
|    ent_coef_loss   | 4.1      |
|    learning_rate   | 0.000587 |
|    n_updates       | 5474324  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=-241.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 4140000  |
| train/             |          |
|    actor_loss      | 8.17     |
|    critic_loss     | 0.266    |
|    ent_coef        | 0.00474  |
|    ent_coef_loss   | -0.0404  |
|    learning_rate   | 0.000586 |
|    n_updates       | 5484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -265     |
| time/              |          |
|    episodes        | 828      |
|    fps             | 36       |
|    time_elapsed    | 113460   |
|    total_timesteps | 4140000  |
---------------------------------
Eval num_timesteps=4150000, episode_reward=-832.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -832     |
| time/              |          |
|    total_timesteps | 4150000  |
| train/             |          |
|    actor_loss      | 4.25     |
|    critic_loss     | 2.37     |
|    ent_coef        | 0.00495  |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000585 |
|    n_updates       | 5494324  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=2814.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4160000  |
| train/             |          |
|    actor_loss      | 5.69     |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.00471  |
|    ent_coef_loss   | 0.0712   |
|    learning_rate   | 0.000584 |
|    n_updates       | 5504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -245     |
| time/              |          |
|    episodes        | 832      |
|    fps             | 36       |
|    time_elapsed    | 114050   |
|    total_timesteps | 4160000  |
---------------------------------
Eval num_timesteps=4170000, episode_reward=-131.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 4170000  |
| train/             |          |
|    actor_loss      | 4.17     |
|    critic_loss     | 0.686    |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | 5.77     |
|    learning_rate   | 0.000583 |
|    n_updates       | 5514324  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=2656.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 4180000  |
| train/             |          |
|    actor_loss      | 7.31     |
|    critic_loss     | 5.18     |
|    ent_coef        | 0.0054   |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.000582 |
|    n_updates       | 5524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 836      |
|    fps             | 36       |
|    time_elapsed    | 114649   |
|    total_timesteps | 4180000  |
---------------------------------
Eval num_timesteps=4190000, episode_reward=-211.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 4190000  |
| train/             |          |
|    actor_loss      | 3.77     |
|    critic_loss     | 1.18     |
|    ent_coef        | 0.00662  |
|    ent_coef_loss   | 2.61     |
|    learning_rate   | 0.000581 |
|    n_updates       | 5534324  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=2653.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4200000  |
| train/             |          |
|    actor_loss      | 8.28     |
|    critic_loss     | 3.44     |
|    ent_coef        | 0.00714  |
|    ent_coef_loss   | -2.22    |
|    learning_rate   | 0.00058  |
|    n_updates       | 5544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -183     |
| time/              |          |
|    episodes        | 840      |
|    fps             | 36       |
|    time_elapsed    | 115242   |
|    total_timesteps | 4200000  |
---------------------------------
Eval num_timesteps=4210000, episode_reward=-205.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 4210000  |
| train/             |          |
|    actor_loss      | 8.79     |
|    critic_loss     | 2.61     |
|    ent_coef        | 0.0086   |
|    ent_coef_loss   | -4.55    |
|    learning_rate   | 0.000579 |
|    n_updates       | 5554324  |
---------------------------------
Eval num_timesteps=4220000, episode_reward=-252.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 4220000  |
| train/             |          |
|    actor_loss      | 7.76     |
|    critic_loss     | 2.17     |
|    ent_coef        | 0.00797  |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000578 |
|    n_updates       | 5564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -130     |
| time/              |          |
|    episodes        | 844      |
|    fps             | 36       |
|    time_elapsed    | 115838   |
|    total_timesteps | 4220000  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=-217.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 4230000  |
| train/             |          |
|    actor_loss      | 9.47     |
|    critic_loss     | 31.1     |
|    ent_coef        | 0.00789  |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000577 |
|    n_updates       | 5574324  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=-317.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 4240000  |
| train/             |          |
|    actor_loss      | -0.269   |
|    critic_loss     | 2.51     |
|    ent_coef        | 0.00884  |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.000576 |
|    n_updates       | 5584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 848      |
|    fps             | 36       |
|    time_elapsed    | 116432   |
|    total_timesteps | 4240000  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=-306.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4250000  |
| train/             |          |
|    actor_loss      | -1.1     |
|    critic_loss     | 2.24     |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | 0.575    |
|    learning_rate   | 0.000575 |
|    n_updates       | 5594324  |
---------------------------------
Eval num_timesteps=4260000, episode_reward=-306.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4260000  |
| train/             |          |
|    actor_loss      | -2.7     |
|    critic_loss     | 6.09     |
|    ent_coef        | 0.0142   |
|    ent_coef_loss   | 5.32     |
|    learning_rate   | 0.000574 |
|    n_updates       | 5604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -160     |
| time/              |          |
|    episodes        | 852      |
|    fps             | 36       |
|    time_elapsed    | 117026   |
|    total_timesteps | 4260000  |
---------------------------------
Eval num_timesteps=4270000, episode_reward=-1079.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.08e+03 |
| time/              |           |
|    total_timesteps | 4270000   |
| train/             |           |
|    actor_loss      | -9.65     |
|    critic_loss     | 68.2      |
|    ent_coef        | 0.0248    |
|    ent_coef_loss   | 4.84      |
|    learning_rate   | 0.000573  |
|    n_updates       | 5614324   |
----------------------------------
Eval num_timesteps=4280000, episode_reward=-1068.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 4280000   |
| train/             |           |
|    actor_loss      | -69.5     |
|    critic_loss     | 91.8      |
|    ent_coef        | 0.0891    |
|    ent_coef_loss   | 0.311     |
|    learning_rate   | 0.000572  |
|    n_updates       | 5624324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -292     |
| time/              |          |
|    episodes        | 856      |
|    fps             | 36       |
|    time_elapsed    | 117618   |
|    total_timesteps | 4280000  |
---------------------------------
Eval num_timesteps=4290000, episode_reward=-1068.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 4290000   |
| train/             |           |
|    actor_loss      | -49.3     |
|    critic_loss     | 20.1      |
|    ent_coef        | 0.044     |
|    ent_coef_loss   | -0.693    |
|    learning_rate   | 0.000571  |
|    n_updates       | 5634324   |
----------------------------------
Eval num_timesteps=4300000, episode_reward=-1897.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.9e+03 |
| time/              |          |
|    total_timesteps | 4300000  |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 12.4     |
|    ent_coef        | 0.0268   |
|    ent_coef_loss   | -3.09    |
|    learning_rate   | 0.00057  |
|    n_updates       | 5644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -357     |
| time/              |          |
|    episodes        | 860      |
|    fps             | 36       |
|    time_elapsed    | 118214   |
|    total_timesteps | 4300000  |
---------------------------------
Eval num_timesteps=4310000, episode_reward=-1921.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.92e+03 |
| time/              |           |
|    total_timesteps | 4310000   |
| train/             |           |
|    actor_loss      | -16.5     |
|    critic_loss     | 14        |
|    ent_coef        | 0.016     |
|    ent_coef_loss   | 4.16      |
|    learning_rate   | 0.000569  |
|    n_updates       | 5654324   |
----------------------------------
Eval num_timesteps=4320000, episode_reward=-616.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -616     |
| time/              |          |
|    total_timesteps | 4320000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 16.6     |
|    ent_coef        | 0.0161   |
|    ent_coef_loss   | 2.08     |
|    learning_rate   | 0.000568 |
|    n_updates       | 5664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -432     |
| time/              |          |
|    episodes        | 864      |
|    fps             | 36       |
|    time_elapsed    | 118808   |
|    total_timesteps | 4320000  |
---------------------------------
Eval num_timesteps=4330000, episode_reward=-610.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -611     |
| time/              |          |
|    total_timesteps | 4330000  |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 7.07     |
|    ent_coef        | 0.0168   |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000567 |
|    n_updates       | 5674324  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=-289.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 4340000  |
| train/             |          |
|    actor_loss      | -6.59    |
|    critic_loss     | 3.35     |
|    ent_coef        | 0.0177   |
|    ent_coef_loss   | -2.41    |
|    learning_rate   | 0.000566 |
|    n_updates       | 5684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -428     |
| time/              |          |
|    episodes        | 868      |
|    fps             | 36       |
|    time_elapsed    | 119404   |
|    total_timesteps | 4340000  |
---------------------------------
Eval num_timesteps=4350000, episode_reward=-309.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 4350000  |
| train/             |          |
|    actor_loss      | -3.57    |
|    critic_loss     | 2.88     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | 0.227    |
|    learning_rate   | 0.000565 |
|    n_updates       | 5694324  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=148.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 149      |
| time/              |          |
|    total_timesteps | 4360000  |
| train/             |          |
|    actor_loss      | 4.08     |
|    critic_loss     | 5.9      |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | -0.188   |
|    learning_rate   | 0.000564 |
|    n_updates       | 5704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -429     |
| time/              |          |
|    episodes        | 872      |
|    fps             | 36       |
|    time_elapsed    | 119995   |
|    total_timesteps | 4360000  |
---------------------------------
Eval num_timesteps=4370000, episode_reward=-801.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -802     |
| time/              |          |
|    total_timesteps | 4370000  |
| train/             |          |
|    actor_loss      | 4.62     |
|    critic_loss     | 13.3     |
|    ent_coef        | 0.0113   |
|    ent_coef_loss   | 4.48     |
|    learning_rate   | 0.000563 |
|    n_updates       | 5714324  |
---------------------------------
Eval num_timesteps=4380000, episode_reward=-197.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 4380000  |
| train/             |          |
|    actor_loss      | 4.56     |
|    critic_loss     | 3.17     |
|    ent_coef        | 0.0082   |
|    ent_coef_loss   | -5.1     |
|    learning_rate   | 0.000562 |
|    n_updates       | 5724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -454     |
| time/              |          |
|    episodes        | 876      |
|    fps             | 36       |
|    time_elapsed    | 120590   |
|    total_timesteps | 4380000  |
---------------------------------
Eval num_timesteps=4390000, episode_reward=-970.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -970     |
| time/              |          |
|    total_timesteps | 4390000  |
| train/             |          |
|    actor_loss      | 5.99     |
|    critic_loss     | 6.04     |
|    ent_coef        | 0.00593  |
|    ent_coef_loss   | -0.913   |
|    learning_rate   | 0.000561 |
|    n_updates       | 5734324  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=-939.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 4400000  |
| train/             |          |
|    actor_loss      | 3.36     |
|    critic_loss     | 1.61     |
|    ent_coef        | 0.00809  |
|    ent_coef_loss   | -1.4     |
|    learning_rate   | 0.00056  |
|    n_updates       | 5744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -488     |
| time/              |          |
|    episodes        | 880      |
|    fps             | 36       |
|    time_elapsed    | 121179   |
|    total_timesteps | 4400000  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=-1287.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.29e+03 |
| time/              |           |
|    total_timesteps | 4410000   |
| train/             |           |
|    actor_loss      | -0.595    |
|    critic_loss     | 1.66      |
|    ent_coef        | 0.0134    |
|    ent_coef_loss   | -0.393    |
|    learning_rate   | 0.000559  |
|    n_updates       | 5754324   |
----------------------------------
Eval num_timesteps=4420000, episode_reward=-1025.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 4420000   |
| train/             |           |
|    actor_loss      | -4.97     |
|    critic_loss     | 3.21      |
|    ent_coef        | 0.0133    |
|    ent_coef_loss   | -1.17     |
|    learning_rate   | 0.000558  |
|    n_updates       | 5764324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -524     |
| time/              |          |
|    episodes        | 884      |
|    fps             | 36       |
|    time_elapsed    | 121765   |
|    total_timesteps | 4420000  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=-1631.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+03 |
| time/              |           |
|    total_timesteps | 4430000   |
| train/             |           |
|    actor_loss      | -3.16     |
|    critic_loss     | 4.67      |
|    ent_coef        | 0.0123    |
|    ent_coef_loss   | -0.298    |
|    learning_rate   | 0.000557  |
|    n_updates       | 5774324   |
----------------------------------
Eval num_timesteps=4440000, episode_reward=-1470.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 4440000   |
| train/             |           |
|    actor_loss      | -4.48     |
|    critic_loss     | 2.57      |
|    ent_coef        | 0.011     |
|    ent_coef_loss   | -2.57     |
|    learning_rate   | 0.000556  |
|    n_updates       | 5784324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -584     |
| time/              |          |
|    episodes        | 888      |
|    fps             | 36       |
|    time_elapsed    | 122351   |
|    total_timesteps | 4440000  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=-224.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 4450000  |
| train/             |          |
|    actor_loss      | 3.19     |
|    critic_loss     | 2.14     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000555 |
|    n_updates       | 5794324  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=-213.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 4460000  |
| train/             |          |
|    actor_loss      | 5.65     |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | -5       |
|    learning_rate   | 0.000554 |
|    n_updates       | 5804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -588     |
| time/              |          |
|    episodes        | 892      |
|    fps             | 36       |
|    time_elapsed    | 122940   |
|    total_timesteps | 4460000  |
---------------------------------
Eval num_timesteps=4470000, episode_reward=-305.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 4470000  |
| train/             |          |
|    actor_loss      | 7.87     |
|    critic_loss     | 3.24     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | 0.928    |
|    learning_rate   | 0.000553 |
|    n_updates       | 5814324  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=-306.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4480000  |
| train/             |          |
|    actor_loss      | 8.75     |
|    critic_loss     | 3.66     |
|    ent_coef        | 0.00996  |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000552 |
|    n_updates       | 5824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -572     |
| time/              |          |
|    episodes        | 896      |
|    fps             | 36       |
|    time_elapsed    | 123530   |
|    total_timesteps | 4480000  |
---------------------------------
Eval num_timesteps=4490000, episode_reward=-306.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 4490000  |
| train/             |          |
|    actor_loss      | 10.2     |
|    critic_loss     | 3.37     |
|    ent_coef        | 0.0098   |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.000551 |
|    n_updates       | 5834324  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=-305.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 4500000  |
| train/             |          |
|    actor_loss      | 11.6     |
|    critic_loss     | 2.76     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | -1.42    |
|    learning_rate   | 0.00055  |
|    n_updates       | 5844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -572     |
| time/              |          |
|    episodes        | 900      |
|    fps             | 36       |
|    time_elapsed    | 124121   |
|    total_timesteps | 4500000  |
---------------------------------
Eval num_timesteps=4510000, episode_reward=-305.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 4510000  |
| train/             |          |
|    actor_loss      | 9.28     |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.00937  |
|    ent_coef_loss   | 4.35     |
|    learning_rate   | 0.000549 |
|    n_updates       | 5854324  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=-306.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4520000  |
| train/             |          |
|    actor_loss      | 11.2     |
|    critic_loss     | 0.883    |
|    ent_coef        | 0.00924  |
|    ent_coef_loss   | -6.38    |
|    learning_rate   | 0.000548 |
|    n_updates       | 5864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -610     |
| time/              |          |
|    episodes        | 904      |
|    fps             | 36       |
|    time_elapsed    | 124712   |
|    total_timesteps | 4520000  |
---------------------------------
Eval num_timesteps=4530000, episode_reward=-306.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4530000  |
| train/             |          |
|    actor_loss      | 10.5     |
|    critic_loss     | 0.65     |
|    ent_coef        | 0.00805  |
|    ent_coef_loss   | -8.19    |
|    learning_rate   | 0.000547 |
|    n_updates       | 5874324  |
---------------------------------
Eval num_timesteps=4540000, episode_reward=-289.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 4540000  |
| train/             |          |
|    actor_loss      | 8.54     |
|    critic_loss     | 1.76     |
|    ent_coef        | 0.00833  |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000546 |
|    n_updates       | 5884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -615     |
| time/              |          |
|    episodes        | 908      |
|    fps             | 36       |
|    time_elapsed    | 125307   |
|    total_timesteps | 4540000  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=-295.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 4550000  |
| train/             |          |
|    actor_loss      | 7.05     |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00756  |
|    ent_coef_loss   | -4.64    |
|    learning_rate   | 0.000545 |
|    n_updates       | 5894324  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=-305.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 4560000  |
| train/             |          |
|    actor_loss      | 10.3     |
|    critic_loss     | 0.766    |
|    ent_coef        | 0.00664  |
|    ent_coef_loss   | -0.725   |
|    learning_rate   | 0.000544 |
|    n_updates       | 5904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -621     |
| time/              |          |
|    episodes        | 912      |
|    fps             | 36       |
|    time_elapsed    | 125905   |
|    total_timesteps | 4560000  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=-298.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 4570000  |
| train/             |          |
|    actor_loss      | 9.08     |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.00629  |
|    ent_coef_loss   | -4.12    |
|    learning_rate   | 0.000543 |
|    n_updates       | 5914324  |
---------------------------------
Eval num_timesteps=4580000, episode_reward=-307.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4580000  |
| train/             |          |
|    actor_loss      | 7.28     |
|    critic_loss     | 1.22     |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000542 |
|    n_updates       | 5924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -595     |
| time/              |          |
|    episodes        | 916      |
|    fps             | 36       |
|    time_elapsed    | 126500   |
|    total_timesteps | 4580000  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=-299.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 4590000  |
| train/             |          |
|    actor_loss      | 7.02     |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | 7.07     |
|    learning_rate   | 0.000541 |
|    n_updates       | 5934324  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=-1255.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.26e+03 |
| time/              |           |
|    total_timesteps | 4600000   |
| train/             |           |
|    actor_loss      | 7.7       |
|    critic_loss     | 0.682     |
|    ent_coef        | 0.00565   |
|    ent_coef_loss   | -5.1      |
|    learning_rate   | 0.00054   |
|    n_updates       | 5944324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -568     |
| time/              |          |
|    episodes        | 920      |
|    fps             | 36       |
|    time_elapsed    | 127098   |
|    total_timesteps | 4600000  |
---------------------------------
Eval num_timesteps=4610000, episode_reward=-373.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 4610000  |
| train/             |          |
|    actor_loss      | 6.05     |
|    critic_loss     | 0.584    |
|    ent_coef        | 0.00456  |
|    ent_coef_loss   | 4.93     |
|    learning_rate   | 0.000539 |
|    n_updates       | 5954324  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=-432.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 4620000  |
| train/             |          |
|    actor_loss      | 7.47     |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.00596  |
|    ent_coef_loss   | 7.2      |
|    learning_rate   | 0.000538 |
|    n_updates       | 5964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -555     |
| time/              |          |
|    episodes        | 924      |
|    fps             | 36       |
|    time_elapsed    | 127703   |
|    total_timesteps | 4620000  |
---------------------------------
Eval num_timesteps=4630000, episode_reward=-1149.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.15e+03 |
| time/              |           |
|    total_timesteps | 4630000   |
| train/             |           |
|    actor_loss      | 6.76      |
|    critic_loss     | 0.424     |
|    ent_coef        | 0.00633   |
|    ent_coef_loss   | 0.578     |
|    learning_rate   | 0.000537  |
|    n_updates       | 5974324   |
----------------------------------
Eval num_timesteps=4640000, episode_reward=-441.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 4640000  |
| train/             |          |
|    actor_loss      | 8.38     |
|    critic_loss     | 0.867    |
|    ent_coef        | 0.00555  |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.000536 |
|    n_updates       | 5984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -566     |
| time/              |          |
|    episodes        | 928      |
|    fps             | 36       |
|    time_elapsed    | 128308   |
|    total_timesteps | 4640000  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=-1036.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 4650000   |
| train/             |           |
|    actor_loss      | 7.09      |
|    critic_loss     | 0.733     |
|    ent_coef        | 0.00477   |
|    ent_coef_loss   | 3.99      |
|    learning_rate   | 0.000535  |
|    n_updates       | 5994324   |
----------------------------------
Eval num_timesteps=4660000, episode_reward=-467.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 4660000  |
| train/             |          |
|    actor_loss      | 5.08     |
|    critic_loss     | 1.32     |
|    ent_coef        | 0.00553  |
|    ent_coef_loss   | -2.44    |
|    learning_rate   | 0.000534 |
|    n_updates       | 6004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -596     |
| time/              |          |
|    episodes        | 932      |
|    fps             | 36       |
|    time_elapsed    | 128912   |
|    total_timesteps | 4660000  |
---------------------------------
Eval num_timesteps=4670000, episode_reward=-1296.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.3e+03 |
| time/              |          |
|    total_timesteps | 4670000  |
| train/             |          |
|    actor_loss      | 6.8      |
|    critic_loss     | 0.754    |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | -1.6     |
|    learning_rate   | 0.000533 |
|    n_updates       | 6014324  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=-1069.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 4680000   |
| train/             |           |
|    actor_loss      | 6.65      |
|    critic_loss     | 4.74      |
|    ent_coef        | 0.0139    |
|    ent_coef_loss   | -0.637    |
|    learning_rate   | 0.000532  |
|    n_updates       | 6024324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -631     |
| time/              |          |
|    episodes        | 936      |
|    fps             | 36       |
|    time_elapsed    | 129512   |
|    total_timesteps | 4680000  |
---------------------------------
Eval num_timesteps=4690000, episode_reward=-1060.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.06e+03 |
| time/              |           |
|    total_timesteps | 4690000   |
| train/             |           |
|    actor_loss      | -3.97     |
|    critic_loss     | 4.1       |
|    ent_coef        | 0.00796   |
|    ent_coef_loss   | 1.01      |
|    learning_rate   | 0.000531  |
|    n_updates       | 6034324   |
----------------------------------
Eval num_timesteps=4700000, episode_reward=-1081.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.08e+03 |
| time/              |           |
|    total_timesteps | 4700000   |
| train/             |           |
|    actor_loss      | -3.48     |
|    critic_loss     | 1.9       |
|    ent_coef        | 0.00545   |
|    ent_coef_loss   | -2.89     |
|    learning_rate   | 0.00053   |
|    n_updates       | 6044324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -693     |
| time/              |          |
|    episodes        | 940      |
|    fps             | 36       |
|    time_elapsed    | 130109   |
|    total_timesteps | 4700000  |
---------------------------------
Eval num_timesteps=4710000, episode_reward=-307.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4710000  |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00621  |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.000529 |
|    n_updates       | 6054324  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=-305.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 4720000  |
| train/             |          |
|    actor_loss      | 2.02     |
|    critic_loss     | 1.96     |
|    ent_coef        | 0.00653  |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.000528 |
|    n_updates       | 6064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -714     |
| time/              |          |
|    episodes        | 944      |
|    fps             | 36       |
|    time_elapsed    | 130700   |
|    total_timesteps | 4720000  |
---------------------------------
Eval num_timesteps=4730000, episode_reward=-659.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -660     |
| time/              |          |
|    total_timesteps | 4730000  |
| train/             |          |
|    actor_loss      | 2.83     |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.00685  |
|    ent_coef_loss   | -0.75    |
|    learning_rate   | 0.000527 |
|    n_updates       | 6074324  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=-961.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -961     |
| time/              |          |
|    total_timesteps | 4740000  |
| train/             |          |
|    actor_loss      | 0.267    |
|    critic_loss     | 8.89     |
|    ent_coef        | 0.00839  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000526 |
|    n_updates       | 6084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -765     |
| time/              |          |
|    episodes        | 948      |
|    fps             | 36       |
|    time_elapsed    | 131294   |
|    total_timesteps | 4740000  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=-1050.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.05e+03 |
| time/              |           |
|    total_timesteps | 4750000   |
| train/             |           |
|    actor_loss      | 2.53      |
|    critic_loss     | 0.631     |
|    ent_coef        | 0.00782   |
|    ent_coef_loss   | 3.28      |
|    learning_rate   | 0.000525  |
|    n_updates       | 6094324   |
----------------------------------
Eval num_timesteps=4760000, episode_reward=-1745.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.75e+03 |
| time/              |           |
|    total_timesteps | 4760000   |
| train/             |           |
|    actor_loss      | 1.01      |
|    critic_loss     | 1.74      |
|    ent_coef        | 0.00855   |
|    ent_coef_loss   | -4.8      |
|    learning_rate   | 0.000524  |
|    n_updates       | 6104324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -803     |
| time/              |          |
|    episodes        | 952      |
|    fps             | 36       |
|    time_elapsed    | 131903   |
|    total_timesteps | 4760000  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=-1471.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 4770000   |
| train/             |           |
|    actor_loss      | -0.925    |
|    critic_loss     | 1.29      |
|    ent_coef        | 0.00853   |
|    ent_coef_loss   | -0.167    |
|    learning_rate   | 0.000523  |
|    n_updates       | 6114324   |
----------------------------------
Eval num_timesteps=4780000, episode_reward=-206.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 4780000  |
| train/             |          |
|    actor_loss      | 4.43     |
|    critic_loss     | 5.66     |
|    ent_coef        | 0.00786  |
|    ent_coef_loss   | -0.0802  |
|    learning_rate   | 0.000522 |
|    n_updates       | 6124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -831     |
| time/              |          |
|    episodes        | 956      |
|    fps             | 36       |
|    time_elapsed    | 132425   |
|    total_timesteps | 4780000  |
---------------------------------
Eval num_timesteps=4790000, episode_reward=-1732.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.73e+03 |
| time/              |           |
|    total_timesteps | 4790000   |
| train/             |           |
|    actor_loss      | -0.256    |
|    critic_loss     | 2.44      |
|    ent_coef        | 0.0126    |
|    ent_coef_loss   | -0.431    |
|    learning_rate   | 0.000521  |
|    n_updates       | 6134324   |
----------------------------------
Eval num_timesteps=4800000, episode_reward=-225.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 4800000  |
| train/             |          |
|    actor_loss      | -4.02    |
|    critic_loss     | 7.24     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -0.827   |
|    learning_rate   | 0.00052  |
|    n_updates       | 6144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -781     |
| time/              |          |
|    episodes        | 960      |
|    fps             | 36       |
|    time_elapsed    | 132927   |
|    total_timesteps | 4800000  |
---------------------------------
Eval num_timesteps=4810000, episode_reward=-253.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 4810000  |
| train/             |          |
|    actor_loss      | -6       |
|    critic_loss     | 4.08     |
|    ent_coef        | 0.00979  |
|    ent_coef_loss   | -4.23    |
|    learning_rate   | 0.000519 |
|    n_updates       | 6154324  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=-1440.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.44e+03 |
| time/              |           |
|    total_timesteps | 4820000   |
| train/             |           |
|    actor_loss      | -5.46     |
|    critic_loss     | 3.89      |
|    ent_coef        | 0.0145    |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 0.000518  |
|    n_updates       | 6164324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -736     |
| time/              |          |
|    episodes        | 964      |
|    fps             | 36       |
|    time_elapsed    | 133434   |
|    total_timesteps | 4820000  |
---------------------------------
Eval num_timesteps=4830000, episode_reward=-629.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -629     |
| time/              |          |
|    total_timesteps | 4830000  |
| train/             |          |
|    actor_loss      | -6.82    |
|    critic_loss     | 4.78     |
|    ent_coef        | 0.0143   |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000517 |
|    n_updates       | 6174324  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=-1457.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.46e+03 |
| time/              |           |
|    total_timesteps | 4840000   |
| train/             |           |
|    actor_loss      | -3.42     |
|    critic_loss     | 27.8      |
|    ent_coef        | 0.0146    |
|    ent_coef_loss   | 5.35      |
|    learning_rate   | 0.000516  |
|    n_updates       | 6184324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -740     |
| time/              |          |
|    episodes        | 968      |
|    fps             | 36       |
|    time_elapsed    | 133942   |
|    total_timesteps | 4840000  |
---------------------------------
Eval num_timesteps=4850000, episode_reward=-121.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 4850000  |
| train/             |          |
|    actor_loss      | -0.183   |
|    critic_loss     | 4.22     |
|    ent_coef        | 0.0172   |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000515 |
|    n_updates       | 6194324  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=65.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 65.2     |
| time/              |          |
|    total_timesteps | 4860000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 13.1     |
|    ent_coef        | 0.017    |
|    ent_coef_loss   | -0.386   |
|    learning_rate   | 0.000514 |
|    n_updates       | 6204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -732     |
| time/              |          |
|    episodes        | 972      |
|    fps             | 36       |
|    time_elapsed    | 134448   |
|    total_timesteps | 4860000  |
---------------------------------
Eval num_timesteps=4870000, episode_reward=-1466.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+03 |
| time/              |           |
|    total_timesteps | 4870000   |
| train/             |           |
|    actor_loss      | 0.309     |
|    critic_loss     | 8.63      |
|    ent_coef        | 0.0109    |
|    ent_coef_loss   | 2.55      |
|    learning_rate   | 0.000513  |
|    n_updates       | 6214324   |
----------------------------------
Eval num_timesteps=4880000, episode_reward=-1374.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.37e+03 |
| time/              |           |
|    total_timesteps | 4880000   |
| train/             |           |
|    actor_loss      | 3.63      |
|    critic_loss     | 5.01      |
|    ent_coef        | 0.0106    |
|    ent_coef_loss   | -2.32     |
|    learning_rate   | 0.000512  |
|    n_updates       | 6224324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -734     |
| time/              |          |
|    episodes        | 976      |
|    fps             | 36       |
|    time_elapsed    | 134955   |
|    total_timesteps | 4880000  |
---------------------------------
Eval num_timesteps=4890000, episode_reward=-1491.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+03 |
| time/              |           |
|    total_timesteps | 4890000   |
| train/             |           |
|    actor_loss      | 11.4      |
|    critic_loss     | 12.5      |
|    ent_coef        | 0.0133    |
|    ent_coef_loss   | -1.6      |
|    learning_rate   | 0.000511  |
|    n_updates       | 6234324   |
----------------------------------
Eval num_timesteps=4900000, episode_reward=-738.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -739     |
| time/              |          |
|    total_timesteps | 4900000  |
| train/             |          |
|    actor_loss      | 11       |
|    critic_loss     | 10.6     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | -9.83    |
|    learning_rate   | 0.00051  |
|    n_updates       | 6244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -751     |
| time/              |          |
|    episodes        | 980      |
|    fps             | 36       |
|    time_elapsed    | 135466   |
|    total_timesteps | 4900000  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=150.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 4910000  |
| train/             |          |
|    actor_loss      | 16.1     |
|    critic_loss     | 15.1     |
|    ent_coef        | 0.0147   |
|    ent_coef_loss   | -8.44    |
|    learning_rate   | 0.000509 |
|    n_updates       | 6254324  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=-429.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 4920000  |
| train/             |          |
|    actor_loss      | 11.9     |
|    critic_loss     | 12.9     |
|    ent_coef        | 0.0176   |
|    ent_coef_loss   | -0.933   |
|    learning_rate   | 0.000508 |
|    n_updates       | 6264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -727     |
| time/              |          |
|    episodes        | 984      |
|    fps             | 36       |
|    time_elapsed    | 135988   |
|    total_timesteps | 4920000  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=-310.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 4930000  |
| train/             |          |
|    actor_loss      | 19.9     |
|    critic_loss     | 11.5     |
|    ent_coef        | 0.0219   |
|    ent_coef_loss   | -0.382   |
|    learning_rate   | 0.000507 |
|    n_updates       | 6274324  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=-308.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    actor_loss      | 21.2     |
|    critic_loss     | 39.9     |
|    ent_coef        | 0.0226   |
|    ent_coef_loss   | 5.7      |
|    learning_rate   | 0.000506 |
|    n_updates       | 6284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -692     |
| time/              |          |
|    episodes        | 988      |
|    fps             | 36       |
|    time_elapsed    | 136546   |
|    total_timesteps | 4940000  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=-1493.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+03 |
| time/              |           |
|    total_timesteps | 4950000   |
| train/             |           |
|    actor_loss      | 18.5      |
|    critic_loss     | 12.6      |
|    ent_coef        | 0.0215    |
|    ent_coef_loss   | -3.66     |
|    learning_rate   | 0.000505  |
|    n_updates       | 6294324   |
----------------------------------
Eval num_timesteps=4960000, episode_reward=-305.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 4960000  |
| train/             |          |
|    actor_loss      | 18.1     |
|    critic_loss     | 25.2     |
|    ent_coef        | 0.0223   |
|    ent_coef_loss   | -1.39    |
|    learning_rate   | 0.000504 |
|    n_updates       | 6304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -710     |
| time/              |          |
|    episodes        | 992      |
|    fps             | 36       |
|    time_elapsed    | 137079   |
|    total_timesteps | 4960000  |
---------------------------------
Eval num_timesteps=4970000, episode_reward=-309.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 4970000  |
| train/             |          |
|    actor_loss      | 17.8     |
|    critic_loss     | 8.7      |
|    ent_coef        | 0.0203   |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000503 |
|    n_updates       | 6314324  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=-303.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 4980000  |
| train/             |          |
|    actor_loss      | 12.4     |
|    critic_loss     | 4.89     |
|    ent_coef        | 0.0217   |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.000502 |
|    n_updates       | 6324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -712     |
| time/              |          |
|    episodes        | 996      |
|    fps             | 36       |
|    time_elapsed    | 137589   |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4990000, episode_reward=165.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 4990000  |
| train/             |          |
|    actor_loss      | 5.59     |
|    critic_loss     | 6.88     |
|    ent_coef        | 0.0203   |
|    ent_coef_loss   | 0.0465   |
|    learning_rate   | 0.000501 |
|    n_updates       | 6334324  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=-309.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 5000000  |
| train/             |          |
|    actor_loss      | 6.44     |
|    critic_loss     | 9.37     |
|    ent_coef        | 0.0198   |
|    ent_coef_loss   | 4.38     |
|    learning_rate   | 0.0005   |
|    n_updates       | 6344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -698     |
| time/              |          |
|    episodes        | 1000     |
|    fps             | 36       |
|    time_elapsed    | 138098   |
|    total_timesteps | 5000000  |
---------------------------------
Eval num_timesteps=5010000, episode_reward=-310.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 5010000  |
| train/             |          |
|    actor_loss      | -0.38    |
|    critic_loss     | 6.18     |
|    ent_coef        | 0.0197   |
|    ent_coef_loss   | 0.96     |
|    learning_rate   | 0.000499 |
|    n_updates       | 6354324  |
---------------------------------
Eval num_timesteps=5020000, episode_reward=-1135.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.14e+03 |
| time/              |           |
|    total_timesteps | 5020000   |
| train/             |           |
|    actor_loss      | -7.54     |
|    critic_loss     | 13.3      |
|    ent_coef        | 0.0224    |
|    ent_coef_loss   | -0.599    |
|    learning_rate   | 0.000498  |
|    n_updates       | 6364324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -698     |
| time/              |          |
|    episodes        | 1004     |
|    fps             | 36       |
|    time_elapsed    | 138607   |
|    total_timesteps | 5020000  |
---------------------------------
Eval num_timesteps=5030000, episode_reward=-650.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -651     |
| time/              |          |
|    total_timesteps | 5030000  |
| train/             |          |
|    actor_loss      | -9.68    |
|    critic_loss     | 19       |
|    ent_coef        | 0.0308   |
|    ent_coef_loss   | -3.54    |
|    learning_rate   | 0.000497 |
|    n_updates       | 6374324  |
---------------------------------
Eval num_timesteps=5040000, episode_reward=150.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 5040000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 79.5     |
|    ent_coef        | 0.0486   |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000496 |
|    n_updates       | 6384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -702     |
| time/              |          |
|    episodes        | 1008     |
|    fps             | 36       |
|    time_elapsed    | 139121   |
|    total_timesteps | 5040000  |
---------------------------------
Eval num_timesteps=5050000, episode_reward=-1173.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.17e+03 |
| time/              |           |
|    total_timesteps | 5050000   |
| train/             |           |
|    actor_loss      | -48.6     |
|    critic_loss     | 40.4      |
|    ent_coef        | 0.061     |
|    ent_coef_loss   | -0.542    |
|    learning_rate   | 0.000495  |
|    n_updates       | 6394324   |
----------------------------------
Eval num_timesteps=5060000, episode_reward=-1952.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.95e+03 |
| time/              |           |
|    total_timesteps | 5060000   |
| train/             |           |
|    actor_loss      | -62.4     |
|    critic_loss     | 53.7      |
|    ent_coef        | 0.0518    |
|    ent_coef_loss   | 0.781     |
|    learning_rate   | 0.000494  |
|    n_updates       | 6404324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -709     |
| time/              |          |
|    episodes        | 1012     |
|    fps             | 36       |
|    time_elapsed    | 139689   |
|    total_timesteps | 5060000  |
---------------------------------
Eval num_timesteps=5070000, episode_reward=-1104.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.1e+03 |
| time/              |          |
|    total_timesteps | 5070000  |
| train/             |          |
|    actor_loss      | -45.2    |
|    critic_loss     | 110      |
|    ent_coef        | 0.0512   |
|    ent_coef_loss   | 2.02     |
|    learning_rate   | 0.000493 |
|    n_updates       | 6414324  |
---------------------------------
Eval num_timesteps=5080000, episode_reward=-907.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 5080000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 79       |
|    ent_coef        | 0.0524   |
|    ent_coef_loss   | 0.764    |
|    learning_rate   | 0.000492 |
|    n_updates       | 6424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -749     |
| time/              |          |
|    episodes        | 1016     |
|    fps             | 36       |
|    time_elapsed    | 140255   |
|    total_timesteps | 5080000  |
---------------------------------
Eval num_timesteps=5090000, episode_reward=-1126.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.13e+03 |
| time/              |           |
|    total_timesteps | 5090000   |
| train/             |           |
|    actor_loss      | -38.3     |
|    critic_loss     | 74.6      |
|    ent_coef        | 0.0463    |
|    ent_coef_loss   | -0.0341   |
|    learning_rate   | 0.000491  |
|    n_updates       | 6434324   |
----------------------------------
Eval num_timesteps=5100000, episode_reward=-696.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -697     |
| time/              |          |
|    total_timesteps | 5100000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 28.3     |
|    ent_coef        | 0.0506   |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.00049  |
|    n_updates       | 6444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -774     |
| time/              |          |
|    episodes        | 1020     |
|    fps             | 36       |
|    time_elapsed    | 140801   |
|    total_timesteps | 5100000  |
---------------------------------
Eval num_timesteps=5110000, episode_reward=-1346.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.35e+03 |
| time/              |           |
|    total_timesteps | 5110000   |
| train/             |           |
|    actor_loss      | -0.696    |
|    critic_loss     | 40.5      |
|    ent_coef        | 0.0466    |
|    ent_coef_loss   | -1.99     |
|    learning_rate   | 0.000489  |
|    n_updates       | 6454324   |
----------------------------------
Eval num_timesteps=5120000, episode_reward=152.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 5120000  |
| train/             |          |
|    actor_loss      | 11.8     |
|    critic_loss     | 47.2     |
|    ent_coef        | 0.046    |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.000488 |
|    n_updates       | 6464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -792     |
| time/              |          |
|    episodes        | 1024     |
|    fps             | 36       |
|    time_elapsed    | 141353   |
|    total_timesteps | 5120000  |
---------------------------------
Eval num_timesteps=5130000, episode_reward=171.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 5130000  |
| train/             |          |
|    actor_loss      | 14.8     |
|    critic_loss     | 37.4     |
|    ent_coef        | 0.0437   |
|    ent_coef_loss   | -0.401   |
|    learning_rate   | 0.000487 |
|    n_updates       | 6474324  |
---------------------------------
Eval num_timesteps=5140000, episode_reward=-1420.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.42e+03 |
| time/              |           |
|    total_timesteps | 5140000   |
| train/             |           |
|    actor_loss      | 17.6      |
|    critic_loss     | 22.2      |
|    ent_coef        | 0.038     |
|    ent_coef_loss   | 1.35      |
|    learning_rate   | 0.000486  |
|    n_updates       | 6484324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -795     |
| time/              |          |
|    episodes        | 1028     |
|    fps             | 36       |
|    time_elapsed    | 141875   |
|    total_timesteps | 5140000  |
---------------------------------
Eval num_timesteps=5150000, episode_reward=-304.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 5150000  |
| train/             |          |
|    actor_loss      | 24.8     |
|    critic_loss     | 102      |
|    ent_coef        | 0.0368   |
|    ent_coef_loss   | -0.47    |
|    learning_rate   | 0.000485 |
|    n_updates       | 6494324  |
---------------------------------
Eval num_timesteps=5160000, episode_reward=-306.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 5160000  |
| train/             |          |
|    actor_loss      | 28.6     |
|    critic_loss     | 42.4     |
|    ent_coef        | 0.03     |
|    ent_coef_loss   | -0.462   |
|    learning_rate   | 0.000484 |
|    n_updates       | 6504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -773     |
| time/              |          |
|    episodes        | 1032     |
|    fps             | 36       |
|    time_elapsed    | 142371   |
|    total_timesteps | 5160000  |
---------------------------------
Eval num_timesteps=5170000, episode_reward=-770.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -771     |
| time/              |          |
|    total_timesteps | 5170000  |
| train/             |          |
|    actor_loss      | 30.5     |
|    critic_loss     | 100      |
|    ent_coef        | 0.0269   |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000483 |
|    n_updates       | 6514324  |
---------------------------------
Eval num_timesteps=5180000, episode_reward=-1005.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 5180000   |
| train/             |           |
|    actor_loss      | 20.9      |
|    critic_loss     | 23.5      |
|    ent_coef        | 0.0273    |
|    ent_coef_loss   | -2.28     |
|    learning_rate   | 0.000482  |
|    n_updates       | 6524324   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -753     |
| time/              |          |
|    episodes        | 1036     |
|    fps             | 36       |
|    time_elapsed    | 142873   |
|    total_timesteps | 5180000  |
