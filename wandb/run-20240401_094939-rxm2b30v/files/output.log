Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_60
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=844.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -17.6    |
|    critic_loss     | 0.229    |
|    ent_coef        | 0.00146  |
|    ent_coef_loss   | 27.2     |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=61.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 61.5     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -40.7    |
|    critic_loss     | 3.22     |
|    ent_coef        | 0.00637  |
|    ent_coef_loss   | 1.8      |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 40       |
|    time_elapsed    | 488      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=117.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -55.8    |
|    critic_loss     | 0.894    |
|    ent_coef        | 0.0159   |
|    ent_coef_loss   | -0.619   |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
Eval num_timesteps=40000, episode_reward=196.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 0.182    |
|    ent_coef        | 0.00684  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 433      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 38       |
|    time_elapsed    | 1039     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=92.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 92.5     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.108    |
|    ent_coef        | 0.00386  |
|    ent_coef_loss   | -0.436   |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=691.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -20.8    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 329      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 37       |
|    time_elapsed    | 1602     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=703.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -18.9    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -2.53    |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=680.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 0.13     |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | -1.58    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 419      |
| time/              |          |
|    episodes        | 16       |
|    fps             | 36       |
|    time_elapsed    | 2173     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=678.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 0.148    |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | -3.64    |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=683.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -13.8    |
|    critic_loss     | 0.0629   |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | -0.379   |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 471      |
| time/              |          |
|    episodes        | 20       |
|    fps             | 36       |
|    time_elapsed    | 2754     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=684.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | 8.03     |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=57.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 57.4     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.00206  |
|    ent_coef_loss   | -3.17    |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 505      |
| time/              |          |
|    episodes        | 24       |
|    fps             | 35       |
|    time_elapsed    | 3343     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=856.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | 7.79     |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
New best mean reward!
Eval num_timesteps=140000, episode_reward=1134.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.962    |
|    ent_coef        | 0.00303  |
|    ent_coef_loss   | 6.64     |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 527      |
| time/              |          |
|    episodes        | 28       |
|    fps             | 35       |
|    time_elapsed    | 3936     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=1161.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.185    |
|    ent_coef        | 0.00231  |
|    ent_coef_loss   | 6.3      |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
New best mean reward!
Eval num_timesteps=160000, episode_reward=855.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.081    |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | -3.89    |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 565      |
| time/              |          |
|    episodes        | 32       |
|    fps             | 35       |
|    time_elapsed    | 4456     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=689.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -12.1    |
|    critic_loss     | 0.0545   |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | -7.3     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=1079.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.0934   |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | 2.87     |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 545      |
| time/              |          |
|    episodes        | 36       |
|    fps             | 36       |
|    time_elapsed    | 4961     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=556.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.0704   |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | 0.617    |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=152.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 8.46     |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 545      |
| time/              |          |
|    episodes        | 40       |
|    fps             | 36       |
|    time_elapsed    | 5463     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=760.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.0787   |
|    ent_coef        | 0.00243  |
|    ent_coef_loss   | 4.83     |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
Eval num_timesteps=220000, episode_reward=239.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 239      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.181    |
|    ent_coef        | 0.0026   |
|    ent_coef_loss   | -0.367   |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 586      |
| time/              |          |
|    episodes        | 44       |
|    fps             | 36       |
|    time_elapsed    | 5963     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=671.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.133    |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | 2.88     |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=900.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.145    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 570      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 37       |
|    time_elapsed    | 6463     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=556.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.076    |
|    ent_coef        | 0.00158  |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
Eval num_timesteps=260000, episode_reward=1202.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0723   |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | -1.64    |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 595      |
| time/              |          |
|    episodes        | 52       |
|    fps             | 37       |
|    time_elapsed    | 6965     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=1196.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0643   |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -3.85    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=692.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 619      |
| time/              |          |
|    episodes        | 56       |
|    fps             | 37       |
|    time_elapsed    | 7473     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=936.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.0828   |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | -4.93    |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=949.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.149    |
|    ent_coef        | 0.00204  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 631      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 37       |
|    time_elapsed    | 7981     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=1264.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.138    |
|    ent_coef        | 0.0022   |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
New best mean reward!
Eval num_timesteps=320000, episode_reward=1169.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 0.907    |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 664      |
| time/              |          |
|    episodes        | 64       |
|    fps             | 37       |
|    time_elapsed    | 8491     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=518.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -14.2    |
|    critic_loss     | 0.263    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=1145.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 671      |
| time/              |          |
|    episodes        | 68       |
|    fps             | 37       |
|    time_elapsed    | 8999     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=326.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -14.8    |
|    critic_loss     | 0.242    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | 2.87     |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=1124.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -14.3    |
|    critic_loss     | 0.0932   |
|    ent_coef        | 0.00249  |
|    ent_coef_loss   | 0.945    |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 655      |
| time/              |          |
|    episodes        | 72       |
|    fps             | 37       |
|    time_elapsed    | 9510     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=860.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
Eval num_timesteps=380000, episode_reward=829.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.223    |
|    ent_coef        | 0.00221  |
|    ent_coef_loss   | 0.336    |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 654      |
| time/              |          |
|    episodes        | 76       |
|    fps             | 37       |
|    time_elapsed    | 10063    |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=1152.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | -7.09    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=1146.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.123    |
|    ent_coef        | 0.00212  |
|    ent_coef_loss   | -0.852   |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 673      |
| time/              |          |
|    episodes        | 80       |
|    fps             | 37       |
|    time_elapsed    | 10586    |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=1147.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.144    |
|    ent_coef        | 0.00186  |
|    ent_coef_loss   | -3.78    |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
Eval num_timesteps=420000, episode_reward=1172.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -11.9    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00212  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 693      |
| time/              |          |
|    episodes        | 84       |
|    fps             | 37       |
|    time_elapsed    | 11096    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=1157.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.535    |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | 1.66     |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=1161.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.225    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -5.28    |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 88       |
|    fps             | 37       |
|    time_elapsed    | 11606    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=921.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -12.3    |
|    critic_loss     | 0.149    |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -0.27    |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=1176.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | -0.445   |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 705      |
| time/              |          |
|    episodes        | 92       |
|    fps             | 37       |
|    time_elapsed    | 12117    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=793.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0854   |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=661.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0961   |
|    ent_coef        | 0.00206  |
|    ent_coef_loss   | -4.38    |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 721      |
| time/              |          |
|    episodes        | 96       |
|    fps             | 38       |
|    time_elapsed    | 12628    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=1241.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.158    |
|    ent_coef        | 0.00215  |
|    ent_coef_loss   | -2.7     |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=1192.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -12.8    |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 728      |
| time/              |          |
|    episodes        | 100      |
|    fps             | 38       |
|    time_elapsed    | 13137    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=1229.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.0895   |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=1186.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.066    |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | -3.17    |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 748      |
| time/              |          |
|    episodes        | 104      |
|    fps             | 38       |
|    time_elapsed    | 13647    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=292.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -13.3    |
|    critic_loss     | 0.16     |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
Eval num_timesteps=540000, episode_reward=1222.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.0919   |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -5.38    |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 780      |
| time/              |          |
|    episodes        | 108      |
|    fps             | 38       |
|    time_elapsed    | 14158    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=866.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.0719   |
|    ent_coef        | 0.00164  |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=613.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.086    |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -8.58    |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 811      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 38       |
|    time_elapsed    | 14666    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=593.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.0305   |
|    ent_coef        | 0.00167  |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=594.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 594      |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -14      |
|    critic_loss     | 0.0977   |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -4.66    |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 808      |
| time/              |          |
|    episodes        | 116      |
|    fps             | 38       |
|    time_elapsed    | 15176    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=560.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.178    |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
Eval num_timesteps=600000, episode_reward=701.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.299    |
|    ent_coef        | 0.00214  |
|    ent_coef_loss   | -0.00724 |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 806      |
| time/              |          |
|    episodes        | 120      |
|    fps             | 38       |
|    time_elapsed    | 15687    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=574.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -13.5    |
|    critic_loss     | 0.556    |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 0.243    |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=260.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0599   |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 1.26     |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 805      |
| time/              |          |
|    episodes        | 124      |
|    fps             | 38       |
|    time_elapsed    | 16195    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=593.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.0662   |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 4.2      |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=599.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.164    |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 808      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 38       |
|    time_elapsed    | 16702    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=262.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -13.7    |
|    critic_loss     | 0.089    |
|    ent_coef        | 0.0019   |
|    ent_coef_loss   | 0.931    |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=256.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.144    |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | 3.3      |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 795      |
| time/              |          |
|    episodes        | 132      |
|    fps             | 38       |
|    time_elapsed    | 17213    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=565.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -12.2    |
|    critic_loss     | 0.0815   |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -0.316   |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=548.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -13.1    |
|    critic_loss     | 0.204    |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 798      |
| time/              |          |
|    episodes        | 136      |
|    fps             | 38       |
|    time_elapsed    | 17723    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=596.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.676    |
|    ent_coef        | 0.00217  |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=568.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 0.0802   |
|    ent_coef        | 0.00204  |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 795      |
| time/              |          |
|    episodes        | 140      |
|    fps             | 38       |
|    time_elapsed    | 18233    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=511.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.187    |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=575.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.147    |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | 7.39     |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 777      |
| time/              |          |
|    episodes        | 144      |
|    fps             | 38       |
|    time_elapsed    | 18740    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=580.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00261  |
|    ent_coef_loss   | -0.571   |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=319.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.411    |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | 4.07     |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 784      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 38       |
|    time_elapsed    | 19249    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=310.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -12.6    |
|    critic_loss     | 0.0773   |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
Eval num_timesteps=760000, episode_reward=1332.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 0.39     |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 774      |
| time/              |          |
|    episodes        | 152      |
|    fps             | 38       |
|    time_elapsed    | 19758    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=1222.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 5.1      |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=377.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -13.4    |
|    critic_loss     | 0.0617   |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | 3.43     |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 765      |
| time/              |          |
|    episodes        | 156      |
|    fps             | 38       |
|    time_elapsed    | 20264    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=273.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -13.9    |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | 2.16     |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=576.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -12.9    |
|    critic_loss     | 0.464    |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | 1.49     |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 766      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 38       |
|    time_elapsed    | 20768    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=336.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.4      |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=617.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -12.5    |
|    critic_loss     | 0.0433   |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 745      |
| time/              |          |
|    episodes        | 164      |
|    fps             | 38       |
|    time_elapsed    | 21274    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=337.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.0945   |
|    ent_coef        | 0.00267  |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=581.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | 4.23     |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 743      |
| time/              |          |
|    episodes        | 168      |
|    fps             | 38       |
|    time_elapsed    | 21781    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=613.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -12.7    |
|    critic_loss     | 0.0864   |
|    ent_coef        | 0.00264  |
|    ent_coef_loss   | -0.256   |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=603.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.0841   |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -1.97    |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 752      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 38       |
|    time_elapsed    | 22290    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=571.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.0449   |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | -4.35    |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=624.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.205    |
|    ent_coef        | 0.00259  |
|    ent_coef_loss   | 3.99     |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 750      |
| time/              |          |
|    episodes        | 176      |
|    fps             | 38       |
|    time_elapsed    | 22801    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=576.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -12.4    |
|    critic_loss     | 0.377    |
|    ent_coef        | 0.00235  |
|    ent_coef_loss   | -0.382   |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=607.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.568    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 732      |
| time/              |          |
|    episodes        | 180      |
|    fps             | 38       |
|    time_elapsed    | 23308    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=581.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.0958   |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -3.67    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=614.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -11.6    |
|    critic_loss     | 0.368    |
|    ent_coef        | 0.00237  |
|    ent_coef_loss   | 5.7      |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 710      |
| time/              |          |
|    episodes        | 184      |
|    fps             | 38       |
|    time_elapsed    | 23816    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=572.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.333    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 4.52     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=573.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -12      |
|    critic_loss     | 0.193    |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | -0.856   |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 687      |
| time/              |          |
|    episodes        | 188      |
|    fps             | 38       |
|    time_elapsed    | 24324    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=1151.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -9.61    |
|    critic_loss     | 0.0311   |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -5.61    |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=587.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -9.74    |
|    critic_loss     | 0.497    |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | 6.63     |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 695      |
| time/              |          |
|    episodes        | 192      |
|    fps             | 38       |
|    time_elapsed    | 24826    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=291.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.198    |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=556.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.199    |
|    ent_coef        | 0.00254  |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 672      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 38       |
|    time_elapsed    | 25334    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=570.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=568.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0626   |
|    ent_coef        | 0.00251  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 658      |
| time/              |          |
|    episodes        | 200      |
|    fps             | 38       |
|    time_elapsed    | 25846    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=565.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -11.4    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00222  |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=567.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 0.97     |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 632      |
| time/              |          |
|    episodes        | 204      |
|    fps             | 38       |
|    time_elapsed    | 26355    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=572.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.191    |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=570.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.0635   |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 616      |
| time/              |          |
|    episodes        | 208      |
|    fps             | 38       |
|    time_elapsed    | 26864    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=573.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0534   |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -4.85    |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=589.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -9.86    |
|    critic_loss     | 0.0516   |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | -5.18    |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 605      |
| time/              |          |
|    episodes        | 212      |
|    fps             | 38       |
|    time_elapsed    | 27376    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=614.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -9.41    |
|    critic_loss     | 0.0307   |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | -0.826   |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=620.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0789   |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 19.2     |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 602      |
| time/              |          |
|    episodes        | 216      |
|    fps             | 38       |
|    time_elapsed    | 27889    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=660.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -11.1    |
|    critic_loss     | 0.336    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=591.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -1.26    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 601      |
| time/              |          |
|    episodes        | 220      |
|    fps             | 38       |
|    time_elapsed    | 28398    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=291.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.0737   |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -2.48    |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=636.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -9.66    |
|    critic_loss     | 0.0649   |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -8.11    |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 595      |
| time/              |          |
|    episodes        | 224      |
|    fps             | 38       |
|    time_elapsed    | 28907    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=598.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -9.83    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | -4.41    |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=329.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -8.98    |
|    critic_loss     | 0.0971   |
|    ent_coef        | 0.00267  |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 589      |
| time/              |          |
|    episodes        | 228      |
|    fps             | 38       |
|    time_elapsed    | 29417    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=401.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -9.77    |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=261.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -9.34    |
|    critic_loss     | 0.109    |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 599      |
| time/              |          |
|    episodes        | 232      |
|    fps             | 38       |
|    time_elapsed    | 29929    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=641.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.296    |
|    ent_coef        | 0.00271  |
|    ent_coef_loss   | 5.45     |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=742.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 613      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 38       |
|    time_elapsed    | 30438    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=735.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.00298  |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=1307.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -9.99    |
|    critic_loss     | 0.0555   |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -1.91    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 621      |
| time/              |          |
|    episodes        | 240      |
|    fps             | 38       |
|    time_elapsed    | 30946    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=1325.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | 3.53     |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=677.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.0907   |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | -7.4     |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 636      |
| time/              |          |
|    episodes        | 244      |
|    fps             | 38       |
|    time_elapsed    | 31455    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=668.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.0998   |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | -2.39    |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=1180.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 651      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 38       |
|    time_elapsed    | 31964    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=1187.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | -3.42    |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=159.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.239    |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | 0.394    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 663      |
| time/              |          |
|    episodes        | 252      |
|    fps             | 38       |
|    time_elapsed    | 32472    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=596.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=1228.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.00313  |
|    ent_coef_loss   | 3.75     |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 663      |
| time/              |          |
|    episodes        | 256      |
|    fps             | 38       |
|    time_elapsed    | 33050    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=691.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.221    |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | 7.27     |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=688.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.174    |
|    ent_coef        | 0.00297  |
|    ent_coef_loss   | 5.87     |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 662      |
| time/              |          |
|    episodes        | 260      |
|    fps             | 38       |
|    time_elapsed    | 33655    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=378.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -9.98    |
|    critic_loss     | 2.25     |
|    ent_coef        | 0.00231  |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=726.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.163    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 663      |
| time/              |          |
|    episodes        | 264      |
|    fps             | 38       |
|    time_elapsed    | 34233    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=673.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | -2.12    |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=1305.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.415    |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | 7.54     |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 666      |
| time/              |          |
|    episodes        | 268      |
|    fps             | 38       |
|    time_elapsed    | 34815    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=1254.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.563    |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | 1.26     |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=692.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -9.67    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 0.817    |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 676      |
| time/              |          |
|    episodes        | 272      |
|    fps             | 38       |
|    time_elapsed    | 35446    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=1302.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.478    |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | 4.98     |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=835.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.282    |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -5.89    |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 705      |
| time/              |          |
|    episodes        | 276      |
|    fps             | 38       |
|    time_elapsed    | 36039    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=1301.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.941    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | 9.59     |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=616.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 1.18     |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 717      |
| time/              |          |
|    episodes        | 280      |
|    fps             | 38       |
|    time_elapsed    | 36622    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=722.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -9.66    |
|    critic_loss     | 0.22     |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=1167.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -9.68    |
|    critic_loss     | 0.022    |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -6.44    |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 723      |
| time/              |          |
|    episodes        | 284      |
|    fps             | 38       |
|    time_elapsed    | 37224    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=656.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.36     |
|    ent_coef        | 0.00302  |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=618.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -9.77    |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | 1.67     |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 730      |
| time/              |          |
|    episodes        | 288      |
|    fps             | 38       |
|    time_elapsed    | 37839    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=613.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -9.48    |
|    critic_loss     | 0.245    |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | 3.08     |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=722.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -9.45    |
|    critic_loss     | 0.082    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 728      |
| time/              |          |
|    episodes        | 292      |
|    fps             | 37       |
|    time_elapsed    | 38464    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=813.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.0817   |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | 1.36     |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=889.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.335    |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 4.55     |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 739      |
| time/              |          |
|    episodes        | 296      |
|    fps             | 37       |
|    time_elapsed    | 39048    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=644.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -9.17    |
|    critic_loss     | 0.16     |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=740.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -9.44    |
|    critic_loss     | 0.148    |
|    ent_coef        | 0.00394  |
|    ent_coef_loss   | 0.747    |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 746      |
| time/              |          |
|    episodes        | 300      |
|    fps             | 37       |
|    time_elapsed    | 39651    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=662.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -9.51    |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -1.76    |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=787.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -8.49    |
|    critic_loss     | 1.2      |
|    ent_coef        | 0.0036   |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 754      |
| time/              |          |
|    episodes        | 304      |
|    fps             | 37       |
|    time_elapsed    | 40268    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=1255.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -9.49    |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=612.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -8.94    |
|    critic_loss     | 0.166    |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -1.1     |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 763      |
| time/              |          |
|    episodes        | 308      |
|    fps             | 37       |
|    time_elapsed    | 40857    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=684.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -9       |
|    critic_loss     | 0.163    |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -0.885   |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=642.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -8.43    |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 771      |
| time/              |          |
|    episodes        | 312      |
|    fps             | 37       |
|    time_elapsed    | 41431    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=1336.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -9.11    |
|    critic_loss     | 0.285    |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | 0.688    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1580000, episode_reward=377.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.389    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | 1.67     |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 784      |
| time/              |          |
|    episodes        | 316      |
|    fps             | 37       |
|    time_elapsed    | 42004    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=652.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 0.502    |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=694.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.0536   |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | -0.0324  |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 789      |
| time/              |          |
|    episodes        | 320      |
|    fps             | 37       |
|    time_elapsed    | 42582    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=651.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -8.96    |
|    critic_loss     | 0.408    |
|    ent_coef        | 0.00313  |
|    ent_coef_loss   | -3.97    |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=693.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | -1.29    |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 800      |
| time/              |          |
|    episodes        | 324      |
|    fps             | 37       |
|    time_elapsed    | 43160    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=713.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -7.76    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -4.65    |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=1335.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -7.75    |
|    critic_loss     | 0.0543   |
|    ent_coef        | 0.00341  |
|    ent_coef_loss   | -3.45    |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 808      |
| time/              |          |
|    episodes        | 328      |
|    fps             | 37       |
|    time_elapsed    | 43736    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=719.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -8.81    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=1364.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -9.09    |
|    critic_loss     | 0.178    |
|    ent_coef        | 0.00356  |
|    ent_coef_loss   | -0.879   |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 807      |
| time/              |          |
|    episodes        | 332      |
|    fps             | 37       |
|    time_elapsed    | 44321    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=818.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.0898   |
|    ent_coef        | 0.00334  |
|    ent_coef_loss   | 1.36     |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=1326.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 816      |
| time/              |          |
|    episodes        | 336      |
|    fps             | 37       |
|    time_elapsed    | 44902    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=1343.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -8.26    |
|    critic_loss     | 0.22     |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -4.45    |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1360.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -9.59    |
|    critic_loss     | 0.254    |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | 3.82     |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 836      |
| time/              |          |
|    episodes        | 340      |
|    fps             | 37       |
|    time_elapsed    | 45485    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=684.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -9.85    |
|    critic_loss     | 0.283    |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=675.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -9.49    |
|    critic_loss     | 0.35     |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 836      |
| time/              |          |
|    episodes        | 344      |
|    fps             | 37       |
|    time_elapsed    | 46079    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=234.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -9.08    |
|    critic_loss     | 0.374    |
|    ent_coef        | 0.00436  |
|    ent_coef_loss   | -3.93    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=746.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.673    |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | -3.27    |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 825      |
| time/              |          |
|    episodes        | 348      |
|    fps             | 37       |
|    time_elapsed    | 46717    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=981.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -9.06    |
|    critic_loss     | 0.678    |
|    ent_coef        | 0.00542  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=985.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.619    |
|    ent_coef        | 0.00547  |
|    ent_coef_loss   | -4.06    |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 814      |
| time/              |          |
|    episodes        | 352      |
|    fps             | 37       |
|    time_elapsed    | 47346    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=1199.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.272    |
|    ent_coef        | 0.0042   |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=810.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -7.28    |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.0047   |
|    ent_coef_loss   | 4.2      |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 808      |
| time/              |          |
|    episodes        | 356      |
|    fps             | 37       |
|    time_elapsed    | 47938    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=416.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -6.46    |
|    critic_loss     | 0.831    |
|    ent_coef        | 0.00615  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=236.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -8.11    |
|    critic_loss     | 0.676    |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 802      |
| time/              |          |
|    episodes        | 360      |
|    fps             | 37       |
|    time_elapsed    | 48532    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=624.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -7.47    |
|    critic_loss     | 1.9      |
|    ent_coef        | 0.00577  |
|    ent_coef_loss   | -3.89    |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=709.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -7.17    |
|    critic_loss     | 0.742    |
|    ent_coef        | 0.00614  |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 801      |
| time/              |          |
|    episodes        | 364      |
|    fps             | 37       |
|    time_elapsed    | 49120    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=690.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -7.84    |
|    critic_loss     | 0.821    |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | 2.62     |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=656.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -8.3     |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00513  |
|    ent_coef_loss   | -3.44    |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 795      |
| time/              |          |
|    episodes        | 368      |
|    fps             | 37       |
|    time_elapsed    | 49709    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=689.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -9.96    |
|    critic_loss     | 0.828    |
|    ent_coef        | 0.00461  |
|    ent_coef_loss   | 0.763    |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=1355.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 0.659    |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | -0.397   |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 787      |
| time/              |          |
|    episodes        | 372      |
|    fps             | 36       |
|    time_elapsed    | 50297    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=750.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.677    |
|    ent_coef        | 0.00468  |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=689.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.302    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 762      |
| time/              |          |
|    episodes        | 376      |
|    fps             | 36       |
|    time_elapsed    | 50883    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=643.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.353    |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 2.89     |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=924.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -9.19    |
|    critic_loss     | 0.409    |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | -3.3     |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 751      |
| time/              |          |
|    episodes        | 380      |
|    fps             | 36       |
|    time_elapsed    | 51478    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=723.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -11.5    |
|    critic_loss     | 0.514    |
|    ent_coef        | 0.00441  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=680.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -9.77    |
|    critic_loss     | 0.936    |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | 0.0979   |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 758      |
| time/              |          |
|    episodes        | 384      |
|    fps             | 36       |
|    time_elapsed    | 52068    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=636.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -9.81    |
|    critic_loss     | 0.485    |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | 3.85     |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=636.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -9.59    |
|    critic_loss     | 0.491    |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 754      |
| time/              |          |
|    episodes        | 388      |
|    fps             | 36       |
|    time_elapsed    | 52656    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=1330.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.229    |
|    ent_coef        | 0.00359  |
|    ent_coef_loss   | 0.0415   |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=901.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -9.79    |
|    critic_loss     | 0.879    |
|    ent_coef        | 0.00393  |
|    ent_coef_loss   | 4.44     |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 754      |
| time/              |          |
|    episodes        | 392      |
|    fps             | 36       |
|    time_elapsed    | 53245    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=773.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.387    |
|    ent_coef        | 0.0039   |
|    ent_coef_loss   | -0.663   |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=639.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.438    |
|    ent_coef        | 0.00361  |
|    ent_coef_loss   | 4.81     |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 761      |
| time/              |          |
|    episodes        | 396      |
|    fps             | 36       |
|    time_elapsed    | 53830    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=683.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.501    |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=729.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.334    |
|    ent_coef        | 0.0039   |
|    ent_coef_loss   | 0.0251   |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 758      |
| time/              |          |
|    episodes        | 400      |
|    fps             | 36       |
|    time_elapsed    | 54419    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=560.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 560      |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=341.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -11.8    |
|    critic_loss     | 0.127    |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | -2.53    |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 762      |
| time/              |          |
|    episodes        | 404      |
|    fps             | 36       |
|    time_elapsed    | 54998    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=750.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.771    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=909.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -10.9    |
|    critic_loss     | 0.858    |
|    ent_coef        | 0.00428  |
|    ent_coef_loss   | 5.01     |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 752      |
| time/              |          |
|    episodes        | 408      |
|    fps             | 36       |
|    time_elapsed    | 55578    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=617.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.386    |
|    ent_coef        | 0.0039   |
|    ent_coef_loss   | -3.21    |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=505.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -11.7    |
|    critic_loss     | 0.949    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 2.7      |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 746      |
| time/              |          |
|    episodes        | 412      |
|    fps             | 36       |
|    time_elapsed    | 56156    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=668.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | -0.187   |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=694.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -9.87    |
|    critic_loss     | 0.445    |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | -2.54    |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 740      |
| time/              |          |
|    episodes        | 416      |
|    fps             | 36       |
|    time_elapsed    | 56745    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=1319.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.284    |
|    ent_coef        | 0.00443  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000791 |
|    n_updates       | 3434324  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=1235.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.324    |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -7.42    |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 743      |
| time/              |          |
|    episodes        | 420      |
|    fps             | 36       |
|    time_elapsed    | 57334    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=1030.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.719    |
|    ent_coef        | 0.00393  |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=667.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -10.8    |
|    critic_loss     | 0.156    |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000788 |
|    n_updates       | 3464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 743      |
| time/              |          |
|    episodes        | 424      |
|    fps             | 36       |
|    time_elapsed    | 57925    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=752.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 0.401    |
|    ent_coef        | 0.00382  |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=818.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -10.7    |
|    critic_loss     | 0.314    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 738      |
| time/              |          |
|    episodes        | 428      |
|    fps             | 36       |
|    time_elapsed    | 58517    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=634.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 0.662    |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=658.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.526    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000784 |
|    n_updates       | 3504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 737      |
| time/              |          |
|    episodes        | 432      |
|    fps             | 36       |
|    time_elapsed    | 59111    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=678.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -9.59    |
|    critic_loss     | 0.46     |
|    ent_coef        | 0.00364  |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000783 |
|    n_updates       | 3514324  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=679.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -9.57    |
|    critic_loss     | 0.611    |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 723      |
| time/              |          |
|    episodes        | 436      |
|    fps             | 36       |
|    time_elapsed    | 59707    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=668.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 0.397    |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | 0.989    |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=689.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.363    |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | -2.76    |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 702      |
| time/              |          |
|    episodes        | 440      |
|    fps             | 36       |
|    time_elapsed    | 60299    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=675.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -10.5    |
|    critic_loss     | 0.389    |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | 1.97     |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=664.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -9.34    |
|    critic_loss     | 0.515    |
|    ent_coef        | 0.0041   |
|    ent_coef_loss   | 0.174    |
|    learning_rate   | 0.000778 |
|    n_updates       | 3564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 692      |
| time/              |          |
|    episodes        | 444      |
|    fps             | 36       |
|    time_elapsed    | 60899    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=702.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.464    |
|    ent_coef        | 0.00382  |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.000777 |
|    n_updates       | 3574324  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=651.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -8.52    |
|    critic_loss     | 0.951    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.000776 |
|    n_updates       | 3584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 692      |
| time/              |          |
|    episodes        | 448      |
|    fps             | 36       |
|    time_elapsed    | 61494    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=675.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -9.49    |
|    critic_loss     | 0.287    |
|    ent_coef        | 0.00412  |
|    ent_coef_loss   | -5.14    |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=696.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -9.43    |
|    critic_loss     | 0.817    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 6.1      |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 693      |
| time/              |          |
|    episodes        | 452      |
|    fps             | 36       |
|    time_elapsed    | 62085    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=664.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -11      |
|    critic_loss     | 0.637    |
|    ent_coef        | 0.00467  |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=681.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -9.92    |
|    critic_loss     | 0.489    |
|    ent_coef        | 0.00456  |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 699      |
| time/              |          |
|    episodes        | 456      |
|    fps             | 36       |
|    time_elapsed    | 62694    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=804.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -8.61    |
|    critic_loss     | 0.968    |
|    ent_coef        | 0.00454  |
|    ent_coef_loss   | -2.36    |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=697.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -7.92    |
|    critic_loss     | 0.307    |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 699      |
| time/              |          |
|    episodes        | 460      |
|    fps             | 36       |
|    time_elapsed    | 63291    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=698.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -8.64    |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.00456  |
|    ent_coef_loss   | -0.976   |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=1112.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 1.94     |
|    ent_coef        | 0.00412  |
|    ent_coef_loss   | 0.39     |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 702      |
| time/              |          |
|    episodes        | 464      |
|    fps             | 36       |
|    time_elapsed    | 63898    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=698.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -9.32    |
|    critic_loss     | 0.378    |
|    ent_coef        | 0.00468  |
|    ent_coef_loss   | 4.92     |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=659.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 0.627    |
|    ent_coef        | 0.00455  |
|    ent_coef_loss   | 6        |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 703      |
| time/              |          |
|    episodes        | 468      |
|    fps             | 36       |
|    time_elapsed    | 64498    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=697.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -9.02    |
|    critic_loss     | 0.247    |
|    ent_coef        | 0.00485  |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=678.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -9.07    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.00512  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000764 |
|    n_updates       | 3704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 701      |
| time/              |          |
|    episodes        | 472      |
|    fps             | 36       |
|    time_elapsed    | 65089    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=653.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -8.75    |
|    critic_loss     | 0.119    |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | 0.435    |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=703.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -9.72    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 700      |
| time/              |          |
|    episodes        | 476      |
|    fps             | 36       |
|    time_elapsed    | 65682    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=678.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -7.69    |
|    critic_loss     | 0.247    |
|    ent_coef        | 0.00501  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=302.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.119    |
|    ent_coef        | 0.00485  |
|    ent_coef_loss   | -0.795   |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 702      |
| time/              |          |
|    episodes        | 480      |
|    fps             | 36       |
|    time_elapsed    | 66273    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=660.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -7.41    |
|    critic_loss     | 0.268    |
|    ent_coef        | 0.00456  |
|    ent_coef_loss   | -0.835   |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=699.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -7.56    |
|    critic_loss     | 0.211    |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 699      |
| time/              |          |
|    episodes        | 484      |
|    fps             | 36       |
|    time_elapsed    | 66866    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=673.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -7.71    |
|    critic_loss     | 0.22     |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=672.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -6.86    |
|    critic_loss     | 0.375    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | 1.78     |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 705      |
| time/              |          |
|    episodes        | 488      |
|    fps             | 36       |
|    time_elapsed    | 67459    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=671.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -9.42    |
|    critic_loss     | 0.187    |
|    ent_coef        | 0.00468  |
|    ent_coef_loss   | 0.279    |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=619.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -8.68    |
|    critic_loss     | 0.251    |
|    ent_coef        | 0.00402  |
|    ent_coef_loss   | -0.268   |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 705      |
| time/              |          |
|    episodes        | 492      |
|    fps             | 36       |
|    time_elapsed    | 68049    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=640.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -8.88    |
|    critic_loss     | 0.206    |
|    ent_coef        | 0.00423  |
|    ent_coef_loss   | 0.425    |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=662.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -9.13    |
|    critic_loss     | 0.802    |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | 2.21     |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 695      |
| time/              |          |
|    episodes        | 496      |
|    fps             | 36       |
|    time_elapsed    | 68643    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=674.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -9.85    |
|    critic_loss     | 0.655    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | -0.195   |
|    learning_rate   | 0.000751 |
|    n_updates       | 3834324  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=692.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -9.24    |
|    critic_loss     | 1.8      |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | 4.58     |
|    learning_rate   | 0.00075  |
|    n_updates       | 3844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 694      |
| time/              |          |
|    episodes        | 500      |
|    fps             | 36       |
|    time_elapsed    | 69235    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=680.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -8.41    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=681.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -9.43    |
|    critic_loss     | 0.62     |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | 0.296    |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 687      |
| time/              |          |
|    episodes        | 504      |
|    fps             | 36       |
|    time_elapsed    | 69825    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=674.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -9.55    |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00341  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=662.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -9.67    |
|    critic_loss     | 0.307    |
|    ent_coef        | 0.00386  |
|    ent_coef_loss   | -1.29    |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 693      |
| time/              |          |
|    episodes        | 508      |
|    fps             | 36       |
|    time_elapsed    | 70419    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=662.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -8.46    |
|    critic_loss     | 0.221    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=697.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -9.17    |
|    critic_loss     | 0.447    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | -1.09    |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 692      |
| time/              |          |
|    episodes        | 512      |
|    fps             | 36       |
|    time_elapsed    | 71012    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=668.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -8.91    |
|    critic_loss     | 0.229    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | 0.51     |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=675.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -7.55    |
|    critic_loss     | 0.601    |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | -3.73    |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 690      |
| time/              |          |
|    episodes        | 516      |
|    fps             | 36       |
|    time_elapsed    | 71604    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=645.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -8.45    |
|    critic_loss     | 0.305    |
|    ent_coef        | 0.00353  |
|    ent_coef_loss   | 3.18     |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=662.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -9.99    |
|    critic_loss     | 0.68     |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -2.99    |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 680      |
| time/              |          |
|    episodes        | 520      |
|    fps             | 36       |
|    time_elapsed    | 72196    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=923.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -9.39    |
|    critic_loss     | 0.737    |
|    ent_coef        | 0.00414  |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=657.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -8.67    |
|    critic_loss     | 0.488    |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 679      |
| time/              |          |
|    episodes        | 524      |
|    fps             | 35       |
|    time_elapsed    | 72783    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=629.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -7.12    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.00595  |
|    ent_coef_loss   | -0.684   |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=894.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -7.11    |
|    critic_loss     | 0.374    |
|    ent_coef        | 0.00671  |
|    ent_coef_loss   | -0.737   |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 678      |
| time/              |          |
|    episodes        | 528      |
|    fps             | 35       |
|    time_elapsed    | 73365    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=903.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -7.6     |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.00707  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=1175.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -6.54    |
|    critic_loss     | 1.78     |
|    ent_coef        | 0.00682  |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 686      |
| time/              |          |
|    episodes        | 532      |
|    fps             | 35       |
|    time_elapsed    | 73953    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=705.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -6.65    |
|    critic_loss     | 0.55     |
|    ent_coef        | 0.00672  |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=896.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -5.38    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00726  |
|    ent_coef_loss   | 0.685    |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 694      |
| time/              |          |
|    episodes        | 536      |
|    fps             | 35       |
|    time_elapsed    | 74543    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=1178.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -5.2     |
|    critic_loss     | 0.797    |
|    ent_coef        | 0.00784  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000731 |
|    n_updates       | 4034324  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=1181.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -4.68    |
|    critic_loss     | 0.602    |
|    ent_coef        | 0.0068   |
|    ent_coef_loss   | -3.48    |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 709      |
| time/              |          |
|    episodes        | 540      |
|    fps             | 35       |
|    time_elapsed    | 75137    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=637.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -5.64    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.00711  |
|    ent_coef_loss   | 2.97     |
|    learning_rate   | 0.000729 |
|    n_updates       | 4054324  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=650.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -5.53    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.00713  |
|    ent_coef_loss   | 5.71     |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 718      |
| time/              |          |
|    episodes        | 544      |
|    fps             | 35       |
|    time_elapsed    | 75729    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=654.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -6.37    |
|    critic_loss     | 0.675    |
|    ent_coef        | 0.00692  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000727 |
|    n_updates       | 4074324  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=634.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -3.96    |
|    critic_loss     | 0.609    |
|    ent_coef        | 0.00767  |
|    ent_coef_loss   | -0.486   |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 718      |
| time/              |          |
|    episodes        | 548      |
|    fps             | 35       |
|    time_elapsed    | 76326    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=680.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -5.04    |
|    critic_loss     | 0.526    |
|    ent_coef        | 0.00817  |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000725 |
|    n_updates       | 4094324  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=650.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -5.35    |
|    critic_loss     | 0.379    |
|    ent_coef        | 0.00788  |
|    ent_coef_loss   | -0.128   |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 716      |
| time/              |          |
|    episodes        | 552      |
|    fps             | 35       |
|    time_elapsed    | 76918    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=640.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -6.26    |
|    critic_loss     | 0.595    |
|    ent_coef        | 0.00705  |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=643.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -4.81    |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.00731  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 556      |
|    fps             | 35       |
|    time_elapsed    | 77512    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=674.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -7.58    |
|    critic_loss     | 0.517    |
|    ent_coef        | 0.00645  |
|    ent_coef_loss   | 3.67     |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=661.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -7.57    |
|    critic_loss     | 0.321    |
|    ent_coef        | 0.00582  |
|    ent_coef_loss   | 1.99     |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 560      |
|    fps             | 35       |
|    time_elapsed    | 78106    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=673.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -6.63    |
|    critic_loss     | 0.356    |
|    ent_coef        | 0.00549  |
|    ent_coef_loss   | -2.77    |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=636.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -5.77    |
|    critic_loss     | 0.167    |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | -7.5     |
|    learning_rate   | 0.000718 |
|    n_updates       | 4164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 715      |
| time/              |          |
|    episodes        | 564      |
|    fps             | 35       |
|    time_elapsed    | 78706    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=662.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -8.25    |
|    critic_loss     | 0.307    |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | 0.113    |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=885.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 0.544    |
|    ent_coef        | 0.00667  |
|    ent_coef_loss   | 4.45     |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 710      |
| time/              |          |
|    episodes        | 568      |
|    fps             | 35       |
|    time_elapsed    | 79310    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=584.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -5.08    |
|    critic_loss     | 35.8     |
|    ent_coef        | 0.0089   |
|    ent_coef_loss   | -2.98    |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=1172.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -7.27    |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.0086   |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 721      |
| time/              |          |
|    episodes        | 572      |
|    fps             | 35       |
|    time_elapsed    | 79912    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=755.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -4.71    |
|    critic_loss     | 1.48     |
|    ent_coef        | 0.00743  |
|    ent_coef_loss   | 3        |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=125.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -8.6     |
|    critic_loss     | 0.986    |
|    ent_coef        | 0.00802  |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 742      |
| time/              |          |
|    episodes        | 576      |
|    fps             | 35       |
|    time_elapsed    | 80510    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=275.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -6.84    |
|    critic_loss     | 0.363    |
|    ent_coef        | 0.00885  |
|    ent_coef_loss   | -4.98    |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=629.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -9.48    |
|    critic_loss     | 1.34     |
|    ent_coef        | 0.00873  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 745      |
| time/              |          |
|    episodes        | 580      |
|    fps             | 35       |
|    time_elapsed    | 81100    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=628.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -8.68    |
|    critic_loss     | 9.81     |
|    ent_coef        | 0.00726  |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=659.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -13      |
|    critic_loss     | 1.63     |
|    ent_coef        | 0.00548  |
|    ent_coef_loss   | -3.44    |
|    learning_rate   | 0.000708 |
|    n_updates       | 4264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 740      |
| time/              |          |
|    episodes        | 584      |
|    fps             | 35       |
|    time_elapsed    | 81692    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=871.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -11.2    |
|    critic_loss     | 2.55     |
|    ent_coef        | 0.00641  |
|    ent_coef_loss   | -2.15    |
|    learning_rate   | 0.000707 |
|    n_updates       | 4274324  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=644.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -8.32    |
|    critic_loss     | 1.48     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.000706 |
|    n_updates       | 4284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 734      |
| time/              |          |
|    episodes        | 588      |
|    fps             | 35       |
|    time_elapsed    | 82285    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=841.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -8.8     |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -3.15    |
|    learning_rate   | 0.000705 |
|    n_updates       | 4294324  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=1191.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -9.88    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.0132   |
|    ent_coef_loss   | -3.52    |
|    learning_rate   | 0.000704 |
|    n_updates       | 4304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 736      |
| time/              |          |
|    episodes        | 592      |
|    fps             | 35       |
|    time_elapsed    | 82886    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=701.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -8.93    |
|    critic_loss     | 13.3     |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | 3.25     |
|    learning_rate   | 0.000703 |
|    n_updates       | 4314324  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=803.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -8.93    |
|    critic_loss     | 0.967    |
|    ent_coef        | 0.0144   |
|    ent_coef_loss   | -0.361   |
|    learning_rate   | 0.000702 |
|    n_updates       | 4324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 749      |
| time/              |          |
|    episodes        | 596      |
|    fps             | 35       |
|    time_elapsed    | 83448    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=853.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -7.05    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.000701 |
|    n_updates       | 4334324  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=275.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -3.36    |
|    critic_loss     | 18.4     |
|    ent_coef        | 0.0151   |
|    ent_coef_loss   | 3.31     |
|    learning_rate   | 0.0007   |
|    n_updates       | 4344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 753      |
| time/              |          |
|    episodes        | 600      |
|    fps             | 35       |
|    time_elapsed    | 83948    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=534.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -1.06    |
|    critic_loss     | 0.891    |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -2.64    |
|    learning_rate   | 0.000699 |
|    n_updates       | 4354324  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=233.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -3.72    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.0132   |
|    ent_coef_loss   | -0.573   |
|    learning_rate   | 0.000698 |
|    n_updates       | 4364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 746      |
| time/              |          |
|    episodes        | 604      |
|    fps             | 35       |
|    time_elapsed    | 84453    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=1125.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 5.2      |
|    ent_coef        | 0.0177   |
|    ent_coef_loss   | -0.956   |
|    learning_rate   | 0.000697 |
|    n_updates       | 4374324  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=751.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | 0.192    |
|    critic_loss     | 3.14     |
|    ent_coef        | 0.0146   |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.000696 |
|    n_updates       | 4384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 733      |
| time/              |          |
|    episodes        | 608      |
|    fps             | 35       |
|    time_elapsed    | 84960    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=1203.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | 1.25     |
|    critic_loss     | 4.26     |
|    ent_coef        | 0.0179   |
|    ent_coef_loss   | -2.18    |
|    learning_rate   | 0.000695 |
|    n_updates       | 4394324  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=749.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | 4.72     |
|    critic_loss     | 20.5     |
|    ent_coef        | 0.0155   |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.000694 |
|    n_updates       | 4404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 752      |
| time/              |          |
|    episodes        | 612      |
|    fps             | 35       |
|    time_elapsed    | 85466    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=575.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | 0.515    |
|    critic_loss     | 3.85     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | 1.55     |
|    learning_rate   | 0.000693 |
|    n_updates       | 4414324  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=1188.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -0.302   |
|    critic_loss     | 2.28     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | 2.5      |
|    learning_rate   | 0.000692 |
|    n_updates       | 4424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 760      |
| time/              |          |
|    episodes        | 616      |
|    fps             | 35       |
|    time_elapsed    | 85973    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=328.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | 0.162    |
|    critic_loss     | 6.08     |
|    ent_coef        | 0.015    |
|    ent_coef_loss   | 0.0434   |
|    learning_rate   | 0.000691 |
|    n_updates       | 4434324  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=1155.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.0219   |
|    ent_coef_loss   | 2.96     |
|    learning_rate   | 0.00069  |
|    n_updates       | 4444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 755      |
| time/              |          |
|    episodes        | 620      |
|    fps             | 35       |
|    time_elapsed    | 86483    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=520.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 4.64     |
|    ent_coef        | 0.0244   |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000689 |
|    n_updates       | 4454324  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=535.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | 4.93     |
|    critic_loss     | 18.5     |
|    ent_coef        | 0.0226   |
|    ent_coef_loss   | 0.324    |
|    learning_rate   | 0.000688 |
|    n_updates       | 4464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 763      |
| time/              |          |
|    episodes        | 624      |
|    fps             | 35       |
|    time_elapsed    | 86993    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=553.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | 3.94     |
|    critic_loss     | 2.63     |
|    ent_coef        | 0.0212   |
|    ent_coef_loss   | 0.846    |
|    learning_rate   | 0.000687 |
|    n_updates       | 4474324  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=1162.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | 7.15     |
|    critic_loss     | 5.39     |
|    ent_coef        | 0.0199   |
|    ent_coef_loss   | -0.888   |
|    learning_rate   | 0.000686 |
|    n_updates       | 4484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 756      |
| time/              |          |
|    episodes        | 628      |
|    fps             | 35       |
|    time_elapsed    | 87550    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=548.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | 8.71     |
|    critic_loss     | 2.16     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000685 |
|    n_updates       | 4494324  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=543.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | 2.06     |
|    critic_loss     | 1.35     |
|    ent_coef        | 0.0152   |
|    ent_coef_loss   | -2.69    |
|    learning_rate   | 0.000684 |
|    n_updates       | 4504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 755      |
| time/              |          |
|    episodes        | 632      |
|    fps             | 35       |
|    time_elapsed    | 88094    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=1161.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | 1.06     |
|    critic_loss     | 9.77     |
|    ent_coef        | 0.0136   |
|    ent_coef_loss   | -0.648   |
|    learning_rate   | 0.000683 |
|    n_updates       | 4514324  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=1166.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -0.34    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | -3.49    |
|    learning_rate   | 0.000682 |
|    n_updates       | 4524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 745      |
| time/              |          |
|    episodes        | 636      |
|    fps             | 35       |
|    time_elapsed    | 88604    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=546.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 3.44     |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | 0.551    |
|    learning_rate   | 0.000681 |
|    n_updates       | 4534324  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=898.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 2.44     |
|    ent_coef        | 0.0118   |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.00068  |
|    n_updates       | 4544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 732      |
| time/              |          |
|    episodes        | 640      |
|    fps             | 35       |
|    time_elapsed    | 89115    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=720.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | 1.32     |
|    critic_loss     | 1.63     |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | -0.327   |
|    learning_rate   | 0.000679 |
|    n_updates       | 4554324  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=1196.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 3.49     |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000678 |
|    n_updates       | 4564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 730      |
| time/              |          |
|    episodes        | 644      |
|    fps             | 35       |
|    time_elapsed    | 89624    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=1194.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | 0.662    |
|    critic_loss     | 4.33     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | 2.71     |
|    learning_rate   | 0.000677 |
|    n_updates       | 4574324  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=218.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 1.48     |
|    ent_coef        | 0.011    |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.000676 |
|    n_updates       | 4584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 725      |
| time/              |          |
|    episodes        | 648      |
|    fps             | 35       |
|    time_elapsed    | 90134    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=598.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 1.74     |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | -0.237   |
|    learning_rate   | 0.000675 |
|    n_updates       | 4594324  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=646.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 4.03     |
|    ent_coef        | 0.0099   |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000674 |
|    n_updates       | 4604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 720      |
| time/              |          |
|    episodes        | 652      |
|    fps             | 35       |
|    time_elapsed    | 90678    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=619.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -3.55    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.00897  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.000673 |
|    n_updates       | 4614324  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=1189.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 1.33     |
|    ent_coef        | 0.00979  |
|    ent_coef_loss   | -2.29    |
|    learning_rate   | 0.000672 |
|    n_updates       | 4624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 738      |
| time/              |          |
|    episodes        | 656      |
|    fps             | 35       |
|    time_elapsed    | 91249    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=918.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.9      |
|    ent_coef        | 0.00878  |
|    ent_coef_loss   | -1.94    |
|    learning_rate   | 0.000671 |
|    n_updates       | 4634324  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=1170.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -3.2     |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.00975  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.00067  |
|    n_updates       | 4644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 754      |
| time/              |          |
|    episodes        | 660      |
|    fps             | 35       |
|    time_elapsed    | 91806    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=869.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 2.23     |
|    ent_coef        | 0.00905  |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 0.000669 |
|    n_updates       | 4654324  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=772.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.936    |
|    ent_coef        | 0.00862  |
|    ent_coef_loss   | -0.985   |
|    learning_rate   | 0.000668 |
|    n_updates       | 4664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 765      |
| time/              |          |
|    episodes        | 664      |
|    fps             | 35       |
|    time_elapsed    | 92353    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=611.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.73     |
|    ent_coef        | 0.00765  |
|    ent_coef_loss   | 1.93     |
|    learning_rate   | 0.000667 |
|    n_updates       | 4674324  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=1151.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -3.59    |
|    critic_loss     | 0.597    |
|    ent_coef        | 0.00715  |
|    ent_coef_loss   | -0.618   |
|    learning_rate   | 0.000666 |
|    n_updates       | 4684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 782      |
| time/              |          |
|    episodes        | 668      |
|    fps             | 35       |
|    time_elapsed    | 92895    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=1156.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -6.54    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00676  |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000665 |
|    n_updates       | 4694324  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=1177.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -7.29    |
|    critic_loss     | 0.524    |
|    ent_coef        | 0.00686  |
|    ent_coef_loss   | -0.556   |
|    learning_rate   | 0.000664 |
|    n_updates       | 4704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 792      |
| time/              |          |
|    episodes        | 672      |
|    fps             | 35       |
|    time_elapsed    | 93391    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=1209.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -6.82    |
|    critic_loss     | 0.791    |
|    ent_coef        | 0.00703  |
|    ent_coef_loss   | 3.9      |
|    learning_rate   | 0.000663 |
|    n_updates       | 4714324  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=1188.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -5.22    |
|    critic_loss     | 1.55     |
|    ent_coef        | 0.00653  |
|    ent_coef_loss   | 5.58     |
|    learning_rate   | 0.000662 |
|    n_updates       | 4724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 792      |
| time/              |          |
|    episodes        | 676      |
|    fps             | 35       |
|    time_elapsed    | 93889    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=1187.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -6.65    |
|    critic_loss     | 1.72     |
|    ent_coef        | 0.00631  |
|    ent_coef_loss   | 0.0702   |
|    learning_rate   | 0.000661 |
|    n_updates       | 4734324  |
