Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to 2gdl/SAC_52
Eval num_timesteps=10000, episode_reward=-320332.29 +/- 2954.59
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | 3.19e+03 |
|    critic_loss     | 148      |
|    ent_coef        | 0.00964  |
|    ent_coef_loss   | 89.8     |
|    learning_rate   | 9.99e-06 |
|    n_updates       | 3689798  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-317531.14 +/- 3452.47
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | 4.32e+03  |
|    critic_loss     | 274       |
|    ent_coef        | 0.0106    |
|    ent_coef_loss   | 65        |
|    learning_rate   | 9.98e-06  |
|    n_updates       | 3699798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.16e+05 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 15        |
|    time_elapsed    | 1286      |
|    total_timesteps | 20000     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-225217.44 +/- 118131.34
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.25e+05 |
| time/              |           |
|    total_timesteps | 30000     |
| train/             |           |
|    actor_loss      | 4.33e+03  |
|    critic_loss     | 68        |
|    ent_coef        | 0.0117    |
|    ent_coef_loss   | 87.1      |
|    learning_rate   | 9.97e-06  |
|    n_updates       | 3709798   |
----------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=-310647.48 +/- 5511.19
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.11e+05 |
| time/              |           |
|    total_timesteps | 40000     |
| train/             |           |
|    actor_loss      | 4.4e+03   |
|    critic_loss     | 5.09e+03  |
|    ent_coef        | 0.0129    |
|    ent_coef_loss   | 64.2      |
|    learning_rate   | 9.96e-06  |
|    n_updates       | 3719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.86e+05 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 15        |
|    time_elapsed    | 2512      |
|    total_timesteps | 40000     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-319825.86 +/- 3688.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 5.04e+03 |
|    critic_loss     | 158      |
|    ent_coef        | 0.0142   |
|    ent_coef_loss   | 47.4     |
|    learning_rate   | 9.95e-06 |
|    n_updates       | 3729798  |
---------------------------------
Eval num_timesteps=60000, episode_reward=-319664.46 +/- 3709.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 5.38e+03 |
|    critic_loss     | 197      |
|    ent_coef        | 0.0157   |
|    ent_coef_loss   | 34.1     |
|    learning_rate   | 9.94e-06 |
|    n_updates       | 3739798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.95e+05 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 16        |
|    time_elapsed    | 3735      |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-319784.21 +/- 3452.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 5.57e+03 |
|    critic_loss     | 131      |
|    ent_coef        | 0.0173   |
|    ent_coef_loss   | 34.3     |
|    learning_rate   | 9.93e-06 |
|    n_updates       | 3749798  |
---------------------------------
Eval num_timesteps=80000, episode_reward=-318446.98 +/- 3753.24
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 80000     |
| train/             |           |
|    actor_loss      | 5.75e+03  |
|    critic_loss     | 127       |
|    ent_coef        | 0.0191    |
|    ent_coef_loss   | 25.6      |
|    learning_rate   | 9.92e-06  |
|    n_updates       | 3759798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3e+05   |
| time/              |          |
|    episodes        | 16       |
|    fps             | 16       |
|    time_elapsed    | 4963     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-318560.28 +/- 3676.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.19e+05 |
| time/              |           |
|    total_timesteps | 90000     |
| train/             |           |
|    actor_loss      | 5.92e+03  |
|    critic_loss     | 54.8      |
|    ent_coef        | 0.021     |
|    ent_coef_loss   | 26.5      |
|    learning_rate   | 9.91e-06  |
|    n_updates       | 3769798   |
----------------------------------
Eval num_timesteps=100000, episode_reward=-318469.47 +/- 3787.72
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 100000    |
| train/             |           |
|    actor_loss      | 6.06e+03  |
|    critic_loss     | 39.8      |
|    ent_coef        | 0.0232    |
|    ent_coef_loss   | 19.1      |
|    learning_rate   | 9.9e-06   |
|    n_updates       | 3779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.04e+05 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 16        |
|    time_elapsed    | 6213      |
|    total_timesteps | 100000    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-316478.43 +/- 4151.16
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 110000    |
| train/             |           |
|    actor_loss      | 6.03e+03  |
|    critic_loss     | 1.86e+03  |
|    ent_coef        | 0.0256    |
|    ent_coef_loss   | 20.5      |
|    learning_rate   | 9.89e-06  |
|    n_updates       | 3789798   |
----------------------------------
Eval num_timesteps=120000, episode_reward=-318040.19 +/- 4314.88
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 120000    |
| train/             |           |
|    actor_loss      | 6.18e+03  |
|    critic_loss     | 78.7      |
|    ent_coef        | 0.0282    |
|    ent_coef_loss   | 16.1      |
|    learning_rate   | 9.88e-06  |
|    n_updates       | 3799798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.06e+05 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 16        |
|    time_elapsed    | 7450      |
|    total_timesteps | 120000    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-316345.26 +/- 4219.10
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 130000    |
| train/             |           |
|    actor_loss      | 6.07e+03  |
|    critic_loss     | 22.5      |
|    ent_coef        | 0.031     |
|    ent_coef_loss   | 9.94      |
|    learning_rate   | 9.87e-06  |
|    n_updates       | 3809798   |
----------------------------------
Eval num_timesteps=140000, episode_reward=-315800.21 +/- 4706.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 140000    |
| train/             |           |
|    actor_loss      | 6.15e+03  |
|    critic_loss     | 48.4      |
|    ent_coef        | 0.0342    |
|    ent_coef_loss   | 16        |
|    learning_rate   | 9.86e-06  |
|    n_updates       | 3819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.08e+05 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 16        |
|    time_elapsed    | 8682      |
|    total_timesteps | 140000    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-313427.45 +/- 4068.15
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 150000    |
| train/             |           |
|    actor_loss      | 6.34e+03  |
|    critic_loss     | 57.8      |
|    ent_coef        | 0.0377    |
|    ent_coef_loss   | 12.9      |
|    learning_rate   | 9.85e-06  |
|    n_updates       | 3829798   |
----------------------------------
Eval num_timesteps=160000, episode_reward=-319672.40 +/- 3781.91
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 6.22e+03 |
|    critic_loss     | 326      |
|    ent_coef        | 0.0414   |
|    ent_coef_loss   | 7.59     |
|    learning_rate   | 9.84e-06 |
|    n_updates       | 3839798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.09e+05 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 16        |
|    time_elapsed    | 9923      |
|    total_timesteps | 160000    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-318013.43 +/- 4347.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 170000    |
| train/             |           |
|    actor_loss      | 6.25e+03  |
|    critic_loss     | 921       |
|    ent_coef        | 0.0456    |
|    ent_coef_loss   | 7.85      |
|    learning_rate   | 9.83e-06  |
|    n_updates       | 3849798   |
----------------------------------
Eval num_timesteps=180000, episode_reward=-316503.09 +/- 4131.24
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 180000    |
| train/             |           |
|    actor_loss      | 6.19e+03  |
|    critic_loss     | 89        |
|    ent_coef        | 0.0503    |
|    ent_coef_loss   | 7.68      |
|    learning_rate   | 9.82e-06  |
|    n_updates       | 3859798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.1e+05 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 16       |
|    time_elapsed    | 11137    |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-318119.30 +/- 4217.66
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 190000    |
| train/             |           |
|    actor_loss      | 6.27e+03  |
|    critic_loss     | 79.8      |
|    ent_coef        | 0.0555    |
|    ent_coef_loss   | 6.85      |
|    learning_rate   | 9.81e-06  |
|    n_updates       | 3869798   |
----------------------------------
Eval num_timesteps=200000, episode_reward=-314553.40 +/- 3504.75
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.15e+05 |
| time/              |           |
|    total_timesteps | 200000    |
| train/             |           |
|    actor_loss      | 6.25e+03  |
|    critic_loss     | 63.2      |
|    ent_coef        | 0.0611    |
|    ent_coef_loss   | 6.3       |
|    learning_rate   | 9.8e-06   |
|    n_updates       | 3879798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.1e+05 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 16       |
|    time_elapsed    | 12352    |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-318058.62 +/- 4291.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 210000    |
| train/             |           |
|    actor_loss      | 6.36e+03  |
|    critic_loss     | 30.4      |
|    ent_coef        | 0.0674    |
|    ent_coef_loss   | 5.85      |
|    learning_rate   | 9.79e-06  |
|    n_updates       | 3889798   |
----------------------------------
Eval num_timesteps=220000, episode_reward=-319777.86 +/- 3570.86
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 6.37e+03 |
|    critic_loss     | 25.7     |
|    ent_coef        | 0.0741   |
|    ent_coef_loss   | 5.23     |
|    learning_rate   | 9.78e-06 |
|    n_updates       | 3899798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.11e+05 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 16        |
|    time_elapsed    | 13562     |
|    total_timesteps | 220000    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-314332.42 +/- 3615.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.14e+05 |
| time/              |           |
|    total_timesteps | 230000    |
| train/             |           |
|    actor_loss      | 6.35e+03  |
|    critic_loss     | 19        |
|    ent_coef        | 0.0814    |
|    ent_coef_loss   | 3.47      |
|    learning_rate   | 9.77e-06  |
|    n_updates       | 3909798   |
----------------------------------
Eval num_timesteps=240000, episode_reward=-312966.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 240000    |
| train/             |           |
|    actor_loss      | 6.14e+03  |
|    critic_loss     | 391       |
|    ent_coef        | 0.0903    |
|    ent_coef_loss   | 7.89      |
|    learning_rate   | 9.76e-06  |
|    n_updates       | 3919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.11e+05 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 16        |
|    time_elapsed    | 14812     |
|    total_timesteps | 240000    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-316197.66 +/- 4424.87
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 250000    |
| train/             |           |
|    actor_loss      | 6.2e+03   |
|    critic_loss     | 18.1      |
|    ent_coef        | 0.0997    |
|    ent_coef_loss   | 8.32      |
|    learning_rate   | 9.75e-06  |
|    n_updates       | 3929798   |
----------------------------------
Eval num_timesteps=260000, episode_reward=-315697.97 +/- 4832.93
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 260000    |
| train/             |           |
|    actor_loss      | 6.15e+03  |
|    critic_loss     | 178       |
|    ent_coef        | 0.11      |
|    ent_coef_loss   | 8.01      |
|    learning_rate   | 9.74e-06  |
|    n_updates       | 3939798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.12e+05 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 16        |
|    time_elapsed    | 16028     |
|    total_timesteps | 260000    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-317432.92 +/- 5121.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 270000    |
| train/             |           |
|    actor_loss      | 6.18e+03  |
|    critic_loss     | 108       |
|    ent_coef        | 0.12      |
|    ent_coef_loss   | 3.01      |
|    learning_rate   | 9.73e-06  |
|    n_updates       | 3949798   |
----------------------------------
Eval num_timesteps=280000, episode_reward=-317174.78 +/- 5438.56
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 280000    |
| train/             |           |
|    actor_loss      | 6.23e+03  |
|    critic_loss     | 112       |
|    ent_coef        | 0.132     |
|    ent_coef_loss   | 3.36      |
|    learning_rate   | 9.72e-06  |
|    n_updates       | 3959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.12e+05 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 16        |
|    time_elapsed    | 17245     |
|    total_timesteps | 280000    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-311892.80 +/- 4860.68
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.12e+05 |
| time/              |           |
|    total_timesteps | 290000    |
| train/             |           |
|    actor_loss      | 6.37e+03  |
|    critic_loss     | 87.3      |
|    ent_coef        | 0.145     |
|    ent_coef_loss   | 2.37      |
|    learning_rate   | 9.71e-06  |
|    n_updates       | 3969798   |
----------------------------------
Eval num_timesteps=300000, episode_reward=-308094.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.08e+05 |
| time/              |           |
|    total_timesteps | 300000    |
| train/             |           |
|    actor_loss      | 6.22e+03  |
|    critic_loss     | 612       |
|    ent_coef        | 0.158     |
|    ent_coef_loss   | 1.39      |
|    learning_rate   | 9.7e-06   |
|    n_updates       | 3979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.12e+05 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 16        |
|    time_elapsed    | 18511     |
|    total_timesteps | 300000    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-310307.86 +/- 5653.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.1e+05 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 6.34e+03 |
|    critic_loss     | 16.5     |
|    ent_coef        | 0.171    |
|    ent_coef_loss   | 0.564    |
|    learning_rate   | 9.69e-06 |
|    n_updates       | 3989798  |
---------------------------------
Eval num_timesteps=320000, episode_reward=-312780.10 +/- 7213.26
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 320000    |
| train/             |           |
|    actor_loss      | 6.29e+03  |
|    critic_loss     | 130       |
|    ent_coef        | 0.175     |
|    ent_coef_loss   | -0.0354   |
|    learning_rate   | 9.68e-06  |
|    n_updates       | 3999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.13e+05 |
| time/              |           |
|    episodes        | 64        |
|    fps             | 16        |
|    time_elapsed    | 19913     |
|    total_timesteps | 320000    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-310619.63 +/- 13466.26
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.11e+05 |
| time/              |           |
|    total_timesteps | 330000    |
| train/             |           |
|    actor_loss      | 6.28e+03  |
|    critic_loss     | 57.6      |
|    ent_coef        | 0.17      |
|    ent_coef_loss   | 0.157     |
|    learning_rate   | 9.67e-06  |
|    n_updates       | 4009798   |
----------------------------------
Eval num_timesteps=340000, episode_reward=-159118.60 +/- 132677.78
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+05 |
| time/              |           |
|    total_timesteps | 340000    |
| train/             |           |
|    actor_loss      | 6.08e+03  |
|    critic_loss     | 140       |
|    ent_coef        | 0.173     |
|    ent_coef_loss   | 0.646     |
|    learning_rate   | 9.66e-06  |
|    n_updates       | 4019798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.09e+05 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 15        |
|    time_elapsed    | 21305     |
|    total_timesteps | 340000    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-162734.57 +/- 129726.14
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.63e+05 |
| time/              |           |
|    total_timesteps | 350000    |
| train/             |           |
|    actor_loss      | 6.17e+03  |
|    critic_loss     | 75.7      |
|    ent_coef        | 0.177     |
|    ent_coef_loss   | 0.387     |
|    learning_rate   | 9.65e-06  |
|    n_updates       | 4029798   |
----------------------------------
Eval num_timesteps=360000, episode_reward=-161761.04 +/- 130520.90
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.62e+05 |
| time/              |           |
|    total_timesteps | 360000    |
| train/             |           |
|    actor_loss      | 6.25e+03  |
|    critic_loss     | 67        |
|    ent_coef        | 0.17      |
|    ent_coef_loss   | -0.276    |
|    learning_rate   | 9.64e-06  |
|    n_updates       | 4039798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -3.1e+05 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 15       |
|    time_elapsed    | 22661    |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-117974.66 +/- 101820.65
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.18e+05 |
| time/              |           |
|    total_timesteps | 370000    |
| train/             |           |
|    actor_loss      | 6.21e+03  |
|    critic_loss     | 241       |
|    ent_coef        | 0.157     |
|    ent_coef_loss   | -0.596    |
|    learning_rate   | 9.63e-06  |
|    n_updates       | 4049798   |
----------------------------------
New best mean reward!
Eval num_timesteps=380000, episode_reward=-316617.75 +/- 6121.35
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 380000    |
| train/             |           |
|    actor_loss      | 6.22e+03  |
|    critic_loss     | 768       |
|    ent_coef        | 0.143     |
|    ent_coef_loss   | -1.46     |
|    learning_rate   | 9.62e-06  |
|    n_updates       | 4059798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.04e+05 |
| time/              |           |
|    episodes        | 76        |
|    fps             | 15        |
|    time_elapsed    | 23996     |
|    total_timesteps | 380000    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-316745.97 +/- 5965.84
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 390000    |
| train/             |           |
|    actor_loss      | 6.16e+03  |
|    critic_loss     | 45.5      |
|    ent_coef        | 0.134     |
|    ent_coef_loss   | 0.346     |
|    learning_rate   | 9.61e-06  |
|    n_updates       | 4069798   |
----------------------------------
Eval num_timesteps=400000, episode_reward=-316807.78 +/- 5889.55
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 400000    |
| train/             |           |
|    actor_loss      | 6.13e+03  |
|    critic_loss     | 104       |
|    ent_coef        | 0.127     |
|    ent_coef_loss   | -0.57     |
|    learning_rate   | 9.6e-06   |
|    n_updates       | 4079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.05e+05 |
| time/              |           |
|    episodes        | 80        |
|    fps             | 15        |
|    time_elapsed    | 25322     |
|    total_timesteps | 400000    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-152636.28 +/- 137971.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.53e+05 |
| time/              |           |
|    total_timesteps | 410000    |
| train/             |           |
|    actor_loss      | 5.98e+03  |
|    critic_loss     | 388       |
|    ent_coef        | 0.117     |
|    ent_coef_loss   | -0.0353   |
|    learning_rate   | 9.59e-06  |
|    n_updates       | 4089798   |
----------------------------------
Eval num_timesteps=420000, episode_reward=-208033.41 +/- 139048.63
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.08e+05 |
| time/              |           |
|    total_timesteps | 420000    |
| train/             |           |
|    actor_loss      | 5.84e+03  |
|    critic_loss     | 269       |
|    ent_coef        | 0.116     |
|    ent_coef_loss   | 1.27      |
|    learning_rate   | 9.58e-06  |
|    n_updates       | 4099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.96e+05 |
| time/              |           |
|    episodes        | 84        |
|    fps             | 15        |
|    time_elapsed    | 26644     |
|    total_timesteps | 420000    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-316406.82 +/- 6320.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 430000    |
| train/             |           |
|    actor_loss      | 6.07e+03  |
|    critic_loss     | 82.6      |
|    ent_coef        | 0.127     |
|    ent_coef_loss   | 1.37      |
|    learning_rate   | 9.57e-06  |
|    n_updates       | 4109798   |
----------------------------------
Eval num_timesteps=440000, episode_reward=-317903.70 +/- 4487.14
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+05 |
| time/              |           |
|    total_timesteps | 440000    |
| train/             |           |
|    actor_loss      | 5.77e+03  |
|    critic_loss     | 808       |
|    ent_coef        | 0.138     |
|    ent_coef_loss   | 2.01      |
|    learning_rate   | 9.56e-06  |
|    n_updates       | 4119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.96e+05 |
| time/              |           |
|    episodes        | 88        |
|    fps             | 15        |
|    time_elapsed    | 27965     |
|    total_timesteps | 440000    |
----------------------------------
Eval num_timesteps=450000, episode_reward=-319834.80 +/- 3465.96
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | 5.88e+03 |
|    critic_loss     | 191      |
|    ent_coef        | 0.151    |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 9.55e-06 |
|    n_updates       | 4129798  |
---------------------------------
Eval num_timesteps=460000, episode_reward=-319737.84 +/- 3660.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+05 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | 5.96e+03 |
|    critic_loss     | 125      |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | 0.913    |
|    learning_rate   | 9.54e-06 |
|    n_updates       | 4139798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.97e+05 |
| time/              |           |
|    episodes        | 92        |
|    fps             | 15        |
|    time_elapsed    | 29293     |
|    total_timesteps | 460000    |
----------------------------------
Eval num_timesteps=470000, episode_reward=-313629.64 +/- 3969.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.14e+05 |
| time/              |           |
|    total_timesteps | 470000    |
| train/             |           |
|    actor_loss      | 5.96e+03  |
|    critic_loss     | 116       |
|    ent_coef        | 0.176     |
|    ent_coef_loss   | -0.296    |
|    learning_rate   | 9.53e-06  |
|    n_updates       | 4149798   |
----------------------------------
Eval num_timesteps=480000, episode_reward=-319430.13 +/- 4274.36
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.19e+05 |
| time/              |           |
|    total_timesteps | 480000    |
| train/             |           |
|    actor_loss      | 5.93e+03  |
|    critic_loss     | 97.2      |
|    ent_coef        | 0.175     |
|    ent_coef_loss   | -0.947    |
|    learning_rate   | 9.52e-06  |
|    n_updates       | 4159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.98e+05 |
| time/              |           |
|    episodes        | 96        |
|    fps             | 15        |
|    time_elapsed    | 30629     |
|    total_timesteps | 480000    |
----------------------------------
Eval num_timesteps=490000, episode_reward=-317249.19 +/- 5288.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 490000    |
| train/             |           |
|    actor_loss      | 5.96e+03  |
|    critic_loss     | 135       |
|    ent_coef        | 0.165     |
|    ent_coef_loss   | -0.756    |
|    learning_rate   | 9.51e-06  |
|    n_updates       | 4169798   |
----------------------------------
Eval num_timesteps=500000, episode_reward=-316941.20 +/- 5666.48
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.17e+05 |
| time/              |           |
|    total_timesteps | 500000    |
| train/             |           |
|    actor_loss      | 5.83e+03  |
|    critic_loss     | 212       |
|    ent_coef        | 0.16      |
|    ent_coef_loss   | 0.154     |
|    learning_rate   | 9.5e-06   |
|    n_updates       | 4179798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.99e+05 |
| time/              |           |
|    episodes        | 100       |
|    fps             | 15        |
|    time_elapsed    | 31986     |
|    total_timesteps | 500000    |
----------------------------------
Eval num_timesteps=510000, episode_reward=-276077.92 +/- 37141.88
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.76e+05 |
| time/              |           |
|    total_timesteps | 510000    |
| train/             |           |
|    actor_loss      | 6.03e+03  |
|    critic_loss     | 117       |
|    ent_coef        | 0.163     |
|    ent_coef_loss   | -0.263    |
|    learning_rate   | 9.49e-06  |
|    n_updates       | 4189798   |
----------------------------------
Eval num_timesteps=520000, episode_reward=-286123.71 +/- 70887.36
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.86e+05 |
| time/              |           |
|    total_timesteps | 520000    |
| train/             |           |
|    actor_loss      | 5.94e+03  |
|    critic_loss     | 359       |
|    ent_coef        | 0.153     |
|    ent_coef_loss   | -0.375    |
|    learning_rate   | 9.48e-06  |
|    n_updates       | 4199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.99e+05 |
| time/              |           |
|    episodes        | 104       |
|    fps             | 15        |
|    time_elapsed    | 33388     |
|    total_timesteps | 520000    |
----------------------------------
Eval num_timesteps=530000, episode_reward=-143299.09 +/- 145555.44
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.43e+05 |
| time/              |           |
|    total_timesteps | 530000    |
| train/             |           |
|    actor_loss      | 6.03e+03  |
|    critic_loss     | 87.3      |
|    ent_coef        | 0.142     |
|    ent_coef_loss   | -1.03     |
|    learning_rate   | 9.47e-06  |
|    n_updates       | 4209798   |
----------------------------------
Eval num_timesteps=540000, episode_reward=-262631.27 +/- 117871.46
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.63e+05 |
| time/              |           |
|    total_timesteps | 540000    |
| train/             |           |
|    actor_loss      | 6.09e+03  |
|    critic_loss     | 26.9      |
|    ent_coef        | 0.131     |
|    ent_coef_loss   | -1.52     |
|    learning_rate   | 9.46e-06  |
|    n_updates       | 4219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.01e+05 |
| time/              |           |
|    episodes        | 108       |
|    fps             | 15        |
|    time_elapsed    | 34735     |
|    total_timesteps | 540000    |
----------------------------------
Eval num_timesteps=550000, episode_reward=-207659.88 +/- 139507.31
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.08e+05 |
| time/              |           |
|    total_timesteps | 550000    |
| train/             |           |
|    actor_loss      | 5.82e+03  |
|    critic_loss     | 580       |
|    ent_coef        | 0.123     |
|    ent_coef_loss   | 0.521     |
|    learning_rate   | 9.45e-06  |
|    n_updates       | 4229798   |
----------------------------------
Eval num_timesteps=560000, episode_reward=-152769.35 +/- 137823.23
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.53e+05 |
| time/              |           |
|    total_timesteps | 560000    |
| train/             |           |
|    actor_loss      | 5.96e+03  |
|    critic_loss     | 70.5      |
|    ent_coef        | 0.121     |
|    ent_coef_loss   | 0.158     |
|    learning_rate   | 9.44e-06  |
|    n_updates       | 4239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.99e+05 |
| time/              |           |
|    episodes        | 112       |
|    fps             | 15        |
|    time_elapsed    | 36085     |
|    total_timesteps | 560000    |
----------------------------------
Eval num_timesteps=570000, episode_reward=-265864.40 +/- 111512.98
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 570000    |
| train/             |           |
|    actor_loss      | 5.63e+03  |
|    critic_loss     | 370       |
|    ent_coef        | 0.133     |
|    ent_coef_loss   | 3.85      |
|    learning_rate   | 9.43e-06  |
|    n_updates       | 4249798   |
----------------------------------
Eval num_timesteps=580000, episode_reward=-41805.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.18e+04 |
| time/              |           |
|    total_timesteps | 580000    |
| train/             |           |
|    actor_loss      | 5.71e+03  |
|    critic_loss     | 297       |
|    ent_coef        | 0.145     |
|    ent_coef_loss   | 1.43      |
|    learning_rate   | 9.42e-06  |
|    n_updates       | 4259798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.93e+05 |
| time/              |           |
|    episodes        | 116       |
|    fps             | 15        |
|    time_elapsed    | 37428     |
|    total_timesteps | 580000    |
----------------------------------
Eval num_timesteps=590000, episode_reward=-149674.26 +/- 140351.24
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | 5.81e+03 |
|    critic_loss     | 67.5     |
|    ent_coef        | 0.148    |
|    ent_coef_loss   | -0.988   |
|    learning_rate   | 9.41e-06 |
|    n_updates       | 4269798  |
---------------------------------
Eval num_timesteps=600000, episode_reward=-148384.66 +/- 141403.20
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+05 |
| time/              |           |
|    total_timesteps | 600000    |
| train/             |           |
|    actor_loss      | 5.61e+03  |
|    critic_loss     | 107       |
|    ent_coef        | 0.137     |
|    ent_coef_loss   | -0.751    |
|    learning_rate   | 9.4e-06   |
|    n_updates       | 4279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.82e+05 |
| time/              |           |
|    episodes        | 120       |
|    fps             | 15        |
|    time_elapsed    | 38759     |
|    total_timesteps | 600000    |
----------------------------------
Eval num_timesteps=610000, episode_reward=-206196.49 +/- 141299.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.06e+05 |
| time/              |           |
|    total_timesteps | 610000    |
| train/             |           |
|    actor_loss      | 5.91e+03  |
|    critic_loss     | 58.4      |
|    ent_coef        | 0.126     |
|    ent_coef_loss   | -1.56     |
|    learning_rate   | 9.39e-06  |
|    n_updates       | 4289798   |
----------------------------------
Eval num_timesteps=620000, episode_reward=-92090.32 +/- 114738.30
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.21e+04 |
| time/              |           |
|    total_timesteps | 620000    |
| train/             |           |
|    actor_loss      | 5.68e+03  |
|    critic_loss     | 55.1      |
|    ent_coef        | 0.116     |
|    ent_coef_loss   | -2.14     |
|    learning_rate   | 9.38e-06  |
|    n_updates       | 4299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.74e+05 |
| time/              |           |
|    episodes        | 124       |
|    fps             | 15        |
|    time_elapsed    | 40082     |
|    total_timesteps | 620000    |
----------------------------------
Eval num_timesteps=630000, episode_reward=-149629.76 +/- 140386.04
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | 5.61e+03 |
|    critic_loss     | 74.5     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 9.37e-06 |
|    n_updates       | 4309798  |
---------------------------------
Eval num_timesteps=640000, episode_reward=-91771.18 +/- 114897.90
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.18e+04 |
| time/              |           |
|    total_timesteps | 640000    |
| train/             |           |
|    actor_loss      | 5.61e+03  |
|    critic_loss     | 30.5      |
|    ent_coef        | 0.098     |
|    ent_coef_loss   | -2.09     |
|    learning_rate   | 9.36e-06  |
|    n_updates       | 4319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.68e+05 |
| time/              |           |
|    episodes        | 128       |
|    fps             | 15        |
|    time_elapsed    | 41406     |
|    total_timesteps | 640000    |
----------------------------------
Eval num_timesteps=650000, episode_reward=-148575.02 +/- 141246.73
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 650000    |
| train/             |           |
|    actor_loss      | 5.61e+03  |
|    critic_loss     | 34.4      |
|    ent_coef        | 0.0908    |
|    ent_coef_loss   | -1.3      |
|    learning_rate   | 9.35e-06  |
|    n_updates       | 4329798   |
----------------------------------
Eval num_timesteps=660000, episode_reward=-148182.74 +/- 141567.05
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+05 |
| time/              |           |
|    total_timesteps | 660000    |
| train/             |           |
|    actor_loss      | 5.52e+03  |
|    critic_loss     | 27.7      |
|    ent_coef        | 0.0859    |
|    ent_coef_loss   | 0.0929    |
|    learning_rate   | 9.34e-06  |
|    n_updates       | 4339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.65e+05 |
| time/              |           |
|    episodes        | 132       |
|    fps             | 15        |
|    time_elapsed    | 42733     |
|    total_timesteps | 660000    |
----------------------------------
Eval num_timesteps=670000, episode_reward=-263561.78 +/- 116009.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 670000    |
| train/             |           |
|    actor_loss      | 5.57e+03  |
|    critic_loss     | 86.8      |
|    ent_coef        | 0.0828    |
|    ent_coef_loss   | -0.912    |
|    learning_rate   | 9.33e-06  |
|    n_updates       | 4349798   |
----------------------------------
Eval num_timesteps=680000, episode_reward=-147913.81 +/- 141786.48
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+05 |
| time/              |           |
|    total_timesteps | 680000    |
| train/             |           |
|    actor_loss      | 5.37e+03  |
|    critic_loss     | 142       |
|    ent_coef        | 0.0818    |
|    ent_coef_loss   | 0.0537    |
|    learning_rate   | 9.32e-06  |
|    n_updates       | 4359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.63e+05 |
| time/              |           |
|    episodes        | 136       |
|    fps             | 15        |
|    time_elapsed    | 44070     |
|    total_timesteps | 680000    |
----------------------------------
Eval num_timesteps=690000, episode_reward=-205975.82 +/- 141569.31
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.06e+05 |
| time/              |           |
|    total_timesteps | 690000    |
| train/             |           |
|    actor_loss      | 5.56e+03  |
|    critic_loss     | 45.5      |
|    ent_coef        | 0.0792    |
|    ent_coef_loss   | -0.638    |
|    learning_rate   | 9.31e-06  |
|    n_updates       | 4369798   |
----------------------------------
Eval num_timesteps=700000, episode_reward=-149039.54 +/- 140868.19
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 700000    |
| train/             |           |
|    actor_loss      | 5.51e+03  |
|    critic_loss     | 83.2      |
|    ent_coef        | 0.0767    |
|    ent_coef_loss   | 0.662     |
|    learning_rate   | 9.3e-06   |
|    n_updates       | 4379798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.6e+05 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 15       |
|    time_elapsed    | 45411    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-206964.50 +/- 140358.77
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 710000    |
| train/             |           |
|    actor_loss      | 5.64e+03  |
|    critic_loss     | 99.4      |
|    ent_coef        | 0.0768    |
|    ent_coef_loss   | -0.0942   |
|    learning_rate   | 9.29e-06  |
|    n_updates       | 4389798   |
----------------------------------
Eval num_timesteps=720000, episode_reward=-149850.34 +/- 140206.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | 5.58e+03 |
|    critic_loss     | 103      |
|    ent_coef        | 0.0745   |
|    ent_coef_loss   | 0.749    |
|    learning_rate   | 9.28e-06 |
|    n_updates       | 4399798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.54e+05 |
| time/              |           |
|    episodes        | 144       |
|    fps             | 15        |
|    time_elapsed    | 46763     |
|    total_timesteps | 720000    |
----------------------------------
Eval num_timesteps=730000, episode_reward=-92929.33 +/- 114318.90
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.29e+04 |
| time/              |           |
|    total_timesteps | 730000    |
| train/             |           |
|    actor_loss      | 5.5e+03   |
|    critic_loss     | 48.7      |
|    ent_coef        | 0.0752    |
|    ent_coef_loss   | -1.05     |
|    learning_rate   | 9.27e-06  |
|    n_updates       | 4409798   |
----------------------------------
Eval num_timesteps=740000, episode_reward=-150680.10 +/- 139529.46
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.51e+05 |
| time/              |           |
|    total_timesteps | 740000    |
| train/             |           |
|    actor_loss      | 5.21e+03  |
|    critic_loss     | 48.6      |
|    ent_coef        | 0.0726    |
|    ent_coef_loss   | -0.851    |
|    learning_rate   | 9.26e-06  |
|    n_updates       | 4419798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.49e+05 |
| time/              |           |
|    episodes        | 148       |
|    fps             | 15        |
|    time_elapsed    | 48146     |
|    total_timesteps | 740000    |
----------------------------------
Eval num_timesteps=750000, episode_reward=-150878.32 +/- 139367.76
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.51e+05 |
| time/              |           |
|    total_timesteps | 750000    |
| train/             |           |
|    actor_loss      | 5.56e+03  |
|    critic_loss     | 20.1      |
|    ent_coef        | 0.0667    |
|    ent_coef_loss   | -1.1      |
|    learning_rate   | 9.25e-06  |
|    n_updates       | 4429798   |
----------------------------------
Eval num_timesteps=760000, episode_reward=-207454.62 +/- 139760.75
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 760000    |
| train/             |           |
|    actor_loss      | 5.45e+03  |
|    critic_loss     | 61.7      |
|    ent_coef        | 0.0623    |
|    ent_coef_loss   | -0.496    |
|    learning_rate   | 9.24e-06  |
|    n_updates       | 4439798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.41e+05 |
| time/              |           |
|    episodes        | 152       |
|    fps             | 15        |
|    time_elapsed    | 49499     |
|    total_timesteps | 760000    |
----------------------------------
Eval num_timesteps=770000, episode_reward=-321569.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+05 |
| time/              |           |
|    total_timesteps | 770000    |
| train/             |           |
|    actor_loss      | 5.33e+03  |
|    critic_loss     | 48.6      |
|    ent_coef        | 0.0594    |
|    ent_coef_loss   | -0.707    |
|    learning_rate   | 9.23e-06  |
|    n_updates       | 4449798   |
----------------------------------
Eval num_timesteps=780000, episode_reward=-206922.85 +/- 140412.89
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 780000    |
| train/             |           |
|    actor_loss      | 5.32e+03  |
|    critic_loss     | 121       |
|    ent_coef        | 0.057     |
|    ent_coef_loss   | -0.943    |
|    learning_rate   | 9.22e-06  |
|    n_updates       | 4459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.41e+05 |
| time/              |           |
|    episodes        | 156       |
|    fps             | 15        |
|    time_elapsed    | 50830     |
|    total_timesteps | 780000    |
----------------------------------
Eval num_timesteps=790000, episode_reward=-207233.41 +/- 140094.85
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 790000    |
| train/             |           |
|    actor_loss      | 5.26e+03  |
|    critic_loss     | 49.7      |
|    ent_coef        | 0.0556    |
|    ent_coef_loss   | -0.0632   |
|    learning_rate   | 9.21e-06  |
|    n_updates       | 4469798   |
----------------------------------
Eval num_timesteps=800000, episode_reward=-207034.43 +/- 140338.22
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 800000    |
| train/             |           |
|    actor_loss      | 5.42e+03  |
|    critic_loss     | 15.6      |
|    ent_coef        | 0.0553    |
|    ent_coef_loss   | -2.1      |
|    learning_rate   | 9.2e-06   |
|    n_updates       | 4479798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.35e+05 |
| time/              |           |
|    episodes        | 160       |
|    fps             | 15        |
|    time_elapsed    | 52177     |
|    total_timesteps | 800000    |
----------------------------------
Eval num_timesteps=810000, episode_reward=-149355.08 +/- 140653.73
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 810000    |
| train/             |           |
|    actor_loss      | 5.26e+03  |
|    critic_loss     | 63.7      |
|    ent_coef        | 0.055     |
|    ent_coef_loss   | -0.748    |
|    learning_rate   | 9.19e-06  |
|    n_updates       | 4489798   |
----------------------------------
Eval num_timesteps=820000, episode_reward=-206666.36 +/- 140788.89
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 820000    |
| train/             |           |
|    actor_loss      | 5.2e+03   |
|    critic_loss     | 78        |
|    ent_coef        | 0.0566    |
|    ent_coef_loss   | -0.598    |
|    learning_rate   | 9.18e-06  |
|    n_updates       | 4499798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.3e+05 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 15       |
|    time_elapsed    | 53503    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=-264196.45 +/- 114849.13
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 830000    |
| train/             |           |
|    actor_loss      | 5.22e+03  |
|    critic_loss     | 43.6      |
|    ent_coef        | 0.0562    |
|    ent_coef_loss   | 0.763     |
|    learning_rate   | 9.17e-06  |
|    n_updates       | 4509798   |
----------------------------------
Eval num_timesteps=840000, episode_reward=-149230.87 +/- 140756.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 840000    |
| train/             |           |
|    actor_loss      | 5.31e+03  |
|    critic_loss     | 121       |
|    ent_coef        | 0.0609    |
|    ent_coef_loss   | 2.62      |
|    learning_rate   | 9.16e-06  |
|    n_updates       | 4519798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.3e+05 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 15       |
|    time_elapsed    | 54829    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-149178.91 +/- 140799.13
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 850000    |
| train/             |           |
|    actor_loss      | 5.48e+03  |
|    critic_loss     | 64.4      |
|    ent_coef        | 0.0656    |
|    ent_coef_loss   | 1.17      |
|    learning_rate   | 9.15e-06  |
|    n_updates       | 4529798   |
----------------------------------
Eval num_timesteps=860000, episode_reward=-206719.98 +/- 140726.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 860000    |
| train/             |           |
|    actor_loss      | 5.35e+03  |
|    critic_loss     | 26.3      |
|    ent_coef        | 0.0687    |
|    ent_coef_loss   | 0.738     |
|    learning_rate   | 9.14e-06  |
|    n_updates       | 4539798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.24e+05 |
| time/              |           |
|    episodes        | 172       |
|    fps             | 15        |
|    time_elapsed    | 56160     |
|    total_timesteps | 860000    |
----------------------------------
Eval num_timesteps=870000, episode_reward=-92219.48 +/- 114701.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.22e+04 |
| time/              |           |
|    total_timesteps | 870000    |
| train/             |           |
|    actor_loss      | 5.03e+03  |
|    critic_loss     | 15.8      |
|    ent_coef        | 0.0731    |
|    ent_coef_loss   | 0.525     |
|    learning_rate   | 9.13e-06  |
|    n_updates       | 4549798   |
----------------------------------
Eval num_timesteps=880000, episode_reward=-34767.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.48e+04 |
| time/              |           |
|    total_timesteps | 880000    |
| train/             |           |
|    actor_loss      | 5.51e+03  |
|    critic_loss     | 24        |
|    ent_coef        | 0.0739    |
|    ent_coef_loss   | -0.828    |
|    learning_rate   | 9.12e-06  |
|    n_updates       | 4559798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.29e+05 |
| time/              |           |
|    episodes        | 176       |
|    fps             | 15        |
|    time_elapsed    | 57498     |
|    total_timesteps | 880000    |
----------------------------------
Eval num_timesteps=890000, episode_reward=-149310.83 +/- 140690.89
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 890000    |
| train/             |           |
|    actor_loss      | 5.27e+03  |
|    critic_loss     | 44.5      |
|    ent_coef        | 0.0711    |
|    ent_coef_loss   | -0.0796   |
|    learning_rate   | 9.11e-06  |
|    n_updates       | 4569798   |
----------------------------------
Eval num_timesteps=900000, episode_reward=-264191.24 +/- 114860.55
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 900000    |
| train/             |           |
|    actor_loss      | 5e+03     |
|    critic_loss     | 40.7      |
|    ent_coef        | 0.0654    |
|    ent_coef_loss   | -2.25     |
|    learning_rate   | 9.1e-06   |
|    n_updates       | 4579798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -2.2e+05 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 15       |
|    time_elapsed    | 58848    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=-149036.34 +/- 140915.14
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 910000    |
| train/             |           |
|    actor_loss      | 5.24e+03  |
|    critic_loss     | 19.6      |
|    ent_coef        | 0.0602    |
|    ent_coef_loss   | -2.02     |
|    learning_rate   | 9.09e-06  |
|    n_updates       | 4589798   |
----------------------------------
Eval num_timesteps=920000, episode_reward=-91467.50 +/- 115076.26
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.15e+04 |
| time/              |           |
|    total_timesteps | 920000    |
| train/             |           |
|    actor_loss      | 5.04e+03  |
|    critic_loss     | 171       |
|    ent_coef        | 0.0552    |
|    ent_coef_loss   | -1.66     |
|    learning_rate   | 9.08e-06  |
|    n_updates       | 4599798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.17e+05 |
| time/              |           |
|    episodes        | 184       |
|    fps             | 15        |
|    time_elapsed    | 60206     |
|    total_timesteps | 920000    |
----------------------------------
Eval num_timesteps=930000, episode_reward=-206647.40 +/- 140812.36
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 930000    |
| train/             |           |
|    actor_loss      | 4.92e+03  |
|    critic_loss     | 24        |
|    ent_coef        | 0.0525    |
|    ent_coef_loss   | 0.087     |
|    learning_rate   | 9.07e-06  |
|    n_updates       | 4609798   |
----------------------------------
Eval num_timesteps=940000, episode_reward=-149607.67 +/- 140447.42
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | 5.06e+03 |
|    critic_loss     | 63.9     |
|    ent_coef        | 0.048    |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 9.06e-06 |
|    n_updates       | 4619798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.12e+05 |
| time/              |           |
|    episodes        | 188       |
|    fps             | 15        |
|    time_elapsed    | 61565     |
|    total_timesteps | 940000    |
----------------------------------
Eval num_timesteps=950000, episode_reward=-150261.46 +/- 139913.13
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | 5.12e+03 |
|    critic_loss     | 103      |
|    ent_coef        | 0.0439   |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 9.05e-06 |
|    n_updates       | 4629798  |
---------------------------------
Eval num_timesteps=960000, episode_reward=-93346.14 +/- 114110.60
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.33e+04 |
| time/              |           |
|    total_timesteps | 960000    |
| train/             |           |
|    actor_loss      | 4.96e+03  |
|    critic_loss     | 43.4      |
|    ent_coef        | 0.0403    |
|    ent_coef_loss   | -1.63     |
|    learning_rate   | 9.04e-06  |
|    n_updates       | 4639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.12e+05 |
| time/              |           |
|    episodes        | 192       |
|    fps             | 15        |
|    time_elapsed    | 62924     |
|    total_timesteps | 960000    |
----------------------------------
Eval num_timesteps=970000, episode_reward=-207312.55 +/- 139933.87
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 970000    |
| train/             |           |
|    actor_loss      | 4.94e+03  |
|    critic_loss     | 21.8      |
|    ent_coef        | 0.037     |
|    ent_coef_loss   | -1.02     |
|    learning_rate   | 9.03e-06  |
|    n_updates       | 4649798   |
----------------------------------
Eval num_timesteps=980000, episode_reward=-149603.70 +/- 140408.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | 5.18e+03 |
|    critic_loss     | 94.2     |
|    ent_coef        | 0.034    |
|    ent_coef_loss   | -2.73    |
|    learning_rate   | 9.02e-06 |
|    n_updates       | 4659798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -2.01e+05 |
| time/              |           |
|    episodes        | 196       |
|    fps             | 15        |
|    time_elapsed    | 64277     |
|    total_timesteps | 980000    |
----------------------------------
Eval num_timesteps=990000, episode_reward=-92363.54 +/- 114601.98
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.24e+04 |
| time/              |           |
|    total_timesteps | 990000    |
| train/             |           |
|    actor_loss      | 5.03e+03  |
|    critic_loss     | 88.6      |
|    ent_coef        | 0.0315    |
|    ent_coef_loss   | -0.664    |
|    learning_rate   | 9.01e-06  |
|    n_updates       | 4669798   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=-91995.85 +/- 114786.88
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -9.2e+04 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | 5.18e+03 |
|    critic_loss     | 23.4     |
|    ent_coef        | 0.0308   |
|    ent_coef_loss   | -0.205   |
|    learning_rate   | 9e-06    |
|    n_updates       | 4679798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.89e+05 |
| time/              |           |
|    episodes        | 200       |
|    fps             | 15        |
|    time_elapsed    | 65625     |
|    total_timesteps | 1000000   |
----------------------------------
Eval num_timesteps=1010000, episode_reward=-206836.11 +/- 140516.37
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1010000   |
| train/             |           |
|    actor_loss      | 5.05e+03  |
|    critic_loss     | 10.5      |
|    ent_coef        | 0.0285    |
|    ent_coef_loss   | -2.28     |
|    learning_rate   | 8.99e-06  |
|    n_updates       | 4689798   |
----------------------------------
Eval num_timesteps=1020000, episode_reward=-92363.92 +/- 114627.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.24e+04 |
| time/              |           |
|    total_timesteps | 1020000   |
| train/             |           |
|    actor_loss      | 4.52e+03  |
|    critic_loss     | 90.1      |
|    ent_coef        | 0.027     |
|    ent_coef_loss   | 1.57      |
|    learning_rate   | 8.98e-06  |
|    n_updates       | 4699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    episodes        | 204       |
|    fps             | 15        |
|    time_elapsed    | 66966     |
|    total_timesteps | 1020000   |
----------------------------------
Eval num_timesteps=1030000, episode_reward=-264223.48 +/- 114791.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1030000   |
| train/             |           |
|    actor_loss      | 4.73e+03  |
|    critic_loss     | 34.6      |
|    ent_coef        | 0.0292    |
|    ent_coef_loss   | 3.71      |
|    learning_rate   | 8.97e-06  |
|    n_updates       | 4709798   |
----------------------------------
Eval num_timesteps=1040000, episode_reward=-209278.77 +/- 137524.72
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.09e+05 |
| time/              |           |
|    total_timesteps | 1040000   |
| train/             |           |
|    actor_loss      | 5.15e+03  |
|    critic_loss     | 77.9      |
|    ent_coef        | 0.0281    |
|    ent_coef_loss   | -2.31     |
|    learning_rate   | 8.96e-06  |
|    n_updates       | 4719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 208       |
|    fps             | 15        |
|    time_elapsed    | 68307     |
|    total_timesteps | 1040000   |
----------------------------------
Eval num_timesteps=1050000, episode_reward=-207071.89 +/- 140226.98
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1050000   |
| train/             |           |
|    actor_loss      | 4.83e+03  |
|    critic_loss     | 82.3      |
|    ent_coef        | 0.0261    |
|    ent_coef_loss   | -0.797    |
|    learning_rate   | 8.95e-06  |
|    n_updates       | 4729798   |
----------------------------------
Eval num_timesteps=1060000, episode_reward=-149493.90 +/- 140496.85
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 1060000   |
| train/             |           |
|    actor_loss      | 4.91e+03  |
|    critic_loss     | 2.94e+04  |
|    ent_coef        | 0.0241    |
|    ent_coef_loss   | 0.974     |
|    learning_rate   | 8.94e-06  |
|    n_updates       | 4739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 212       |
|    fps             | 15        |
|    time_elapsed    | 69590     |
|    total_timesteps | 1060000   |
----------------------------------
Eval num_timesteps=1070000, episode_reward=-149730.18 +/- 140303.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | 4.66e+03 |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.0241   |
|    ent_coef_loss   | 0.0594   |
|    learning_rate   | 8.93e-06 |
|    n_updates       | 4749798  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=-264469.95 +/- 114296.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1080000   |
| train/             |           |
|    actor_loss      | 4.83e+03  |
|    critic_loss     | 58.1      |
|    ent_coef        | 0.0244    |
|    ent_coef_loss   | -0.258    |
|    learning_rate   | 8.92e-06  |
|    n_updates       | 4759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 216       |
|    fps             | 15        |
|    time_elapsed    | 70872     |
|    total_timesteps | 1080000   |
----------------------------------
Eval num_timesteps=1090000, episode_reward=-150489.03 +/- 139726.07
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | 4.97e+03 |
|    critic_loss     | 149      |
|    ent_coef        | 0.0254   |
|    ent_coef_loss   | 0.629    |
|    learning_rate   | 8.91e-06 |
|    n_updates       | 4769798  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=-150640.46 +/- 139602.89
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.51e+05 |
| time/              |           |
|    total_timesteps | 1100000   |
| train/             |           |
|    actor_loss      | 4.63e+03  |
|    critic_loss     | 31.3      |
|    ent_coef        | 0.0258    |
|    ent_coef_loss   | 0.724     |
|    learning_rate   | 8.9e-06   |
|    n_updates       | 4779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.93e+05 |
| time/              |           |
|    episodes        | 220       |
|    fps             | 15        |
|    time_elapsed    | 72163     |
|    total_timesteps | 1100000   |
----------------------------------
Eval num_timesteps=1110000, episode_reward=-264563.41 +/- 114109.23
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.65e+05 |
| time/              |           |
|    total_timesteps | 1110000   |
| train/             |           |
|    actor_loss      | 4.55e+03  |
|    critic_loss     | 7.18      |
|    ent_coef        | 0.0242    |
|    ent_coef_loss   | -0.368    |
|    learning_rate   | 8.89e-06  |
|    n_updates       | 4789798   |
----------------------------------
Eval num_timesteps=1120000, episode_reward=-207199.98 +/- 140134.75
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1120000   |
| train/             |           |
|    actor_loss      | 4.45e+03  |
|    critic_loss     | 45.7      |
|    ent_coef        | 0.0227    |
|    ent_coef_loss   | -0.511    |
|    learning_rate   | 8.88e-06  |
|    n_updates       | 4799798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+05 |
| time/              |           |
|    episodes        | 224       |
|    fps             | 15        |
|    time_elapsed    | 73463     |
|    total_timesteps | 1120000   |
----------------------------------
Eval num_timesteps=1130000, episode_reward=-149140.15 +/- 140787.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 1130000   |
| train/             |           |
|    actor_loss      | 4.81e+03  |
|    critic_loss     | 152       |
|    ent_coef        | 0.021     |
|    ent_coef_loss   | -0.00738  |
|    learning_rate   | 8.87e-06  |
|    n_updates       | 4809798   |
----------------------------------
Eval num_timesteps=1140000, episode_reward=-264133.81 +/- 114970.27
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1140000   |
| train/             |           |
|    actor_loss      | 4.46e+03  |
|    critic_loss     | 61.1      |
|    ent_coef        | 0.0205    |
|    ent_coef_loss   | 0.868     |
|    learning_rate   | 8.86e-06  |
|    n_updates       | 4819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+05 |
| time/              |           |
|    episodes        | 228       |
|    fps             | 15        |
|    time_elapsed    | 74769     |
|    total_timesteps | 1140000   |
----------------------------------
Eval num_timesteps=1150000, episode_reward=-263937.37 +/- 115362.22
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1150000   |
| train/             |           |
|    actor_loss      | 4.63e+03  |
|    critic_loss     | 11.3      |
|    ent_coef        | 0.0213    |
|    ent_coef_loss   | 0.0472    |
|    learning_rate   | 8.85e-06  |
|    n_updates       | 4829798   |
----------------------------------
Eval num_timesteps=1160000, episode_reward=-263954.33 +/- 115227.11
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1160000   |
| train/             |           |
|    actor_loss      | 4.51e+03  |
|    critic_loss     | 16.2      |
|    ent_coef        | 0.0224    |
|    ent_coef_loss   | 0.764     |
|    learning_rate   | 8.84e-06  |
|    n_updates       | 4839798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.93e+05 |
| time/              |           |
|    episodes        | 232       |
|    fps             | 15        |
|    time_elapsed    | 76078     |
|    total_timesteps | 1160000   |
----------------------------------
Eval num_timesteps=1170000, episode_reward=-91227.06 +/- 115170.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.12e+04 |
| time/              |           |
|    total_timesteps | 1170000   |
| train/             |           |
|    actor_loss      | 4.34e+03  |
|    critic_loss     | 48.4      |
|    ent_coef        | 0.024     |
|    ent_coef_loss   | 1.28      |
|    learning_rate   | 8.83e-06  |
|    n_updates       | 4849798   |
----------------------------------
Eval num_timesteps=1180000, episode_reward=-148973.79 +/- 140922.32
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 1180000   |
| train/             |           |
|    actor_loss      | 4.5e+03   |
|    critic_loss     | 59.2      |
|    ent_coef        | 0.0259    |
|    ent_coef_loss   | 0.793     |
|    learning_rate   | 8.82e-06  |
|    n_updates       | 4859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    episodes        | 236       |
|    fps             | 15        |
|    time_elapsed    | 77383     |
|    total_timesteps | 1180000   |
----------------------------------
Eval num_timesteps=1190000, episode_reward=-149730.12 +/- 140305.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | 4.36e+03 |
|    critic_loss     | 166      |
|    ent_coef        | 0.0281   |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 8.81e-06 |
|    n_updates       | 4869798  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=-149828.01 +/- 140225.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | 4.46e+03 |
|    critic_loss     | 15.6     |
|    ent_coef        | 0.0304   |
|    ent_coef_loss   | 0.548    |
|    learning_rate   | 8.8e-06  |
|    n_updates       | 4879798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 240       |
|    fps             | 15        |
|    time_elapsed    | 78679     |
|    total_timesteps | 1200000   |
----------------------------------
Eval num_timesteps=1210000, episode_reward=-149945.98 +/- 140129.05
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | 4.38e+03 |
|    critic_loss     | 25.7     |
|    ent_coef        | 0.0329   |
|    ent_coef_loss   | 0.571    |
|    learning_rate   | 8.79e-06 |
|    n_updates       | 4889798  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=-207233.39 +/- 140030.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1220000   |
| train/             |           |
|    actor_loss      | 4.23e+03  |
|    critic_loss     | 31.9      |
|    ent_coef        | 0.0355    |
|    ent_coef_loss   | 1.09      |
|    learning_rate   | 8.78e-06  |
|    n_updates       | 4899798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 244       |
|    fps             | 15        |
|    time_elapsed    | 79969     |
|    total_timesteps | 1220000   |
----------------------------------
Eval num_timesteps=1230000, episode_reward=-92570.43 +/- 114498.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.26e+04 |
| time/              |           |
|    total_timesteps | 1230000   |
| train/             |           |
|    actor_loss      | 4.16e+03  |
|    critic_loss     | 261       |
|    ent_coef        | 0.0381    |
|    ent_coef_loss   | 0.717     |
|    learning_rate   | 8.77e-06  |
|    n_updates       | 4909798   |
----------------------------------
Eval num_timesteps=1240000, episode_reward=-206983.66 +/- 140336.76
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1240000   |
| train/             |           |
|    actor_loss      | 4.41e+03  |
|    critic_loss     | 104       |
|    ent_coef        | 0.0405    |
|    ent_coef_loss   | -0.103    |
|    learning_rate   | 8.76e-06  |
|    n_updates       | 4919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    episodes        | 248       |
|    fps             | 15        |
|    time_elapsed    | 81251     |
|    total_timesteps | 1240000   |
----------------------------------
Eval num_timesteps=1250000, episode_reward=-206704.26 +/- 140740.56
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1250000   |
| train/             |           |
|    actor_loss      | 4.34e+03  |
|    critic_loss     | 12.5      |
|    ent_coef        | 0.0432    |
|    ent_coef_loss   | 1.51      |
|    learning_rate   | 8.75e-06  |
|    n_updates       | 4929798   |
----------------------------------
Eval num_timesteps=1260000, episode_reward=-89868.09 +/- 115875.38
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.99e+04 |
| time/              |           |
|    total_timesteps | 1260000   |
| train/             |           |
|    actor_loss      | 4.38e+03  |
|    critic_loss     | 14.6      |
|    ent_coef        | 0.0473    |
|    ent_coef_loss   | 1.17      |
|    learning_rate   | 8.74e-06  |
|    n_updates       | 4939798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 15       |
|    time_elapsed    | 82524    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=-38594.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.86e+04 |
| time/              |           |
|    total_timesteps | 1270000   |
| train/             |           |
|    actor_loss      | 4.32e+03  |
|    critic_loss     | 21.2      |
|    ent_coef        | 0.0513    |
|    ent_coef_loss   | 2         |
|    learning_rate   | 8.73e-06  |
|    n_updates       | 4949798   |
----------------------------------
Eval num_timesteps=1280000, episode_reward=-154071.56 +/- 136801.80
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 1280000   |
| train/             |           |
|    actor_loss      | 4.31e+03  |
|    critic_loss     | 17.8      |
|    ent_coef        | 0.0543    |
|    ent_coef_loss   | -0.348    |
|    learning_rate   | 8.72e-06  |
|    n_updates       | 4959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 256       |
|    fps             | 15        |
|    time_elapsed    | 83819     |
|    total_timesteps | 1280000   |
----------------------------------
Eval num_timesteps=1290000, episode_reward=-156173.29 +/- 135044.65
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 1290000   |
| train/             |           |
|    actor_loss      | 4.22e+03  |
|    critic_loss     | 34.7      |
|    ent_coef        | 0.0503    |
|    ent_coef_loss   | -0.42     |
|    learning_rate   | 8.71e-06  |
|    n_updates       | 4969798   |
----------------------------------
Eval num_timesteps=1300000, episode_reward=-99611.53 +/- 110978.27
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.96e+04 |
| time/              |           |
|    total_timesteps | 1300000   |
| train/             |           |
|    actor_loss      | 4.22e+03  |
|    critic_loss     | 36.9      |
|    ent_coef        | 0.0469    |
|    ent_coef_loss   | -0.305    |
|    learning_rate   | 8.7e-06   |
|    n_updates       | 4979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 260       |
|    fps             | 15        |
|    time_elapsed    | 85121     |
|    total_timesteps | 1300000   |
----------------------------------
Eval num_timesteps=1310000, episode_reward=-35346.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.53e+04 |
| time/              |           |
|    total_timesteps | 1310000   |
| train/             |           |
|    actor_loss      | 4.34e+03  |
|    critic_loss     | 213       |
|    ent_coef        | 0.0443    |
|    ent_coef_loss   | -0.565    |
|    learning_rate   | 8.69e-06  |
|    n_updates       | 4989798   |
----------------------------------
Eval num_timesteps=1320000, episode_reward=-206810.75 +/- 140548.35
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1320000   |
| train/             |           |
|    actor_loss      | 4.15e+03  |
|    critic_loss     | 14.8      |
|    ent_coef        | 0.0423    |
|    ent_coef_loss   | -0.362    |
|    learning_rate   | 8.68e-06  |
|    n_updates       | 4999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 264       |
|    fps             | 15        |
|    time_elapsed    | 86414     |
|    total_timesteps | 1320000   |
----------------------------------
Eval num_timesteps=1330000, episode_reward=-264411.48 +/- 114415.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1330000   |
| train/             |           |
|    actor_loss      | 4.12e+03  |
|    critic_loss     | 488       |
|    ent_coef        | 0.0408    |
|    ent_coef_loss   | 0.206     |
|    learning_rate   | 8.67e-06  |
|    n_updates       | 5009798   |
----------------------------------
Eval num_timesteps=1340000, episode_reward=-150277.89 +/- 139899.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | 4.58e+03 |
|    critic_loss     | 13.7     |
|    ent_coef        | 0.0422   |
|    ent_coef_loss   | -0.495   |
|    learning_rate   | 8.66e-06 |
|    n_updates       | 5019798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 268       |
|    fps             | 15        |
|    time_elapsed    | 87711     |
|    total_timesteps | 1340000   |
----------------------------------
Eval num_timesteps=1350000, episode_reward=-321618.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+05 |
| time/              |           |
|    total_timesteps | 1350000   |
| train/             |           |
|    actor_loss      | 4.36e+03  |
|    critic_loss     | 21.1      |
|    ent_coef        | 0.0413    |
|    ent_coef_loss   | -0.749    |
|    learning_rate   | 8.65e-06  |
|    n_updates       | 5029798   |
----------------------------------
Eval num_timesteps=1360000, episode_reward=-207142.71 +/- 140204.97
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1360000   |
| train/             |           |
|    actor_loss      | 3.98e+03  |
|    critic_loss     | 34.9      |
|    ent_coef        | 0.0386    |
|    ent_coef_loss   | -0.457    |
|    learning_rate   | 8.64e-06  |
|    n_updates       | 5039798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 272       |
|    fps             | 15        |
|    time_elapsed    | 89016     |
|    total_timesteps | 1360000   |
----------------------------------
Eval num_timesteps=1370000, episode_reward=-207443.06 +/- 139837.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1370000   |
| train/             |           |
|    actor_loss      | 4.09e+03  |
|    critic_loss     | 568       |
|    ent_coef        | 0.0376    |
|    ent_coef_loss   | -0.33     |
|    learning_rate   | 8.63e-06  |
|    n_updates       | 5049798   |
----------------------------------
Eval num_timesteps=1380000, episode_reward=-149860.53 +/- 140241.09
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | 4.11e+03 |
|    critic_loss     | 10.4     |
|    ent_coef        | 0.0376   |
|    ent_coef_loss   | -0.456   |
|    learning_rate   | 8.62e-06 |
|    n_updates       | 5059798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.76e+05 |
| time/              |           |
|    episodes        | 276       |
|    fps             | 15        |
|    time_elapsed    | 90322     |
|    total_timesteps | 1380000   |
----------------------------------
Eval num_timesteps=1390000, episode_reward=-149880.61 +/- 140223.56
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | 4.05e+03 |
|    critic_loss     | 24.2     |
|    ent_coef        | 0.0396   |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 8.61e-06 |
|    n_updates       | 5069798  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=-35434.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.54e+04 |
| time/              |           |
|    total_timesteps | 1400000   |
| train/             |           |
|    actor_loss      | 4.15e+03  |
|    critic_loss     | 32.1      |
|    ent_coef        | 0.0434    |
|    ent_coef_loss   | 9.25      |
|    learning_rate   | 8.6e-06   |
|    n_updates       | 5079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.79e+05 |
| time/              |           |
|    episodes        | 280       |
|    fps             | 15        |
|    time_elapsed    | 91620     |
|    total_timesteps | 1400000   |
----------------------------------
Eval num_timesteps=1410000, episode_reward=-207002.65 +/- 140373.63
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1410000   |
| train/             |           |
|    actor_loss      | 3.84e+03  |
|    critic_loss     | 24.2      |
|    ent_coef        | 0.0473    |
|    ent_coef_loss   | 7.2       |
|    learning_rate   | 8.59e-06  |
|    n_updates       | 5089798   |
----------------------------------
Eval num_timesteps=1420000, episode_reward=-206902.97 +/- 140495.04
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1420000   |
| train/             |           |
|    actor_loss      | 3.74e+03  |
|    critic_loss     | 9.14      |
|    ent_coef        | 0.0512    |
|    ent_coef_loss   | 3.1       |
|    learning_rate   | 8.58e-06  |
|    n_updates       | 5099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.82e+05 |
| time/              |           |
|    episodes        | 284       |
|    fps             | 15        |
|    time_elapsed    | 92918     |
|    total_timesteps | 1420000   |
----------------------------------
Eval num_timesteps=1430000, episode_reward=-206814.44 +/- 140606.79
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1430000   |
| train/             |           |
|    actor_loss      | 3.88e+03  |
|    critic_loss     | 11.3      |
|    ent_coef        | 0.0515    |
|    ent_coef_loss   | 0.51      |
|    learning_rate   | 8.57e-06  |
|    n_updates       | 5109798   |
----------------------------------
Eval num_timesteps=1440000, episode_reward=-264310.51 +/- 114620.51
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1440000   |
| train/             |           |
|    actor_loss      | 3.97e+03  |
|    critic_loss     | 2.69e+04  |
|    ent_coef        | 0.0509    |
|    ent_coef_loss   | -0.322    |
|    learning_rate   | 8.56e-06  |
|    n_updates       | 5119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.82e+05 |
| time/              |           |
|    episodes        | 288       |
|    fps             | 15        |
|    time_elapsed    | 94195     |
|    total_timesteps | 1440000   |
----------------------------------
Eval num_timesteps=1450000, episode_reward=-91883.90 +/- 114867.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.19e+04 |
| time/              |           |
|    total_timesteps | 1450000   |
| train/             |           |
|    actor_loss      | 3.97e+03  |
|    critic_loss     | 5.14      |
|    ent_coef        | 0.0489    |
|    ent_coef_loss   | 3.18      |
|    learning_rate   | 8.55e-06  |
|    n_updates       | 5129798   |
----------------------------------
Eval num_timesteps=1460000, episode_reward=-149540.29 +/- 140501.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.5e+05 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | 3.77e+03 |
|    critic_loss     | 51.3     |
|    ent_coef        | 0.0527   |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 8.54e-06 |
|    n_updates       | 5139798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.76e+05 |
| time/              |           |
|    episodes        | 292       |
|    fps             | 15        |
|    time_elapsed    | 95469     |
|    total_timesteps | 1460000   |
----------------------------------
Eval num_timesteps=1470000, episode_reward=-149493.17 +/- 140541.56
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 1470000   |
| train/             |           |
|    actor_loss      | 3.44e+03  |
|    critic_loss     | 15.8      |
|    ent_coef        | 0.0504    |
|    ent_coef_loss   | -0.466    |
|    learning_rate   | 8.53e-06  |
|    n_updates       | 5149798   |
----------------------------------
Eval num_timesteps=1480000, episode_reward=-264459.99 +/- 114322.15
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.64e+05 |
| time/              |           |
|    total_timesteps | 1480000   |
| train/             |           |
|    actor_loss      | 3.93e+03  |
|    critic_loss     | 8.14      |
|    ent_coef        | 0.0466    |
|    ent_coef_loss   | -0.965    |
|    learning_rate   | 8.52e-06  |
|    n_updates       | 5159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 296       |
|    fps             | 15        |
|    time_elapsed    | 96751     |
|    total_timesteps | 1480000   |
----------------------------------
Eval num_timesteps=1490000, episode_reward=-91965.48 +/- 114826.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -9.2e+04 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | 3.7e+03  |
|    critic_loss     | 15.4     |
|    ent_coef        | 0.044    |
|    ent_coef_loss   | 2.86     |
|    learning_rate   | 8.51e-06 |
|    n_updates       | 5169798  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=-148220.16 +/- 141581.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+05 |
| time/              |           |
|    total_timesteps | 1500000   |
| train/             |           |
|    actor_loss      | 3.65e+03  |
|    critic_loss     | 5.47      |
|    ent_coef        | 0.0429    |
|    ent_coef_loss   | -0.594    |
|    learning_rate   | 8.5e-06   |
|    n_updates       | 5179798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 15       |
|    time_elapsed    | 98024    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=-206409.63 +/- 141102.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.06e+05 |
| time/              |           |
|    total_timesteps | 1510000   |
| train/             |           |
|    actor_loss      | 3.61e+03  |
|    critic_loss     | 9.88      |
|    ent_coef        | 0.0458    |
|    ent_coef_loss   | 0.088     |
|    learning_rate   | 8.49e-06  |
|    n_updates       | 5189798   |
----------------------------------
Eval num_timesteps=1520000, episode_reward=-148046.44 +/- 141722.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.48e+05 |
| time/              |           |
|    total_timesteps | 1520000   |
| train/             |           |
|    actor_loss      | 3.99e+03  |
|    critic_loss     | 14.3      |
|    ent_coef        | 0.0429    |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 8.48e-06  |
|    n_updates       | 5199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 304       |
|    fps             | 15        |
|    time_elapsed    | 99298     |
|    total_timesteps | 1520000   |
----------------------------------
Eval num_timesteps=1530000, episode_reward=-206319.66 +/- 141213.18
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.06e+05 |
| time/              |           |
|    total_timesteps | 1530000   |
| train/             |           |
|    actor_loss      | 3.91e+03  |
|    critic_loss     | 4.58      |
|    ent_coef        | 0.0402    |
|    ent_coef_loss   | -0.533    |
|    learning_rate   | 8.47e-06  |
|    n_updates       | 5209798   |
----------------------------------
Eval num_timesteps=1540000, episode_reward=-152403.46 +/- 138164.47
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.52e+05 |
| time/              |           |
|    total_timesteps | 1540000   |
| train/             |           |
|    actor_loss      | 3.84e+03  |
|    critic_loss     | 580       |
|    ent_coef        | 0.0374    |
|    ent_coef_loss   | -0.618    |
|    learning_rate   | 8.46e-06  |
|    n_updates       | 5219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 308       |
|    fps             | 15        |
|    time_elapsed    | 100602    |
|    total_timesteps | 1540000   |
----------------------------------
Eval num_timesteps=1550000, episode_reward=-207495.27 +/- 139773.17
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.07e+05 |
| time/              |           |
|    total_timesteps | 1550000   |
| train/             |           |
|    actor_loss      | 3.89e+03  |
|    critic_loss     | 19.6      |
|    ent_coef        | 0.0352    |
|    ent_coef_loss   | -0.774    |
|    learning_rate   | 8.45e-06  |
|    n_updates       | 5229798   |
----------------------------------
Eval num_timesteps=1560000, episode_reward=-205628.20 +/- 142061.14
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.06e+05 |
| time/              |           |
|    total_timesteps | 1560000   |
| train/             |           |
|    actor_loss      | 3.78e+03  |
|    critic_loss     | 7.44      |
|    ent_coef        | 0.0341    |
|    ent_coef_loss   | 0.254     |
|    learning_rate   | 8.44e-06  |
|    n_updates       | 5239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 312       |
|    fps             | 15        |
|    time_elapsed    | 101902    |
|    total_timesteps | 1560000   |
----------------------------------
Eval num_timesteps=1570000, episode_reward=-158433.65 +/- 133241.23
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.58e+05 |
| time/              |           |
|    total_timesteps | 1570000   |
| train/             |           |
|    actor_loss      | 3.41e+03  |
|    critic_loss     | 17.2      |
|    ent_coef        | 0.0365    |
|    ent_coef_loss   | 1.01      |
|    learning_rate   | 8.43e-06  |
|    n_updates       | 5249798   |
----------------------------------
Eval num_timesteps=1580000, episode_reward=-213235.19 +/- 132742.32
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.13e+05 |
| time/              |           |
|    total_timesteps | 1580000   |
| train/             |           |
|    actor_loss      | 3.89e+03  |
|    critic_loss     | 5.31      |
|    ent_coef        | 0.04      |
|    ent_coef_loss   | 3.35      |
|    learning_rate   | 8.42e-06  |
|    n_updates       | 5259798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.79e+05 |
| time/              |           |
|    episodes        | 316       |
|    fps             | 15        |
|    time_elapsed    | 103215    |
|    total_timesteps | 1580000   |
----------------------------------
Eval num_timesteps=1590000, episode_reward=-154307.39 +/- 136610.83
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 1590000   |
| train/             |           |
|    actor_loss      | 3.6e+03   |
|    critic_loss     | 13.5      |
|    ent_coef        | 0.0402    |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 8.41e-06  |
|    n_updates       | 5269798   |
----------------------------------
Eval num_timesteps=1600000, episode_reward=-156101.80 +/- 135144.61
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 1600000   |
| train/             |           |
|    actor_loss      | 3.82e+03  |
|    critic_loss     | 14.3      |
|    ent_coef        | 0.0411    |
|    ent_coef_loss   | 1.78      |
|    learning_rate   | 8.4e-06   |
|    n_updates       | 5279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.74e+05 |
| time/              |           |
|    episodes        | 320       |
|    fps             | 15        |
|    time_elapsed    | 104604    |
|    total_timesteps | 1600000   |
----------------------------------
Eval num_timesteps=1610000, episode_reward=-266817.76 +/- 109606.76
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.67e+05 |
| time/              |           |
|    total_timesteps | 1610000   |
| train/             |           |
|    actor_loss      | 3.89e+03  |
|    critic_loss     | 7.98      |
|    ent_coef        | 0.0404    |
|    ent_coef_loss   | -1.29     |
|    learning_rate   | 8.39e-06  |
|    n_updates       | 5289798   |
----------------------------------
Eval num_timesteps=1620000, episode_reward=-212912.36 +/- 133137.85
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.13e+05 |
| time/              |           |
|    total_timesteps | 1620000   |
| train/             |           |
|    actor_loss      | 3.55e+03  |
|    critic_loss     | 198       |
|    ent_coef        | 0.0394    |
|    ent_coef_loss   | 1.06      |
|    learning_rate   | 8.38e-06  |
|    n_updates       | 5299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.66e+05 |
| time/              |           |
|    episodes        | 324       |
|    fps             | 15        |
|    time_elapsed    | 105957    |
|    total_timesteps | 1620000   |
----------------------------------
Eval num_timesteps=1630000, episode_reward=-158728.78 +/- 132999.19
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.59e+05 |
| time/              |           |
|    total_timesteps | 1630000   |
| train/             |           |
|    actor_loss      | 3.63e+03  |
|    critic_loss     | 4.51      |
|    ent_coef        | 0.0398    |
|    ent_coef_loss   | 1.71      |
|    learning_rate   | 8.37e-06  |
|    n_updates       | 5309798   |
----------------------------------
Eval num_timesteps=1640000, episode_reward=-156400.53 +/- 134900.53
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 1640000   |
| train/             |           |
|    actor_loss      | 3.68e+03  |
|    critic_loss     | 22.1      |
|    ent_coef        | 0.0383    |
|    ent_coef_loss   | 0.422     |
|    learning_rate   | 8.36e-06  |
|    n_updates       | 5319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.66e+05 |
| time/              |           |
|    episodes        | 328       |
|    fps             | 15        |
|    time_elapsed    | 107321    |
|    total_timesteps | 1640000   |
----------------------------------
Eval num_timesteps=1650000, episode_reward=-46470.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.65e+04 |
| time/              |           |
|    total_timesteps | 1650000   |
| train/             |           |
|    actor_loss      | 3.46e+03  |
|    critic_loss     | 4.06      |
|    ent_coef        | 0.0392    |
|    ent_coef_loss   | 3.29      |
|    learning_rate   | 8.35e-06  |
|    n_updates       | 5329798   |
----------------------------------
Eval num_timesteps=1660000, episode_reward=-156576.04 +/- 134756.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 1660000   |
| train/             |           |
|    actor_loss      | 3.57e+03  |
|    critic_loss     | 845       |
|    ent_coef        | 0.0423    |
|    ent_coef_loss   | 1.52      |
|    learning_rate   | 8.34e-06  |
|    n_updates       | 5339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.66e+05 |
| time/              |           |
|    episodes        | 332       |
|    fps             | 15        |
|    time_elapsed    | 108693    |
|    total_timesteps | 1660000   |
----------------------------------
Eval num_timesteps=1670000, episode_reward=-158403.13 +/- 133266.78
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.58e+05 |
| time/              |           |
|    total_timesteps | 1670000   |
| train/             |           |
|    actor_loss      | 3.7e+03   |
|    critic_loss     | 759       |
|    ent_coef        | 0.0429    |
|    ent_coef_loss   | -0.655    |
|    learning_rate   | 8.33e-06  |
|    n_updates       | 5349798   |
----------------------------------
Eval num_timesteps=1680000, episode_reward=-170875.84 +/- 123080.72
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.71e+05 |
| time/              |           |
|    total_timesteps | 1680000   |
| train/             |           |
|    actor_loss      | 3.26e+03  |
|    critic_loss     | 6.29      |
|    ent_coef        | 0.0401    |
|    ent_coef_loss   | -0.611    |
|    learning_rate   | 8.32e-06  |
|    n_updates       | 5359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.69e+05 |
| time/              |           |
|    episodes        | 336       |
|    fps             | 15        |
|    time_elapsed    | 110047    |
|    total_timesteps | 1680000   |
----------------------------------
Eval num_timesteps=1690000, episode_reward=-307505.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.08e+05 |
| time/              |           |
|    total_timesteps | 1690000   |
| train/             |           |
|    actor_loss      | 3.6e+03   |
|    critic_loss     | 27.8      |
|    ent_coef        | 0.0371    |
|    ent_coef_loss   | -0.602    |
|    learning_rate   | 8.31e-06  |
|    n_updates       | 5369798   |
----------------------------------
Eval num_timesteps=1700000, episode_reward=-313671.95 +/- 6488.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.14e+05 |
| time/              |           |
|    total_timesteps | 1700000   |
| train/             |           |
|    actor_loss      | 3.55e+03  |
|    critic_loss     | 60.2      |
|    ent_coef        | 0.0342    |
|    ent_coef_loss   | -1.35     |
|    learning_rate   | 8.3e-06   |
|    n_updates       | 5379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.78e+05 |
| time/              |           |
|    episodes        | 340       |
|    fps             | 15        |
|    time_elapsed    | 111391    |
|    total_timesteps | 1700000   |
----------------------------------
Eval num_timesteps=1710000, episode_reward=-308917.44 +/- 6350.28
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.09e+05 |
| time/              |           |
|    total_timesteps | 1710000   |
| train/             |           |
|    actor_loss      | 3.38e+03  |
|    critic_loss     | 871       |
|    ent_coef        | 0.0342    |
|    ent_coef_loss   | 1.02      |
|    learning_rate   | 8.29e-06  |
|    n_updates       | 5389798   |
----------------------------------
Eval num_timesteps=1720000, episode_reward=-312627.49 +/- 7340.55
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 1720000   |
| train/             |           |
|    actor_loss      | 3.44e+03  |
|    critic_loss     | 23.2      |
|    ent_coef        | 0.0372    |
|    ent_coef_loss   | 2         |
|    learning_rate   | 8.28e-06  |
|    n_updates       | 5399798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 344       |
|    fps             | 15        |
|    time_elapsed    | 112738    |
|    total_timesteps | 1720000   |
----------------------------------
Eval num_timesteps=1730000, episode_reward=-312702.08 +/- 7235.34
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 1730000   |
| train/             |           |
|    actor_loss      | 3.73e+03  |
|    critic_loss     | 23.4      |
|    ent_coef        | 0.0405    |
|    ent_coef_loss   | 2.58      |
|    learning_rate   | 8.27e-06  |
|    n_updates       | 5409798   |
----------------------------------
Eval num_timesteps=1740000, episode_reward=-182527.12 +/- 113521.75
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.83e+05 |
| time/              |           |
|    total_timesteps | 1740000   |
| train/             |           |
|    actor_loss      | 3.43e+03  |
|    critic_loss     | 18.2      |
|    ent_coef        | 0.044     |
|    ent_coef_loss   | 3.41      |
|    learning_rate   | 8.26e-06  |
|    n_updates       | 5419798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 348       |
|    fps             | 15        |
|    time_elapsed    | 114085    |
|    total_timesteps | 1740000   |
----------------------------------
Eval num_timesteps=1750000, episode_reward=-316066.64 +/- 10996.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+05 |
| time/              |           |
|    total_timesteps | 1750000   |
| train/             |           |
|    actor_loss      | 3.73e+03  |
|    critic_loss     | 41.4      |
|    ent_coef        | 0.0476    |
|    ent_coef_loss   | 3         |
|    learning_rate   | 8.25e-06  |
|    n_updates       | 5429798   |
----------------------------------
Eval num_timesteps=1760000, episode_reward=-312732.83 +/- 7210.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 1760000   |
| train/             |           |
|    actor_loss      | 3.58e+03  |
|    critic_loss     | 96.7      |
|    ent_coef        | 0.0519    |
|    ent_coef_loss   | 5.67      |
|    learning_rate   | 8.24e-06  |
|    n_updates       | 5439798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.84e+05 |
| time/              |           |
|    episodes        | 352       |
|    fps             | 15        |
|    time_elapsed    | 115433    |
|    total_timesteps | 1760000   |
----------------------------------
Eval num_timesteps=1770000, episode_reward=-312692.19 +/- 7245.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.13e+05 |
| time/              |           |
|    total_timesteps | 1770000   |
| train/             |           |
|    actor_loss      | 3.53e+03  |
|    critic_loss     | 48.3      |
|    ent_coef        | 0.0565    |
|    ent_coef_loss   | 6.31      |
|    learning_rate   | 8.23e-06  |
|    n_updates       | 5449798   |
----------------------------------
Eval num_timesteps=1780000, episode_reward=-313913.52 +/- 15410.24
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.14e+05 |
| time/              |           |
|    total_timesteps | 1780000   |
| train/             |           |
|    actor_loss      | 3.24e+03  |
|    critic_loss     | 67.8      |
|    ent_coef        | 0.0611    |
|    ent_coef_loss   | 4.67      |
|    learning_rate   | 8.22e-06  |
|    n_updates       | 5459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    episodes        | 356       |
|    fps             | 15        |
|    time_elapsed    | 116780    |
|    total_timesteps | 1780000   |
----------------------------------
Eval num_timesteps=1790000, episode_reward=-263053.32 +/- 117127.99
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.63e+05 |
| time/              |           |
|    total_timesteps | 1790000   |
| train/             |           |
|    actor_loss      | 3.7e+03   |
|    critic_loss     | 88.2      |
|    ent_coef        | 0.0664    |
|    ent_coef_loss   | 5.23      |
|    learning_rate   | 8.21e-06  |
|    n_updates       | 5469798   |
----------------------------------
Eval num_timesteps=1800000, episode_reward=-318955.90 +/- 5113.92
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.19e+05 |
| time/              |           |
|    total_timesteps | 1800000   |
| train/             |           |
|    actor_loss      | 3.78e+03  |
|    critic_loss     | 317       |
|    ent_coef        | 0.0721    |
|    ent_coef_loss   | 4.42      |
|    learning_rate   | 8.2e-06   |
|    n_updates       | 5479798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 15       |
|    time_elapsed    | 118174   |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=-307613.53 +/- 7031.14
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.08e+05 |
| time/              |           |
|    total_timesteps | 1810000   |
| train/             |           |
|    actor_loss      | 3.48e+03  |
|    critic_loss     | 1.83e+03  |
|    ent_coef        | 0.0782    |
|    ent_coef_loss   | 6.25      |
|    learning_rate   | 8.19e-06  |
|    n_updates       | 5489798   |
----------------------------------
Eval num_timesteps=1820000, episode_reward=-265989.98 +/- 111371.10
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 1820000   |
| train/             |           |
|    actor_loss      | 4.29e+03  |
|    critic_loss     | 75.2      |
|    ent_coef        | 0.0849    |
|    ent_coef_loss   | 6.54      |
|    learning_rate   | 8.18e-06  |
|    n_updates       | 5499798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.95e+05 |
| time/              |           |
|    episodes        | 364       |
|    fps             | 15        |
|    time_elapsed    | 119555    |
|    total_timesteps | 1820000   |
----------------------------------
Eval num_timesteps=1830000, episode_reward=-96171.85 +/- 112751.50
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.62e+04 |
| time/              |           |
|    total_timesteps | 1830000   |
| train/             |           |
|    actor_loss      | 3.82e+03  |
|    critic_loss     | 66.6      |
|    ent_coef        | 0.0922    |
|    ent_coef_loss   | 7.06      |
|    learning_rate   | 8.17e-06  |
|    n_updates       | 5509798   |
----------------------------------
Eval num_timesteps=1840000, episode_reward=-148798.50 +/- 141064.91
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.49e+05 |
| time/              |           |
|    total_timesteps | 1840000   |
| train/             |           |
|    actor_loss      | 3.84e+03  |
|    critic_loss     | 89.3      |
|    ent_coef        | 0.0997    |
|    ent_coef_loss   | 0.481     |
|    learning_rate   | 8.16e-06  |
|    n_updates       | 5519798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+05 |
| time/              |           |
|    episodes        | 368       |
|    fps             | 15        |
|    time_elapsed    | 120906    |
|    total_timesteps | 1840000   |
----------------------------------
Eval num_timesteps=1850000, episode_reward=-287403.66 +/- 41905.87
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.87e+05 |
| time/              |           |
|    total_timesteps | 1850000   |
| train/             |           |
|    actor_loss      | 3.93e+03  |
|    critic_loss     | 59.1      |
|    ent_coef        | 0.107     |
|    ent_coef_loss   | 2.06      |
|    learning_rate   | 8.15e-06  |
|    n_updates       | 5529798   |
----------------------------------
Eval num_timesteps=1860000, episode_reward=-94310.32 +/- 113654.65
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.43e+04 |
| time/              |           |
|    total_timesteps | 1860000   |
| train/             |           |
|    actor_loss      | 4.11e+03  |
|    critic_loss     | 197       |
|    ent_coef        | 0.115     |
|    ent_coef_loss   | 2.33      |
|    learning_rate   | 8.14e-06  |
|    n_updates       | 5539798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+05 |
| time/              |           |
|    episodes        | 372       |
|    fps             | 15        |
|    time_elapsed    | 122242    |
|    total_timesteps | 1860000   |
----------------------------------
Eval num_timesteps=1870000, episode_reward=-260977.14 +/- 121287.11
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.61e+05 |
| time/              |           |
|    total_timesteps | 1870000   |
| train/             |           |
|    actor_loss      | 3.79e+03  |
|    critic_loss     | 92.4      |
|    ent_coef        | 0.124     |
|    ent_coef_loss   | 1.12      |
|    learning_rate   | 8.13e-06  |
|    n_updates       | 5549798   |
----------------------------------
Eval num_timesteps=1880000, episode_reward=-139732.92 +/- 148510.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.4e+05 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | 4.04e+03 |
|    critic_loss     | 102      |
|    ent_coef        | 0.132    |
|    ent_coef_loss   | 0.719    |
|    learning_rate   | 8.12e-06 |
|    n_updates       | 5559798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.98e+05 |
| time/              |           |
|    episodes        | 376       |
|    fps             | 15        |
|    time_elapsed    | 123580    |
|    total_timesteps | 1880000   |
----------------------------------
Eval num_timesteps=1890000, episode_reward=-18810.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.88e+04 |
| time/              |           |
|    total_timesteps | 1890000   |
| train/             |           |
|    actor_loss      | 4.19e+03  |
|    critic_loss     | 3.87e+03  |
|    ent_coef        | 0.139     |
|    ent_coef_loss   | 0.626     |
|    learning_rate   | 8.11e-06  |
|    n_updates       | 5569798   |
----------------------------------
New best mean reward!
Eval num_timesteps=1900000, episode_reward=-204741.35 +/- 143146.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.05e+05 |
| time/              |           |
|    total_timesteps | 1900000   |
| train/             |           |
|    actor_loss      | 3.71e+03  |
|    critic_loss     | 48.2      |
|    ent_coef        | 0.15      |
|    ent_coef_loss   | 0.608     |
|    learning_rate   | 8.1e-06   |
|    n_updates       | 5579798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.94e+05 |
| time/              |           |
|    episodes        | 380       |
|    fps             | 15        |
|    time_elapsed    | 124966    |
|    total_timesteps | 1900000   |
----------------------------------
Eval num_timesteps=1910000, episode_reward=-264723.98 +/- 113791.63
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.65e+05 |
| time/              |           |
|    total_timesteps | 1910000   |
| train/             |           |
|    actor_loss      | 4.06e+03  |
|    critic_loss     | 54.3      |
|    ent_coef        | 0.157     |
|    ent_coef_loss   | -0.419    |
|    learning_rate   | 8.09e-06  |
|    n_updates       | 5589798   |
----------------------------------
Eval num_timesteps=1920000, episode_reward=-210742.41 +/- 135796.24
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 1920000   |
| train/             |           |
|    actor_loss      | 3.92e+03  |
|    critic_loss     | 397       |
|    ent_coef        | 0.159     |
|    ent_coef_loss   | -0.549    |
|    learning_rate   | 8.08e-06  |
|    n_updates       | 5599798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.97e+05 |
| time/              |           |
|    episodes        | 384       |
|    fps             | 15        |
|    time_elapsed    | 126304    |
|    total_timesteps | 1920000   |
----------------------------------
Eval num_timesteps=1930000, episode_reward=-211643.07 +/- 134691.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.12e+05 |
| time/              |           |
|    total_timesteps | 1930000   |
| train/             |           |
|    actor_loss      | 4.02e+03  |
|    critic_loss     | 29.4      |
|    ent_coef        | 0.151     |
|    ent_coef_loss   | -0.854    |
|    learning_rate   | 8.07e-06  |
|    n_updates       | 5609798   |
----------------------------------
Eval num_timesteps=1940000, episode_reward=-266890.56 +/- 109453.16
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.67e+05 |
| time/              |           |
|    total_timesteps | 1940000   |
| train/             |           |
|    actor_loss      | 3.66e+03  |
|    critic_loss     | 93.6      |
|    ent_coef        | 0.156     |
|    ent_coef_loss   | -0.133    |
|    learning_rate   | 8.06e-06  |
|    n_updates       | 5619798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.92e+05 |
| time/              |           |
|    episodes        | 388       |
|    fps             | 15        |
|    time_elapsed    | 127616    |
|    total_timesteps | 1940000   |
----------------------------------
Eval num_timesteps=1950000, episode_reward=-157444.16 +/- 134004.71
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 1950000   |
| train/             |           |
|    actor_loss      | 3.66e+03  |
|    critic_loss     | 39.9      |
|    ent_coef        | 0.162     |
|    ent_coef_loss   | 0.539     |
|    learning_rate   | 8.05e-06  |
|    n_updates       | 5629798   |
----------------------------------
Eval num_timesteps=1960000, episode_reward=-265990.34 +/- 111150.69
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 1960000   |
| train/             |           |
|    actor_loss      | 3.61e+03  |
|    critic_loss     | 33.1      |
|    ent_coef        | 0.166     |
|    ent_coef_loss   | 0.0147    |
|    learning_rate   | 8.04e-06  |
|    n_updates       | 5639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.92e+05 |
| time/              |           |
|    episodes        | 392       |
|    fps             | 15        |
|    time_elapsed    | 128935    |
|    total_timesteps | 1960000   |
----------------------------------
Eval num_timesteps=1970000, episode_reward=-156559.45 +/- 134726.90
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 1970000   |
| train/             |           |
|    actor_loss      | 4.13e+03  |
|    critic_loss     | 230       |
|    ent_coef        | 0.167     |
|    ent_coef_loss   | -0.0316   |
|    learning_rate   | 8.03e-06  |
|    n_updates       | 5649798   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=-157057.45 +/- 134320.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 1980000   |
| train/             |           |
|    actor_loss      | 3.67e+03  |
|    critic_loss     | 25.7      |
|    ent_coef        | 0.174     |
|    ent_coef_loss   | 0.462     |
|    learning_rate   | 8.02e-06  |
|    n_updates       | 5659798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 15       |
|    time_elapsed    | 130256   |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=-157139.83 +/- 134252.77
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 1990000   |
| train/             |           |
|    actor_loss      | 3.82e+03  |
|    critic_loss     | 47.3      |
|    ent_coef        | 0.182     |
|    ent_coef_loss   | 0.904     |
|    learning_rate   | 8.01e-06  |
|    n_updates       | 5669798   |
----------------------------------
Eval num_timesteps=2000000, episode_reward=-156898.24 +/- 134449.88
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 2000000   |
| train/             |           |
|    actor_loss      | 3.79e+03  |
|    critic_loss     | 83.7      |
|    ent_coef        | 0.193     |
|    ent_coef_loss   | 0.36      |
|    learning_rate   | 8e-06     |
|    n_updates       | 5679798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    episodes        | 400       |
|    fps             | 15        |
|    time_elapsed    | 131582    |
|    total_timesteps | 2000000   |
----------------------------------
Eval num_timesteps=2010000, episode_reward=-266791.48 +/- 109547.05
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.67e+05 |
| time/              |           |
|    total_timesteps | 2010000   |
| train/             |           |
|    actor_loss      | 3.51e+03  |
|    critic_loss     | 58.3      |
|    ent_coef        | 0.199     |
|    ent_coef_loss   | 0.15      |
|    learning_rate   | 7.99e-06  |
|    n_updates       | 5689798   |
----------------------------------
Eval num_timesteps=2020000, episode_reward=-266456.54 +/- 110215.83
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2020000   |
| train/             |           |
|    actor_loss      | 4e+03     |
|    critic_loss     | 1.24e+03  |
|    ent_coef        | 0.204     |
|    ent_coef_loss   | -0.151    |
|    learning_rate   | 7.98e-06  |
|    n_updates       | 5699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.88e+05 |
| time/              |           |
|    episodes        | 404       |
|    fps             | 15        |
|    time_elapsed    | 132903    |
|    total_timesteps | 2020000   |
----------------------------------
Eval num_timesteps=2030000, episode_reward=-100035.96 +/- 110764.44
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1e+05   |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | 3.83e+03 |
|    critic_loss     | 57.5     |
|    ent_coef        | 0.209    |
|    ent_coef_loss   | -0.214   |
|    learning_rate   | 7.97e-06 |
|    n_updates       | 5709798  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=-210828.17 +/- 135623.97
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2040000   |
| train/             |           |
|    actor_loss      | 3.68e+03  |
|    critic_loss     | 168       |
|    ent_coef        | 0.215     |
|    ent_coef_loss   | -0.347    |
|    learning_rate   | 7.96e-06  |
|    n_updates       | 5719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.85e+05 |
| time/              |           |
|    episodes        | 408       |
|    fps             | 15        |
|    time_elapsed    | 134224    |
|    total_timesteps | 2040000   |
----------------------------------
Eval num_timesteps=2050000, episode_reward=-99268.94 +/- 111148.23
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.93e+04 |
| time/              |           |
|    total_timesteps | 2050000   |
| train/             |           |
|    actor_loss      | 3.88e+03  |
|    critic_loss     | 87.9      |
|    ent_coef        | 0.206     |
|    ent_coef_loss   | -0.27     |
|    learning_rate   | 7.95e-06  |
|    n_updates       | 5729798   |
----------------------------------
Eval num_timesteps=2060000, episode_reward=-154462.49 +/- 136439.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 2060000   |
| train/             |           |
|    actor_loss      | 3.86e+03  |
|    critic_loss     | 31.4      |
|    ent_coef        | 0.192     |
|    ent_coef_loss   | -0.825    |
|    learning_rate   | 7.94e-06  |
|    n_updates       | 5739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.88e+05 |
| time/              |           |
|    episodes        | 412       |
|    fps             | 15        |
|    time_elapsed    | 135540    |
|    total_timesteps | 2060000   |
----------------------------------
Eval num_timesteps=2070000, episode_reward=-155045.53 +/- 135963.13
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2070000   |
| train/             |           |
|    actor_loss      | 3.62e+03  |
|    critic_loss     | 1.53e+03  |
|    ent_coef        | 0.18      |
|    ent_coef_loss   | -0.504    |
|    learning_rate   | 7.93e-06  |
|    n_updates       | 5749798   |
----------------------------------
Eval num_timesteps=2080000, episode_reward=-210901.01 +/- 135536.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2080000   |
| train/             |           |
|    actor_loss      | 3.73e+03  |
|    critic_loss     | 53.8      |
|    ent_coef        | 0.174     |
|    ent_coef_loss   | -0.27     |
|    learning_rate   | 7.92e-06  |
|    n_updates       | 5759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.85e+05 |
| time/              |           |
|    episodes        | 416       |
|    fps             | 15        |
|    time_elapsed    | 136879    |
|    total_timesteps | 2080000   |
----------------------------------
Eval num_timesteps=2090000, episode_reward=-100039.02 +/- 110763.34
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1e+05   |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | 3.68e+03 |
|    critic_loss     | 621      |
|    ent_coef        | 0.163    |
|    ent_coef_loss   | -0.607   |
|    learning_rate   | 7.91e-06 |
|    n_updates       | 5769798  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=-154741.42 +/- 136211.12
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2100000   |
| train/             |           |
|    actor_loss      | 3.36e+03  |
|    critic_loss     | 57.3      |
|    ent_coef        | 0.163     |
|    ent_coef_loss   | -0.0409   |
|    learning_rate   | 7.9e-06   |
|    n_updates       | 5779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.85e+05 |
| time/              |           |
|    episodes        | 420       |
|    fps             | 15        |
|    time_elapsed    | 138190    |
|    total_timesteps | 2100000   |
----------------------------------
Eval num_timesteps=2110000, episode_reward=-209897.87 +/- 136828.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | 3.59e+03 |
|    critic_loss     | 18.6     |
|    ent_coef        | 0.153    |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 7.89e-06 |
|    n_updates       | 5789798  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=-153885.51 +/- 136953.37
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 2120000   |
| train/             |           |
|    actor_loss      | 3.33e+03  |
|    critic_loss     | 36.6      |
|    ent_coef        | 0.142     |
|    ent_coef_loss   | -0.613    |
|    learning_rate   | 7.88e-06  |
|    n_updates       | 5799798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 15       |
|    time_elapsed    | 139505   |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=-154027.93 +/- 136837.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 2130000   |
| train/             |           |
|    actor_loss      | 3.64e+03  |
|    critic_loss     | 145       |
|    ent_coef        | 0.132     |
|    ent_coef_loss   | -0.73     |
|    learning_rate   | 7.87e-06  |
|    n_updates       | 5809798   |
----------------------------------
Eval num_timesteps=2140000, episode_reward=-210292.09 +/- 136349.14
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | 3.75e+03 |
|    critic_loss     | 35.3     |
|    ent_coef        | 0.123    |
|    ent_coef_loss   | -0.683   |
|    learning_rate   | 7.86e-06 |
|    n_updates       | 5819798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.93e+05 |
| time/              |           |
|    episodes        | 428       |
|    fps             | 15        |
|    time_elapsed    | 140818    |
|    total_timesteps | 2140000   |
----------------------------------
Eval num_timesteps=2150000, episode_reward=-210071.52 +/- 136620.60
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | 3.49e+03 |
|    critic_loss     | 53.1     |
|    ent_coef        | 0.119    |
|    ent_coef_loss   | 0.146    |
|    learning_rate   | 7.85e-06 |
|    n_updates       | 5829798  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=-100286.59 +/- 110667.85
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1e+05   |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | 3.76e+03 |
|    critic_loss     | 23.3     |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 7.84e-06 |
|    n_updates       | 5839798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.96e+05 |
| time/              |           |
|    episodes        | 432       |
|    fps             | 15        |
|    time_elapsed    | 142135    |
|    total_timesteps | 2160000   |
----------------------------------
Eval num_timesteps=2170000, episode_reward=-211147.18 +/- 135304.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2170000   |
| train/             |           |
|    actor_loss      | 3.86e+03  |
|    critic_loss     | 17.4      |
|    ent_coef        | 0.129     |
|    ent_coef_loss   | 0.0825    |
|    learning_rate   | 7.83e-06  |
|    n_updates       | 5849798   |
----------------------------------
Eval num_timesteps=2180000, episode_reward=-100893.65 +/- 110364.93
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 2180000   |
| train/             |           |
|    actor_loss      | 3.68e+03  |
|    critic_loss     | 1.64e+03  |
|    ent_coef        | 0.136     |
|    ent_coef_loss   | 0.453     |
|    learning_rate   | 7.82e-06  |
|    n_updates       | 5859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.96e+05 |
| time/              |           |
|    episodes        | 436       |
|    fps             | 15        |
|    time_elapsed    | 143463    |
|    total_timesteps | 2180000   |
----------------------------------
Eval num_timesteps=2190000, episode_reward=-45296.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.53e+04 |
| time/              |           |
|    total_timesteps | 2190000   |
| train/             |           |
|    actor_loss      | 3.83e+03  |
|    critic_loss     | 30.7      |
|    ent_coef        | 0.145     |
|    ent_coef_loss   | 0.0122    |
|    learning_rate   | 7.81e-06  |
|    n_updates       | 5869798   |
----------------------------------
Eval num_timesteps=2200000, episode_reward=-210793.14 +/- 135738.46
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2200000   |
| train/             |           |
|    actor_loss      | 3.59e+03  |
|    critic_loss     | 38.9      |
|    ent_coef        | 0.154     |
|    ent_coef_loss   | 0.39      |
|    learning_rate   | 7.8e-06   |
|    n_updates       | 5879798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.9e+05 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 15       |
|    time_elapsed    | 144797   |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=-209898.52 +/- 136833.02
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | 4.02e+03 |
|    critic_loss     | 142      |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | 0.162    |
|    learning_rate   | 7.79e-06 |
|    n_updates       | 5889798  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=-39762.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.98e+04 |
| time/              |           |
|    total_timesteps | 2220000   |
| train/             |           |
|    actor_loss      | 3.74e+03  |
|    critic_loss     | 41.9      |
|    ent_coef        | 0.162     |
|    ent_coef_loss   | 0.11      |
|    learning_rate   | 7.78e-06  |
|    n_updates       | 5899798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.82e+05 |
| time/              |           |
|    episodes        | 444       |
|    fps             | 15        |
|    time_elapsed    | 146130    |
|    total_timesteps | 2220000   |
----------------------------------
Eval num_timesteps=2230000, episode_reward=-208989.84 +/- 137944.95
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.09e+05 |
| time/              |           |
|    total_timesteps | 2230000   |
| train/             |           |
|    actor_loss      | 3.72e+03  |
|    critic_loss     | 26.3      |
|    ent_coef        | 0.158     |
|    ent_coef_loss   | -0.0349   |
|    learning_rate   | 7.77e-06  |
|    n_updates       | 5909798   |
----------------------------------
Eval num_timesteps=2240000, episode_reward=-265382.49 +/- 112477.49
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.65e+05 |
| time/              |           |
|    total_timesteps | 2240000   |
| train/             |           |
|    actor_loss      | 4e+03     |
|    critic_loss     | 507       |
|    ent_coef        | 0.15      |
|    ent_coef_loss   | -0.363    |
|    learning_rate   | 7.76e-06  |
|    n_updates       | 5919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 448       |
|    fps             | 15        |
|    time_elapsed    | 147454    |
|    total_timesteps | 2240000   |
----------------------------------
Eval num_timesteps=2250000, episode_reward=-265484.62 +/- 112272.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.65e+05 |
| time/              |           |
|    total_timesteps | 2250000   |
| train/             |           |
|    actor_loss      | 3.77e+03  |
|    critic_loss     | 1.61e+03  |
|    ent_coef        | 0.145     |
|    ent_coef_loss   | 0.315     |
|    learning_rate   | 7.75e-06  |
|    n_updates       | 5929798   |
----------------------------------
Eval num_timesteps=2260000, episode_reward=-97359.29 +/- 112130.57
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.74e+04 |
| time/              |           |
|    total_timesteps | 2260000   |
| train/             |           |
|    actor_loss      | 3.69e+03  |
|    critic_loss     | 1.64e+03  |
|    ent_coef        | 0.148     |
|    ent_coef_loss   | 0.542     |
|    learning_rate   | 7.74e-06  |
|    n_updates       | 5939798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.81e+05 |
| time/              |           |
|    episodes        | 452       |
|    fps             | 15        |
|    time_elapsed    | 148774    |
|    total_timesteps | 2260000   |
----------------------------------
Eval num_timesteps=2270000, episode_reward=-209753.84 +/- 137007.84
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | 3.59e+03 |
|    critic_loss     | 14.3     |
|    ent_coef        | 0.154    |
|    ent_coef_loss   | -0.252   |
|    learning_rate   | 7.73e-06 |
|    n_updates       | 5949798  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=-210264.88 +/- 136381.64
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | 3.71e+03 |
|    critic_loss     | 20.6     |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | 0.0208   |
|    learning_rate   | 7.72e-06 |
|    n_updates       | 5959798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.76e+05 |
| time/              |           |
|    episodes        | 456       |
|    fps             | 15        |
|    time_elapsed    | 150094    |
|    total_timesteps | 2280000   |
----------------------------------
Eval num_timesteps=2290000, episode_reward=-99667.54 +/- 110975.99
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.97e+04 |
| time/              |           |
|    total_timesteps | 2290000   |
| train/             |           |
|    actor_loss      | 3.81e+03  |
|    critic_loss     | 13.9      |
|    ent_coef        | 0.162     |
|    ent_coef_loss   | 0.15      |
|    learning_rate   | 7.71e-06  |
|    n_updates       | 5969798   |
----------------------------------
Eval num_timesteps=2300000, episode_reward=-154770.63 +/- 136231.27
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2300000   |
| train/             |           |
|    actor_loss      | 3.66e+03  |
|    critic_loss     | 41.4      |
|    ent_coef        | 0.169     |
|    ent_coef_loss   | 0.00354   |
|    learning_rate   | 7.7e-06   |
|    n_updates       | 5979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.76e+05 |
| time/              |           |
|    episodes        | 460       |
|    fps             | 15        |
|    time_elapsed    | 151403    |
|    total_timesteps | 2300000   |
----------------------------------
Eval num_timesteps=2310000, episode_reward=-210269.64 +/- 136374.49
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | 3.93e+03 |
|    critic_loss     | 60.3     |
|    ent_coef        | 0.17     |
|    ent_coef_loss   | -0.19    |
|    learning_rate   | 7.69e-06 |
|    n_updates       | 5989798  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=-154226.39 +/- 136675.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.54e+05 |
| time/              |           |
|    total_timesteps | 2320000   |
| train/             |           |
|    actor_loss      | 3.7e+03   |
|    critic_loss     | 17.1      |
|    ent_coef        | 0.168     |
|    ent_coef_loss   | -0.399    |
|    learning_rate   | 7.68e-06  |
|    n_updates       | 5999798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.7e+05 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 15       |
|    time_elapsed    | 152722   |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=-98155.52 +/- 111731.81
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.82e+04 |
| time/              |           |
|    total_timesteps | 2330000   |
| train/             |           |
|    actor_loss      | 3.84e+03  |
|    critic_loss     | 69.1      |
|    ent_coef        | 0.166     |
|    ent_coef_loss   | 0.087     |
|    learning_rate   | 7.67e-06  |
|    n_updates       | 6009798   |
----------------------------------
Eval num_timesteps=2340000, episode_reward=-209823.49 +/- 136920.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | 3.82e+03 |
|    critic_loss     | 13.5     |
|    ent_coef        | 0.158    |
|    ent_coef_loss   | -0.469   |
|    learning_rate   | 7.66e-06 |
|    n_updates       | 6019798  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.7e+05 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 15       |
|    time_elapsed    | 154054   |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=-98696.78 +/- 111460.80
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.87e+04 |
| time/              |           |
|    total_timesteps | 2350000   |
| train/             |           |
|    actor_loss      | 3.67e+03  |
|    critic_loss     | 14.6      |
|    ent_coef        | 0.148     |
|    ent_coef_loss   | -0.48     |
|    learning_rate   | 7.65e-06  |
|    n_updates       | 6029798   |
----------------------------------
Eval num_timesteps=2360000, episode_reward=-210447.90 +/- 136156.15
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | 3.67e+03 |
|    critic_loss     | 52       |
|    ent_coef        | 0.139    |
|    ent_coef_loss   | -0.762   |
|    learning_rate   | 7.64e-06 |
|    n_updates       | 6039798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.68e+05 |
| time/              |           |
|    episodes        | 472       |
|    fps             | 15        |
|    time_elapsed    | 155318    |
|    total_timesteps | 2360000   |
----------------------------------
Eval num_timesteps=2370000, episode_reward=-155001.24 +/- 136000.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2370000   |
| train/             |           |
|    actor_loss      | 3.71e+03  |
|    critic_loss     | 73.4      |
|    ent_coef        | 0.129     |
|    ent_coef_loss   | -0.666    |
|    learning_rate   | 7.63e-06  |
|    n_updates       | 6049798   |
----------------------------------
Eval num_timesteps=2380000, episode_reward=-210457.69 +/- 136080.67
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | 3.59e+03 |
|    critic_loss     | 15.7     |
|    ent_coef        | 0.12     |
|    ent_coef_loss   | -0.502   |
|    learning_rate   | 7.62e-06 |
|    n_updates       | 6059798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.71e+05 |
| time/              |           |
|    episodes        | 476       |
|    fps             | 15        |
|    time_elapsed    | 156568    |
|    total_timesteps | 2380000   |
----------------------------------
Eval num_timesteps=2390000, episode_reward=-210680.78 +/- 135807.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2390000   |
| train/             |           |
|    actor_loss      | 3.58e+03  |
|    critic_loss     | 21.3      |
|    ent_coef        | 0.113     |
|    ent_coef_loss   | -0.107    |
|    learning_rate   | 7.61e-06  |
|    n_updates       | 6069798   |
----------------------------------
Eval num_timesteps=2400000, episode_reward=-265996.29 +/- 111141.19
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2400000   |
| train/             |           |
|    actor_loss      | 3.88e+03  |
|    critic_loss     | 19.5      |
|    ent_coef        | 0.106     |
|    ent_coef_loss   | -0.256    |
|    learning_rate   | 7.6e-06   |
|    n_updates       | 6079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.78e+05 |
| time/              |           |
|    episodes        | 480       |
|    fps             | 15        |
|    time_elapsed    | 157810    |
|    total_timesteps | 2400000   |
----------------------------------
Eval num_timesteps=2410000, episode_reward=-210480.38 +/- 136052.07
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | 3.65e+03 |
|    critic_loss     | 11       |
|    ent_coef        | 0.0991   |
|    ent_coef_loss   | -0.0174  |
|    learning_rate   | 7.59e-06 |
|    n_updates       | 6089798  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=-210556.56 +/- 135959.19
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2420000   |
| train/             |           |
|    actor_loss      | 3.62e+03  |
|    critic_loss     | 46.4      |
|    ent_coef        | 0.0932    |
|    ent_coef_loss   | -0.938    |
|    learning_rate   | 7.58e-06  |
|    n_updates       | 6099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.75e+05 |
| time/              |           |
|    episodes        | 484       |
|    fps             | 15        |
|    time_elapsed    | 159053    |
|    total_timesteps | 2420000   |
----------------------------------
Eval num_timesteps=2430000, episode_reward=-266100.39 +/- 110933.63
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2430000   |
| train/             |           |
|    actor_loss      | 3.52e+03  |
|    critic_loss     | 20.9      |
|    ent_coef        | 0.0878    |
|    ent_coef_loss   | -0.295    |
|    learning_rate   | 7.57e-06  |
|    n_updates       | 6109798   |
----------------------------------
Eval num_timesteps=2440000, episode_reward=-155066.09 +/- 135989.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2440000   |
| train/             |           |
|    actor_loss      | 3.93e+03  |
|    critic_loss     | 14        |
|    ent_coef        | 0.0827    |
|    ent_coef_loss   | -0.502    |
|    learning_rate   | 7.56e-06  |
|    n_updates       | 6119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 488       |
|    fps             | 15        |
|    time_elapsed    | 160286    |
|    total_timesteps | 2440000   |
----------------------------------
Eval num_timesteps=2450000, episode_reward=-210713.36 +/- 135698.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2450000   |
| train/             |           |
|    actor_loss      | 4.07e+03  |
|    critic_loss     | 10.6      |
|    ent_coef        | 0.0849    |
|    ent_coef_loss   | 0.883     |
|    learning_rate   | 7.55e-06  |
|    n_updates       | 6129798   |
----------------------------------
Eval num_timesteps=2460000, episode_reward=-321511.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+05 |
| time/              |           |
|    total_timesteps | 2460000   |
| train/             |           |
|    actor_loss      | 3.73e+03  |
|    critic_loss     | 14.7      |
|    ent_coef        | 0.0914    |
|    ent_coef_loss   | 1.47      |
|    learning_rate   | 7.54e-06  |
|    n_updates       | 6139798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 492       |
|    fps             | 15        |
|    time_elapsed    | 161512    |
|    total_timesteps | 2460000   |
----------------------------------
Eval num_timesteps=2470000, episode_reward=-99717.83 +/- 110949.04
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.97e+04 |
| time/              |           |
|    total_timesteps | 2470000   |
| train/             |           |
|    actor_loss      | 3.82e+03  |
|    critic_loss     | 16.5      |
|    ent_coef        | 0.0986    |
|    ent_coef_loss   | 1.56      |
|    learning_rate   | 7.53e-06  |
|    n_updates       | 6149798   |
----------------------------------
Eval num_timesteps=2480000, episode_reward=-156642.06 +/- 134615.77
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.57e+05 |
| time/              |           |
|    total_timesteps | 2480000   |
| train/             |           |
|    actor_loss      | 3.61e+03  |
|    critic_loss     | 20.8      |
|    ent_coef        | 0.106     |
|    ent_coef_loss   | 0.658     |
|    learning_rate   | 7.52e-06  |
|    n_updates       | 6159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 496       |
|    fps             | 15        |
|    time_elapsed    | 162749    |
|    total_timesteps | 2480000   |
----------------------------------
Eval num_timesteps=2490000, episode_reward=-156302.73 +/- 134892.74
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2490000   |
| train/             |           |
|    actor_loss      | 3.93e+03  |
|    critic_loss     | 25.1      |
|    ent_coef        | 0.114     |
|    ent_coef_loss   | 1.37      |
|    learning_rate   | 7.51e-06  |
|    n_updates       | 6169798   |
----------------------------------
Eval num_timesteps=2500000, episode_reward=-210690.52 +/- 135728.41
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2500000   |
| train/             |           |
|    actor_loss      | 3.72e+03  |
|    critic_loss     | 82        |
|    ent_coef        | 0.122     |
|    ent_coef_loss   | 1.83      |
|    learning_rate   | 7.5e-06   |
|    n_updates       | 6179798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 15       |
|    time_elapsed    | 163992   |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=-154616.39 +/- 136270.47
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2510000   |
| train/             |           |
|    actor_loss      | 3.49e+03  |
|    critic_loss     | 180       |
|    ent_coef        | 0.13      |
|    ent_coef_loss   | 0.354     |
|    learning_rate   | 7.49e-06  |
|    n_updates       | 6189798   |
----------------------------------
Eval num_timesteps=2520000, episode_reward=-210225.33 +/- 136298.93
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | 3.45e+03 |
|    critic_loss     | 17.3     |
|    ent_coef        | 0.134    |
|    ent_coef_loss   | -0.00933 |
|    learning_rate   | 7.48e-06 |
|    n_updates       | 6199798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.86e+05 |
| time/              |           |
|    episodes        | 504       |
|    fps             | 15        |
|    time_elapsed    | 165238    |
|    total_timesteps | 2520000   |
----------------------------------
Eval num_timesteps=2530000, episode_reward=-321512.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+05 |
| time/              |           |
|    total_timesteps | 2530000   |
| train/             |           |
|    actor_loss      | 3.53e+03  |
|    critic_loss     | 48.6      |
|    ent_coef        | 0.134     |
|    ent_coef_loss   | -0.0706   |
|    learning_rate   | 7.47e-06  |
|    n_updates       | 6209798   |
----------------------------------
Eval num_timesteps=2540000, episode_reward=-154543.31 +/- 136144.27
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2540000   |
| train/             |           |
|    actor_loss      | 3.64e+03  |
|    critic_loss     | 153       |
|    ent_coef        | 0.144     |
|    ent_coef_loss   | 8.75      |
|    learning_rate   | 7.46e-06  |
|    n_updates       | 6219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.94e+05 |
| time/              |           |
|    episodes        | 508       |
|    fps             | 15        |
|    time_elapsed    | 166476    |
|    total_timesteps | 2540000   |
----------------------------------
Eval num_timesteps=2550000, episode_reward=-210422.74 +/- 135649.91
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -2.1e+05 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | 3.21e+03 |
|    critic_loss     | 70.7     |
|    ent_coef        | 0.155    |
|    ent_coef_loss   | 7.19     |
|    learning_rate   | 7.45e-06 |
|    n_updates       | 6229798  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=-46694.00 +/- 1152.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.67e+04 |
| time/              |           |
|    total_timesteps | 2560000   |
| train/             |           |
|    actor_loss      | 3.23e+03  |
|    critic_loss     | 58.2      |
|    ent_coef        | 0.167     |
|    ent_coef_loss   | 10.3      |
|    learning_rate   | 7.44e-06  |
|    n_updates       | 6239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.88e+05 |
| time/              |           |
|    episodes        | 512       |
|    fps             | 15        |
|    time_elapsed    | 167716    |
|    total_timesteps | 2560000   |
----------------------------------
Eval num_timesteps=2570000, episode_reward=-45972.31 +/- 474.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -4.6e+04 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | 3.52e+03 |
|    critic_loss     | 197      |
|    ent_coef        | 0.18     |
|    ent_coef_loss   | 11.8     |
|    learning_rate   | 7.43e-06 |
|    n_updates       | 6249798  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=-120192.77 +/- 61740.70
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+05 |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | 3.77e+03 |
|    critic_loss     | 189      |
|    ent_coef        | 0.193    |
|    ent_coef_loss   | 7.41     |
|    learning_rate   | 7.42e-06 |
|    n_updates       | 6259798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.85e+05 |
| time/              |           |
|    episodes        | 516       |
|    fps             | 15        |
|    time_elapsed    | 168947    |
|    total_timesteps | 2580000   |
----------------------------------
Eval num_timesteps=2590000, episode_reward=-210969.89 +/- 135321.79
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2590000   |
| train/             |           |
|    actor_loss      | 3.46e+03  |
|    critic_loss     | 33        |
|    ent_coef        | 0.202     |
|    ent_coef_loss   | 0.649     |
|    learning_rate   | 7.41e-06  |
|    n_updates       | 6269798   |
----------------------------------
Eval num_timesteps=2600000, episode_reward=-156065.18 +/- 134999.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2600000   |
| train/             |           |
|    actor_loss      | 3.62e+03  |
|    critic_loss     | 39        |
|    ent_coef        | 0.218     |
|    ent_coef_loss   | 1.9       |
|    learning_rate   | 7.4e-06   |
|    n_updates       | 6279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.85e+05 |
| time/              |           |
|    episodes        | 520       |
|    fps             | 15        |
|    time_elapsed    | 170180    |
|    total_timesteps | 2600000   |
----------------------------------
Eval num_timesteps=2610000, episode_reward=-156187.76 +/- 134901.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2610000   |
| train/             |           |
|    actor_loss      | 3.83e+03  |
|    critic_loss     | 219       |
|    ent_coef        | 0.235     |
|    ent_coef_loss   | 1.2       |
|    learning_rate   | 7.39e-06  |
|    n_updates       | 6289798   |
----------------------------------
Eval num_timesteps=2620000, episode_reward=-321459.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.21e+05 |
| time/              |           |
|    total_timesteps | 2620000   |
| train/             |           |
|    actor_loss      | 3.46e+03  |
|    critic_loss     | 61.3      |
|    ent_coef        | 0.252     |
|    ent_coef_loss   | 1.11      |
|    learning_rate   | 7.38e-06  |
|    n_updates       | 6299798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 15       |
|    time_elapsed    | 171411   |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=-101127.72 +/- 110165.05
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 2630000   |
| train/             |           |
|    actor_loss      | 3.79e+03  |
|    critic_loss     | 151       |
|    ent_coef        | 0.272     |
|    ent_coef_loss   | 1.5       |
|    learning_rate   | 7.37e-06  |
|    n_updates       | 6309798   |
----------------------------------
Eval num_timesteps=2640000, episode_reward=-266449.50 +/- 110231.46
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2640000   |
| train/             |           |
|    actor_loss      | 3.85e+03  |
|    critic_loss     | 36.9      |
|    ent_coef        | 0.275     |
|    ent_coef_loss   | 0.146     |
|    learning_rate   | 7.36e-06  |
|    n_updates       | 6319798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 15       |
|    time_elapsed    | 172637   |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=-211366.64 +/- 134963.82
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2650000   |
| train/             |           |
|    actor_loss      | 3.77e+03  |
|    critic_loss     | 93.1      |
|    ent_coef        | 0.275     |
|    ent_coef_loss   | 0.469     |
|    learning_rate   | 7.35e-06  |
|    n_updates       | 6329798   |
----------------------------------
Eval num_timesteps=2660000, episode_reward=-156420.78 +/- 134839.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2660000   |
| train/             |           |
|    actor_loss      | 3.92e+03  |
|    critic_loss     | 113       |
|    ent_coef        | 0.266     |
|    ent_coef_loss   | -0.154    |
|    learning_rate   | 7.34e-06  |
|    n_updates       | 6339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 532       |
|    fps             | 15        |
|    time_elapsed    | 173871    |
|    total_timesteps | 2660000   |
----------------------------------
Eval num_timesteps=2670000, episode_reward=-321563.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+05 |
| time/              |           |
|    total_timesteps | 2670000   |
| train/             |           |
|    actor_loss      | 3.77e+03  |
|    critic_loss     | 25.8      |
|    ent_coef        | 0.256     |
|    ent_coef_loss   | 0.0439    |
|    learning_rate   | 7.33e-06  |
|    n_updates       | 6349798   |
----------------------------------
Eval num_timesteps=2680000, episode_reward=-266413.03 +/- 110301.83
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2680000   |
| train/             |           |
|    actor_loss      | 3.86e+03  |
|    critic_loss     | 11.4      |
|    ent_coef        | 0.248     |
|    ent_coef_loss   | -0.37     |
|    learning_rate   | 7.32e-06  |
|    n_updates       | 6359798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 15       |
|    time_elapsed    | 175102   |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=-100826.43 +/- 110369.68
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 2690000   |
| train/             |           |
|    actor_loss      | 3.71e+03  |
|    critic_loss     | 63.8      |
|    ent_coef        | 0.231     |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 7.31e-06  |
|    n_updates       | 6369798   |
----------------------------------
Eval num_timesteps=2700000, episode_reward=-211160.28 +/- 135215.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2700000   |
| train/             |           |
|    actor_loss      | 4.07e+03  |
|    critic_loss     | 3.48e+04  |
|    ent_coef        | 0.216     |
|    ent_coef_loss   | -0.122    |
|    learning_rate   | 7.3e-06   |
|    n_updates       | 6379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.77e+05 |
| time/              |           |
|    episodes        | 540       |
|    fps             | 15        |
|    time_elapsed    | 176341    |
|    total_timesteps | 2700000   |
----------------------------------
Eval num_timesteps=2710000, episode_reward=-155756.36 +/- 135379.72
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2710000   |
| train/             |           |
|    actor_loss      | 3.65e+03  |
|    critic_loss     | 30.4      |
|    ent_coef        | 0.205     |
|    ent_coef_loss   | -0.153    |
|    learning_rate   | 7.29e-06  |
|    n_updates       | 6389798   |
----------------------------------
Eval num_timesteps=2720000, episode_reward=-44113.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.41e+04 |
| time/              |           |
|    total_timesteps | 2720000   |
| train/             |           |
|    actor_loss      | 3.37e+03  |
|    critic_loss     | 1.68e+03  |
|    ent_coef        | 0.2       |
|    ent_coef_loss   | -0.164    |
|    learning_rate   | 7.28e-06  |
|    n_updates       | 6399798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 544       |
|    fps             | 15        |
|    time_elapsed    | 177588    |
|    total_timesteps | 2720000   |
----------------------------------
Eval num_timesteps=2730000, episode_reward=-210594.93 +/- 135907.80
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2730000   |
| train/             |           |
|    actor_loss      | 3.67e+03  |
|    critic_loss     | 28.4      |
|    ent_coef        | 0.187     |
|    ent_coef_loss   | -0.175    |
|    learning_rate   | 7.27e-06  |
|    n_updates       | 6409798   |
----------------------------------
Eval num_timesteps=2740000, episode_reward=-210642.59 +/- 135851.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2740000   |
| train/             |           |
|    actor_loss      | 4.04e+03  |
|    critic_loss     | 49.3      |
|    ent_coef        | 0.178     |
|    ent_coef_loss   | -0.569    |
|    learning_rate   | 7.26e-06  |
|    n_updates       | 6419798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 15       |
|    time_elapsed    | 178831   |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=-266172.59 +/- 110777.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2750000   |
| train/             |           |
|    actor_loss      | 3.41e+03  |
|    critic_loss     | 21.9      |
|    ent_coef        | 0.169     |
|    ent_coef_loss   | -0.252    |
|    learning_rate   | 7.25e-06  |
|    n_updates       | 6429798   |
----------------------------------
Eval num_timesteps=2760000, episode_reward=-155170.18 +/- 135811.83
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.55e+05 |
| time/              |           |
|    total_timesteps | 2760000   |
| train/             |           |
|    actor_loss      | 3.7e+03   |
|    critic_loss     | 41.2      |
|    ent_coef        | 0.18      |
|    ent_coef_loss   | 0.495     |
|    learning_rate   | 7.24e-06  |
|    n_updates       | 6439798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 552      |
|    fps             | 15       |
|    time_elapsed    | 180062   |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=-211295.62 +/- 135043.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2770000   |
| train/             |           |
|    actor_loss      | 3.76e+03  |
|    critic_loss     | 26.8      |
|    ent_coef        | 0.193     |
|    ent_coef_loss   | 0.829     |
|    learning_rate   | 7.23e-06  |
|    n_updates       | 6449798   |
----------------------------------
Eval num_timesteps=2780000, episode_reward=-212282.17 +/- 133899.52
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.12e+05 |
| time/              |           |
|    total_timesteps | 2780000   |
| train/             |           |
|    actor_loss      | 3.64e+03  |
|    critic_loss     | 78.4      |
|    ent_coef        | 0.209     |
|    ent_coef_loss   | 3.08      |
|    learning_rate   | 7.22e-06  |
|    n_updates       | 6459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.83e+05 |
| time/              |           |
|    episodes        | 556       |
|    fps             | 15        |
|    time_elapsed    | 181286    |
|    total_timesteps | 2780000   |
----------------------------------
Eval num_timesteps=2790000, episode_reward=-155785.70 +/- 135399.12
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2790000   |
| train/             |           |
|    actor_loss      | 3.41e+03  |
|    critic_loss     | 71.6      |
|    ent_coef        | 0.223     |
|    ent_coef_loss   | 1.26      |
|    learning_rate   | 7.21e-06  |
|    n_updates       | 6469798   |
----------------------------------
Eval num_timesteps=2800000, episode_reward=-155726.87 +/- 135404.88
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.56e+05 |
| time/              |           |
|    total_timesteps | 2800000   |
| train/             |           |
|    actor_loss      | 3.56e+03  |
|    critic_loss     | 36        |
|    ent_coef        | 0.222     |
|    ent_coef_loss   | -0.179    |
|    learning_rate   | 7.2e-06   |
|    n_updates       | 6479798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 15       |
|    time_elapsed    | 182504   |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=-266296.66 +/- 110328.34
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.66e+05 |
| time/              |           |
|    total_timesteps | 2810000   |
| train/             |           |
|    actor_loss      | 3.51e+03  |
|    critic_loss     | 24.6      |
|    ent_coef        | 0.208     |
|    ent_coef_loss   | -0.738    |
|    learning_rate   | 7.19e-06  |
|    n_updates       | 6489798   |
----------------------------------
Eval num_timesteps=2820000, episode_reward=-46101.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.61e+04 |
| time/              |           |
|    total_timesteps | 2820000   |
| train/             |           |
|    actor_loss      | 3.8e+03   |
|    critic_loss     | 26.2      |
|    ent_coef        | 0.195     |
|    ent_coef_loss   | -0.324    |
|    learning_rate   | 7.18e-06  |
|    n_updates       | 6499798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.8e+05 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 15       |
|    time_elapsed    | 183726   |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=-211339.12 +/- 134870.15
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.11e+05 |
| time/              |           |
|    total_timesteps | 2830000   |
| train/             |           |
|    actor_loss      | 3.25e+03  |
|    critic_loss     | 10        |
|    ent_coef        | 0.186     |
|    ent_coef_loss   | 0.0344    |
|    learning_rate   | 7.17e-06  |
|    n_updates       | 6509798   |
