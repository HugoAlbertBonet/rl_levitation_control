Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_62
Eval num_timesteps=10000, episode_reward=-248.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -17      |
|    critic_loss     | 1.84     |
|    ent_coef        | 0.00687  |
|    ent_coef_loss   | -1.65    |
|    learning_rate   | 0.000999 |
|    n_updates       | 184425   |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=424.73 +/- 355.02
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 4.1      |
|    ent_coef        | 0.00421  |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.000998 |
|    n_updates       | 194425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 409      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 34       |
|    time_elapsed    | 576      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1067.92 +/- 129.54
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -17.2    |
|    critic_loss     | 0.185    |
|    ent_coef        | 0.00443  |
|    ent_coef_loss   | 4.1      |
|    learning_rate   | 0.000997 |
|    n_updates       | 204425   |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=623.50 +/- 22.11
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -18.6    |
|    critic_loss     | 0.192    |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | -0.263   |
|    learning_rate   | 0.000996 |
|    n_updates       | 214425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 781      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 35       |
|    time_elapsed    | 1137     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1534.51 +/- 2.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -19.8    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.0033   |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.000995 |
|    n_updates       | 224425   |
---------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=1525.78 +/- 0.14
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -24.3    |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.00309  |
|    ent_coef_loss   | -2.66    |
|    learning_rate   | 0.000994 |
|    n_updates       | 234425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 966      |
| time/              |          |
|    episodes        | 12       |
|    fps             | 35       |
|    time_elapsed    | 1713     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1546.43 +/- 3.13
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.39     |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -4.6     |
|    learning_rate   | 0.000993 |
|    n_updates       | 244425   |
---------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=1528.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -22.5    |
|    critic_loss     | 0.525    |
|    ent_coef        | 0.00213  |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.000992 |
|    n_updates       | 254425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 34       |
|    time_elapsed    | 2294     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1520.33 +/- 3.99
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -21.1    |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | 8.5      |
|    learning_rate   | 0.000991 |
|    n_updates       | 264425   |
---------------------------------
Eval num_timesteps=100000, episode_reward=1846.28 +/- 14.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 0.151    |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.00099  |
|    n_updates       | 274425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 34       |
|    time_elapsed    | 2879     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=520.19 +/- 816.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -21.5    |
|    critic_loss     | 0.202    |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | 7.72     |
|    learning_rate   | 0.000989 |
|    n_updates       | 284425   |
---------------------------------
Eval num_timesteps=120000, episode_reward=783.46 +/- 3.31
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -21.9    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00186  |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000988 |
|    n_updates       | 294425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 34       |
|    time_elapsed    | 3452     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1538.08 +/- 13.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -21.8    |
|    critic_loss     | 0.29     |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | -7.39    |
|    learning_rate   | 0.000987 |
|    n_updates       | 304425   |
---------------------------------
Eval num_timesteps=140000, episode_reward=679.86 +/- 23.76
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -23.2    |
|    critic_loss     | 0.862    |
|    ent_coef        | 0.00234  |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.000986 |
|    n_updates       | 314425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 35       |
|    time_elapsed    | 3956     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=416.81 +/- 896.68
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 2.5      |
|    ent_coef        | 0.00491  |
|    ent_coef_loss   | -6.23    |
|    learning_rate   | 0.000985 |
|    n_updates       | 324425   |
---------------------------------
Eval num_timesteps=160000, episode_reward=180.33 +/- 677.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.00857  |
|    ent_coef_loss   | -0.947   |
|    learning_rate   | 0.000984 |
|    n_updates       | 334425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 35       |
|    time_elapsed    | 4456     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-290.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 4.59     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000983 |
|    n_updates       | 344425   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-242.08 +/- 52.39
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 4.14     |
|    ent_coef        | 0.0131   |
|    ent_coef_loss   | -6.46    |
|    learning_rate   | 0.000982 |
|    n_updates       | 354425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 36       |
|    time_elapsed    | 4955     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1174.64 +/- 436.05
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -40.1    |
|    critic_loss     | 0.777    |
|    ent_coef        | 0.00865  |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.000981 |
|    n_updates       | 364425   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1520.58 +/- 0.70
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.559    |
|    ent_coef        | 0.00567  |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 0.00098  |
|    n_updates       | 374425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    episodes        | 40       |
|    fps             | 36       |
|    time_elapsed    | 5454     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1512.38 +/- 13.54
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.339    |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | 4.52     |
|    learning_rate   | 0.000979 |
|    n_updates       | 384425   |
---------------------------------
Eval num_timesteps=220000, episode_reward=1532.64 +/- 21.93
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.576    |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.000978 |
|    n_updates       | 394425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 36       |
|    time_elapsed    | 5953     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=1535.07 +/- 0.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -23.8    |
|    critic_loss     | 0.316    |
|    ent_coef        | 0.00295  |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000977 |
|    n_updates       | 404425   |
---------------------------------
Eval num_timesteps=240000, episode_reward=1608.85 +/- 16.93
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -25      |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | 2.74     |
|    learning_rate   | 0.000976 |
|    n_updates       | 414425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 37       |
|    time_elapsed    | 6456     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=1576.91 +/- 59.32
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.218    |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | -5.09    |
|    learning_rate   | 0.000975 |
|    n_updates       | 424425   |
---------------------------------
Eval num_timesteps=260000, episode_reward=1181.68 +/- 435.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -23.5    |
|    critic_loss     | 0.181    |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | -2.72    |
|    learning_rate   | 0.000974 |
|    n_updates       | 434425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 37       |
|    time_elapsed    | 6962     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=352.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -23.1    |
|    critic_loss     | 0.169    |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 2.97     |
|    learning_rate   | 0.000973 |
|    n_updates       | 444425   |
---------------------------------
Eval num_timesteps=280000, episode_reward=746.52 +/- 22.69
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -25.4    |
|    critic_loss     | 0.0642   |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000972 |
|    n_updates       | 454425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 37       |
|    time_elapsed    | 7469     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1552.83 +/- 12.89
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -23.7    |
|    critic_loss     | 0.556    |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -0.944   |
|    learning_rate   | 0.000971 |
|    n_updates       | 464425   |
---------------------------------
Eval num_timesteps=300000, episode_reward=1041.69 +/- 493.42
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 0.393    |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -0.972   |
|    learning_rate   | 0.00097  |
|    n_updates       | 474425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 60       |
|    fps             | 37       |
|    time_elapsed    | 7974     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=1537.72 +/- 8.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -21.7    |
|    critic_loss     | 0.553    |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | -3.94    |
|    learning_rate   | 0.000969 |
|    n_updates       | 484425   |
---------------------------------
Eval num_timesteps=320000, episode_reward=1562.94 +/- 16.60
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00182  |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 0.000968 |
|    n_updates       | 494425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 37       |
|    time_elapsed    | 8479     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2117.44 +/- 33.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | 6.04     |
|    learning_rate   | 0.000967 |
|    n_updates       | 504425   |
---------------------------------
New best mean reward!
Eval num_timesteps=340000, episode_reward=1535.22 +/- 7.24
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 0.000966 |
|    n_updates       | 514425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 37       |
|    time_elapsed    | 9001     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=1655.18 +/- 141.13
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -24.7    |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.00202  |
|    ent_coef_loss   | 0.448    |
|    learning_rate   | 0.000965 |
|    n_updates       | 524425   |
---------------------------------
Eval num_timesteps=360000, episode_reward=2828.26 +/- 164.67
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -24.6    |
|    critic_loss     | 0.274    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -5.61    |
|    learning_rate   | 0.000964 |
|    n_updates       | 534425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 37       |
|    time_elapsed    | 9548     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=1530.30 +/- 21.95
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.176    |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -0.483   |
|    learning_rate   | 0.000963 |
|    n_updates       | 544425   |
---------------------------------
Eval num_timesteps=380000, episode_reward=1549.44 +/- 18.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -26      |
|    critic_loss     | 0.225    |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 0.15     |
|    learning_rate   | 0.000962 |
|    n_updates       | 554425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 37       |
|    time_elapsed    | 10059    |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=652.89 +/- 2.82
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 0.35     |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000961 |
|    n_updates       | 564425   |
---------------------------------
Eval num_timesteps=400000, episode_reward=1568.03 +/- 20.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 0.279    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.00096  |
|    n_updates       | 574425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 37       |
|    time_elapsed    | 10570    |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=649.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 0.439    |
|    ent_coef        | 0.00378  |
|    ent_coef_loss   | 4.24     |
|    learning_rate   | 0.000959 |
|    n_updates       | 584425   |
---------------------------------
Eval num_timesteps=420000, episode_reward=1534.71 +/- 2.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.685    |
|    ent_coef        | 0.0047   |
|    ent_coef_loss   | -4.85    |
|    learning_rate   | 0.000958 |
|    n_updates       | 594425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 37       |
|    time_elapsed    | 11079    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=81.76 +/- 452.96
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 81.8     |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 5.01     |
|    ent_coef        | 0.0055   |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000957 |
|    n_updates       | 604425   |
---------------------------------
Eval num_timesteps=440000, episode_reward=233.21 +/- 424.47
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -32.8    |
|    critic_loss     | 2.42     |
|    ent_coef        | 0.00773  |
|    ent_coef_loss   | 4.91     |
|    learning_rate   | 0.000956 |
|    n_updates       | 614425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 37       |
|    time_elapsed    | 11589    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=223.27 +/- 438.80
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -42.7    |
|    critic_loss     | 3.73     |
|    ent_coef        | 0.00662  |
|    ent_coef_loss   | 5.45     |
|    learning_rate   | 0.000955 |
|    n_updates       | 624425   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-147.22 +/- 77.89
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -39.2    |
|    critic_loss     | 5.26     |
|    ent_coef        | 0.0098   |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000954 |
|    n_updates       | 634425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 38       |
|    time_elapsed    | 12097    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=652.25 +/- 12.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 1        |
|    ent_coef        | 0.0082   |
|    ent_coef_loss   | -4.97    |
|    learning_rate   | 0.000953 |
|    n_updates       | 644425   |
---------------------------------
Eval num_timesteps=480000, episode_reward=495.49 +/- 121.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 1.24     |
|    ent_coef        | 0.00617  |
|    ent_coef_loss   | 5.18     |
|    learning_rate   | 0.000952 |
|    n_updates       | 654425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 38       |
|    time_elapsed    | 12607    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=1532.59 +/- 5.24
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 2.14     |
|    ent_coef        | 0.00613  |
|    ent_coef_loss   | 3.69     |
|    learning_rate   | 0.000951 |
|    n_updates       | 664425   |
---------------------------------
Eval num_timesteps=500000, episode_reward=86.08 +/- 475.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 86.1     |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 3.05     |
|    ent_coef        | 0.00576  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.00095  |
|    n_updates       | 674425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 38       |
|    time_elapsed    | 13116    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=627.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 3.12     |
|    ent_coef        | 0.00638  |
|    ent_coef_loss   | -0.627   |
|    learning_rate   | 0.000949 |
|    n_updates       | 684425   |
---------------------------------
Eval num_timesteps=520000, episode_reward=1127.75 +/- 518.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 2.07     |
|    ent_coef        | 0.007    |
|    ent_coef_loss   | 0.586    |
|    learning_rate   | 0.000948 |
|    n_updates       | 694425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 38       |
|    time_elapsed    | 13625    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=1185.31 +/- 739.90
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.00648  |
|    ent_coef_loss   | -8.78    |
|    learning_rate   | 0.000947 |
|    n_updates       | 704425   |
---------------------------------
Eval num_timesteps=540000, episode_reward=888.03 +/- 521.77
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 2.79     |
|    ent_coef        | 0.00618  |
|    ent_coef_loss   | -0.168   |
|    learning_rate   | 0.000946 |
|    n_updates       | 714425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 38       |
|    time_elapsed    | 14133    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=1539.09 +/- 3.07
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 0.76     |
|    ent_coef        | 0.00585  |
|    ent_coef_loss   | 8.78     |
|    learning_rate   | 0.000945 |
|    n_updates       | 724425   |
---------------------------------
Eval num_timesteps=560000, episode_reward=1543.63 +/- 19.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -26.3    |
|    critic_loss     | 1.86     |
|    ent_coef        | 0.00473  |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000944 |
|    n_updates       | 734425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 38       |
|    time_elapsed    | 14641    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=1003.93 +/- 462.72
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 0.324    |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | -2.78    |
|    learning_rate   | 0.000943 |
|    n_updates       | 744425   |
---------------------------------
Eval num_timesteps=580000, episode_reward=997.86 +/- 444.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00378  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000942 |
|    n_updates       | 754425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 38       |
|    time_elapsed    | 15148    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=1539.62 +/- 5.18
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -23.1    |
|    critic_loss     | 0.289    |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000941 |
|    n_updates       | 764425   |
---------------------------------
Eval num_timesteps=600000, episode_reward=1533.44 +/- 2.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -23      |
|    critic_loss     | 0.241    |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | -1.26    |
|    learning_rate   | 0.00094  |
|    n_updates       | 774425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 38       |
|    time_elapsed    | 15654    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=1537.14 +/- 6.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -23.2    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.00307  |
|    ent_coef_loss   | -0.497   |
|    learning_rate   | 0.000939 |
|    n_updates       | 784425   |
---------------------------------
Eval num_timesteps=620000, episode_reward=1547.92 +/- 13.25
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -22.5    |
|    critic_loss     | 0.438    |
|    ent_coef        | 0.00294  |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000938 |
|    n_updates       | 794425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 38       |
|    time_elapsed    | 16160    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=1526.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.394    |
|    ent_coef        | 0.00271  |
|    ent_coef_loss   | 4.14     |
|    learning_rate   | 0.000937 |
|    n_updates       | 804425   |
---------------------------------
Eval num_timesteps=640000, episode_reward=1928.35 +/- 58.69
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | -0.187   |
|    learning_rate   | 0.000936 |
|    n_updates       | 814425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 38       |
|    time_elapsed    | 16666    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1887.42 +/- 41.42
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -24      |
|    critic_loss     | 0.189    |
|    ent_coef        | 0.00226  |
|    ent_coef_loss   | -3.55    |
|    learning_rate   | 0.000935 |
|    n_updates       | 824425   |
---------------------------------
Eval num_timesteps=660000, episode_reward=1789.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -23.9    |
|    critic_loss     | 0.362    |
|    ent_coef        | 0.00216  |
|    ent_coef_loss   | 2.41     |
|    learning_rate   | 0.000934 |
|    n_updates       | 834425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 38       |
|    time_elapsed    | 17173    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=1991.64 +/- 68.93
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.0839   |
|    ent_coef        | 0.00213  |
|    ent_coef_loss   | -3.33    |
|    learning_rate   | 0.000933 |
|    n_updates       | 844425   |
---------------------------------
Eval num_timesteps=680000, episode_reward=892.15 +/- 239.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.289    |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000932 |
|    n_updates       | 854425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 38       |
|    time_elapsed    | 17678    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=1863.34 +/- 28.24
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00204  |
|    ent_coef_loss   | 0.787    |
|    learning_rate   | 0.000931 |
|    n_updates       | 864425   |
---------------------------------
Eval num_timesteps=700000, episode_reward=1881.25 +/- 27.32
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -23.6    |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | -1.42    |
|    learning_rate   | 0.00093  |
|    n_updates       | 874425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 38       |
|    time_elapsed    | 18184    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=1463.41 +/- 3.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 0.0846   |
|    ent_coef        | 0.00199  |
|    ent_coef_loss   | -0.219   |
|    learning_rate   | 0.000929 |
|    n_updates       | 884425   |
---------------------------------
Eval num_timesteps=720000, episode_reward=1529.94 +/- 8.05
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -23.2    |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.00214  |
|    ent_coef_loss   | 0.405    |
|    learning_rate   | 0.000928 |
|    n_updates       | 894425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 38       |
|    time_elapsed    | 18692    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=2145.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -23.3    |
|    critic_loss     | 0.135    |
|    ent_coef        | 0.0021   |
|    ent_coef_loss   | 0.647    |
|    learning_rate   | 0.000927 |
|    n_updates       | 904425   |
---------------------------------
Eval num_timesteps=740000, episode_reward=2339.70 +/- 0.75
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -22.9    |
|    critic_loss     | 0.208    |
|    ent_coef        | 0.00221  |
|    ent_coef_loss   | 2.31     |
|    learning_rate   | 0.000926 |
|    n_updates       | 914425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 38       |
|    time_elapsed    | 19198    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=1536.51 +/- 4.72
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -23.9    |
|    critic_loss     | 0.137    |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | 5.13     |
|    learning_rate   | 0.000925 |
|    n_updates       | 924425   |
---------------------------------
Eval num_timesteps=760000, episode_reward=1546.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -24.6    |
|    critic_loss     | 1        |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | -0.683   |
|    learning_rate   | 0.000924 |
|    n_updates       | 934425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 38       |
|    time_elapsed    | 19703    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2272.88 +/- 34.29
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -25.5    |
|    critic_loss     | 0.0839   |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000923 |
|    n_updates       | 944425   |
---------------------------------
Eval num_timesteps=780000, episode_reward=2204.10 +/- 66.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -24.7    |
|    critic_loss     | 0.473    |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.000922 |
|    n_updates       | 954425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 38       |
|    time_elapsed    | 20205    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2319.88 +/- 1.70
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -22.7    |
|    critic_loss     | 0.249    |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.000921 |
|    n_updates       | 964425   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2171.82 +/- 50.16
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -25.5    |
|    critic_loss     | 0.198    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -5.33    |
|    learning_rate   | 0.00092  |
|    n_updates       | 974425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 38       |
|    time_elapsed    | 20712    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=2227.23 +/- 2.44
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -25.9    |
|    critic_loss     | 0.0998   |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | -4.87    |
|    learning_rate   | 0.000919 |
|    n_updates       | 984425   |
---------------------------------
Eval num_timesteps=820000, episode_reward=2270.96 +/- 10.39
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 2.26     |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -6.64    |
|    learning_rate   | 0.000918 |
|    n_updates       | 994425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 38       |
|    time_elapsed    | 21219    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2378.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.389    |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | -2.59    |
|    learning_rate   | 0.000917 |
|    n_updates       | 1004425  |
---------------------------------
Eval num_timesteps=840000, episode_reward=1955.35 +/- 9.40
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -25.6    |
|    critic_loss     | 0.575    |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.000916 |
|    n_updates       | 1014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 38       |
|    time_elapsed    | 21729    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2157.64 +/- 17.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -25.1    |
|    critic_loss     | 0.204    |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | -4.82    |
|    learning_rate   | 0.000915 |
|    n_updates       | 1024425  |
---------------------------------
Eval num_timesteps=860000, episode_reward=2420.11 +/- 5.10
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -26.3    |
|    critic_loss     | 0.604    |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.000914 |
|    n_updates       | 1034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 38       |
|    time_elapsed    | 22237    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2404.87 +/- 17.23
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -25.8    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.00229  |
|    ent_coef_loss   | 5.59     |
|    learning_rate   | 0.000913 |
|    n_updates       | 1044425  |
---------------------------------
Eval num_timesteps=880000, episode_reward=1590.11 +/- 34.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -26.4    |
|    critic_loss     | 0.235    |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | 0.974    |
|    learning_rate   | 0.000912 |
|    n_updates       | 1054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 176      |
|    fps             | 38       |
|    time_elapsed    | 22745    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2212.15 +/- 31.43
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -26.2    |
|    critic_loss     | 0.409    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | 3.46     |
|    learning_rate   | 0.000911 |
|    n_updates       | 1064425  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2241.41 +/- 24.25
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -25.7    |
|    critic_loss     | 0.44     |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | 3        |
|    learning_rate   | 0.00091  |
|    n_updates       | 1074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 38       |
|    time_elapsed    | 23252    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=1774.47 +/- 31.21
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 0.135    |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | 4.76     |
|    learning_rate   | 0.000909 |
|    n_updates       | 1084425  |
---------------------------------
Eval num_timesteps=920000, episode_reward=1569.12 +/- 44.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.595    |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | 9.37     |
|    learning_rate   | 0.000908 |
|    n_updates       | 1094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 38       |
|    time_elapsed    | 23757    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2104.40 +/- 23.38
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -24.8    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -4.37    |
|    learning_rate   | 0.000907 |
|    n_updates       | 1104425  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2203.11 +/- 5.55
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -25.7    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.0025   |
|    ent_coef_loss   | 0.337    |
|    learning_rate   | 0.000906 |
|    n_updates       | 1114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 38       |
|    time_elapsed    | 24258    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2086.49 +/- 9.49
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -24.7    |
|    critic_loss     | 0.703    |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.000905 |
|    n_updates       | 1124425  |
---------------------------------
Eval num_timesteps=960000, episode_reward=2133.53 +/- 3.84
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -25.4    |
|    critic_loss     | 0.223    |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.000904 |
|    n_updates       | 1134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 38       |
|    time_elapsed    | 24764    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=907.95 +/- 40.60
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -26      |
|    critic_loss     | 0.203    |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000903 |
|    n_updates       | 1144425  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2211.60 +/- 64.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -26.5    |
|    critic_loss     | 0.505    |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | 2.15     |
|    learning_rate   | 0.000902 |
|    n_updates       | 1154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 38       |
|    time_elapsed    | 25272    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=1889.01 +/- 27.18
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.213    |
|    ent_coef        | 0.00296  |
|    ent_coef_loss   | -2.23    |
|    learning_rate   | 0.000901 |
|    n_updates       | 1164425  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=1789.29 +/- 23.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 0.0893   |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | -0.97    |
|    learning_rate   | 0.0009   |
|    n_updates       | 1174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 38       |
|    time_elapsed    | 25778    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=2504.49 +/- 1.70
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -27      |
|    critic_loss     | 0.264    |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 3.61     |
|    learning_rate   | 0.000899 |
|    n_updates       | 1184425  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=2516.49 +/- 11.58
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -26.3    |
|    critic_loss     | 0.23     |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -0.0475  |
|    learning_rate   | 0.000898 |
|    n_updates       | 1194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 38       |
|    time_elapsed    | 26285    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=1907.38 +/- 9.96
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 0.409    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | 7.85     |
|    learning_rate   | 0.000897 |
|    n_updates       | 1204425  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=1781.35 +/- 4.91
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -26.8    |
|    critic_loss     | 0.307    |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | -0.628   |
|    learning_rate   | 0.000896 |
|    n_updates       | 1214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 38       |
|    time_elapsed    | 26796    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=1841.06 +/- 11.88
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 0.801    |
|    learning_rate   | 0.000895 |
|    n_updates       | 1224425  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=1787.20 +/- 24.19
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 0.409    |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | -1.37    |
|    learning_rate   | 0.000894 |
|    n_updates       | 1234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 38       |
|    time_elapsed    | 27304    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2319.01 +/- 23.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -26.7    |
|    critic_loss     | 0.349    |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | -4.29    |
|    learning_rate   | 0.000893 |
|    n_updates       | 1244425  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=1599.55 +/- 429.54
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -26.5    |
|    critic_loss     | 0.397    |
|    ent_coef        | 0.00269  |
|    ent_coef_loss   | -0.822   |
|    learning_rate   | 0.000892 |
|    n_updates       | 1254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 38       |
|    time_elapsed    | 27811    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2336.39 +/- 1.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -25.9    |
|    critic_loss     | 0.326    |
|    ent_coef        | 0.00271  |
|    ent_coef_loss   | 0.962    |
|    learning_rate   | 0.000891 |
|    n_updates       | 1264425  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=1838.35 +/- 61.69
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 0.775    |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -7.26    |
|    learning_rate   | 0.00089  |
|    n_updates       | 1274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 38       |
|    time_elapsed    | 28319    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=1863.24 +/- 34.06
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -24.4    |
|    critic_loss     | 0.727    |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | 6.74     |
|    learning_rate   | 0.000889 |
|    n_updates       | 1284425  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=1625.50 +/- 84.80
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -25.7    |
|    critic_loss     | 2.69     |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.000888 |
|    n_updates       | 1294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 38       |
|    time_elapsed    | 28826    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=1581.59 +/- 0.28
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -26.1    |
|    critic_loss     | 0.465    |
|    ent_coef        | 0.00305  |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.000887 |
|    n_updates       | 1304425  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=1615.05 +/- 47.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -26.2    |
|    critic_loss     | 0.205    |
|    ent_coef        | 0.00297  |
|    ent_coef_loss   | 4.2      |
|    learning_rate   | 0.000886 |
|    n_updates       | 1314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.97e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 38       |
|    time_elapsed    | 29335    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=1644.24 +/- 49.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -25.7    |
|    critic_loss     | 0.299    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 5.09     |
|    learning_rate   | 0.000885 |
|    n_updates       | 1324425  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=1770.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -26      |
|    critic_loss     | 1.88     |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | 1.91     |
|    learning_rate   | 0.000884 |
|    n_updates       | 1334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 38       |
|    time_elapsed    | 29845    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=1120.05 +/- 513.76
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -26.6    |
|    critic_loss     | 0.273    |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | 2.75     |
|    learning_rate   | 0.000883 |
|    n_updates       | 1344425  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=1532.47 +/- 1.55
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -26      |
|    critic_loss     | 0.348    |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | -4.5     |
|    learning_rate   | 0.000882 |
|    n_updates       | 1354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 38       |
|    time_elapsed    | 30353    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=1579.31 +/- 2.47
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -25.9    |
|    critic_loss     | 0.573    |
|    ent_coef        | 0.00256  |
|    ent_coef_loss   | 0.797    |
|    learning_rate   | 0.000881 |
|    n_updates       | 1364425  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=1576.99 +/- 8.49
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.244    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -0.215   |
|    learning_rate   | 0.00088  |
|    n_updates       | 1374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 38       |
|    time_elapsed    | 30863    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=787.94 +/- 41.51
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -27      |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 0.341    |
|    learning_rate   | 0.000879 |
|    n_updates       | 1384425  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=1569.79 +/- 0.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.522    |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -3.71    |
|    learning_rate   | 0.000878 |
|    n_updates       | 1394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 38       |
|    time_elapsed    | 31371    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=1662.79 +/- 1144.27
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.544    |
|    ent_coef        | 0.00262  |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.000877 |
|    n_updates       | 1404425  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=1579.95 +/- 22.39
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.439    |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | -5.45    |
|    learning_rate   | 0.000876 |
|    n_updates       | 1414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 38       |
|    time_elapsed    | 31876    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=1486.29 +/- 23.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.498    |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.000875 |
|    n_updates       | 1424425  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=1557.69 +/- 20.14
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.215    |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | 3.4      |
|    learning_rate   | 0.000874 |
|    n_updates       | 1434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 38       |
|    time_elapsed    | 32466    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=1576.35 +/- 0.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | -2.2     |
|    learning_rate   | 0.000873 |
|    n_updates       | 1444425  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=1545.58 +/- 18.06
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00266  |
|    ent_coef_loss   | -0.837   |
|    learning_rate   | 0.000872 |
|    n_updates       | 1454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.91e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 38       |
|    time_elapsed    | 33063    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=1587.32 +/- 2.54
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.527    |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | 1.66     |
|    learning_rate   | 0.000871 |
|    n_updates       | 1464425  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=1758.09 +/- 9.91
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -30      |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.00087  |
|    n_updates       | 1474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 38       |
|    time_elapsed    | 33639    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=1977.92 +/- 54.59
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.000869 |
|    n_updates       | 1484425  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=1984.50 +/- 15.72
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 0.389    |
|    ent_coef        | 0.00351  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000868 |
|    n_updates       | 1494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 38       |
|    time_elapsed    | 34227    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2511.70 +/- 9.84
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.754    |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | 0.173    |
|    learning_rate   | 0.000867 |
|    n_updates       | 1504425  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=1737.12 +/- 971.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 0.454    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000866 |
|    n_updates       | 1514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 38       |
|    time_elapsed    | 34849    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=618.95 +/- 192.58
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 0.259    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | -2.81    |
|    learning_rate   | 0.000865 |
|    n_updates       | 1524425  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2508.88 +/- 2.65
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 0.27     |
|    ent_coef        | 0.00414  |
|    ent_coef_loss   | 0.692    |
|    learning_rate   | 0.000864 |
|    n_updates       | 1534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 38       |
|    time_elapsed    | 35438    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2101.84 +/- 463.40
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 3.33     |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | -3.62    |
|    learning_rate   | 0.000863 |
|    n_updates       | 1544425  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=1718.81 +/- 368.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.586    |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | 0.318    |
|    learning_rate   | 0.000862 |
|    n_updates       | 1554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 276      |
|    fps             | 38       |
|    time_elapsed    | 36017    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2426.40 +/- 3.88
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.00392  |
|    ent_coef_loss   | 0.215    |
|    learning_rate   | 0.000861 |
|    n_updates       | 1564425  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=1629.67 +/- 77.77
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 0.952    |
|    ent_coef        | 0.00361  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.00086  |
|    n_updates       | 1574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 38       |
|    time_elapsed    | 36617    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=716.55 +/- 51.76
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.238    |
|    ent_coef        | 0.00367  |
|    ent_coef_loss   | 0.89     |
|    learning_rate   | 0.000859 |
|    n_updates       | 1584425  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=1575.83 +/- 11.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.902    |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000858 |
|    n_updates       | 1594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 38       |
|    time_elapsed    | 37230    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=1872.56 +/- 246.34
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 0.65     |
|    ent_coef        | 0.00385  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000857 |
|    n_updates       | 1604425  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=1824.95 +/- 49.64
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.315    |
|    ent_coef        | 0.00365  |
|    ent_coef_loss   | -0.954   |
|    learning_rate   | 0.000856 |
|    n_updates       | 1614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 38       |
|    time_elapsed    | 37849    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2006.51 +/- 0.63
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.346    |
|    ent_coef        | 0.00366  |
|    ent_coef_loss   | -0.813   |
|    learning_rate   | 0.000855 |
|    n_updates       | 1624425  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2280.53 +/- 645.35
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 1.83     |
|    ent_coef        | 0.00423  |
|    ent_coef_loss   | -2.94    |
|    learning_rate   | 0.000854 |
|    n_updates       | 1634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 37       |
|    time_elapsed    | 38426    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=1925.39 +/- 40.60
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.00459  |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000853 |
|    n_updates       | 1644425  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2121.14 +/- 20.12
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 1        |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 3.28     |
|    learning_rate   | 0.000852 |
|    n_updates       | 1654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 37       |
|    time_elapsed    | 39032    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2076.14 +/- 14.47
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.326    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | 4.15     |
|    learning_rate   | 0.000851 |
|    n_updates       | 1664425  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=1646.41 +/- 669.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.888    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.00085  |
|    n_updates       | 1674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 37       |
|    time_elapsed    | 39645    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=1905.39 +/- 26.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.963    |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | 1.67     |
|    learning_rate   | 0.000849 |
|    n_updates       | 1684425  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=1717.50 +/- 161.86
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 0.456    |
|    ent_coef        | 0.00502  |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 0.000848 |
|    n_updates       | 1694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 304      |
|    fps             | 37       |
|    time_elapsed    | 40227    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=797.70 +/- 59.16
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.307    |
|    ent_coef        | 0.00444  |
|    ent_coef_loss   | 5.82     |
|    learning_rate   | 0.000847 |
|    n_updates       | 1704425  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=1263.04 +/- 444.31
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.283    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000846 |
|    n_updates       | 1714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 37       |
|    time_elapsed    | 40798    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=730.69 +/- 16.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.369    |
|    ent_coef        | 0.00398  |
|    ent_coef_loss   | -2.12    |
|    learning_rate   | 0.000845 |
|    n_updates       | 1724425  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=741.54 +/- 10.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 0.389    |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | 6.24     |
|    learning_rate   | 0.000844 |
|    n_updates       | 1734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.75e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 37       |
|    time_elapsed    | 41367    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2198.52 +/- 171.33
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | 0.0706   |
|    learning_rate   | 0.000843 |
|    n_updates       | 1744425  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2669.90 +/- 602.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.0039   |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.000842 |
|    n_updates       | 1754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 37       |
|    time_elapsed    | 41941    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=2181.40 +/- 265.78
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 0.431    |
|    ent_coef        | 0.00378  |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.000841 |
|    n_updates       | 1764425  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=2009.05 +/- 981.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.314    |
|    ent_coef        | 0.00391  |
|    ent_coef_loss   | -1.64    |
|    learning_rate   | 0.00084  |
|    n_updates       | 1774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 37       |
|    time_elapsed    | 42516    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=1597.50 +/- 89.27
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.667    |
|    ent_coef        | 0.00351  |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.000839 |
|    n_updates       | 1784425  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=1887.70 +/- 555.25
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 0.317    |
|    ent_coef        | 0.00354  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000838 |
|    n_updates       | 1794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 37       |
|    time_elapsed    | 43089    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2344.02 +/- 398.05
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.377    |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | -0.325   |
|    learning_rate   | 0.000837 |
|    n_updates       | 1804425  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=1789.47 +/- 73.85
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 0.796    |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | -4.34    |
|    learning_rate   | 0.000836 |
|    n_updates       | 1814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 37       |
|    time_elapsed    | 43670    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=1581.74 +/- 59.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.000835 |
|    n_updates       | 1824425  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=1724.77 +/- 48.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.582    |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | -3.28    |
|    learning_rate   | 0.000834 |
|    n_updates       | 1834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 37       |
|    time_elapsed    | 44249    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2599.44 +/- 820.95
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 0.634    |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | -2.73    |
|    learning_rate   | 0.000833 |
|    n_updates       | 1844425  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2377.66 +/- 920.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 0.449    |
|    ent_coef        | 0.00313  |
|    ent_coef_loss   | 2.5      |
|    learning_rate   | 0.000832 |
|    n_updates       | 1854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 37       |
|    time_elapsed    | 44827    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=968.50 +/- 704.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -31.2    |
|    critic_loss     | 1.71     |
|    ent_coef        | 0.00325  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000831 |
|    n_updates       | 1864425  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2725.71 +/- 257.87
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.433    |
|    ent_coef        | 0.00323  |
|    ent_coef_loss   | -0.611   |
|    learning_rate   | 0.00083  |
|    n_updates       | 1874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 37       |
|    time_elapsed    | 45420    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2849.35 +/- 111.03
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.646    |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -0.6     |
|    learning_rate   | 0.000829 |
|    n_updates       | 1884425  |
---------------------------------
New best mean reward!
Eval num_timesteps=1720000, episode_reward=2661.73 +/- 336.24
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00309  |
|    ent_coef_loss   | 0.432    |
|    learning_rate   | 0.000828 |
|    n_updates       | 1894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 37       |
|    time_elapsed    | 46056    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=1590.59 +/- 51.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -33.6    |
|    critic_loss     | 0.684    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | 4.71     |
|    learning_rate   | 0.000827 |
|    n_updates       | 1904425  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=1565.51 +/- 38.68
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | 0.535    |
|    learning_rate   | 0.000826 |
|    n_updates       | 1914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 37       |
|    time_elapsed    | 46684    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=1982.06 +/- 341.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.908    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -0.844   |
|    learning_rate   | 0.000825 |
|    n_updates       | 1924425  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=1619.35 +/- 29.48
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.718    |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | -0.267   |
|    learning_rate   | 0.000824 |
|    n_updates       | 1934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 37       |
|    time_elapsed    | 47276    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2612.55 +/- 417.34
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 0.545    |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000823 |
|    n_updates       | 1944425  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2334.15 +/- 23.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 1        |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 0.839    |
|    learning_rate   | 0.000822 |
|    n_updates       | 1954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 37       |
|    time_elapsed    | 47865    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2375.22 +/- 114.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | 3.57     |
|    learning_rate   | 0.000821 |
|    n_updates       | 1964425  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=1790.42 +/- 303.25
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 0.682    |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | 4.32     |
|    learning_rate   | 0.00082  |
|    n_updates       | 1974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 37       |
|    time_elapsed    | 48452    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=1868.65 +/- 45.47
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.628    |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | 0.857    |
|    learning_rate   | 0.000819 |
|    n_updates       | 1984425  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2445.36 +/- 417.55
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.938    |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 0.288    |
|    learning_rate   | 0.000818 |
|    n_updates       | 1994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 37       |
|    time_elapsed    | 49041    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=2358.82 +/- 669.36
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.575    |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | -0.887   |
|    learning_rate   | 0.000817 |
|    n_updates       | 2004425  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2666.72 +/- 545.94
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.245    |
|    ent_coef        | 0.00367  |
|    ent_coef_loss   | 2.45     |
|    learning_rate   | 0.000816 |
|    n_updates       | 2014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 37       |
|    time_elapsed    | 49627    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2434.23 +/- 730.18
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 0.322    |
|    ent_coef        | 0.00365  |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.000815 |
|    n_updates       | 2024425  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=1613.57 +/- 1154.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 0.47     |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.000814 |
|    n_updates       | 2034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 37       |
|    time_elapsed    | 50211    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2127.09 +/- 722.15
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 0.716    |
|    ent_coef        | 0.00376  |
|    ent_coef_loss   | 1.4      |
|    learning_rate   | 0.000813 |
|    n_updates       | 2044425  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=1792.75 +/- 108.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 0.606    |
|    ent_coef        | 0.00391  |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.000812 |
|    n_updates       | 2054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 37       |
|    time_elapsed    | 50801    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=1567.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -33.4    |
|    critic_loss     | 0.738    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | -0.426   |
|    learning_rate   | 0.000811 |
|    n_updates       | 2064425  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=1186.63 +/- 187.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -34.2    |
|    critic_loss     | 0.657    |
|    ent_coef        | 0.00398  |
|    ent_coef_loss   | 5.18     |
|    learning_rate   | 0.00081  |
|    n_updates       | 2074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 36       |
|    time_elapsed    | 51391    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=1648.76 +/- 89.84
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -32.8    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000809 |
|    n_updates       | 2084425  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=1636.31 +/- 120.73
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | 5.25     |
|    learning_rate   | 0.000808 |
|    n_updates       | 2094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 36       |
|    time_elapsed    | 51980    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=1714.96 +/- 112.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 0.751    |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000807 |
|    n_updates       | 2104425  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2881.66 +/- 657.09
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -32.3    |
|    critic_loss     | 0.477    |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.000806 |
|    n_updates       | 2114425  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 36       |
|    time_elapsed    | 52568    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=1950.39 +/- 184.77
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.95e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -33.7    |
|    critic_loss     | 0.862    |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | -2.62    |
|    learning_rate   | 0.000805 |
|    n_updates       | 2124425  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=1663.49 +/- 126.56
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 0.615    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.000804 |
|    n_updates       | 2134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 36       |
|    time_elapsed    | 53153    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=1601.14 +/- 33.80
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 2.05     |
|    ent_coef        | 0.00391  |
|    ent_coef_loss   | 0.967    |
|    learning_rate   | 0.000803 |
|    n_updates       | 2144425  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2115.70 +/- 687.51
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -34      |
|    critic_loss     | 0.537    |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | 3.94     |
|    learning_rate   | 0.000802 |
|    n_updates       | 2154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 36       |
|    time_elapsed    | 53744    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=779.19 +/- 67.89
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 3.05     |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 0.6      |
|    learning_rate   | 0.000801 |
|    n_updates       | 2164425  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=1653.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -32.7    |
|    critic_loss     | 2.8      |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 2.5      |
|    learning_rate   | 0.0008   |
|    n_updates       | 2174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 36       |
|    time_elapsed    | 54322    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2023.29 +/- 33.70
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 0.614    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 0.000799 |
|    n_updates       | 2184425  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=2431.11 +/- 613.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -33      |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000798 |
|    n_updates       | 2194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 36       |
|    time_elapsed    | 54896    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2326.97 +/- 301.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.334    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | -4.53    |
|    learning_rate   | 0.000797 |
|    n_updates       | 2204425  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=2608.50 +/- 518.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -33.1    |
|    critic_loss     | 0.324    |
|    ent_coef        | 0.00395  |
|    ent_coef_loss   | -3.21    |
|    learning_rate   | 0.000796 |
|    n_updates       | 2214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 408      |
|    fps             | 36       |
|    time_elapsed    | 55472    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=1078.19 +/- 470.49
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | 4.29     |
|    learning_rate   | 0.000795 |
|    n_updates       | 2224425  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=1799.30 +/- 226.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -33.5    |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.00452  |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.000794 |
|    n_updates       | 2234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 36       |
|    time_elapsed    | 56056    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2157.47 +/- 723.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.718    |
|    ent_coef        | 0.00473  |
|    ent_coef_loss   | 0.777    |
|    learning_rate   | 0.000793 |
|    n_updates       | 2244425  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=733.13 +/- 59.49
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 0.483    |
|    ent_coef        | 0.00471  |
|    ent_coef_loss   | 3.63     |
|    learning_rate   | 0.000792 |
|    n_updates       | 2254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 36       |
|    time_elapsed    | 56644    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=1826.23 +/- 499.38
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.934    |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | 2.05     |
|    learning_rate   | 0.000791 |
|    n_updates       | 2264425  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=2338.09 +/- 85.91
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -32.5    |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.0048   |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.00079  |
|    n_updates       | 2274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 36       |
|    time_elapsed    | 57229    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2193.80 +/- 665.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 0.729    |
|    ent_coef        | 0.00474  |
|    ent_coef_loss   | -0.32    |
|    learning_rate   | 0.000789 |
|    n_updates       | 2284425  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=1749.03 +/- 224.75
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -34.6    |
|    critic_loss     | 0.747    |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | 3.78     |
|    learning_rate   | 0.000788 |
|    n_updates       | 2294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 36       |
|    time_elapsed    | 57818    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2680.29 +/- 563.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -32.1    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.00571  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000787 |
|    n_updates       | 2304425  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=1356.30 +/- 691.31
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -31.5    |
|    critic_loss     | 0.7      |
|    ent_coef        | 0.00618  |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.000786 |
|    n_updates       | 2314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 428      |
|    fps             | 36       |
|    time_elapsed    | 58411    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=1807.65 +/- 496.71
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00611  |
|    ent_coef_loss   | 2.05     |
|    learning_rate   | 0.000785 |
|    n_updates       | 2324425  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=1915.88 +/- 584.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 0.805    |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.000784 |
|    n_updates       | 2334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 36       |
|    time_elapsed    | 59007    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=2178.17 +/- 351.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00481  |
|    ent_coef_loss   | -0.897   |
|    learning_rate   | 0.000783 |
|    n_updates       | 2344425  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=1631.03 +/- 41.56
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00501  |
|    ent_coef_loss   | 3.21     |
|    learning_rate   | 0.000782 |
|    n_updates       | 2354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 36       |
|    time_elapsed    | 59594    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=1804.83 +/- 64.61
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 0.845    |
|    ent_coef        | 0.00499  |
|    ent_coef_loss   | 1.42     |
|    learning_rate   | 0.000781 |
|    n_updates       | 2364425  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=1822.46 +/- 142.19
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -29.1    |
|    critic_loss     | 1.45     |
|    ent_coef        | 0.00507  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.00078  |
|    n_updates       | 2374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.87e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 36       |
|    time_elapsed    | 60190    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=2472.70 +/- 461.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.515    |
|    ent_coef        | 0.00512  |
|    ent_coef_loss   | -2.45    |
|    learning_rate   | 0.000779 |
|    n_updates       | 2384425  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=2125.96 +/- 517.64
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 2.97     |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 7.02     |
|    learning_rate   | 0.000778 |
|    n_updates       | 2394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 36       |
|    time_elapsed    | 60786    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2125.13 +/- 699.62
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 5.92     |
|    ent_coef        | 0.00505  |
|    ent_coef_loss   | -3.51    |
|    learning_rate   | 0.000777 |
|    n_updates       | 2404425  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2320.57 +/- 790.85
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 0.863    |
|    ent_coef        | 0.00496  |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000776 |
|    n_updates       | 2414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.83e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 36       |
|    time_elapsed    | 61374    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2017.29 +/- 1076.47
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 0.514    |
|    ent_coef        | 0.00482  |
|    ent_coef_loss   | -0.519   |
|    learning_rate   | 0.000775 |
|    n_updates       | 2424425  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2050.01 +/- 620.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 0.53     |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.000774 |
|    n_updates       | 2434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 36       |
|    time_elapsed    | 61975    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2646.69 +/- 553.34
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 1.74     |
|    ent_coef        | 0.00498  |
|    ent_coef_loss   | -2.59    |
|    learning_rate   | 0.000773 |
|    n_updates       | 2444425  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=1556.52 +/- 1116.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.644    |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | -0.461   |
|    learning_rate   | 0.000772 |
|    n_updates       | 2454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 36       |
|    time_elapsed    | 62569    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2037.64 +/- 916.65
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -30.7    |
|    critic_loss     | 0.424    |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.000771 |
|    n_updates       | 2464425  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=1572.72 +/- 50.07
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.426    |
|    ent_coef        | 0.00518  |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.00077  |
|    n_updates       | 2474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 36       |
|    time_elapsed    | 63174    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=1361.53 +/- 15.96
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 0.883    |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | 3.26     |
|    learning_rate   | 0.000769 |
|    n_updates       | 2484425  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2059.57 +/- 586.73
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 0.767    |
|    ent_coef        | 0.00488  |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000768 |
|    n_updates       | 2494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 36       |
|    time_elapsed    | 63769    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=1203.01 +/- 832.39
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 0.44     |
|    ent_coef        | 0.00521  |
|    ent_coef_loss   | 0.734    |
|    learning_rate   | 0.000767 |
|    n_updates       | 2504425  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=1586.46 +/- 23.82
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -33.2    |
|    critic_loss     | 0.877    |
|    ent_coef        | 0.00491  |
|    ent_coef_loss   | -2.39    |
|    learning_rate   | 0.000766 |
|    n_updates       | 2514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.89e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 36       |
|    time_elapsed    | 64360    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=722.53 +/- 15.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -31.7    |
|    critic_loss     | 0.731    |
|    ent_coef        | 0.00503  |
|    ent_coef_loss   | 0.222    |
|    learning_rate   | 0.000765 |
|    n_updates       | 2524425  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=1573.92 +/- 19.29
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -31.8    |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.00503  |
|    ent_coef_loss   | -0.776   |
|    learning_rate   | 0.000764 |
|    n_updates       | 2534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 36       |
|    time_elapsed    | 64950    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=711.80 +/- 13.44
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 0.873    |
|    ent_coef        | 0.00463  |
|    ent_coef_loss   | -1.1     |
|    learning_rate   | 0.000763 |
|    n_updates       | 2544425  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=-94.56 +/- 25.56
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -94.6    |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 0.392    |
|    ent_coef        | 0.00459  |
|    ent_coef_loss   | -5.32    |
|    learning_rate   | 0.000762 |
|    n_updates       | 2554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 36       |
|    time_elapsed    | 65545    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=1552.43 +/- 27.43
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00459  |
|    ent_coef_loss   | 5.1      |
|    learning_rate   | 0.000761 |
|    n_updates       | 2564425  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=1593.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 2.48     |
|    ent_coef        | 0.00448  |
|    ent_coef_loss   | -2.66    |
|    learning_rate   | 0.00076  |
|    n_updates       | 2574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.85e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 36       |
|    time_elapsed    | 66136    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=1234.22 +/- 397.33
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 1.34     |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | 0.962    |
|    learning_rate   | 0.000759 |
|    n_updates       | 2584425  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=1675.21 +/- 87.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00454  |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000758 |
|    n_updates       | 2594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.81e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 36       |
|    time_elapsed    | 66730    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2061.41 +/- 606.34
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 2.39     |
|    ent_coef        | 0.00447  |
|    ent_coef_loss   | 0.86     |
|    learning_rate   | 0.000757 |
|    n_updates       | 2604425  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=1795.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -28.9    |
|    critic_loss     | 0.652    |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | -4.54    |
|    learning_rate   | 0.000756 |
|    n_updates       | 2614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 36       |
|    time_elapsed    | 67325    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=1617.43 +/- 17.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -30      |
|    critic_loss     | 0.709    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | 3.51     |
|    learning_rate   | 0.000755 |
|    n_updates       | 2624425  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=1572.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -31.3    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | -2.92    |
|    learning_rate   | 0.000754 |
|    n_updates       | 2634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 36       |
|    time_elapsed    | 67917    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=1676.19 +/- 70.28
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.652    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | -0.597   |
|    learning_rate   | 0.000753 |
|    n_updates       | 2644425  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=1809.53 +/- 138.09
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -31.1    |
|    critic_loss     | 0.553    |
|    ent_coef        | 0.00423  |
|    ent_coef_loss   | 4.73     |
|    learning_rate   | 0.000752 |
|    n_updates       | 2654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 36       |
|    time_elapsed    | 68512    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=1596.14 +/- 78.18
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 1.85     |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | -1.54    |
|    learning_rate   | 0.000751 |
|    n_updates       | 2664425  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2249.36 +/- 585.92
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -32.6    |
|    critic_loss     | 0.923    |
|    ent_coef        | 0.00412  |
|    ent_coef_loss   | -0.89    |
|    learning_rate   | 0.00075  |
|    n_updates       | 2674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.79e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 36       |
|    time_elapsed    | 69107    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=880.46 +/- 326.26
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 6.35     |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | 0.334    |
|    learning_rate   | 0.000749 |
|    n_updates       | 2684425  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=694.45 +/- 27.02
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 0.841    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | 8.32     |
|    learning_rate   | 0.000748 |
|    n_updates       | 2694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.77e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 36       |
|    time_elapsed    | 69698    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=695.38 +/- 11.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 0.563    |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | 4.43     |
|    learning_rate   | 0.000747 |
|    n_updates       | 2704425  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=1528.96 +/- 17.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 0.781    |
|    ent_coef        | 0.0046   |
|    ent_coef_loss   | 8.48     |
|    learning_rate   | 0.000746 |
|    n_updates       | 2714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 36       |
|    time_elapsed    | 70290    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=1419.06 +/- 255.19
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 3.62     |
|    ent_coef        | 0.00468  |
|    ent_coef_loss   | -2.7     |
|    learning_rate   | 0.000745 |
|    n_updates       | 2724425  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=1232.42 +/- 248.44
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00453  |
|    ent_coef_loss   | -0.454   |
|    learning_rate   | 0.000744 |
|    n_updates       | 2734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.73e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 36       |
|    time_elapsed    | 70882    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=1974.31 +/- 1028.93
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -31.9    |
|    critic_loss     | 2.61     |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | -2.51    |
|    learning_rate   | 0.000743 |
|    n_updates       | 2744425  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=781.54 +/- 44.12
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -33.3    |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00557  |
|    ent_coef_loss   | -4.02    |
|    learning_rate   | 0.000742 |
|    n_updates       | 2754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.69e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 36       |
|    time_elapsed    | 71469    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=748.31 +/- 43.27
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 3.48     |
|    ent_coef        | 0.00492  |
|    ent_coef_loss   | 1.4      |
|    learning_rate   | 0.000741 |
|    n_updates       | 2764425  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=1108.36 +/- 248.41
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 4.27     |
|    ent_coef        | 0.00564  |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.00074  |
|    n_updates       | 2774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.67e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 36       |
|    time_elapsed    | 72055    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=706.81 +/- 15.42
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -33.8    |
|    critic_loss     | 9.95     |
|    ent_coef        | 0.00589  |
|    ent_coef_loss   | -7.32    |
|    learning_rate   | 0.000739 |
|    n_updates       | 2784425  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=1537.15 +/- 2.16
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -35.6    |
|    critic_loss     | 1.46     |
|    ent_coef        | 0.00661  |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000738 |
|    n_updates       | 2794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 36       |
|    time_elapsed    | 72638    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=668.67 +/- 62.28
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -34.8    |
|    critic_loss     | 2.36     |
|    ent_coef        | 0.00657  |
|    ent_coef_loss   | -3.79    |
|    learning_rate   | 0.000737 |
|    n_updates       | 2804425  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=1045.23 +/- 495.80
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.00678  |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.000736 |
|    n_updates       | 2814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 36       |
|    time_elapsed    | 73222    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=771.12 +/- 58.94
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -30.9    |
|    critic_loss     | 0.82     |
|    ent_coef        | 0.00659  |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.000735 |
|    n_updates       | 2824425  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=678.00 +/- 58.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -30.8    |
|    critic_loss     | 1.75     |
|    ent_coef        | 0.00631  |
|    ent_coef_loss   | -3.35    |
|    learning_rate   | 0.000734 |
|    n_updates       | 2834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 36       |
|    time_elapsed    | 73813    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=1017.69 +/- 460.68
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -32.4    |
|    critic_loss     | 1.58     |
|    ent_coef        | 0.00581  |
|    ent_coef_loss   | 0.187    |
|    learning_rate   | 0.000733 |
|    n_updates       | 2844425  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=689.14 +/- 18.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 1.16     |
|    ent_coef        | 0.00594  |
|    ent_coef_loss   | 5.16     |
|    learning_rate   | 0.000732 |
|    n_updates       | 2854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 36       |
|    time_elapsed    | 74402    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=1865.63 +/- 663.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.658    |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | -0.647   |
|    learning_rate   | 0.000731 |
|    n_updates       | 2864425  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2173.13 +/- 1207.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.00541  |
|    ent_coef_loss   | 2.67     |
|    learning_rate   | 0.00073  |
|    n_updates       | 2874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 540      |
|    fps             | 36       |
|    time_elapsed    | 74989    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=1221.20 +/- 381.84
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -29.6    |
|    critic_loss     | 0.963    |
|    ent_coef        | 0.00504  |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.000729 |
|    n_updates       | 2884425  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=1627.52 +/- 113.80
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 1.13     |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | 0.918    |
|    learning_rate   | 0.000728 |
|    n_updates       | 2894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 35       |
|    time_elapsed    | 75584    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=802.23 +/- 423.48
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -31      |
|    critic_loss     | 0.352    |
|    ent_coef        | 0.00426  |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.000727 |
|    n_updates       | 2904425  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=1649.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -30.2    |
|    critic_loss     | 0.498    |
|    ent_coef        | 0.00472  |
|    ent_coef_loss   | -1.98    |
|    learning_rate   | 0.000726 |
|    n_updates       | 2914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 35       |
|    time_elapsed    | 76176    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=667.62 +/- 38.29
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -29.7    |
|    critic_loss     | 2.27     |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | 3.54     |
|    learning_rate   | 0.000725 |
|    n_updates       | 2924425  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=1621.60 +/- 71.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -32      |
|    critic_loss     | 1.21     |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | 1.82     |
|    learning_rate   | 0.000724 |
|    n_updates       | 2934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 552      |
|    fps             | 35       |
|    time_elapsed    | 76770    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=1672.30 +/- 173.08
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -30.1    |
|    critic_loss     | 0.659    |
|    ent_coef        | 0.00608  |
|    ent_coef_loss   | -5.38    |
|    learning_rate   | 0.000723 |
|    n_updates       | 2944425  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=1572.91 +/- 45.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -32.9    |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.00635  |
|    ent_coef_loss   | -7.79    |
|    learning_rate   | 0.000722 |
|    n_updates       | 2954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 556      |
|    fps             | 35       |
|    time_elapsed    | 77362    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=1645.01 +/- 136.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.855    |
|    ent_coef        | 0.00671  |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.000721 |
|    n_updates       | 2964425  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=743.00 +/- 79.11
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 2.05     |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.00072  |
|    n_updates       | 2974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 35       |
|    time_elapsed    | 77960    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=772.36 +/- 61.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.74     |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | -0.789   |
|    learning_rate   | 0.000719 |
|    n_updates       | 2984425  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=886.02 +/- 589.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -32.2    |
|    critic_loss     | 3.62     |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | 3.78     |
|    learning_rate   | 0.000718 |
|    n_updates       | 2994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 35       |
|    time_elapsed    | 78564    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=1233.14 +/- 367.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 0.42     |
|    ent_coef        | 0.00538  |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000717 |
|    n_updates       | 3004425  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=1551.31 +/- 21.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.578    |
|    ent_coef        | 0.00496  |
|    ent_coef_loss   | -4.73    |
|    learning_rate   | 0.000716 |
|    n_updates       | 3014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    episodes        | 568      |
|    fps             | 35       |
|    time_elapsed    | 79162    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2840.41 +/- 633.59
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -29.4    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | 3.36     |
|    learning_rate   | 0.000715 |
|    n_updates       | 3024425  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=1365.60 +/- 203.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.583    |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | 1.14     |
|    learning_rate   | 0.000714 |
|    n_updates       | 3034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    episodes        | 572      |
|    fps             | 35       |
|    time_elapsed    | 79762    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=2377.05 +/- 687.83
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | 0.444    |
|    learning_rate   | 0.000713 |
|    n_updates       | 3044425  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=2429.32 +/- 701.15
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -33.9    |
|    critic_loss     | 0.517    |
|    ent_coef        | 0.00607  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000712 |
|    n_updates       | 3054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    episodes        | 576      |
|    fps             | 35       |
|    time_elapsed    | 80357    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=1531.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -30.4    |
|    critic_loss     | 2.21     |
|    ent_coef        | 0.00577  |
|    ent_coef_loss   | 3.94     |
|    learning_rate   | 0.000711 |
|    n_updates       | 3064425  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=1738.22 +/- 102.12
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -30.6    |
|    critic_loss     | 0.431    |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.00071  |
|    n_updates       | 3074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    episodes        | 580      |
|    fps             | 35       |
|    time_elapsed    | 80948    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=2185.35 +/- 798.60
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00572  |
|    ent_coef_loss   | -6.17    |
|    learning_rate   | 0.000709 |
|    n_updates       | 3084425  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=1855.29 +/- 640.46
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -31.6    |
|    critic_loss     | 1.07     |
|    ent_coef        | 0.00588  |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000708 |
|    n_updates       | 3094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 584      |
|    fps             | 35       |
|    time_elapsed    | 81539    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=2183.84 +/- 796.09
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.00525  |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000707 |
|    n_updates       | 3104425  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=1672.13 +/- 69.27
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -30.3    |
|    critic_loss     | 1.66     |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | -4.69    |
|    learning_rate   | 0.000706 |
|    n_updates       | 3114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    episodes        | 588      |
|    fps             | 35       |
|    time_elapsed    | 82134    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=1821.95 +/- 582.22
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -30.5    |
|    critic_loss     | 0.723    |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | -0.563   |
|    learning_rate   | 0.000705 |
|    n_updates       | 3124425  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=1277.49 +/- 353.59
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 0.342    |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.000704 |
|    n_updates       | 3134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 592      |
|    fps             | 35       |
|    time_elapsed    | 82715    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=1810.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -28.2    |
|    critic_loss     | 0.852    |
|    ent_coef        | 0.00435  |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.000703 |
|    n_updates       | 3144425  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=1123.43 +/- 225.64
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.987    |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | -1.93    |
|    learning_rate   | 0.000702 |
|    n_updates       | 3154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 596      |
|    fps             | 35       |
|    time_elapsed    | 83213    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=1599.23 +/- 33.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 1.07     |
|    ent_coef        | 0.00428  |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.000701 |
|    n_updates       | 3164425  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=1605.99 +/- 28.45
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -28.6    |
|    critic_loss     | 3.55     |
|    ent_coef        | 0.00462  |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.0007   |
|    n_updates       | 3174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    episodes        | 600      |
|    fps             | 35       |
|    time_elapsed    | 83713    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=1588.68 +/- 24.26
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -29.9    |
|    critic_loss     | 0.413    |
|    ent_coef        | 0.00392  |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.000699 |
|    n_updates       | 3184425  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=1543.21 +/- 24.96
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -27.1    |
|    critic_loss     | 0.504    |
|    ent_coef        | 0.00403  |
|    ent_coef_loss   | -3.42    |
|    learning_rate   | 0.000698 |
|    n_updates       | 3194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 604      |
|    fps             | 35       |
|    time_elapsed    | 84219    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=1580.36 +/- 47.68
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 2.68     |
|    ent_coef        | 0.00396  |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000697 |
|    n_updates       | 3204425  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=1638.40 +/- 73.38
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -27.6    |
|    critic_loss     | 0.279    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | -5.69    |
|    learning_rate   | 0.000696 |
|    n_updates       | 3214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 608      |
|    fps             | 35       |
|    time_elapsed    | 84725    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=1591.71 +/- 16.16
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -28.7    |
|    critic_loss     | 0.428    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.000695 |
|    n_updates       | 3224425  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=1592.09 +/- 22.20
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.257    |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | -2.12    |
|    learning_rate   | 0.000694 |
|    n_updates       | 3234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    episodes        | 612      |
|    fps             | 35       |
|    time_elapsed    | 85230    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=1703.18 +/- 192.59
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -29.2    |
|    critic_loss     | 0.85     |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 3.76     |
|    learning_rate   | 0.000693 |
|    n_updates       | 3244425  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=1708.47 +/- 131.94
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -29.5    |
|    critic_loss     | 0.557    |
|    ent_coef        | 0.00398  |
|    ent_coef_loss   | 5.34     |
|    learning_rate   | 0.000692 |
|    n_updates       | 3254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    episodes        | 616      |
|    fps             | 35       |
|    time_elapsed    | 85737    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=1611.21 +/- 57.97
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -27.8    |
|    critic_loss     | 0.727    |
|    ent_coef        | 0.0037   |
|    ent_coef_loss   | 0.0324   |
|    learning_rate   | 0.000691 |
|    n_updates       | 3264425  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=1630.42 +/- 45.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -28.8    |
|    critic_loss     | 0.269    |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.00069  |
|    n_updates       | 3274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    episodes        | 620      |
|    fps             | 35       |
|    time_elapsed    | 86246    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=1643.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -28.2    |
|    critic_loss     | 0.435    |
|    ent_coef        | 0.00543  |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000689 |
|    n_updates       | 3284425  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=1608.36 +/- 88.57
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.535    |
|    ent_coef        | 0.00535  |
|    ent_coef_loss   | -4.68    |
|    learning_rate   | 0.000688 |
|    n_updates       | 3294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    episodes        | 624      |
|    fps             | 35       |
|    time_elapsed    | 86794    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=2350.20 +/- 667.54
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.35e+03 |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -28.2    |
|    critic_loss     | 0.259    |
|    ent_coef        | 0.00507  |
|    ent_coef_loss   | -0.676   |
|    learning_rate   | 0.000687 |
|    n_updates       | 3304425  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=1602.00 +/- 86.57
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -28.2    |
|    critic_loss     | 0.434    |
|    ent_coef        | 0.00467  |
|    ent_coef_loss   | 2.66     |
|    learning_rate   | 0.000686 |
|    n_updates       | 3314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 628      |
|    fps             | 35       |
|    time_elapsed    | 87337    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=1638.44 +/- 87.17
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -27.8    |
|    critic_loss     | 0.322    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000685 |
|    n_updates       | 3324425  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=1563.21 +/- 26.52
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -26.5    |
|    critic_loss     | 0.356    |
|    ent_coef        | 0.00461  |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.000684 |
|    n_updates       | 3334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    episodes        | 632      |
|    fps             | 35       |
|    time_elapsed    | 87853    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=1562.09 +/- 25.63
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.709    |
|    ent_coef        | 0.00403  |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.000683 |
|    n_updates       | 3344425  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=1553.68 +/- 27.30
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -27.4    |
|    critic_loss     | 0.595    |
|    ent_coef        | 0.00421  |
|    ent_coef_loss   | 1.28     |
|    learning_rate   | 0.000682 |
|    n_updates       | 3354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 636      |
|    fps             | 35       |
|    time_elapsed    | 88362    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=1633.60 +/- 119.50
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.608    |
|    ent_coef        | 0.00424  |
|    ent_coef_loss   | 0.418    |
|    learning_rate   | 0.000681 |
|    n_updates       | 3364425  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=1552.20 +/- 25.37
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 0.382    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | -3.49    |
|    learning_rate   | 0.00068  |
|    n_updates       | 3374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 640      |
|    fps             | 36       |
|    time_elapsed    | 88868    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=1540.60 +/- 18.44
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 0.169    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | -6.65    |
|    learning_rate   | 0.000679 |
|    n_updates       | 3384425  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=1544.08 +/- 24.28
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -29.8    |
|    critic_loss     | 0.286    |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.000678 |
|    n_updates       | 3394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    episodes        | 644      |
|    fps             | 36       |
|    time_elapsed    | 89374    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=1534.85 +/- 4.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -28      |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | 0.24     |
|    learning_rate   | 0.000677 |
|    n_updates       | 3404425  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=716.02 +/- 17.05
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.963    |
|    ent_coef        | 0.00412  |
|    ent_coef_loss   | 3.42     |
|    learning_rate   | 0.000676 |
|    n_updates       | 3414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    episodes        | 648      |
|    fps             | 36       |
|    time_elapsed    | 89908    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=1569.99 +/- 32.81
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -29.3    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00489  |
|    ent_coef_loss   | -3.24    |
|    learning_rate   | 0.000675 |
|    n_updates       | 3424425  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=2291.52 +/- 619.98
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -28.1    |
|    critic_loss     | 0.712    |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | 0.867    |
|    learning_rate   | 0.000674 |
|    n_updates       | 3434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 652      |
|    fps             | 36       |
|    time_elapsed    | 90480    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=1811.02 +/- 556.12
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -28.3    |
|    critic_loss     | 0.608    |
|    ent_coef        | 0.00451  |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.000673 |
|    n_updates       | 3444425  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=1542.81 +/- 21.31
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 3.16     |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | 0.945    |
|    learning_rate   | 0.000672 |
|    n_updates       | 3454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 656      |
|    fps             | 36       |
|    time_elapsed    | 91038    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=2121.15 +/- 720.53
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -28.5    |
|    critic_loss     | 0.448    |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | 0.163    |
|    learning_rate   | 0.000671 |
|    n_updates       | 3464425  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=2166.23 +/- 776.74
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -29      |
|    critic_loss     | 0.252    |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.00067  |
|    n_updates       | 3474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 660      |
|    fps             | 36       |
|    time_elapsed    | 91588    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=2400.48 +/- 703.07
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -28.4    |
|    critic_loss     | 0.275    |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | -0.712   |
|    learning_rate   | 0.000669 |
|    n_updates       | 3484425  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=1065.37 +/- 254.48
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.619    |
|    ent_coef        | 0.00393  |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000668 |
|    n_updates       | 3494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 664      |
|    fps             | 36       |
|    time_elapsed    | 92128    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=1819.39 +/- 570.36
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -27.5    |
|    critic_loss     | 0.449    |
|    ent_coef        | 0.00391  |
|    ent_coef_loss   | -0.856   |
|    learning_rate   | 0.000667 |
|    n_updates       | 3504425  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=1819.52 +/- 553.55
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -27.9    |
|    critic_loss     | 0.236    |
|    ent_coef        | 0.00453  |
|    ent_coef_loss   | -0.449   |
|    learning_rate   | 0.000666 |
|    n_updates       | 3514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 668      |
|    fps             | 36       |
|    time_elapsed    | 92632    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=1562.82 +/- 26.66
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -26.9    |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.00511  |
|    ent_coef_loss   | -0.869   |
|    learning_rate   | 0.000665 |
|    n_updates       | 3524425  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=1566.46 +/- 30.15
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -27.2    |
|    critic_loss     | 0.415    |
|    ent_coef        | 0.00433  |
|    ent_coef_loss   | -2.04    |
|    learning_rate   | 0.000664 |
|    n_updates       | 3534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    episodes        | 672      |
|    fps             | 36       |
|    time_elapsed    | 93130    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=2643.43 +/- 556.65
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -27.7    |
|    critic_loss     | 0.584    |
|    ent_coef        | 0.00389  |
|    ent_coef_loss   | -2.89    |
|    learning_rate   | 0.000663 |
|    n_updates       | 3544425  |
