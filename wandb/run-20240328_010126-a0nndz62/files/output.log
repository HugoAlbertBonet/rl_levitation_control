Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_40
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=2687.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 1.23     |
|    ent_coef        | 0.00452  |
|    ent_coef_loss   | 4.89     |
|    learning_rate   | 0.000999 |
|    n_updates       | 184425   |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=2689.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -65.8    |
|    critic_loss     | 0.437    |
|    ent_coef        | 0.00482  |
|    ent_coef_loss   | -0.512   |
|    learning_rate   | 0.000998 |
|    n_updates       | 194425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 4        |
|    fps             | 47       |
|    time_elapsed    | 424      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2526.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -67.8    |
|    critic_loss     | 0.448    |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -6.41    |
|    learning_rate   | 0.000997 |
|    n_updates       | 204425   |
---------------------------------
Eval num_timesteps=40000, episode_reward=2525.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -66.4    |
|    critic_loss     | 0.226    |
|    ent_coef        | 0.00313  |
|    ent_coef_loss   | -3.54    |
|    learning_rate   | 0.000996 |
|    n_updates       | 214425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 46       |
|    time_elapsed    | 855      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=2525.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -66      |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.00217  |
|    ent_coef_loss   | 3.63     |
|    learning_rate   | 0.000995 |
|    n_updates       | 224425   |
---------------------------------
Eval num_timesteps=60000, episode_reward=2703.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -61.8    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00228  |
|    ent_coef_loss   | 0.585    |
|    learning_rate   | 0.000994 |
|    n_updates       | 234425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 47       |
|    time_elapsed    | 1272     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=2696.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -59.5    |
|    critic_loss     | 0.0773   |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 0.708    |
|    learning_rate   | 0.000993 |
|    n_updates       | 244425   |
---------------------------------
Eval num_timesteps=80000, episode_reward=2689.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 0.0481   |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | 0.441    |
|    learning_rate   | 0.000992 |
|    n_updates       | 254425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 47       |
|    time_elapsed    | 1696     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=2691.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -59.3    |
|    critic_loss     | 0.0492   |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.000991 |
|    n_updates       | 264425   |
---------------------------------
Eval num_timesteps=100000, episode_reward=2534.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -59.6    |
|    critic_loss     | 0.0158   |
|    ent_coef        | 0.00132  |
|    ent_coef_loss   | -3.13    |
|    learning_rate   | 0.00099  |
|    n_updates       | 274425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 47       |
|    time_elapsed    | 2112     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2515.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -57.6    |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.0011   |
|    ent_coef_loss   | -3.46    |
|    learning_rate   | 0.000989 |
|    n_updates       | 284425   |
---------------------------------
Eval num_timesteps=120000, episode_reward=2514.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 0.0161   |
|    ent_coef        | 0.000731 |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000988 |
|    n_updates       | 294425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 47       |
|    time_elapsed    | 2528     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=2688.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 0.0134   |
|    ent_coef        | 0.00101  |
|    ent_coef_loss   | 0.00944  |
|    learning_rate   | 0.000987 |
|    n_updates       | 304425   |
---------------------------------
Eval num_timesteps=140000, episode_reward=2689.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.00476  |
|    ent_coef        | 0.000826 |
|    ent_coef_loss   | -7.78    |
|    learning_rate   | 0.000986 |
|    n_updates       | 314425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 28       |
|    fps             | 47       |
|    time_elapsed    | 2945     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2530.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0139   |
|    ent_coef        | 0.000776 |
|    ent_coef_loss   | 4.93     |
|    learning_rate   | 0.000985 |
|    n_updates       | 324425   |
---------------------------------
Eval num_timesteps=160000, episode_reward=2696.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0291   |
|    ent_coef        | 0.000955 |
|    ent_coef_loss   | 0.12     |
|    learning_rate   | 0.000984 |
|    n_updates       | 334425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 32       |
|    fps             | 47       |
|    time_elapsed    | 3361     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=2688.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0197   |
|    ent_coef        | 0.00083  |
|    ent_coef_loss   | 5.4      |
|    learning_rate   | 0.000983 |
|    n_updates       | 344425   |
---------------------------------
Eval num_timesteps=180000, episode_reward=2696.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0131   |
|    ent_coef        | 0.000677 |
|    ent_coef_loss   | 6.81     |
|    learning_rate   | 0.000982 |
|    n_updates       | 354425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 47       |
|    time_elapsed    | 3776     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=2688.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0181   |
|    ent_coef        | 0.000491 |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.000981 |
|    n_updates       | 364425   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2538.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00975  |
|    ent_coef        | 0.00043  |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.00098  |
|    n_updates       | 374425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 47       |
|    time_elapsed    | 4191     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2518.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0127   |
|    ent_coef        | 0.000502 |
|    ent_coef_loss   | -4.9     |
|    learning_rate   | 0.000979 |
|    n_updates       | 384425   |
---------------------------------
Eval num_timesteps=220000, episode_reward=2690.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.000607 |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.000978 |
|    n_updates       | 394425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 47       |
|    time_elapsed    | 4608     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2519.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0212   |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 0.0567   |
|    learning_rate   | 0.000977 |
|    n_updates       | 404425   |
---------------------------------
Eval num_timesteps=240000, episode_reward=2515.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.000388 |
|    ent_coef_loss   | -0.291   |
|    learning_rate   | 0.000976 |
|    n_updates       | 414425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 47       |
|    time_elapsed    | 5024     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2518.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00563  |
|    ent_coef        | 0.000376 |
|    ent_coef_loss   | -7.77    |
|    learning_rate   | 0.000975 |
|    n_updates       | 424425   |
---------------------------------
Eval num_timesteps=260000, episode_reward=2517.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0477   |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | -4.24    |
|    learning_rate   | 0.000974 |
|    n_updates       | 434425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 47       |
|    time_elapsed    | 5440     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=2519.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0107   |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | 0.193    |
|    learning_rate   | 0.000973 |
|    n_updates       | 444425   |
---------------------------------
Eval num_timesteps=280000, episode_reward=2518.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.00989  |
|    ent_coef        | 0.000298 |
|    ent_coef_loss   | 24.5     |
|    learning_rate   | 0.000972 |
|    n_updates       | 454425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 56       |
|    fps             | 47       |
|    time_elapsed    | 5855     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=2519.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.00937  |
|    ent_coef        | 0.000175 |
|    ent_coef_loss   | -7.67    |
|    learning_rate   | 0.000971 |
|    n_updates       | 464425   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2524.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0118   |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.00097  |
|    n_updates       | 474425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 60       |
|    fps             | 47       |
|    time_elapsed    | 6269     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=2514.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.00539  |
|    ent_coef        | 0.000302 |
|    ent_coef_loss   | -4       |
|    learning_rate   | 0.000969 |
|    n_updates       | 484425   |
---------------------------------
Eval num_timesteps=320000, episode_reward=2516.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0167   |
|    ent_coef        | 0.000218 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.000968 |
|    n_updates       | 494425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 47       |
|    time_elapsed    | 6684     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2524.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0327   |
|    ent_coef        | 0.000205 |
|    ent_coef_loss   | -7.83    |
|    learning_rate   | 0.000967 |
|    n_updates       | 504425   |
---------------------------------
Eval num_timesteps=340000, episode_reward=2519.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0141   |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | 14.6     |
|    learning_rate   | 0.000966 |
|    n_updates       | 514425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 47       |
|    time_elapsed    | 7097     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=2528.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0202   |
|    ent_coef        | 0.00014  |
|    ent_coef_loss   | -3.49    |
|    learning_rate   | 0.000965 |
|    n_updates       | 524425   |
---------------------------------
Eval num_timesteps=360000, episode_reward=2696.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0248   |
|    ent_coef        | 0.000202 |
|    ent_coef_loss   | 3.06     |
|    learning_rate   | 0.000964 |
|    n_updates       | 534425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 47       |
|    time_elapsed    | 7508     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=2525.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0858   |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | -5.31    |
|    learning_rate   | 0.000963 |
|    n_updates       | 544425   |
---------------------------------
Eval num_timesteps=380000, episode_reward=2521.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0192   |
|    ent_coef        | 0.00019  |
|    ent_coef_loss   | 5.69     |
|    learning_rate   | 0.000962 |
|    n_updates       | 554425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 47       |
|    time_elapsed    | 7937     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=2523.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00987  |
|    ent_coef        | 0.000176 |
|    ent_coef_loss   | 3.16     |
|    learning_rate   | 0.000961 |
|    n_updates       | 564425   |
---------------------------------
Eval num_timesteps=400000, episode_reward=2518.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.012    |
|    ent_coef        | 0.000188 |
|    ent_coef_loss   | 9.44     |
|    learning_rate   | 0.00096  |
|    n_updates       | 574425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 47       |
|    time_elapsed    | 8351     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2524.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0094   |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | 4.2      |
|    learning_rate   | 0.000959 |
|    n_updates       | 584425   |
---------------------------------
Eval num_timesteps=420000, episode_reward=2527.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0291   |
|    ent_coef        | 0.000292 |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.000958 |
|    n_updates       | 594425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 47       |
|    time_elapsed    | 8766     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2526.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0055   |
|    ent_coef        | 0.000262 |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000957 |
|    n_updates       | 604425   |
---------------------------------
Eval num_timesteps=440000, episode_reward=2529.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00299  |
|    ent_coef        | 0.00025  |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.000956 |
|    n_updates       | 614425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 47       |
|    time_elapsed    | 9181     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2523.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0231   |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | -7.24    |
|    learning_rate   | 0.000955 |
|    n_updates       | 624425   |
---------------------------------
Eval num_timesteps=460000, episode_reward=2517.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00639  |
|    ent_coef        | 0.000167 |
|    ent_coef_loss   | 0.703    |
|    learning_rate   | 0.000954 |
|    n_updates       | 634425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 47       |
|    time_elapsed    | 9597     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2530.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0112   |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 9.35     |
|    learning_rate   | 0.000953 |
|    n_updates       | 644425   |
---------------------------------
Eval num_timesteps=480000, episode_reward=2523.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.0157   |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | 6.98     |
|    learning_rate   | 0.000952 |
|    n_updates       | 654425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 47       |
|    time_elapsed    | 10011    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=2691.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00232  |
|    ent_coef        | 0.000194 |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.000951 |
|    n_updates       | 664425   |
---------------------------------
Eval num_timesteps=500000, episode_reward=2522.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.012    |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.00095  |
|    n_updates       | 674425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 47       |
|    time_elapsed    | 10425    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=2520.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00398  |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | -0.957   |
|    learning_rate   | 0.000949 |
|    n_updates       | 684425   |
---------------------------------
Eval num_timesteps=520000, episode_reward=2518.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0236   |
|    ent_coef        | 0.000159 |
|    ent_coef_loss   | 0.271    |
|    learning_rate   | 0.000948 |
|    n_updates       | 694425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 47       |
|    time_elapsed    | 10838    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2685.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00741  |
|    ent_coef        | 0.000167 |
|    ent_coef_loss   | 7.03     |
|    learning_rate   | 0.000947 |
|    n_updates       | 704425   |
---------------------------------
Eval num_timesteps=540000, episode_reward=2522.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00695  |
|    ent_coef        | 0.000142 |
|    ent_coef_loss   | 19.6     |
|    learning_rate   | 0.000946 |
|    n_updates       | 714425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 48       |
|    time_elapsed    | 11249    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2517.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00475  |
|    ent_coef        | 0.000173 |
|    ent_coef_loss   | 13.2     |
|    learning_rate   | 0.000945 |
|    n_updates       | 724425   |
---------------------------------
Eval num_timesteps=560000, episode_reward=2695.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00872  |
|    ent_coef        | 0.000152 |
|    ent_coef_loss   | -0.38    |
|    learning_rate   | 0.000944 |
|    n_updates       | 734425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 48       |
|    time_elapsed    | 11661    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=2520.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00418  |
|    ent_coef        | 0.00018  |
|    ent_coef_loss   | -28.2    |
|    learning_rate   | 0.000943 |
|    n_updates       | 744425   |
---------------------------------
Eval num_timesteps=580000, episode_reward=2517.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.018    |
|    ent_coef        | 9.8e-05  |
|    ent_coef_loss   | 26.1     |
|    learning_rate   | 0.000942 |
|    n_updates       | 754425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 48       |
|    time_elapsed    | 12073    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2522.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0368   |
|    ent_coef        | 8.89e-05 |
|    ent_coef_loss   | -4.59    |
|    learning_rate   | 0.000941 |
|    n_updates       | 764425   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2523.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00548  |
|    ent_coef        | 8.52e-05 |
|    ent_coef_loss   | 8.14     |
|    learning_rate   | 0.00094  |
|    n_updates       | 774425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 48       |
|    time_elapsed    | 12486    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2514.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | 5.9      |
|    learning_rate   | 0.000939 |
|    n_updates       | 784425   |
---------------------------------
Eval num_timesteps=620000, episode_reward=2523.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00543  |
|    ent_coef        | 0.000132 |
|    ent_coef_loss   | 17       |
|    learning_rate   | 0.000938 |
|    n_updates       | 794425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 48       |
|    time_elapsed    | 12900    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2523.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00458  |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -23.3    |
|    learning_rate   | 0.000937 |
|    n_updates       | 804425   |
---------------------------------
Eval num_timesteps=640000, episode_reward=2524.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.0106   |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -25.3    |
|    learning_rate   | 0.000936 |
|    n_updates       | 814425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 48       |
|    time_elapsed    | 13315    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2527.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.0423   |
|    ent_coef        | 9.82e-05 |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.000935 |
|    n_updates       | 824425   |
---------------------------------
Eval num_timesteps=660000, episode_reward=2525.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00279  |
|    ent_coef        | 9.58e-05 |
|    ent_coef_loss   | -5.53    |
|    learning_rate   | 0.000934 |
|    n_updates       | 834425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 48       |
|    time_elapsed    | 13730    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=2686.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0109   |
|    ent_coef        | 9.31e-05 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000933 |
|    n_updates       | 844425   |
---------------------------------
Eval num_timesteps=680000, episode_reward=2522.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00507  |
|    ent_coef        | 0.000142 |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.000932 |
|    n_updates       | 854425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 48       |
|    time_elapsed    | 14145    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2525.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0128   |
|    ent_coef        | 6.88e-05 |
|    ent_coef_loss   | 27.7     |
|    learning_rate   | 0.000931 |
|    n_updates       | 864425   |
---------------------------------
Eval num_timesteps=700000, episode_reward=2521.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00151  |
|    ent_coef        | 0.0001   |
|    ent_coef_loss   | -28.2    |
|    learning_rate   | 0.00093  |
|    n_updates       | 874425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 48       |
|    time_elapsed    | 14558    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=2696.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00819  |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -1.54    |
|    learning_rate   | 0.000929 |
|    n_updates       | 884425   |
---------------------------------
Eval num_timesteps=720000, episode_reward=2520.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.016    |
|    ent_coef        | 7.22e-05 |
|    ent_coef_loss   | -19.6    |
|    learning_rate   | 0.000928 |
|    n_updates       | 894425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 48       |
|    time_elapsed    | 14971    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=2692.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0118   |
|    ent_coef        | 6.91e-05 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.000927 |
|    n_updates       | 904425   |
---------------------------------
Eval num_timesteps=740000, episode_reward=2524.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00386  |
|    ent_coef        | 8.89e-05 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.000926 |
|    n_updates       | 914425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 48       |
|    time_elapsed    | 15382    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2700.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00387  |
|    ent_coef        | 0.000103 |
|    ent_coef_loss   | 22.5     |
|    learning_rate   | 0.000925 |
|    n_updates       | 924425   |
---------------------------------
Eval num_timesteps=760000, episode_reward=2699.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.00459  |
|    ent_coef        | 9.78e-05 |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.000924 |
|    n_updates       | 934425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 48       |
|    time_elapsed    | 15795    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2535.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.00744  |
|    ent_coef        | 9.93e-05 |
|    ent_coef_loss   | -33.3    |
|    learning_rate   | 0.000923 |
|    n_updates       | 944425   |
---------------------------------
Eval num_timesteps=780000, episode_reward=2518.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00576  |
|    ent_coef        | 8.46e-05 |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.000922 |
|    n_updates       | 954425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 48       |
|    time_elapsed    | 16209    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2532.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00767  |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -32.6    |
|    learning_rate   | 0.000921 |
|    n_updates       | 964425   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2532.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00524  |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -6.17    |
|    learning_rate   | 0.00092  |
|    n_updates       | 974425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 48       |
|    time_elapsed    | 16623    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=2520.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.00831  |
|    ent_coef        | 0.000101 |
|    ent_coef_loss   | 9.53     |
|    learning_rate   | 0.000919 |
|    n_updates       | 984425   |
---------------------------------
Eval num_timesteps=820000, episode_reward=2694.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00885  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 5.21     |
|    learning_rate   | 0.000918 |
|    n_updates       | 994425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 48       |
|    time_elapsed    | 17036    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2703.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00329  |
|    ent_coef        | 8.27e-05 |
|    ent_coef_loss   | -38.9    |
|    learning_rate   | 0.000917 |
|    n_updates       | 1004425  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2692.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0101   |
|    ent_coef        | 9.33e-05 |
|    ent_coef_loss   | 8.62     |
|    learning_rate   | 0.000916 |
|    n_updates       | 1014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 48       |
|    time_elapsed    | 17450    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2707.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.019    |
|    ent_coef        | 7.87e-05 |
|    ent_coef_loss   | -5.86    |
|    learning_rate   | 0.000915 |
|    n_updates       | 1024425  |
---------------------------------
New best mean reward!
Eval num_timesteps=860000, episode_reward=2696.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00483  |
|    ent_coef        | 8.5e-05  |
|    ent_coef_loss   | 15       |
|    learning_rate   | 0.000914 |
|    n_updates       | 1034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 48       |
|    time_elapsed    | 17863    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2700.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00732  |
|    ent_coef        | 6.67e-05 |
|    ent_coef_loss   | 8.19     |
|    learning_rate   | 0.000913 |
|    n_updates       | 1044425  |
---------------------------------
Eval num_timesteps=880000, episode_reward=2699.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00685  |
|    ent_coef        | 6.83e-05 |
|    ent_coef_loss   | 42.5     |
|    learning_rate   | 0.000912 |
|    n_updates       | 1054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 48       |
|    time_elapsed    | 18276    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2531.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0105   |
|    ent_coef        | 8.12e-05 |
|    ent_coef_loss   | 4.6      |
|    learning_rate   | 0.000911 |
|    n_updates       | 1064425  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2698.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00523  |
|    ent_coef        | 9.88e-05 |
|    ent_coef_loss   | 9.18     |
|    learning_rate   | 0.00091  |
|    n_updates       | 1074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 48       |
|    time_elapsed    | 18688    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2700.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00232  |
|    ent_coef        | 9.6e-05  |
|    ent_coef_loss   | 49.5     |
|    learning_rate   | 0.000909 |
|    n_updates       | 1084425  |
---------------------------------
Eval num_timesteps=920000, episode_reward=2795.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0112   |
|    ent_coef        | 0.000106 |
|    ent_coef_loss   | -2.72    |
|    learning_rate   | 0.000908 |
|    n_updates       | 1094425  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 184      |
|    fps             | 48       |
|    time_elapsed    | 19104    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2702.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0139   |
|    ent_coef        | 6.92e-05 |
|    ent_coef_loss   | 6.69     |
|    learning_rate   | 0.000907 |
|    n_updates       | 1104425  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2534.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0206   |
|    ent_coef        | 9.18e-05 |
|    ent_coef_loss   | 20.1     |
|    learning_rate   | 0.000906 |
|    n_updates       | 1114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 188      |
|    fps             | 48       |
|    time_elapsed    | 19520    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2525.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00894  |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | -21.5    |
|    learning_rate   | 0.000905 |
|    n_updates       | 1124425  |
---------------------------------
Eval num_timesteps=960000, episode_reward=2694.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00471  |
|    ent_coef        | 9.5e-05  |
|    ent_coef_loss   | 1.48     |
|    learning_rate   | 0.000904 |
|    n_updates       | 1134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 192      |
|    fps             | 48       |
|    time_elapsed    | 19935    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=2527.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0229   |
|    ent_coef        | 0.000121 |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.000903 |
|    n_updates       | 1144425  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2707.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0544   |
|    ent_coef        | 8.59e-05 |
|    ent_coef_loss   | -6.43    |
|    learning_rate   | 0.000902 |
|    n_updates       | 1154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 196      |
|    fps             | 48       |
|    time_elapsed    | 20350    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=2693.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00679  |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.000901 |
|    n_updates       | 1164425  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=2697.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00933  |
|    ent_coef        | 9.46e-05 |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.0009   |
|    n_updates       | 1174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 200      |
|    fps             | 48       |
|    time_elapsed    | 20763    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=2581.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.023    |
|    ent_coef        | 8.89e-05 |
|    ent_coef_loss   | -0.351   |
|    learning_rate   | 0.000899 |
|    n_updates       | 1184425  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=2703.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0104   |
|    ent_coef        | 7.1e-05  |
|    ent_coef_loss   | 21.2     |
|    learning_rate   | 0.000898 |
|    n_updates       | 1194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 204      |
|    fps             | 48       |
|    time_elapsed    | 21179    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=2701.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0228   |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | 18.6     |
|    learning_rate   | 0.000897 |
|    n_updates       | 1204425  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=2538.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00698  |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | 9.81     |
|    learning_rate   | 0.000896 |
|    n_updates       | 1214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 208      |
|    fps             | 48       |
|    time_elapsed    | 21594    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=2702.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00424  |
|    ent_coef        | 0.000115 |
|    ent_coef_loss   | 15       |
|    learning_rate   | 0.000895 |
|    n_updates       | 1224425  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=2525.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0023   |
|    ent_coef        | 6.77e-05 |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.000894 |
|    n_updates       | 1234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 212      |
|    fps             | 48       |
|    time_elapsed    | 22009    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2535.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00903  |
|    ent_coef        | 6.4e-05  |
|    ent_coef_loss   | 7.73     |
|    learning_rate   | 0.000893 |
|    n_updates       | 1244425  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=2545.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00364  |
|    ent_coef        | 8.98e-05 |
|    ent_coef_loss   | 0.659    |
|    learning_rate   | 0.000892 |
|    n_updates       | 1254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 216      |
|    fps             | 48       |
|    time_elapsed    | 22424    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2720.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00321  |
|    ent_coef        | 5.98e-05 |
|    ent_coef_loss   | -28.4    |
|    learning_rate   | 0.000891 |
|    n_updates       | 1264425  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=2726.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00667  |
|    ent_coef        | 6.82e-05 |
|    ent_coef_loss   | 25.1     |
|    learning_rate   | 0.00089  |
|    n_updates       | 1274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 48       |
|    time_elapsed    | 22839    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=2537.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00466  |
|    ent_coef        | 9.65e-05 |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.000889 |
|    n_updates       | 1284425  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=2714.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0013   |
|    ent_coef        | 7.43e-05 |
|    ent_coef_loss   | -53.5    |
|    learning_rate   | 0.000888 |
|    n_updates       | 1294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 48       |
|    time_elapsed    | 23254    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2572.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00871  |
|    ent_coef        | 7.71e-05 |
|    ent_coef_loss   | -30.2    |
|    learning_rate   | 0.000887 |
|    n_updates       | 1304425  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=2711.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00973  |
|    ent_coef        | 0.0001   |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.000886 |
|    n_updates       | 1314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 48       |
|    time_elapsed    | 23669    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=2531.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0159   |
|    ent_coef        | 0.000109 |
|    ent_coef_loss   | -18.8    |
|    learning_rate   | 0.000885 |
|    n_updates       | 1324425  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2698.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00314  |
|    ent_coef        | 9.93e-05 |
|    ent_coef_loss   | 17.9     |
|    learning_rate   | 0.000884 |
|    n_updates       | 1334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 48       |
|    time_elapsed    | 24084    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2694.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00329  |
|    ent_coef        | 0.000103 |
|    ent_coef_loss   | 5.89     |
|    learning_rate   | 0.000883 |
|    n_updates       | 1344425  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2704.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00634  |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | 26.7     |
|    learning_rate   | 0.000882 |
|    n_updates       | 1354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 48       |
|    time_elapsed    | 24496    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2588.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0094   |
|    ent_coef        | 0.000139 |
|    ent_coef_loss   | -7.47    |
|    learning_rate   | 0.000881 |
|    n_updates       | 1364425  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2736.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0448   |
|    ent_coef        | 0.000131 |
|    ent_coef_loss   | 21.6     |
|    learning_rate   | 0.00088  |
|    n_updates       | 1374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 48       |
|    time_elapsed    | 24876    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=2558.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.00476  |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | -2.15    |
|    learning_rate   | 0.000879 |
|    n_updates       | 1384425  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=2701.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00159  |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | 0.715    |
|    learning_rate   | 0.000878 |
|    n_updates       | 1394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 48       |
|    time_elapsed    | 25255    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2739.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00513  |
|    ent_coef        | 0.000122 |
|    ent_coef_loss   | 25.4     |
|    learning_rate   | 0.000877 |
|    n_updates       | 1404425  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=2724.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00388  |
|    ent_coef        | 0.000147 |
|    ent_coef_loss   | -7.23    |
|    learning_rate   | 0.000876 |
|    n_updates       | 1414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 48       |
|    time_elapsed    | 25634    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=2719.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.00392  |
|    ent_coef        | 0.000118 |
|    ent_coef_loss   | 22.5     |
|    learning_rate   | 0.000875 |
|    n_updates       | 1424425  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=2697.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0333   |
|    ent_coef        | 0.000134 |
|    ent_coef_loss   | -14.4    |
|    learning_rate   | 0.000874 |
|    n_updates       | 1434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 48       |
|    time_elapsed    | 26013    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2705.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00262  |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.000873 |
|    n_updates       | 1444425  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=2732.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00291  |
|    ent_coef        | 0.000163 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.000872 |
|    n_updates       | 1454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 48       |
|    time_elapsed    | 26392    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=2715.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0142   |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | -2.28    |
|    learning_rate   | 0.000871 |
|    n_updates       | 1464425  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2694.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.00751  |
|    ent_coef        | 0.000227 |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.00087  |
|    n_updates       | 1474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 48       |
|    time_elapsed    | 26773    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=2694.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.00279  |
|    ent_coef        | 0.000237 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.000869 |
|    n_updates       | 1484425  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=2532.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.000868 |
|    n_updates       | 1494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 48       |
|    time_elapsed    | 27154    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2522.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.00681  |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | 0.885    |
|    learning_rate   | 0.000867 |
|    n_updates       | 1504425  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2521.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0134   |
|    ent_coef        | 0.000156 |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.000866 |
|    n_updates       | 1514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 48       |
|    time_elapsed    | 27535    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2534.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.00681  |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | -25      |
|    learning_rate   | 0.000865 |
|    n_updates       | 1524425  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2523.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0283   |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | 5.31     |
|    learning_rate   | 0.000864 |
|    n_updates       | 1534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 48       |
|    time_elapsed    | 27914    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2545.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0143   |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | -13      |
|    learning_rate   | 0.000863 |
|    n_updates       | 1544425  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2582.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00351  |
|    ent_coef        | 0.000146 |
|    ent_coef_loss   | -28.7    |
|    learning_rate   | 0.000862 |
|    n_updates       | 1554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 48       |
|    time_elapsed    | 28294    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2705.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0111   |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | 7.66     |
|    learning_rate   | 0.000861 |
|    n_updates       | 1564425  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2712.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.00448  |
|    ent_coef        | 0.000132 |
|    ent_coef_loss   | 13.3     |
|    learning_rate   | 0.00086  |
|    n_updates       | 1574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 48       |
|    time_elapsed    | 28673    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=2558.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 9.17e-05 |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.000859 |
|    n_updates       | 1584425  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2705.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00756  |
|    ent_coef        | 0.000131 |
|    ent_coef_loss   | -2.4     |
|    learning_rate   | 0.000858 |
|    n_updates       | 1594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 48       |
|    time_elapsed    | 29054    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=2707.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0191   |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | -8.46    |
|    learning_rate   | 0.000857 |
|    n_updates       | 1604425  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=2707.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00822  |
|    ent_coef        | 0.000125 |
|    ent_coef_loss   | -21.5    |
|    learning_rate   | 0.000856 |
|    n_updates       | 1614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 48       |
|    time_elapsed    | 29434    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2704.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0145   |
|    ent_coef        | 9.41e-05 |
|    ent_coef_loss   | 29.4     |
|    learning_rate   | 0.000855 |
|    n_updates       | 1624425  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2703.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0104   |
|    ent_coef        | 0.000111 |
|    ent_coef_loss   | -8.94    |
|    learning_rate   | 0.000854 |
|    n_updates       | 1634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 48       |
|    time_elapsed    | 29815    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2706.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.00711  |
|    ent_coef        | 0.000121 |
|    ent_coef_loss   | -11.5    |
|    learning_rate   | 0.000853 |
|    n_updates       | 1644425  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2717.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00819  |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -20.8    |
|    learning_rate   | 0.000852 |
|    n_updates       | 1654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 49       |
|    time_elapsed    | 30194    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2719.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.00333  |
|    ent_coef        | 9.38e-05 |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000851 |
|    n_updates       | 1664425  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2550.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0179   |
|    ent_coef        | 9.66e-05 |
|    ent_coef_loss   | 19.7     |
|    learning_rate   | 0.00085  |
|    n_updates       | 1674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 49       |
|    time_elapsed    | 30573    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2719.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.000122 |
|    ent_coef_loss   | -8.05    |
|    learning_rate   | 0.000849 |
|    n_updates       | 1684425  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=2709.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.000848 |
|    n_updates       | 1694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 49       |
|    time_elapsed    | 30951    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=2707.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0212   |
|    ent_coef        | 0.000146 |
|    ent_coef_loss   | 7.84     |
|    learning_rate   | 0.000847 |
|    n_updates       | 1704425  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2715.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0109   |
|    ent_coef        | 0.000171 |
|    ent_coef_loss   | 4.44     |
|    learning_rate   | 0.000846 |
|    n_updates       | 1714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 49       |
|    time_elapsed    | 31331    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2707.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0273   |
|    ent_coef        | 0.000186 |
|    ent_coef_loss   | -38      |
|    learning_rate   | 0.000845 |
|    n_updates       | 1724425  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2566.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00667  |
|    ent_coef        | 0.000171 |
|    ent_coef_loss   | -3.49    |
|    learning_rate   | 0.000844 |
|    n_updates       | 1734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 49       |
|    time_elapsed    | 31710    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2564.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0147   |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | -5.15    |
|    learning_rate   | 0.000843 |
|    n_updates       | 1744425  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2720.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0141   |
|    ent_coef        | 0.000191 |
|    ent_coef_loss   | -2.78    |
|    learning_rate   | 0.000842 |
|    n_updates       | 1754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 49       |
|    time_elapsed    | 32092    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=2575.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0177   |
|    ent_coef        | 0.000162 |
|    ent_coef_loss   | 23       |
|    learning_rate   | 0.000841 |
|    n_updates       | 1764425  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=2710.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.00481  |
|    ent_coef        | 0.000182 |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.00084  |
|    n_updates       | 1774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 49       |
|    time_elapsed    | 32470    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=2708.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0269   |
|    ent_coef        | 0.000187 |
|    ent_coef_loss   | 14.2     |
|    learning_rate   | 0.000839 |
|    n_updates       | 1784425  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=2557.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0133   |
|    ent_coef        | 0.00016  |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.000838 |
|    n_updates       | 1794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 49       |
|    time_elapsed    | 32849    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2562.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.00998  |
|    ent_coef        | 0.000172 |
|    ent_coef_loss   | 0.868    |
|    learning_rate   | 0.000837 |
|    n_updates       | 1804425  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2719.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0473   |
|    ent_coef        | 0.000205 |
|    ent_coef_loss   | 11.3     |
|    learning_rate   | 0.000836 |
|    n_updates       | 1814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 49       |
|    time_elapsed    | 33227    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2553.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0113   |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | -7.26    |
|    learning_rate   | 0.000835 |
|    n_updates       | 1824425  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2550.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0166   |
|    ent_coef        | 0.000187 |
|    ent_coef_loss   | -9.57    |
|    learning_rate   | 0.000834 |
|    n_updates       | 1834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 49       |
|    time_elapsed    | 33604    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2561.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.038    |
|    ent_coef        | 0.000246 |
|    ent_coef_loss   | -4.8     |
|    learning_rate   | 0.000833 |
|    n_updates       | 1844425  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2562.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.434    |
|    ent_coef        | 0.000232 |
|    ent_coef_loss   | 1.22     |
|    learning_rate   | 0.000832 |
|    n_updates       | 1854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 49       |
|    time_elapsed    | 33984    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2553.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0137   |
|    ent_coef        | 0.000291 |
|    ent_coef_loss   | 14.6     |
|    learning_rate   | 0.000831 |
|    n_updates       | 1864425  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2693.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0646   |
|    ent_coef        | 0.000267 |
|    ent_coef_loss   | -4.68    |
|    learning_rate   | 0.00083  |
|    n_updates       | 1874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 49       |
|    time_elapsed    | 34362    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2559.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0319   |
|    ent_coef        | 0.000334 |
|    ent_coef_loss   | 7.43     |
|    learning_rate   | 0.000829 |
|    n_updates       | 1884425  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2567.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0138   |
|    ent_coef        | 0.000319 |
|    ent_coef_loss   | -3.79    |
|    learning_rate   | 0.000828 |
|    n_updates       | 1894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 49       |
|    time_elapsed    | 34747    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2578.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.09     |
|    ent_coef        | 0.000343 |
|    ent_coef_loss   | -8.36    |
|    learning_rate   | 0.000827 |
|    n_updates       | 1904425  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=2561.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.000826 |
|    n_updates       | 1914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 49       |
|    time_elapsed    | 35125    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2561.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.124    |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | 9.82     |
|    learning_rate   | 0.000825 |
|    n_updates       | 1924425  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=2558.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0145   |
|    ent_coef        | 0.000412 |
|    ent_coef_loss   | -0.748   |
|    learning_rate   | 0.000824 |
|    n_updates       | 1934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 49       |
|    time_elapsed    | 35503    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2711.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.026    |
|    ent_coef        | 0.000355 |
|    ent_coef_loss   | 8.36     |
|    learning_rate   | 0.000823 |
|    n_updates       | 1944425  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2567.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0261   |
|    ent_coef        | 0.000307 |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.000822 |
|    n_updates       | 1954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 49       |
|    time_elapsed    | 35882    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2571.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0726   |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | -8.86    |
|    learning_rate   | 0.000821 |
|    n_updates       | 1964425  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2565.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0378   |
|    ent_coef        | 0.000322 |
|    ent_coef_loss   | -3.44    |
|    learning_rate   | 0.00082  |
|    n_updates       | 1974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 49       |
|    time_elapsed    | 36262    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2564.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0766   |
|    ent_coef        | 0.00024  |
|    ent_coef_loss   | -6.24    |
|    learning_rate   | 0.000819 |
|    n_updates       | 1984425  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2583.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0318   |
|    ent_coef        | 0.000274 |
|    ent_coef_loss   | -5.33    |
|    learning_rate   | 0.000818 |
|    n_updates       | 1994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 49       |
|    time_elapsed    | 36641    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=2563.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0491   |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | -5.49    |
|    learning_rate   | 0.000817 |
|    n_updates       | 2004425  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2572.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0264   |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | -6.85    |
|    learning_rate   | 0.000816 |
|    n_updates       | 2014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 49       |
|    time_elapsed    | 37019    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2562.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0519   |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | -0.78    |
|    learning_rate   | 0.000815 |
|    n_updates       | 2024425  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=2588.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0209   |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | -2.2     |
|    learning_rate   | 0.000814 |
|    n_updates       | 2034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 49       |
|    time_elapsed    | 37397    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2564.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0656   |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | 2.23     |
|    learning_rate   | 0.000813 |
|    n_updates       | 2044425  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=2706.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0356   |
|    ent_coef        | 0.000417 |
|    ent_coef_loss   | -1       |
|    learning_rate   | 0.000812 |
|    n_updates       | 2054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 49       |
|    time_elapsed    | 37776    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2559.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0603   |
|    ent_coef        | 0.000411 |
|    ent_coef_loss   | 9.52     |
|    learning_rate   | 0.000811 |
|    n_updates       | 2064425  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2569.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0525   |
|    ent_coef        | 0.000423 |
|    ent_coef_loss   | 0.00972  |
|    learning_rate   | 0.00081  |
|    n_updates       | 2074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 49       |
|    time_elapsed    | 38155    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=2557.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0183   |
|    ent_coef        | 0.000551 |
|    ent_coef_loss   | 4.7      |
|    learning_rate   | 0.000809 |
|    n_updates       | 2084425  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=2551.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.052    |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 5.17     |
|    learning_rate   | 0.000808 |
|    n_updates       | 2094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 49       |
|    time_elapsed    | 38543    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2705.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0108   |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | -3.47    |
|    learning_rate   | 0.000807 |
|    n_updates       | 2104425  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2557.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0405   |
|    ent_coef        | 0.000387 |
|    ent_coef_loss   | -15.6    |
|    learning_rate   | 0.000806 |
|    n_updates       | 2114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 49       |
|    time_elapsed    | 38911    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2709.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0452   |
|    ent_coef        | 0.000355 |
|    ent_coef_loss   | -3.22    |
|    learning_rate   | 0.000805 |
|    n_updates       | 2124425  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2564.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0406   |
|    ent_coef        | 0.000319 |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.000804 |
|    n_updates       | 2134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 49       |
|    time_elapsed    | 39276    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=2561.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0239   |
|    ent_coef        | 0.00019  |
|    ent_coef_loss   | 0.819    |
|    learning_rate   | 0.000803 |
|    n_updates       | 2144425  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2516.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0485   |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 43.9     |
|    learning_rate   | 0.000802 |
|    n_updates       | 2154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 49       |
|    time_elapsed    | 39639    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=2558.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0467   |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | 23.6     |
|    learning_rate   | 0.000801 |
|    n_updates       | 2164425  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2554.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0462   |
|    ent_coef        | 0.000176 |
|    ent_coef_loss   | -7.79    |
|    learning_rate   | 0.0008   |
|    n_updates       | 2174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 49       |
|    time_elapsed    | 40001    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2569.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0214   |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -22.5    |
|    learning_rate   | 0.000799 |
|    n_updates       | 2184425  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=2559.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0125   |
|    ent_coef        | 0.000168 |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.000798 |
|    n_updates       | 2194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 50       |
|    time_elapsed    | 40365    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2556.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0193   |
|    ent_coef        | 0.000217 |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.000797 |
|    n_updates       | 2204425  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=2559.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0133   |
|    ent_coef        | 0.000197 |
|    ent_coef_loss   | 12.6     |
|    learning_rate   | 0.000796 |
|    n_updates       | 2214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 50       |
|    time_elapsed    | 40729    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2551.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00905  |
|    ent_coef        | 0.000192 |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.000795 |
|    n_updates       | 2224425  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2532.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0383   |
|    ent_coef        | 0.000277 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.000794 |
|    n_updates       | 2234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 50       |
|    time_elapsed    | 41094    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2718.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0457   |
|    ent_coef        | 0.000369 |
|    ent_coef_loss   | -2.93    |
|    learning_rate   | 0.000793 |
|    n_updates       | 2244425  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=2559.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0344   |
|    ent_coef        | 0.000404 |
|    ent_coef_loss   | -21.3    |
|    learning_rate   | 0.000792 |
|    n_updates       | 2254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 50       |
|    time_elapsed    | 41459    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=2594.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0738   |
|    ent_coef        | 0.000318 |
|    ent_coef_loss   | 1.65     |
|    learning_rate   | 0.000791 |
|    n_updates       | 2264425  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=2578.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0216   |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -9.58    |
|    learning_rate   | 0.00079  |
|    n_updates       | 2274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 50       |
|    time_elapsed    | 41822    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2575.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.00292  |
|    ent_coef        | 0.000318 |
|    ent_coef_loss   | -4.93    |
|    learning_rate   | 0.000789 |
|    n_updates       | 2284425  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=2706.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0226   |
|    ent_coef        | 0.000205 |
|    ent_coef_loss   | -2.52    |
|    learning_rate   | 0.000788 |
|    n_updates       | 2294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 50       |
|    time_elapsed    | 42187    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2540.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0309   |
|    ent_coef        | 0.000178 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.000787 |
|    n_updates       | 2304425  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=2581.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0673   |
|    ent_coef        | 0.000172 |
|    ent_coef_loss   | 12.5     |
|    learning_rate   | 0.000786 |
|    n_updates       | 2314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 50       |
|    time_elapsed    | 42552    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=2708.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0733   |
|    ent_coef        | 0.000204 |
|    ent_coef_loss   | 15.4     |
|    learning_rate   | 0.000785 |
|    n_updates       | 2324425  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=2704.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0573   |
|    ent_coef        | 0.000226 |
|    ent_coef_loss   | 20.8     |
|    learning_rate   | 0.000784 |
|    n_updates       | 2334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 50       |
|    time_elapsed    | 42916    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=2715.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0192   |
|    ent_coef        | 0.000183 |
|    ent_coef_loss   | -6.53    |
|    learning_rate   | 0.000783 |
|    n_updates       | 2344425  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=2778.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0397   |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | -5.11    |
|    learning_rate   | 0.000782 |
|    n_updates       | 2354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 50       |
|    time_elapsed    | 43279    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=2718.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0651   |
|    ent_coef        | 0.000243 |
|    ent_coef_loss   | 23.4     |
|    learning_rate   | 0.000781 |
|    n_updates       | 2364425  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=2704.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0764   |
|    ent_coef        | 0.000245 |
|    ent_coef_loss   | -9.23    |
|    learning_rate   | 0.00078  |
|    n_updates       | 2374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 50       |
|    time_elapsed    | 43643    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=2586.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00659  |
|    ent_coef        | 0.00026  |
|    ent_coef_loss   | 16.8     |
|    learning_rate   | 0.000779 |
|    n_updates       | 2384425  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=2716.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.27     |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.000778 |
|    n_updates       | 2394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 50       |
|    time_elapsed    | 44006    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2705.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0226   |
|    ent_coef        | 0.000273 |
|    ent_coef_loss   | -17.8    |
|    learning_rate   | 0.000777 |
|    n_updates       | 2404425  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2705.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.000234 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.000776 |
|    n_updates       | 2414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 50       |
|    time_elapsed    | 44371    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2708.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0244   |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | -6.17    |
|    learning_rate   | 0.000775 |
|    n_updates       | 2424425  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2704.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0361   |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | 0.218    |
|    learning_rate   | 0.000774 |
|    n_updates       | 2434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 50       |
|    time_elapsed    | 44734    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2708.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00869  |
|    ent_coef        | 0.000349 |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.000773 |
|    n_updates       | 2444425  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=2713.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0113   |
|    ent_coef        | 0.000293 |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.000772 |
|    n_updates       | 2454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 50       |
|    time_elapsed    | 45097    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2689.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00875  |
|    ent_coef        | 0.0002   |
|    ent_coef_loss   | 0.756    |
|    learning_rate   | 0.000771 |
|    n_updates       | 2464425  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=2688.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0344   |
|    ent_coef        | 0.000224 |
|    ent_coef_loss   | -9.8     |
|    learning_rate   | 0.00077  |
|    n_updates       | 2474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 50       |
|    time_elapsed    | 45463    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=2709.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0364   |
|    ent_coef        | 0.000287 |
|    ent_coef_loss   | 12.5     |
|    learning_rate   | 0.000769 |
|    n_updates       | 2484425  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2569.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0285   |
|    ent_coef        | 0.000259 |
|    ent_coef_loss   | -6.55    |
|    learning_rate   | 0.000768 |
|    n_updates       | 2494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 50       |
|    time_elapsed    | 45827    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=2563.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0517   |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | -3.36    |
|    learning_rate   | 0.000767 |
|    n_updates       | 2504425  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=2567.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.000217 |
|    ent_coef_loss   | -14      |
|    learning_rate   | 0.000766 |
|    n_updates       | 2514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 50       |
|    time_elapsed    | 46190    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=2549.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0356   |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.000765 |
|    n_updates       | 2524425  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=2703.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0178   |
|    ent_coef        | 0.000274 |
|    ent_coef_loss   | 9.44     |
|    learning_rate   | 0.000764 |
|    n_updates       | 2534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 50       |
|    time_elapsed    | 46553    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=2561.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000763 |
|    n_updates       | 2544425  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=2578.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.171    |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.000762 |
|    n_updates       | 2554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 50       |
|    time_elapsed    | 46917    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=2705.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0138   |
|    ent_coef        | 0.000243 |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000761 |
|    n_updates       | 2564425  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2572.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0472   |
|    ent_coef        | 0.000253 |
|    ent_coef_loss   | -17.8    |
|    learning_rate   | 0.00076  |
|    n_updates       | 2574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 50       |
|    time_elapsed    | 47282    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=2536.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0511   |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.000759 |
|    n_updates       | 2584425  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=2569.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.0219   |
|    ent_coef        | 0.000353 |
|    ent_coef_loss   | -14.3    |
|    learning_rate   | 0.000758 |
|    n_updates       | 2594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 50       |
|    time_elapsed    | 47649    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2565.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0237   |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | -4.48    |
|    learning_rate   | 0.000757 |
|    n_updates       | 2604425  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=2581.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.055    |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | -3.36    |
|    learning_rate   | 0.000756 |
|    n_updates       | 2614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 50       |
|    time_elapsed    | 48013    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2569.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0107   |
|    ent_coef        | 0.000347 |
|    ent_coef_loss   | 4.46     |
|    learning_rate   | 0.000755 |
|    n_updates       | 2624425  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=2577.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0454   |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | -3.04    |
|    learning_rate   | 0.000754 |
|    n_updates       | 2634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 50       |
|    time_elapsed    | 48375    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=2573.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.13     |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | -6.98    |
|    learning_rate   | 0.000753 |
|    n_updates       | 2644425  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=2709.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0302   |
|    ent_coef        | 0.000187 |
|    ent_coef_loss   | -22.9    |
|    learning_rate   | 0.000752 |
|    n_updates       | 2654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 50       |
|    time_elapsed    | 48738    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=2710.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.044    |
|    ent_coef        | 0.000312 |
|    ent_coef_loss   | -7.02    |
|    learning_rate   | 0.000751 |
|    n_updates       | 2664425  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2554.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0987   |
|    ent_coef        | 0.000312 |
|    ent_coef_loss   | 13.7     |
|    learning_rate   | 0.00075  |
|    n_updates       | 2674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 50       |
|    time_elapsed    | 49103    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=2562.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.166    |
|    ent_coef        | 0.000731 |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.000749 |
|    n_updates       | 2684425  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=2543.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.000928 |
|    ent_coef_loss   | 2.51     |
|    learning_rate   | 0.000748 |
|    n_updates       | 2694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 50       |
|    time_elapsed    | 49466    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=2715.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0533   |
|    ent_coef        | 0.0011   |
|    ent_coef_loss   | 3.01     |
|    learning_rate   | 0.000747 |
|    n_updates       | 2704425  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=2559.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.32     |
|    ent_coef        | 0.000711 |
|    ent_coef_loss   | -4.04    |
|    learning_rate   | 0.000746 |
|    n_updates       | 2714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 50       |
|    time_elapsed    | 49830    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2556.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0975   |
|    ent_coef        | 0.000633 |
|    ent_coef_loss   | -5.32    |
|    learning_rate   | 0.000745 |
|    n_updates       | 2724425  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=2571.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0256   |
|    ent_coef        | 0.000476 |
|    ent_coef_loss   | -8.17    |
|    learning_rate   | 0.000744 |
|    n_updates       | 2734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 50       |
|    time_elapsed    | 50196    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=2566.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0257   |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | -9.54    |
|    learning_rate   | 0.000743 |
|    n_updates       | 2744425  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2701.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0671   |
|    ent_coef        | 0.000369 |
|    ent_coef_loss   | 2.04     |
|    learning_rate   | 0.000742 |
|    n_updates       | 2754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 51       |
|    time_elapsed    | 50560    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=2563.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.158    |
|    ent_coef        | 0.000216 |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.000741 |
|    n_updates       | 2764425  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2564.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0726   |
|    ent_coef        | 0.000352 |
|    ent_coef_loss   | -6.59    |
|    learning_rate   | 0.00074  |
|    n_updates       | 2774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 51       |
|    time_elapsed    | 50926    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=2697.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.000366 |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000739 |
|    n_updates       | 2784425  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=2546.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0389   |
|    ent_coef        | 0.000309 |
|    ent_coef_loss   | -3.51    |
|    learning_rate   | 0.000738 |
|    n_updates       | 2794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 51       |
|    time_elapsed    | 51289    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=2577.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00777  |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | 0.876    |
|    learning_rate   | 0.000737 |
|    n_updates       | 2804425  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=2566.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0642   |
|    ent_coef        | 0.000275 |
|    ent_coef_loss   | 1.19     |
|    learning_rate   | 0.000736 |
|    n_updates       | 2814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 51       |
|    time_elapsed    | 51653    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2574.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0348   |
|    ent_coef        | 0.000298 |
|    ent_coef_loss   | 2.71     |
|    learning_rate   | 0.000735 |
|    n_updates       | 2824425  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2565.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.301    |
|    ent_coef        | 0.00035  |
|    ent_coef_loss   | 6.39     |
|    learning_rate   | 0.000734 |
|    n_updates       | 2834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 51       |
|    time_elapsed    | 52017    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=2567.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | -0.692   |
|    learning_rate   | 0.000733 |
|    n_updates       | 2844425  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=2703.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0687   |
|    ent_coef        | 0.000329 |
|    ent_coef_loss   | 3.35     |
|    learning_rate   | 0.000732 |
|    n_updates       | 2854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 51       |
|    time_elapsed    | 52379    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=2562.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.278    |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 5.15     |
|    learning_rate   | 0.000731 |
|    n_updates       | 2864425  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2563.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0924   |
|    ent_coef        | 0.000178 |
|    ent_coef_loss   | 3.66     |
|    learning_rate   | 0.00073  |
|    n_updates       | 2874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 540      |
|    fps             | 51       |
|    time_elapsed    | 52742    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=2543.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | -5.07    |
|    learning_rate   | 0.000729 |
|    n_updates       | 2884425  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=2705.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0609   |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | 7.32     |
|    learning_rate   | 0.000728 |
|    n_updates       | 2894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 544      |
|    fps             | 51       |
|    time_elapsed    | 53104    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=2698.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.000268 |
|    ent_coef_loss   | 1.2      |
|    learning_rate   | 0.000727 |
|    n_updates       | 2904425  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=2705.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0309   |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000726 |
|    n_updates       | 2914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 51       |
|    time_elapsed    | 53467    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2571.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0406   |
|    ent_coef        | 0.000309 |
|    ent_coef_loss   | 0.517    |
|    learning_rate   | 0.000725 |
|    n_updates       | 2924425  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=2578.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0981   |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | -8.14    |
|    learning_rate   | 0.000724 |
|    n_updates       | 2934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 552      |
|    fps             | 51       |
|    time_elapsed    | 53830    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=2565.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0452   |
|    ent_coef        | 0.000275 |
|    ent_coef_loss   | 0.000942 |
|    learning_rate   | 0.000723 |
|    n_updates       | 2944425  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2538.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0342   |
|    ent_coef        | 0.000346 |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.000722 |
|    n_updates       | 2954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 556      |
|    fps             | 51       |
|    time_elapsed    | 54195    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=2708.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0468   |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | 6.09     |
|    learning_rate   | 0.000721 |
|    n_updates       | 2964425  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2565.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0644   |
|    ent_coef        | 0.000409 |
|    ent_coef_loss   | 4.96     |
|    learning_rate   | 0.00072  |
|    n_updates       | 2974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 560      |
|    fps             | 51       |
|    time_elapsed    | 54557    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=2697.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0357   |
|    ent_coef        | 0.000343 |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.000719 |
|    n_updates       | 2984425  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=2708.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.000365 |
|    ent_coef_loss   | 16.5     |
|    learning_rate   | 0.000718 |
|    n_updates       | 2994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 51       |
|    time_elapsed    | 54922    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=2704.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0214   |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | -11.8    |
|    learning_rate   | 0.000717 |
|    n_updates       | 3004425  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=2704.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0382   |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | 0.603    |
|    learning_rate   | 0.000716 |
|    n_updates       | 3014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 568      |
|    fps             | 51       |
|    time_elapsed    | 55286    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2578.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0156   |
|    ent_coef        | 0.000252 |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.000715 |
|    n_updates       | 3024425  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=2706.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0138   |
|    ent_coef        | 0.000242 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.000714 |
|    n_updates       | 3034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 572      |
|    fps             | 51       |
|    time_elapsed    | 55652    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=2705.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0617   |
|    ent_coef        | 0.000289 |
|    ent_coef_loss   | 9.63     |
|    learning_rate   | 0.000713 |
|    n_updates       | 3044425  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=2704.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0505   |
|    ent_coef        | 0.000231 |
|    ent_coef_loss   | 4.59     |
|    learning_rate   | 0.000712 |
|    n_updates       | 3054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 576      |
|    fps             | 51       |
|    time_elapsed    | 56015    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=2575.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0095   |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -7.77    |
|    learning_rate   | 0.000711 |
|    n_updates       | 3064425  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=2540.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0915   |
|    ent_coef        | 0.000676 |
|    ent_coef_loss   | -5.36    |
|    learning_rate   | 0.00071  |
|    n_updates       | 3074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 580      |
|    fps             | 51       |
|    time_elapsed    | 56380    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=2593.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0842   |
|    ent_coef        | 0.000439 |
|    ent_coef_loss   | -5.73    |
|    learning_rate   | 0.000709 |
|    n_updates       | 3084425  |
---------------------------------
Wrapping the env in a DummyVecEnv.
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Logging to 5gdl/SAC_42
Eval num_timesteps=10000, episode_reward=2691.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0815   |
|    ent_coef        | 0.0046   |
|    ent_coef_loss   | -3.4     |
|    learning_rate   | 0.000999 |
|    n_updates       | 184425   |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=2515.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -66.9    |
|    critic_loss     | 0.094    |
|    ent_coef        | 0.00285  |
|    ent_coef_loss   | -8.11    |
|    learning_rate   | 0.000998 |
|    n_updates       | 194425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 50       |
|    time_elapsed    | 396      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2696.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -62.7    |
|    critic_loss     | 0.0495   |
|    ent_coef        | 0.00282  |
|    ent_coef_loss   | 5.46     |
|    learning_rate   | 0.000997 |
|    n_updates       | 204425   |
---------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=2690.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -60.3    |
|    critic_loss     | 0.0248   |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.000996 |
|    n_updates       | 214425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 50       |
|    time_elapsed    | 784      |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=2695.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -58.5    |
|    critic_loss     | 0.0264   |
|    ent_coef        | 0.00104  |
|    ent_coef_loss   | -3.89    |
|    learning_rate   | 0.000995 |
|    n_updates       | 224425   |
---------------------------------
Eval num_timesteps=60000, episode_reward=2690.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 0.00872  |
|    ent_coef        | 0.000924 |
|    ent_coef_loss   | -7.14    |
|    learning_rate   | 0.000994 |
|    n_updates       | 234425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 51       |
|    time_elapsed    | 1167     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=2516.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -55.3    |
|    critic_loss     | 0.0139   |
|    ent_coef        | 0.000684 |
|    ent_coef_loss   | 0.558    |
|    learning_rate   | 0.000993 |
|    n_updates       | 244425   |
---------------------------------
Eval num_timesteps=80000, episode_reward=2695.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.0132   |
|    ent_coef        | 0.00082  |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.000992 |
|    n_updates       | 254425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 51       |
|    time_elapsed    | 1543     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=2702.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -55.7    |
|    critic_loss     | 0.0359   |
|    ent_coef        | 0.000742 |
|    ent_coef_loss   | 0.0102   |
|    learning_rate   | 0.000991 |
|    n_updates       | 264425   |
---------------------------------
New best mean reward!
Eval num_timesteps=100000, episode_reward=2691.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -55.8    |
|    critic_loss     | 0.013    |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.00099  |
|    n_updates       | 274425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 52       |
|    time_elapsed    | 1919     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2693.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 0.021    |
|    ent_coef        | 0.000426 |
|    ent_coef_loss   | -4.83    |
|    learning_rate   | 0.000989 |
|    n_updates       | 284425   |
---------------------------------
Eval num_timesteps=120000, episode_reward=2695.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -54.9    |
|    critic_loss     | 0.253    |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | 0.352    |
|    learning_rate   | 0.000988 |
|    n_updates       | 294425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 52       |
|    time_elapsed    | 2297     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.0265   |
|    ent_coef        | 0.000243 |
|    ent_coef_loss   | 19.6     |
|    learning_rate   | 0.000987 |
|    n_updates       | 304425   |
---------------------------------
Eval num_timesteps=140000, episode_reward=2699.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.0194   |
|    ent_coef        | 0.000277 |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.000986 |
|    n_updates       | 314425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 52       |
|    time_elapsed    | 2676     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2698.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 0.0123   |
|    ent_coef        | 0.000253 |
|    ent_coef_loss   | -20.8    |
|    learning_rate   | 0.000985 |
|    n_updates       | 324425   |
---------------------------------
Eval num_timesteps=160000, episode_reward=2692.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00444  |
|    ent_coef        | 0.000374 |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.000984 |
|    n_updates       | 334425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 52       |
|    time_elapsed    | 3057     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=2519.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00339  |
|    ent_coef        | 0.000289 |
|    ent_coef_loss   | -3.69    |
|    learning_rate   | 0.000983 |
|    n_updates       | 344425   |
---------------------------------
Eval num_timesteps=180000, episode_reward=2515.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0195   |
|    ent_coef        | 0.000179 |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.000982 |
|    n_updates       | 354425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 52       |
|    time_elapsed    | 3437     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=2512.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0345   |
|    ent_coef        | 0.000199 |
|    ent_coef_loss   | -4.22    |
|    learning_rate   | 0.000981 |
|    n_updates       | 364425   |
---------------------------------
Eval num_timesteps=200000, episode_reward=2513.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0318   |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | 21.1     |
|    learning_rate   | 0.00098  |
|    n_updates       | 374425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 52       |
|    time_elapsed    | 3815     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2513.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00847  |
|    ent_coef        | 0.000238 |
|    ent_coef_loss   | -0.479   |
|    learning_rate   | 0.000979 |
|    n_updates       | 384425   |
---------------------------------
Eval num_timesteps=220000, episode_reward=2516.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0329   |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | -6.36    |
|    learning_rate   | 0.000978 |
|    n_updates       | 394425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 52       |
|    time_elapsed    | 4194     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2513.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0137   |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | -9.95    |
|    learning_rate   | 0.000977 |
|    n_updates       | 404425   |
---------------------------------
Eval num_timesteps=240000, episode_reward=2702.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.00412  |
|    ent_coef        | 0.00014  |
|    ent_coef_loss   | -17.1    |
|    learning_rate   | 0.000976 |
|    n_updates       | 414425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 52       |
|    time_elapsed    | 4570     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2693.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00661  |
|    ent_coef        | 0.000148 |
|    ent_coef_loss   | 15.9     |
|    learning_rate   | 0.000975 |
|    n_updates       | 424425   |
---------------------------------
Eval num_timesteps=260000, episode_reward=2698.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0039   |
|    ent_coef        | 0.000144 |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.000974 |
|    n_updates       | 434425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 52       |
|    time_elapsed    | 4946     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=2515.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00399  |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -8.14    |
|    learning_rate   | 0.000973 |
|    n_updates       | 444425   |
---------------------------------
Eval num_timesteps=280000, episode_reward=2699.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00572  |
|    ent_coef        | 0.000146 |
|    ent_coef_loss   | 8.66     |
|    learning_rate   | 0.000972 |
|    n_updates       | 454425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 52       |
|    time_elapsed    | 5322     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=2697.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00905  |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | -1.31    |
|    learning_rate   | 0.000971 |
|    n_updates       | 464425   |
---------------------------------
Eval num_timesteps=300000, episode_reward=2699.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00485  |
|    ent_coef        | 0.000159 |
|    ent_coef_loss   | -6.18    |
|    learning_rate   | 0.00097  |
|    n_updates       | 474425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 60       |
|    fps             | 52       |
|    time_elapsed    | 5696     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=2697.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0124   |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -15      |
|    learning_rate   | 0.000969 |
|    n_updates       | 484425   |
---------------------------------
Eval num_timesteps=320000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00313  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.000968 |
|    n_updates       | 494425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 52       |
|    time_elapsed    | 6076     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=2704.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00222  |
|    ent_coef        | 0.000121 |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000967 |
|    n_updates       | 504425   |
---------------------------------
New best mean reward!
Eval num_timesteps=340000, episode_reward=2531.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0054   |
|    ent_coef        | 0.000133 |
|    ent_coef_loss   | -6.57    |
|    learning_rate   | 0.000966 |
|    n_updates       | 514425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 52       |
|    time_elapsed    | 6453     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=2698.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.000167 |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000965 |
|    n_updates       | 524425   |
---------------------------------
Eval num_timesteps=360000, episode_reward=2512.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00741  |
|    ent_coef        | 0.000114 |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.000964 |
|    n_updates       | 534425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 52       |
|    time_elapsed    | 6831     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=2515.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00191  |
|    ent_coef        | 0.000125 |
|    ent_coef_loss   | 3.02     |
|    learning_rate   | 0.000963 |
|    n_updates       | 544425   |
---------------------------------
Eval num_timesteps=380000, episode_reward=2514.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00427  |
|    ent_coef        | 0.000174 |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.000962 |
|    n_updates       | 554425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 52       |
|    time_elapsed    | 7207     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=2700.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.00566  |
|    ent_coef        | 0.000143 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.000961 |
|    n_updates       | 564425   |
---------------------------------
Eval num_timesteps=400000, episode_reward=2703.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0201   |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | -9.49    |
|    learning_rate   | 0.00096  |
|    n_updates       | 574425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 52       |
|    time_elapsed    | 7585     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2701.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00373  |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.000959 |
|    n_updates       | 584425   |
---------------------------------
Eval num_timesteps=420000, episode_reward=2514.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00304  |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | -7.81    |
|    learning_rate   | 0.000958 |
|    n_updates       | 594425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 52       |
|    time_elapsed    | 7964     |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2701.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0059   |
|    ent_coef        | 8.47e-05 |
|    ent_coef_loss   | -9.48    |
|    learning_rate   | 0.000957 |
|    n_updates       | 604425   |
---------------------------------
Eval num_timesteps=440000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00508  |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | 12.4     |
|    learning_rate   | 0.000956 |
|    n_updates       | 614425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 88       |
|    fps             | 52       |
|    time_elapsed    | 8344     |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2528.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0102   |
|    ent_coef        | 0.000141 |
|    ent_coef_loss   | 0.987    |
|    learning_rate   | 0.000955 |
|    n_updates       | 624425   |
---------------------------------
Eval num_timesteps=460000, episode_reward=2690.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00392  |
|    ent_coef        | 8.26e-05 |
|    ent_coef_loss   | 8.1      |
|    learning_rate   | 0.000954 |
|    n_updates       | 634425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 52       |
|    time_elapsed    | 8720     |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2697.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0159   |
|    ent_coef        | 0.000137 |
|    ent_coef_loss   | 18.9     |
|    learning_rate   | 0.000953 |
|    n_updates       | 644425   |
---------------------------------
Eval num_timesteps=480000, episode_reward=2699.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0139   |
|    ent_coef        | 0.000112 |
|    ent_coef_loss   | 6.63     |
|    learning_rate   | 0.000952 |
|    n_updates       | 654425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 52       |
|    time_elapsed    | 9097     |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=2700.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00396  |
|    ent_coef        | 0.000131 |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000951 |
|    n_updates       | 664425   |
---------------------------------
Eval num_timesteps=500000, episode_reward=2698.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00747  |
|    ent_coef        | 9.27e-05 |
|    ent_coef_loss   | 1.91     |
|    learning_rate   | 0.00095  |
|    n_updates       | 674425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 52       |
|    time_elapsed    | 9476     |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=2701.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00873  |
|    ent_coef        | 0.000114 |
|    ent_coef_loss   | -4.59    |
|    learning_rate   | 0.000949 |
|    n_updates       | 684425   |
---------------------------------
Eval num_timesteps=520000, episode_reward=2702.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00273  |
|    ent_coef        | 0.000103 |
|    ent_coef_loss   | 24.3     |
|    learning_rate   | 0.000948 |
|    n_updates       | 694425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 52       |
|    time_elapsed    | 9852     |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2700.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00688  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 26.3     |
|    learning_rate   | 0.000947 |
|    n_updates       | 704425   |
---------------------------------
Eval num_timesteps=540000, episode_reward=2699.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00156  |
|    ent_coef        | 0.000106 |
|    ent_coef_loss   | -20.6    |
|    learning_rate   | 0.000946 |
|    n_updates       | 714425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 52       |
|    time_elapsed    | 10230    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=2700.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00675  |
|    ent_coef        | 0.000127 |
|    ent_coef_loss   | 29       |
|    learning_rate   | 0.000945 |
|    n_updates       | 724425   |
---------------------------------
Eval num_timesteps=560000, episode_reward=2702.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00497  |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | 8.3      |
|    learning_rate   | 0.000944 |
|    n_updates       | 734425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 52       |
|    time_elapsed    | 10606    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=2703.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00735  |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | 4.27     |
|    learning_rate   | 0.000943 |
|    n_updates       | 744425   |
---------------------------------
Eval num_timesteps=580000, episode_reward=2701.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0123   |
|    ent_coef        | 0.000192 |
|    ent_coef_loss   | -3.72    |
|    learning_rate   | 0.000942 |
|    n_updates       | 754425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 116      |
|    fps             | 52       |
|    time_elapsed    | 10983    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=2700.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00386  |
|    ent_coef        | 0.00017  |
|    ent_coef_loss   | -5.91    |
|    learning_rate   | 0.000941 |
|    n_updates       | 764425   |
---------------------------------
Eval num_timesteps=600000, episode_reward=2700.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0152   |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | 18.6     |
|    learning_rate   | 0.00094  |
|    n_updates       | 774425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 52       |
|    time_elapsed    | 11358    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2701.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00223  |
|    ent_coef        | 0.000124 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.000939 |
|    n_updates       | 784425   |
---------------------------------
Eval num_timesteps=620000, episode_reward=2701.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0032   |
|    ent_coef        | 0.000145 |
|    ent_coef_loss   | 4.59     |
|    learning_rate   | 0.000938 |
|    n_updates       | 794425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 52       |
|    time_elapsed    | 11733    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=2693.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00261  |
|    ent_coef        | 0.000134 |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000937 |
|    n_updates       | 804425   |
---------------------------------
Eval num_timesteps=640000, episode_reward=2706.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00902  |
|    ent_coef        | 0.000101 |
|    ent_coef_loss   | 0.843    |
|    learning_rate   | 0.000936 |
|    n_updates       | 814425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 128      |
|    fps             | 52       |
|    time_elapsed    | 12109    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=2704.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00878  |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -29.3    |
|    learning_rate   | 0.000935 |
|    n_updates       | 824425   |
---------------------------------
Eval num_timesteps=660000, episode_reward=2706.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0172   |
|    ent_coef        | 0.000187 |
|    ent_coef_loss   | 9.98     |
|    learning_rate   | 0.000934 |
|    n_updates       | 834425   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 52       |
|    time_elapsed    | 12484    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=2706.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0075   |
|    ent_coef        | 0.000148 |
|    ent_coef_loss   | -22.9    |
|    learning_rate   | 0.000933 |
|    n_updates       | 844425   |
---------------------------------
Eval num_timesteps=680000, episode_reward=2706.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.019    |
|    ent_coef        | 0.000146 |
|    ent_coef_loss   | 4.61     |
|    learning_rate   | 0.000932 |
|    n_updates       | 854425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 52       |
|    time_elapsed    | 12859    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=2705.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0124   |
|    ent_coef        | 0.000138 |
|    ent_coef_loss   | -11.5    |
|    learning_rate   | 0.000931 |
|    n_updates       | 864425   |
---------------------------------
Eval num_timesteps=700000, episode_reward=2704.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00425  |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | -6.74    |
|    learning_rate   | 0.00093  |
|    n_updates       | 874425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 140      |
|    fps             | 52       |
|    time_elapsed    | 13235    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=2705.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0103   |
|    ent_coef        | 0.000225 |
|    ent_coef_loss   | 4.05     |
|    learning_rate   | 0.000929 |
|    n_updates       | 884425   |
---------------------------------
Eval num_timesteps=720000, episode_reward=2705.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.014    |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 4        |
|    learning_rate   | 0.000928 |
|    n_updates       | 894425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 52       |
|    time_elapsed    | 13608    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=2698.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00573  |
|    ent_coef        | 0.000157 |
|    ent_coef_loss   | -5.6     |
|    learning_rate   | 0.000927 |
|    n_updates       | 904425   |
---------------------------------
Eval num_timesteps=740000, episode_reward=2699.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00287  |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | -8.26    |
|    learning_rate   | 0.000926 |
|    n_updates       | 914425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 52       |
|    time_elapsed    | 13986    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=2700.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0273   |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | 11.4     |
|    learning_rate   | 0.000925 |
|    n_updates       | 924425   |
---------------------------------
Eval num_timesteps=760000, episode_reward=2700.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0226   |
|    ent_coef        | 0.00014  |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.000924 |
|    n_updates       | 934425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 52       |
|    time_elapsed    | 14362    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=2705.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00462  |
|    ent_coef        | 0.000143 |
|    ent_coef_loss   | -31      |
|    learning_rate   | 0.000923 |
|    n_updates       | 944425   |
---------------------------------
Eval num_timesteps=780000, episode_reward=2705.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00706  |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | -14.6    |
|    learning_rate   | 0.000922 |
|    n_updates       | 954425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 52       |
|    time_elapsed    | 14740    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=2698.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00824  |
|    ent_coef        | 8.22e-05 |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.000921 |
|    n_updates       | 964425   |
---------------------------------
Eval num_timesteps=800000, episode_reward=2700.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00916  |
|    ent_coef        | 0.000134 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00092  |
|    n_updates       | 974425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 160      |
|    fps             | 52       |
|    time_elapsed    | 15116    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=2700.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00658  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 17.1     |
|    learning_rate   | 0.000919 |
|    n_updates       | 984425   |
---------------------------------
Eval num_timesteps=820000, episode_reward=2700.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.000918 |
|    n_updates       | 994425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 52       |
|    time_elapsed    | 15491    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=2700.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00798  |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | -4.17    |
|    learning_rate   | 0.000917 |
|    n_updates       | 1004425  |
---------------------------------
Eval num_timesteps=840000, episode_reward=2703.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00463  |
|    ent_coef        | 9.56e-05 |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.000916 |
|    n_updates       | 1014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 52       |
|    time_elapsed    | 15866    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2700.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00761  |
|    ent_coef        | 0.000136 |
|    ent_coef_loss   | -4.63    |
|    learning_rate   | 0.000915 |
|    n_updates       | 1024425  |
---------------------------------
Eval num_timesteps=860000, episode_reward=2707.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.000893 |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -6.93    |
|    learning_rate   | 0.000914 |
|    n_updates       | 1034425  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 52       |
|    time_elapsed    | 16241    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=2704.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00763  |
|    ent_coef        | 0.000109 |
|    ent_coef_loss   | -0.0788  |
|    learning_rate   | 0.000913 |
|    n_updates       | 1044425  |
---------------------------------
Eval num_timesteps=880000, episode_reward=2696.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00437  |
|    ent_coef        | 0.000155 |
|    ent_coef_loss   | 6.31     |
|    learning_rate   | 0.000912 |
|    n_updates       | 1054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 52       |
|    time_elapsed    | 16619    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2841.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0123   |
|    ent_coef        | 0.000135 |
|    ent_coef_loss   | 7.7      |
|    learning_rate   | 0.000911 |
|    n_updates       | 1064425  |
---------------------------------
New best mean reward!
Eval num_timesteps=900000, episode_reward=2704.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00599  |
|    ent_coef        | 0.000163 |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.00091  |
|    n_updates       | 1074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 52       |
|    time_elapsed    | 16996    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=2704.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0136   |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | 1.65     |
|    learning_rate   | 0.000909 |
|    n_updates       | 1084425  |
---------------------------------
Eval num_timesteps=920000, episode_reward=2705.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00335  |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.000908 |
|    n_updates       | 1094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 52       |
|    time_elapsed    | 17375    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=2706.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.000195 |
|    ent_coef_loss   | 10.2     |
|    learning_rate   | 0.000907 |
|    n_updates       | 1104425  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2705.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00268  |
|    ent_coef        | 0.000129 |
|    ent_coef_loss   | -23.4    |
|    learning_rate   | 0.000906 |
|    n_updates       | 1114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 188      |
|    fps             | 52       |
|    time_elapsed    | 17750    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2705.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0363   |
|    ent_coef        | 0.000107 |
|    ent_coef_loss   | 3.95     |
|    learning_rate   | 0.000905 |
|    n_updates       | 1124425  |
---------------------------------
Eval num_timesteps=960000, episode_reward=2704.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0171   |
|    ent_coef        | 0.000125 |
|    ent_coef_loss   | 32.5     |
|    learning_rate   | 0.000904 |
|    n_updates       | 1134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 192      |
|    fps             | 52       |
|    time_elapsed    | 18126    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=2699.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00284  |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | 3.5      |
|    learning_rate   | 0.000903 |
|    n_updates       | 1144425  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2700.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0111   |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | 1.76     |
|    learning_rate   | 0.000902 |
|    n_updates       | 1154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 196      |
|    fps             | 52       |
|    time_elapsed    | 18503    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 6.69e-05 |
|    ent_coef_loss   | 3.19     |
|    learning_rate   | 0.000901 |
|    n_updates       | 1164425  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=2700.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00683  |
|    ent_coef        | 8.93e-05 |
|    ent_coef_loss   | -4.75    |
|    learning_rate   | 0.0009   |
|    n_updates       | 1174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 200      |
|    fps             | 52       |
|    time_elapsed    | 18879    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=2699.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00438  |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000899 |
|    n_updates       | 1184425  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=2699.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00621  |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | -28.7    |
|    learning_rate   | 0.000898 |
|    n_updates       | 1194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 52       |
|    time_elapsed    | 19258    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=2700.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0123   |
|    ent_coef        | 7.97e-05 |
|    ent_coef_loss   | -4.11    |
|    learning_rate   | 0.000897 |
|    n_updates       | 1204425  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=2700.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00251  |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -15      |
|    learning_rate   | 0.000896 |
|    n_updates       | 1214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 52       |
|    time_elapsed    | 19637    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=2699.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00257  |
|    ent_coef        | 8.87e-05 |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.000895 |
|    n_updates       | 1224425  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=2700.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00773  |
|    ent_coef        | 7.57e-05 |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.000894 |
|    n_updates       | 1234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 212      |
|    fps             | 52       |
|    time_elapsed    | 20015    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=2699.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00578  |
|    ent_coef        | 6.66e-05 |
|    ent_coef_loss   | 7.76     |
|    learning_rate   | 0.000893 |
|    n_updates       | 1244425  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=2700.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00308  |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | 18.1     |
|    learning_rate   | 0.000892 |
|    n_updates       | 1254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 52       |
|    time_elapsed    | 20394    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | 8.3      |
|    learning_rate   | 0.000891 |
|    n_updates       | 1264425  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=2698.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 0.0001   |
|    ent_coef_loss   | 8.85     |
|    learning_rate   | 0.00089  |
|    n_updates       | 1274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 52       |
|    time_elapsed    | 20769    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=2698.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0112   |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.000889 |
|    n_updates       | 1284425  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=2698.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00321  |
|    ent_coef        | 0.000103 |
|    ent_coef_loss   | -20.5    |
|    learning_rate   | 0.000888 |
|    n_updates       | 1294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 224      |
|    fps             | 52       |
|    time_elapsed    | 21147    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2699.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00594  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 19       |
|    learning_rate   | 0.000887 |
|    n_updates       | 1304425  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=2699.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00738  |
|    ent_coef        | 0.000147 |
|    ent_coef_loss   | -5.64    |
|    learning_rate   | 0.000886 |
|    n_updates       | 1314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 52       |
|    time_elapsed    | 21524    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=2699.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00649  |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.000885 |
|    n_updates       | 1324425  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=2699.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00628  |
|    ent_coef        | 9.86e-05 |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000884 |
|    n_updates       | 1334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 52       |
|    time_elapsed    | 21901    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=2537.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00974  |
|    ent_coef        | 0.000144 |
|    ent_coef_loss   | -8.94    |
|    learning_rate   | 0.000883 |
|    n_updates       | 1344425  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=2698.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00481  |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | -3.09    |
|    learning_rate   | 0.000882 |
|    n_updates       | 1354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 52       |
|    time_elapsed    | 22279    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=2543.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00424  |
|    ent_coef        | 9.6e-05  |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.000881 |
|    n_updates       | 1364425  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=2700.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.003    |
|    ent_coef        | 7.98e-05 |
|    ent_coef_loss   | -9.1     |
|    learning_rate   | 0.00088  |
|    n_updates       | 1374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 240      |
|    fps             | 52       |
|    time_elapsed    | 22660    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=2698.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00152  |
|    ent_coef        | 0.000101 |
|    ent_coef_loss   | 4.23     |
|    learning_rate   | 0.000879 |
|    n_updates       | 1384425  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=2699.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00315  |
|    ent_coef        | 9.28e-05 |
|    ent_coef_loss   | 5.22     |
|    learning_rate   | 0.000878 |
|    n_updates       | 1394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 244      |
|    fps             | 52       |
|    time_elapsed    | 23035    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=2697.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00276  |
|    ent_coef        | 9.65e-05 |
|    ent_coef_loss   | 17.1     |
|    learning_rate   | 0.000877 |
|    n_updates       | 1404425  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=2698.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00315  |
|    ent_coef        | 0.000155 |
|    ent_coef_loss   | 8.03     |
|    learning_rate   | 0.000876 |
|    n_updates       | 1414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 248      |
|    fps             | 52       |
|    time_elapsed    | 23412    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=2699.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00491  |
|    ent_coef        | 0.000154 |
|    ent_coef_loss   | -8.81    |
|    learning_rate   | 0.000875 |
|    n_updates       | 1424425  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=2701.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.000145 |
|    ent_coef_loss   | 10.8     |
|    learning_rate   | 0.000874 |
|    n_updates       | 1434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 252      |
|    fps             | 52       |
|    time_elapsed    | 23788    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2706.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0015   |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -5.72    |
|    learning_rate   | 0.000873 |
|    n_updates       | 1444425  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=2704.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00582  |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.000872 |
|    n_updates       | 1454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 256      |
|    fps             | 52       |
|    time_elapsed    | 24167    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=2706.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0117   |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -4.76    |
|    learning_rate   | 0.000871 |
|    n_updates       | 1464425  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=2706.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00418  |
|    ent_coef        | 0.000193 |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.00087  |
|    n_updates       | 1474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 52       |
|    time_elapsed    | 24543    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=2700.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00233  |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | -0.717   |
|    learning_rate   | 0.000869 |
|    n_updates       | 1484425  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=2698.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.011    |
|    ent_coef        | 0.000259 |
|    ent_coef_loss   | 27.3     |
|    learning_rate   | 0.000868 |
|    n_updates       | 1494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 52       |
|    time_elapsed    | 24918    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=2699.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 0.000313 |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.000867 |
|    n_updates       | 1504425  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=2699.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0167   |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | 13.7     |
|    learning_rate   | 0.000866 |
|    n_updates       | 1514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 52       |
|    time_elapsed    | 25294    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=2700.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00452  |
|    ent_coef        | 0.0002   |
|    ent_coef_loss   | -5.51    |
|    learning_rate   | 0.000865 |
|    n_updates       | 1524425  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=2700.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 0.000177 |
|    ent_coef_loss   | -12.3    |
|    learning_rate   | 0.000864 |
|    n_updates       | 1534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 52       |
|    time_elapsed    | 25671    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=2698.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00248  |
|    ent_coef        | 0.000142 |
|    ent_coef_loss   | -17.1    |
|    learning_rate   | 0.000863 |
|    n_updates       | 1544425  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=2701.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00709  |
|    ent_coef        | 9.09e-05 |
|    ent_coef_loss   | -14.9    |
|    learning_rate   | 0.000862 |
|    n_updates       | 1554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 52       |
|    time_elapsed    | 26047    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=2701.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00534  |
|    ent_coef        | 0.000247 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.000861 |
|    n_updates       | 1564425  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=2698.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0055   |
|    ent_coef        | 0.000129 |
|    ent_coef_loss   | -7.71    |
|    learning_rate   | 0.00086  |
|    n_updates       | 1574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 52       |
|    time_elapsed    | 26428    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=2700.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00521  |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | 6.04     |
|    learning_rate   | 0.000859 |
|    n_updates       | 1584425  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=2700.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00335  |
|    ent_coef        | 8.41e-05 |
|    ent_coef_loss   | -8.88    |
|    learning_rate   | 0.000858 |
|    n_updates       | 1594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 52       |
|    time_elapsed    | 26806    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=2700.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00802  |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.000857 |
|    n_updates       | 1604425  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=2700.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00599  |
|    ent_coef        | 7.04e-05 |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.000856 |
|    n_updates       | 1614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 52       |
|    time_elapsed    | 27182    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2700.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00132  |
|    ent_coef        | 6.91e-05 |
|    ent_coef_loss   | 44.8     |
|    learning_rate   | 0.000855 |
|    n_updates       | 1624425  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2701.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00441  |
|    ent_coef        | 6.96e-05 |
|    ent_coef_loss   | 8.92     |
|    learning_rate   | 0.000854 |
|    n_updates       | 1634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 52       |
|    time_elapsed    | 27560    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2693.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.000681 |
|    ent_coef        | 9.49e-05 |
|    ent_coef_loss   | 6.79     |
|    learning_rate   | 0.000853 |
|    n_updates       | 1644425  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=2701.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00228  |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | -16.1    |
|    learning_rate   | 0.000852 |
|    n_updates       | 1654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 52       |
|    time_elapsed    | 27936    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2701.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0136   |
|    ent_coef        | 0.000101 |
|    ent_coef_loss   | 15       |
|    learning_rate   | 0.000851 |
|    n_updates       | 1664425  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=2704.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.00371  |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | 2.78     |
|    learning_rate   | 0.00085  |
|    n_updates       | 1674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 52       |
|    time_elapsed    | 28312    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=2702.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00252  |
|    ent_coef        | 0.000168 |
|    ent_coef_loss   | 60       |
|    learning_rate   | 0.000849 |
|    n_updates       | 1684425  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=2700.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0084   |
|    ent_coef        | 0.000136 |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.000848 |
|    n_updates       | 1694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 52       |
|    time_elapsed    | 28687    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=2700.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00429  |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -29.9    |
|    learning_rate   | 0.000847 |
|    n_updates       | 1704425  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=2698.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00347  |
|    ent_coef        | 9.38e-05 |
|    ent_coef_loss   | -6.98    |
|    learning_rate   | 0.000846 |
|    n_updates       | 1714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 52       |
|    time_elapsed    | 29063    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=2700.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0022   |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | 19       |
|    learning_rate   | 0.000845 |
|    n_updates       | 1724425  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=2702.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00279  |
|    ent_coef        | 0.000148 |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.000844 |
|    n_updates       | 1734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 52       |
|    time_elapsed    | 29439    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2700.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0121   |
|    ent_coef        | 0.000122 |
|    ent_coef_loss   | 18.3     |
|    learning_rate   | 0.000843 |
|    n_updates       | 1744425  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=2701.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0286   |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | 36       |
|    learning_rate   | 0.000842 |
|    n_updates       | 1754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 52       |
|    time_elapsed    | 29817    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=2701.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00403  |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -21.1    |
|    learning_rate   | 0.000841 |
|    n_updates       | 1764425  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=2701.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.0185   |
|    ent_coef        | 0.000109 |
|    ent_coef_loss   | 4.37     |
|    learning_rate   | 0.00084  |
|    n_updates       | 1774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 52       |
|    time_elapsed    | 30194    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=2699.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0049   |
|    ent_coef        | 0.000112 |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000839 |
|    n_updates       | 1784425  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=2702.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0138   |
|    ent_coef        | 0.000166 |
|    ent_coef_loss   | -22.4    |
|    learning_rate   | 0.000838 |
|    n_updates       | 1794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 52       |
|    time_elapsed    | 30587    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2704.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00186  |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -24.9    |
|    learning_rate   | 0.000837 |
|    n_updates       | 1804425  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=2705.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0278   |
|    ent_coef        | 8.9e-05  |
|    ent_coef_loss   | -23.8    |
|    learning_rate   | 0.000836 |
|    n_updates       | 1814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 52       |
|    time_elapsed    | 31070    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=2703.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00582  |
|    ent_coef        | 0.000208 |
|    ent_coef_loss   | 2.94     |
|    learning_rate   | 0.000835 |
|    n_updates       | 1824425  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=2513.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.0154   |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | -3.3     |
|    learning_rate   | 0.000834 |
|    n_updates       | 1834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 52       |
|    time_elapsed    | 31586    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=2699.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.0079   |
|    ent_coef        | 0.000401 |
|    ent_coef_loss   | -5.29    |
|    learning_rate   | 0.000833 |
|    n_updates       | 1844425  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2700.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0174   |
|    ent_coef        | 0.000363 |
|    ent_coef_loss   | -7.76    |
|    learning_rate   | 0.000832 |
|    n_updates       | 1854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 52       |
|    time_elapsed    | 32081    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=2707.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0119   |
|    ent_coef        | 0.000267 |
|    ent_coef_loss   | -5.52    |
|    learning_rate   | 0.000831 |
|    n_updates       | 1864425  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=2693.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.00591  |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | -6.61    |
|    learning_rate   | 0.00083  |
|    n_updates       | 1874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 52       |
|    time_elapsed    | 32577    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=2693.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.00951  |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | -7.42    |
|    learning_rate   | 0.000829 |
|    n_updates       | 1884425  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=2689.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.0105   |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | -4.46    |
|    learning_rate   | 0.000828 |
|    n_updates       | 1894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 52       |
|    time_elapsed    | 33072    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=2692.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.00761  |
|    ent_coef        | 0.000174 |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.000827 |
|    n_updates       | 1904425  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=2689.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0069   |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | 4.44     |
|    learning_rate   | 0.000826 |
|    n_updates       | 1914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 51       |
|    time_elapsed    | 33568    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=2515.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.014    |
|    ent_coef        | 0.000168 |
|    ent_coef_loss   | 5.01     |
|    learning_rate   | 0.000825 |
|    n_updates       | 1924425  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=2515.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00165  |
|    ent_coef        | 0.000147 |
|    ent_coef_loss   | -21.1    |
|    learning_rate   | 0.000824 |
|    n_updates       | 1934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 51       |
|    time_elapsed    | 34065    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=2515.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00321  |
|    ent_coef        | 0.000175 |
|    ent_coef_loss   | 0.249    |
|    learning_rate   | 0.000823 |
|    n_updates       | 1944425  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2691.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00399  |
|    ent_coef        | 0.000139 |
|    ent_coef_loss   | -2.88    |
|    learning_rate   | 0.000822 |
|    n_updates       | 1954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 51       |
|    time_elapsed    | 34562    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=2689.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0116   |
|    ent_coef        | 0.000138 |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.000821 |
|    n_updates       | 1964425  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2689.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000143 |
|    ent_coef_loss   | -18.8    |
|    learning_rate   | 0.00082  |
|    n_updates       | 1974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 51       |
|    time_elapsed    | 35058    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2689.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.022    |
|    ent_coef        | 0.000174 |
|    ent_coef_loss   | -0.836   |
|    learning_rate   | 0.000819 |
|    n_updates       | 1984425  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=2512.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00285  |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | -7.84    |
|    learning_rate   | 0.000818 |
|    n_updates       | 1994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 51       |
|    time_elapsed    | 35556    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=2699.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0067   |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000817 |
|    n_updates       | 2004425  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=2690.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00398  |
|    ent_coef        | 0.000125 |
|    ent_coef_loss   | -7.82    |
|    learning_rate   | 0.000816 |
|    n_updates       | 2014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 51       |
|    time_elapsed    | 36054    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=2689.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0193   |
|    ent_coef        | 0.000132 |
|    ent_coef_loss   | -7.67    |
|    learning_rate   | 0.000815 |
|    n_updates       | 2024425  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=2702.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0045   |
|    ent_coef        | 0.00013  |
|    ent_coef_loss   | 6.02     |
|    learning_rate   | 0.000814 |
|    n_updates       | 2034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 50       |
|    time_elapsed    | 36554    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=2688.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00334  |
|    ent_coef        | 0.000175 |
|    ent_coef_loss   | -0.234   |
|    learning_rate   | 0.000813 |
|    n_updates       | 2044425  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=2686.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00128  |
|    ent_coef        | 0.000181 |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.000812 |
|    n_updates       | 2054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 50       |
|    time_elapsed    | 37052    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=2686.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0334   |
|    ent_coef        | 0.00016  |
|    ent_coef_loss   | 1.48     |
|    learning_rate   | 0.000811 |
|    n_updates       | 2064425  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=2686.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00416  |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | 16.6     |
|    learning_rate   | 0.00081  |
|    n_updates       | 2074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 50       |
|    time_elapsed    | 37550    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=2686.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00373  |
|    ent_coef        | 0.000176 |
|    ent_coef_loss   | 8.86     |
|    learning_rate   | 0.000809 |
|    n_updates       | 2084425  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=2687.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00754  |
|    ent_coef        | 0.00019  |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.000808 |
|    n_updates       | 2094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 50       |
|    time_elapsed    | 38050    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=2695.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00293  |
|    ent_coef        | 0.000171 |
|    ent_coef_loss   | -9.63    |
|    learning_rate   | 0.000807 |
|    n_updates       | 2104425  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=2693.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00677  |
|    ent_coef        | 0.000159 |
|    ent_coef_loss   | -2.97    |
|    learning_rate   | 0.000806 |
|    n_updates       | 2114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 50       |
|    time_elapsed    | 38550    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=2703.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0055   |
|    ent_coef        | 0.000111 |
|    ent_coef_loss   | -5.46    |
|    learning_rate   | 0.000805 |
|    n_updates       | 2124425  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2706.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00513  |
|    ent_coef        | 9.76e-05 |
|    ent_coef_loss   | -9.46    |
|    learning_rate   | 0.000804 |
|    n_updates       | 2134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 50       |
|    time_elapsed    | 39049    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=2705.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00442  |
|    ent_coef        | 0.000124 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000803 |
|    n_updates       | 2144425  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2702.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00207  |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 16.8     |
|    learning_rate   | 0.000802 |
|    n_updates       | 2154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 50       |
|    time_elapsed    | 39547    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=2695.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00312  |
|    ent_coef        | 9.61e-05 |
|    ent_coef_loss   | -16.2    |
|    learning_rate   | 0.000801 |
|    n_updates       | 2164425  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=2707.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0076   |
|    ent_coef        | 0.000107 |
|    ent_coef_loss   | -8.23    |
|    learning_rate   | 0.0008   |
|    n_updates       | 2174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 49       |
|    time_elapsed    | 40045    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=2706.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00646  |
|    ent_coef        | 8.81e-05 |
|    ent_coef_loss   | 28.6     |
|    learning_rate   | 0.000799 |
|    n_updates       | 2184425  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=2700.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0092   |
|    ent_coef        | 9.76e-05 |
|    ent_coef_loss   | 6.5      |
|    learning_rate   | 0.000798 |
|    n_updates       | 2194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 49       |
|    time_elapsed    | 40542    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2699.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00482  |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | 21.1     |
|    learning_rate   | 0.000797 |
|    n_updates       | 2204425  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=2698.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00485  |
|    ent_coef        | 0.000118 |
|    ent_coef_loss   | -3.96    |
|    learning_rate   | 0.000796 |
|    n_updates       | 2214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 49       |
|    time_elapsed    | 41040    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2696.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00281  |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | -10.2    |
|    learning_rate   | 0.000795 |
|    n_updates       | 2224425  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=2693.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00215  |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.000794 |
|    n_updates       | 2234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 49       |
|    time_elapsed    | 41537    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=2688.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00969  |
|    ent_coef        | 9.9e-05  |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.000793 |
|    n_updates       | 2244425  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=2701.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0032   |
|    ent_coef        | 8.2e-05  |
|    ent_coef_loss   | -31      |
|    learning_rate   | 0.000792 |
|    n_updates       | 2254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 416      |
|    fps             | 49       |
|    time_elapsed    | 42036    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=2693.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00359  |
|    ent_coef        | 8.46e-05 |
|    ent_coef_loss   | -23.9    |
|    learning_rate   | 0.000791 |
|    n_updates       | 2264425  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=3014.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00224  |
|    ent_coef        | 9.89e-05 |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.00079  |
|    n_updates       | 2274425  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 49       |
|    time_elapsed    | 42533    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2689.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00344  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.000789 |
|    n_updates       | 2284425  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=2695.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00583  |
|    ent_coef        | 0.000125 |
|    ent_coef_loss   | 10.9     |
|    learning_rate   | 0.000788 |
|    n_updates       | 2294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 49       |
|    time_elapsed    | 43028    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=2696.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0501   |
|    ent_coef        | 0.000156 |
|    ent_coef_loss   | -9.22    |
|    learning_rate   | 0.000787 |
|    n_updates       | 2304425  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=2694.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00485  |
|    ent_coef        | 0.000213 |
|    ent_coef_loss   | 14.4     |
|    learning_rate   | 0.000786 |
|    n_updates       | 2314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 49       |
|    time_elapsed    | 43531    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=2696.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00346  |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | 3.47     |
|    learning_rate   | 0.000785 |
|    n_updates       | 2324425  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=2694.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00347  |
|    ent_coef        | 0.000188 |
|    ent_coef_loss   | 7.75     |
|    learning_rate   | 0.000784 |
|    n_updates       | 2334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 49       |
|    time_elapsed    | 44035    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=2698.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00138  |
|    ent_coef        | 0.000144 |
|    ent_coef_loss   | -0.146   |
|    learning_rate   | 0.000783 |
|    n_updates       | 2344425  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=2700.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00417  |
|    ent_coef        | 0.000149 |
|    ent_coef_loss   | 16.1     |
|    learning_rate   | 0.000782 |
|    n_updates       | 2354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 48       |
|    time_elapsed    | 44530    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=2693.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0032   |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 26       |
|    learning_rate   | 0.000781 |
|    n_updates       | 2364425  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=2697.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00258  |
|    ent_coef        | 0.000137 |
|    ent_coef_loss   | -8.7     |
|    learning_rate   | 0.00078  |
|    n_updates       | 2374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 48       |
|    time_elapsed    | 45023    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=2693.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00234  |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | -7.99    |
|    learning_rate   | 0.000779 |
|    n_updates       | 2384425  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=2694.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00269  |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | 0.544    |
|    learning_rate   | 0.000778 |
|    n_updates       | 2394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 48       |
|    time_elapsed    | 45517    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2696.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00187  |
|    ent_coef        | 0.000102 |
|    ent_coef_loss   | -9.99    |
|    learning_rate   | 0.000777 |
|    n_updates       | 2404425  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2699.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00285  |
|    ent_coef        | 0.000118 |
|    ent_coef_loss   | 18.8     |
|    learning_rate   | 0.000776 |
|    n_updates       | 2414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 48       |
|    time_elapsed    | 46012    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2700.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00195  |
|    ent_coef        | 0.000113 |
|    ent_coef_loss   | -0.77    |
|    learning_rate   | 0.000775 |
|    n_updates       | 2424425  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=2700.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000445 |
|    ent_coef        | 8.66e-05 |
|    ent_coef_loss   | 12.3     |
|    learning_rate   | 0.000774 |
|    n_updates       | 2434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 48       |
|    time_elapsed    | 46507    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=2700.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00125  |
|    ent_coef        | 7.06e-05 |
|    ent_coef_loss   | -32.9    |
|    learning_rate   | 0.000773 |
|    n_updates       | 2444425  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=2702.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -32.8    |
|    learning_rate   | 0.000772 |
|    n_updates       | 2454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 48       |
|    time_elapsed    | 47000    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=2702.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.016    |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -22.8    |
|    learning_rate   | 0.000771 |
|    n_updates       | 2464425  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=2701.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00564  |
|    ent_coef        | 9.29e-05 |
|    ent_coef_loss   | 17.3     |
|    learning_rate   | 0.00077  |
|    n_updates       | 2474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 48       |
|    time_elapsed    | 47494    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=2704.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.008    |
|    ent_coef        | 8.2e-05  |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.000769 |
|    n_updates       | 2484425  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=2701.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00777  |
|    ent_coef        | 6.14e-05 |
|    ent_coef_loss   | -5.44    |
|    learning_rate   | 0.000768 |
|    n_updates       | 2494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 48       |
|    time_elapsed    | 47988    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=2700.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00625  |
|    ent_coef        | 8.73e-05 |
|    ent_coef_loss   | -36.5    |
|    learning_rate   | 0.000767 |
|    n_updates       | 2504425  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=2698.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00398  |
|    ent_coef        | 4.69e-05 |
|    ent_coef_loss   | -24.7    |
|    learning_rate   | 0.000766 |
|    n_updates       | 2514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 48       |
|    time_elapsed    | 48484    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=2700.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0174   |
|    ent_coef        | 5.17e-05 |
|    ent_coef_loss   | -9.2     |
|    learning_rate   | 0.000765 |
|    n_updates       | 2524425  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0022   |
|    ent_coef        | 7.31e-05 |
|    ent_coef_loss   | 33       |
|    learning_rate   | 0.000764 |
|    n_updates       | 2534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 48       |
|    time_elapsed    | 48978    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=2696.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0574   |
|    ent_coef        | 5.38e-05 |
|    ent_coef_loss   | -4.09    |
|    learning_rate   | 0.000763 |
|    n_updates       | 2544425  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=2700.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0016   |
|    ent_coef        | 4.78e-05 |
|    ent_coef_loss   | 9.45     |
|    learning_rate   | 0.000762 |
|    n_updates       | 2554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 48       |
|    time_elapsed    | 49475    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=2700.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00863  |
|    ent_coef        | 8.84e-05 |
|    ent_coef_loss   | -11.1    |
|    learning_rate   | 0.000761 |
|    n_updates       | 2564425  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2702.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00185  |
|    ent_coef        | 2.92e-05 |
|    ent_coef_loss   | 37.1     |
|    learning_rate   | 0.00076  |
|    n_updates       | 2574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 480      |
|    fps             | 48       |
|    time_elapsed    | 49971    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=2700.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00522  |
|    ent_coef        | 6.63e-05 |
|    ent_coef_loss   | 4.16     |
|    learning_rate   | 0.000759 |
|    n_updates       | 2584425  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=2703.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00991  |
|    ent_coef        | 6.01e-05 |
|    ent_coef_loss   | 3.19     |
|    learning_rate   | 0.000758 |
|    n_updates       | 2594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 48       |
|    time_elapsed    | 50410    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2703.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00725  |
|    ent_coef        | 5.26e-05 |
|    ent_coef_loss   | 1.9      |
|    learning_rate   | 0.000757 |
|    n_updates       | 2604425  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=2700.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0055   |
|    ent_coef        | 4.98e-05 |
|    ent_coef_loss   | 5.31     |
|    learning_rate   | 0.000756 |
|    n_updates       | 2614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 47       |
|    time_elapsed    | 50835    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2700.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00211  |
|    ent_coef        | 5.86e-05 |
|    ent_coef_loss   | 9.93     |
|    learning_rate   | 0.000755 |
|    n_updates       | 2624425  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=2700.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00459  |
|    ent_coef        | 5.43e-05 |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.000754 |
|    n_updates       | 2634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 48       |
|    time_elapsed    | 51247    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=2701.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00106  |
|    ent_coef        | 6.21e-05 |
|    ent_coef_loss   | -6.51    |
|    learning_rate   | 0.000753 |
|    n_updates       | 2644425  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=2698.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 6.72e-05 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.000752 |
|    n_updates       | 2654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 496      |
|    fps             | 48       |
|    time_elapsed    | 51660    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=2696.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0315   |
|    ent_coef        | 5.63e-05 |
|    ent_coef_loss   | -31.3    |
|    learning_rate   | 0.000751 |
|    n_updates       | 2664425  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2701.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00197  |
|    ent_coef        | 5.88e-05 |
|    ent_coef_loss   | 48       |
|    learning_rate   | 0.00075  |
|    n_updates       | 2674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 48       |
|    time_elapsed    | 52073    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=2699.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.000721 |
|    ent_coef        | 3.55e-05 |
|    ent_coef_loss   | 7.7      |
|    learning_rate   | 0.000749 |
|    n_updates       | 2684425  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=2698.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0035   |
|    ent_coef        | 4.64e-05 |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.000748 |
|    n_updates       | 2694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 48       |
|    time_elapsed    | 52486    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=2700.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00181  |
|    ent_coef        | 7.66e-05 |
|    ent_coef_loss   | 29.1     |
|    learning_rate   | 0.000747 |
|    n_updates       | 2704425  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=2700.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00102  |
|    ent_coef        | 5.95e-05 |
|    ent_coef_loss   | -33.2    |
|    learning_rate   | 0.000746 |
|    n_updates       | 2714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 48       |
|    time_elapsed    | 52899    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=2701.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00694  |
|    ent_coef        | 5.25e-05 |
|    ent_coef_loss   | 54.4     |
|    learning_rate   | 0.000745 |
|    n_updates       | 2724425  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=2702.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00275  |
|    ent_coef        | 4.4e-05  |
|    ent_coef_loss   | -2.94    |
|    learning_rate   | 0.000744 |
|    n_updates       | 2734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 48       |
|    time_elapsed    | 53310    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=2703.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00749  |
|    ent_coef        | 4.36e-05 |
|    ent_coef_loss   | -22.2    |
|    learning_rate   | 0.000743 |
|    n_updates       | 2744425  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2700.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.012    |
|    ent_coef        | 5.14e-05 |
|    ent_coef_loss   | -28.3    |
|    learning_rate   | 0.000742 |
|    n_updates       | 2754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 48       |
|    time_elapsed    | 53723    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=2702.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00185  |
|    ent_coef        | 3.74e-05 |
|    ent_coef_loss   | 3.07     |
|    learning_rate   | 0.000741 |
|    n_updates       | 2764425  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2702.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00657  |
|    ent_coef        | 5.24e-05 |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.00074  |
|    n_updates       | 2774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 48       |
|    time_elapsed    | 54136    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=2698.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00629  |
|    ent_coef        | 4.72e-05 |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000739 |
|    n_updates       | 2784425  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=2526.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00158  |
|    ent_coef        | 6.75e-05 |
|    ent_coef_loss   | 48       |
|    learning_rate   | 0.000738 |
|    n_updates       | 2794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 524      |
|    fps             | 48       |
|    time_elapsed    | 54549    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=2719.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 4.02e-05 |
|    ent_coef_loss   | -27.9    |
|    learning_rate   | 0.000737 |
|    n_updates       | 2804425  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=2703.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0242   |
|    ent_coef        | 5.5e-05  |
|    ent_coef_loss   | -5.8     |
|    learning_rate   | 0.000736 |
|    n_updates       | 2814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 528      |
|    fps             | 48       |
|    time_elapsed    | 54962    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2703.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00407  |
|    ent_coef        | 7.45e-05 |
|    ent_coef_loss   | 48.8     |
|    learning_rate   | 0.000735 |
|    n_updates       | 2824425  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2704.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.044    |
|    ent_coef        | 6.09e-05 |
|    ent_coef_loss   | 8.63     |
|    learning_rate   | 0.000734 |
|    n_updates       | 2834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 532      |
|    fps             | 48       |
|    time_elapsed    | 55374    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=2700.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0119   |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | -25.2    |
|    learning_rate   | 0.000733 |
|    n_updates       | 2844425  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=2689.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0031   |
|    ent_coef        | 7.05e-05 |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.000732 |
|    n_updates       | 2854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 536      |
|    fps             | 48       |
|    time_elapsed    | 55787    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=2701.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00478  |
|    ent_coef        | 6.67e-05 |
|    ent_coef_loss   | 27.9     |
|    learning_rate   | 0.000731 |
|    n_updates       | 2864425  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=2701.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0128   |
|    ent_coef        | 3.27e-05 |
|    ent_coef_loss   | 70.8     |
|    learning_rate   | 0.00073  |
|    n_updates       | 2874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 540      |
|    fps             | 48       |
|    time_elapsed    | 56200    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=2704.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00227  |
|    ent_coef        | 6.18e-05 |
|    ent_coef_loss   | 9.58     |
|    learning_rate   | 0.000729 |
|    n_updates       | 2884425  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=2699.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0117   |
|    ent_coef        | 6.27e-05 |
|    ent_coef_loss   | 14.4     |
|    learning_rate   | 0.000728 |
|    n_updates       | 2894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 544      |
|    fps             | 48       |
|    time_elapsed    | 56614    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=2697.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00325  |
|    ent_coef        | 2.74e-05 |
|    ent_coef_loss   | -59.4    |
|    learning_rate   | 0.000727 |
|    n_updates       | 2904425  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=2701.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00145  |
|    ent_coef        | 3.8e-05  |
|    ent_coef_loss   | 43.9     |
|    learning_rate   | 0.000726 |
|    n_updates       | 2914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 548      |
|    fps             | 48       |
|    time_elapsed    | 57026    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=2701.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0107   |
|    ent_coef        | 3.4e-05  |
|    ent_coef_loss   | 34.4     |
|    learning_rate   | 0.000725 |
|    n_updates       | 2924425  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=2698.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00385  |
|    ent_coef        | 3.59e-05 |
|    ent_coef_loss   | -25.2    |
|    learning_rate   | 0.000724 |
|    n_updates       | 2934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 552      |
|    fps             | 48       |
|    time_elapsed    | 57439    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00515  |
|    ent_coef        | 2.84e-05 |
|    ent_coef_loss   | 13.3     |
|    learning_rate   | 0.000723 |
|    n_updates       | 2944425  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2701.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0051   |
|    ent_coef        | 5.58e-05 |
|    ent_coef_loss   | -5.06    |
|    learning_rate   | 0.000722 |
|    n_updates       | 2954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 556      |
|    fps             | 48       |
|    time_elapsed    | 57852    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=2699.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0112   |
|    ent_coef        | 6.43e-05 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.000721 |
|    n_updates       | 2964425  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2695.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00223  |
|    ent_coef        | 7.23e-05 |
|    ent_coef_loss   | 37.7     |
|    learning_rate   | 0.00072  |
|    n_updates       | 2974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 560      |
|    fps             | 48       |
|    time_elapsed    | 58265    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=2697.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 5.69e-05 |
|    ent_coef_loss   | -31      |
|    learning_rate   | 0.000719 |
|    n_updates       | 2984425  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=2697.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 5.7e-05  |
|    ent_coef_loss   | -49.9    |
|    learning_rate   | 0.000718 |
|    n_updates       | 2994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 564      |
|    fps             | 48       |
|    time_elapsed    | 58679    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=2698.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0175   |
|    ent_coef        | 4.06e-05 |
|    ent_coef_loss   | 13.3     |
|    learning_rate   | 0.000717 |
|    n_updates       | 3004425  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=2693.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0122   |
|    ent_coef        | 4.46e-05 |
|    ent_coef_loss   | 16       |
|    learning_rate   | 0.000716 |
|    n_updates       | 3014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 568      |
|    fps             | 48       |
|    time_elapsed    | 59092    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=2701.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00915  |
|    ent_coef        | 4.43e-05 |
|    ent_coef_loss   | 4.93     |
|    learning_rate   | 0.000715 |
|    n_updates       | 3024425  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=2694.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00634  |
|    ent_coef        | 5.96e-05 |
|    ent_coef_loss   | 23.9     |
|    learning_rate   | 0.000714 |
|    n_updates       | 3034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 572      |
|    fps             | 48       |
|    time_elapsed    | 59505    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=2690.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.000101 |
|    ent_coef_loss   | 2.51     |
|    learning_rate   | 0.000713 |
|    n_updates       | 3044425  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=2694.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00329  |
|    ent_coef        | 5.95e-05 |
|    ent_coef_loss   | 6.38     |
|    learning_rate   | 0.000712 |
|    n_updates       | 3054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 576      |
|    fps             | 48       |
|    time_elapsed    | 59919    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=2703.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00594  |
|    ent_coef        | 6.64e-05 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.000711 |
|    n_updates       | 3064425  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=2699.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0012   |
|    ent_coef        | 6.04e-05 |
|    ent_coef_loss   | -21.1    |
|    learning_rate   | 0.00071  |
|    n_updates       | 3074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 580      |
|    fps             | 48       |
|    time_elapsed    | 60332    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=2694.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 6.02e-05 |
|    ent_coef_loss   | 19.6     |
|    learning_rate   | 0.000709 |
|    n_updates       | 3084425  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=2702.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00221  |
|    ent_coef        | 6.8e-05  |
|    ent_coef_loss   | 7.26     |
|    learning_rate   | 0.000708 |
|    n_updates       | 3094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 584      |
|    fps             | 48       |
|    time_elapsed    | 60746    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00372  |
|    ent_coef        | 4.53e-05 |
|    ent_coef_loss   | 11.4     |
|    learning_rate   | 0.000707 |
|    n_updates       | 3104425  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=2704.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 7.92e-05 |
|    ent_coef_loss   | -9.18    |
|    learning_rate   | 0.000706 |
|    n_updates       | 3114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 588      |
|    fps             | 48       |
|    time_elapsed    | 61159    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=2700.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00259  |
|    ent_coef        | 5.59e-05 |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.000705 |
|    n_updates       | 3124425  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=2701.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00217  |
|    ent_coef        | 6.55e-05 |
|    ent_coef_loss   | -3.63    |
|    learning_rate   | 0.000704 |
|    n_updates       | 3134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 592      |
|    fps             | 48       |
|    time_elapsed    | 61573    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=2698.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0037   |
|    ent_coef        | 8.07e-05 |
|    ent_coef_loss   | -0.962   |
|    learning_rate   | 0.000703 |
|    n_updates       | 3144425  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=2701.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00443  |
|    ent_coef        | 8.35e-05 |
|    ent_coef_loss   | -21.8    |
|    learning_rate   | 0.000702 |
|    n_updates       | 3154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 596      |
|    fps             | 48       |
|    time_elapsed    | 61987    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=2701.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0066   |
|    ent_coef        | 6.99e-05 |
|    ent_coef_loss   | -25.4    |
|    learning_rate   | 0.000701 |
|    n_updates       | 3164425  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=2700.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00096  |
|    ent_coef        | 4.36e-05 |
|    ent_coef_loss   | -28.7    |
|    learning_rate   | 0.0007   |
|    n_updates       | 3174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 600      |
|    fps             | 48       |
|    time_elapsed    | 62402    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=2699.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00541  |
|    ent_coef        | 3.01e-05 |
|    ent_coef_loss   | 42.4     |
|    learning_rate   | 0.000699 |
|    n_updates       | 3184425  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=2700.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00446  |
|    ent_coef        | 5.55e-05 |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.000698 |
|    n_updates       | 3194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 604      |
|    fps             | 48       |
|    time_elapsed    | 62816    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=2700.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00194  |
|    ent_coef        | 3.72e-05 |
|    ent_coef_loss   | 82.8     |
|    learning_rate   | 0.000697 |
|    n_updates       | 3204425  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=2701.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00572  |
|    ent_coef        | 4.23e-05 |
|    ent_coef_loss   | 22.6     |
|    learning_rate   | 0.000696 |
|    n_updates       | 3214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 608      |
|    fps             | 48       |
|    time_elapsed    | 63229    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=2700.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00141  |
|    ent_coef        | 4.44e-05 |
|    ent_coef_loss   | 8.5      |
|    learning_rate   | 0.000695 |
|    n_updates       | 3224425  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=2700.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0021   |
|    ent_coef        | 4.83e-05 |
|    ent_coef_loss   | 26.8     |
|    learning_rate   | 0.000694 |
|    n_updates       | 3234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 612      |
|    fps             | 48       |
|    time_elapsed    | 63644    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=2699.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 5.58e-05 |
|    ent_coef_loss   | -20.7    |
|    learning_rate   | 0.000693 |
|    n_updates       | 3244425  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=2701.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00162  |
|    ent_coef        | 6.77e-05 |
|    ent_coef_loss   | 6.61     |
|    learning_rate   | 0.000692 |
|    n_updates       | 3254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 616      |
|    fps             | 48       |
|    time_elapsed    | 64059    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=2700.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000905 |
|    ent_coef        | 3.59e-05 |
|    ent_coef_loss   | -5.36    |
|    learning_rate   | 0.000691 |
|    n_updates       | 3264425  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=2695.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.000353 |
|    ent_coef        | 3.73e-05 |
|    ent_coef_loss   | 3.77     |
|    learning_rate   | 0.00069  |
|    n_updates       | 3274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 620      |
|    fps             | 48       |
|    time_elapsed    | 64474    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=2701.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00601  |
|    ent_coef        | 2.39e-05 |
|    ent_coef_loss   | -29      |
|    learning_rate   | 0.000689 |
|    n_updates       | 3284425  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=2705.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00278  |
|    ent_coef        | 3.68e-05 |
|    ent_coef_loss   | 17.5     |
|    learning_rate   | 0.000688 |
|    n_updates       | 3294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 624      |
|    fps             | 48       |
|    time_elapsed    | 64889    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=2703.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00272  |
|    ent_coef        | 4.65e-05 |
|    ent_coef_loss   | -56.1    |
|    learning_rate   | 0.000687 |
|    n_updates       | 3304425  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=2705.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00373  |
|    ent_coef        | 3.37e-05 |
|    ent_coef_loss   | 49.7     |
|    learning_rate   | 0.000686 |
|    n_updates       | 3314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 628      |
|    fps             | 48       |
|    time_elapsed    | 65304    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=2704.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00215  |
|    ent_coef        | 3.56e-05 |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000685 |
|    n_updates       | 3324425  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=2700.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00552  |
|    ent_coef        | 4.08e-05 |
|    ent_coef_loss   | 27.7     |
|    learning_rate   | 0.000684 |
|    n_updates       | 3334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 632      |
|    fps             | 48       |
|    time_elapsed    | 65719    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=2697.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00984  |
|    ent_coef        | 4.5e-05  |
|    ent_coef_loss   | -36      |
|    learning_rate   | 0.000683 |
|    n_updates       | 3344425  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=2700.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00228  |
|    ent_coef        | 5.12e-05 |
|    ent_coef_loss   | -22      |
|    learning_rate   | 0.000682 |
|    n_updates       | 3354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 636      |
|    fps             | 48       |
|    time_elapsed    | 66135    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=2699.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0136   |
|    ent_coef        | 5.28e-05 |
|    ent_coef_loss   | -16.3    |
|    learning_rate   | 0.000681 |
|    n_updates       | 3364425  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=2698.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00016  |
|    ent_coef        | 3.89e-05 |
|    ent_coef_loss   | -39.8    |
|    learning_rate   | 0.00068  |
|    n_updates       | 3374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 640      |
|    fps             | 48       |
|    time_elapsed    | 66551    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=2691.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00158  |
|    ent_coef        | 4.25e-05 |
|    ent_coef_loss   | 6.44     |
|    learning_rate   | 0.000679 |
|    n_updates       | 3384425  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=2702.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00423  |
|    ent_coef        | 5.25e-05 |
|    ent_coef_loss   | -28.6    |
|    learning_rate   | 0.000678 |
|    n_updates       | 3394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 644      |
|    fps             | 48       |
|    time_elapsed    | 67062    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=2700.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00332  |
|    ent_coef        | 3.5e-05  |
|    ent_coef_loss   | 14.3     |
|    learning_rate   | 0.000677 |
|    n_updates       | 3404425  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=2705.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 4.45e-05 |
|    ent_coef_loss   | 48.9     |
|    learning_rate   | 0.000676 |
|    n_updates       | 3414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 648      |
|    fps             | 48       |
|    time_elapsed    | 67478    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=2706.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000499 |
|    ent_coef        | 4.59e-05 |
|    ent_coef_loss   | 14.2     |
|    learning_rate   | 0.000675 |
|    n_updates       | 3424425  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=2703.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00198  |
|    ent_coef        | 2.68e-05 |
|    ent_coef_loss   | 9.42     |
|    learning_rate   | 0.000674 |
|    n_updates       | 3434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 652      |
|    fps             | 48       |
|    time_elapsed    | 67882    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=2706.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000318 |
|    ent_coef        | 2.03e-05 |
|    ent_coef_loss   | -42.1    |
|    learning_rate   | 0.000673 |
|    n_updates       | 3444425  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=2699.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00245  |
|    ent_coef        | 3.58e-05 |
|    ent_coef_loss   | 24.1     |
|    learning_rate   | 0.000672 |
|    n_updates       | 3454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 656      |
|    fps             | 48       |
|    time_elapsed    | 68284    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=2699.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00398  |
|    ent_coef        | 4.64e-05 |
|    ent_coef_loss   | 17.1     |
|    learning_rate   | 0.000671 |
|    n_updates       | 3464425  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=2699.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000477 |
|    ent_coef        | 4.85e-05 |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00067  |
|    n_updates       | 3474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 660      |
|    fps             | 48       |
|    time_elapsed    | 68689    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000772 |
|    ent_coef        | 4.13e-05 |
|    ent_coef_loss   | 20.2     |
|    learning_rate   | 0.000669 |
|    n_updates       | 3484425  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=2700.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.000724 |
|    ent_coef        | 5.46e-05 |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000668 |
|    n_updates       | 3494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 664      |
|    fps             | 48       |
|    time_elapsed    | 69093    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=2699.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00265  |
|    ent_coef        | 3.39e-05 |
|    ent_coef_loss   | -27.3    |
|    learning_rate   | 0.000667 |
|    n_updates       | 3504425  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=2698.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0017   |
|    ent_coef        | 3.38e-05 |
|    ent_coef_loss   | 23.2     |
|    learning_rate   | 0.000666 |
|    n_updates       | 3514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 668      |
|    fps             | 48       |
|    time_elapsed    | 69568    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=2698.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0024   |
|    ent_coef        | 2.39e-05 |
|    ent_coef_loss   | -55.1    |
|    learning_rate   | 0.000665 |
|    n_updates       | 3524425  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=2700.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000336 |
|    ent_coef        | 2.86e-05 |
|    ent_coef_loss   | -29.6    |
|    learning_rate   | 0.000664 |
|    n_updates       | 3534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 672      |
|    fps             | 47       |
|    time_elapsed    | 70066    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=2699.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00324  |
|    ent_coef        | 1.69e-05 |
|    ent_coef_loss   | 24.9     |
|    learning_rate   | 0.000663 |
|    n_updates       | 3544425  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=2699.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00125  |
|    ent_coef        | 1.48e-05 |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.000662 |
|    n_updates       | 3554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 676      |
|    fps             | 47       |
|    time_elapsed    | 70563    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=2699.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00526  |
|    ent_coef        | 1.88e-05 |
|    ent_coef_loss   | -37.4    |
|    learning_rate   | 0.000661 |
|    n_updates       | 3564425  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00318  |
|    ent_coef        | 2.18e-05 |
|    ent_coef_loss   | -9.65    |
|    learning_rate   | 0.00066  |
|    n_updates       | 3574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 680      |
|    fps             | 47       |
|    time_elapsed    | 71058    |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00316  |
|    ent_coef        | 2.19e-05 |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000659 |
|    n_updates       | 3584425  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=2698.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00316  |
|    ent_coef        | 1.51e-05 |
|    ent_coef_loss   | -40.5    |
|    learning_rate   | 0.000658 |
|    n_updates       | 3594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 684      |
|    fps             | 47       |
|    time_elapsed    | 71553    |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=2699.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00342  |
|    ent_coef        | 3.33e-05 |
|    ent_coef_loss   | -41.7    |
|    learning_rate   | 0.000657 |
|    n_updates       | 3604425  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=2699.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3440000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00406  |
|    ent_coef        | 1.82e-05 |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.000656 |
|    n_updates       | 3614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 688      |
|    fps             | 47       |
|    time_elapsed    | 72048    |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=2698.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000805 |
|    ent_coef        | 3.33e-05 |
|    ent_coef_loss   | 11.9     |
|    learning_rate   | 0.000655 |
|    n_updates       | 3624425  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00103  |
|    ent_coef        | 3.78e-05 |
|    ent_coef_loss   | -4.29    |
|    learning_rate   | 0.000654 |
|    n_updates       | 3634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 692      |
|    fps             | 47       |
|    time_elapsed    | 72464    |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=2698.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00792  |
|    ent_coef        | 3.2e-05  |
|    ent_coef_loss   | -17.2    |
|    learning_rate   | 0.000653 |
|    n_updates       | 3644425  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=2696.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00159  |
|    ent_coef        | 3.21e-05 |
|    ent_coef_loss   | 60.9     |
|    learning_rate   | 0.000652 |
|    n_updates       | 3654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 696      |
|    fps             | 47       |
|    time_elapsed    | 72867    |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=2699.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000299 |
|    ent_coef        | 3.62e-05 |
|    ent_coef_loss   | -34.7    |
|    learning_rate   | 0.000651 |
|    n_updates       | 3664425  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=2698.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3500000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00294  |
|    ent_coef        | 3.32e-05 |
|    ent_coef_loss   | -22.1    |
|    learning_rate   | 0.00065  |
|    n_updates       | 3674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 700      |
|    fps             | 47       |
|    time_elapsed    | 73271    |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=2700.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3510000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0015   |
|    ent_coef        | 3.24e-05 |
|    ent_coef_loss   | 51.5     |
|    learning_rate   | 0.000649 |
|    n_updates       | 3684425  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=2702.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3520000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00105  |
|    ent_coef        | 3.43e-05 |
|    ent_coef_loss   | 9.17     |
|    learning_rate   | 0.000648 |
|    n_updates       | 3694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 704      |
|    fps             | 47       |
|    time_elapsed    | 73673    |
|    total_timesteps | 3520000  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=2702.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3530000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0042   |
|    ent_coef        | 6.05e-05 |
|    ent_coef_loss   | 27.2     |
|    learning_rate   | 0.000647 |
|    n_updates       | 3704425  |
---------------------------------
Eval num_timesteps=3540000, episode_reward=2701.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3540000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 3.97e-05 |
|    ent_coef_loss   | -22.1    |
|    learning_rate   | 0.000646 |
|    n_updates       | 3714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 708      |
|    fps             | 47       |
|    time_elapsed    | 74075    |
|    total_timesteps | 3540000  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=2698.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 4.4e-05  |
|    ent_coef_loss   | 33.4     |
|    learning_rate   | 0.000645 |
|    n_updates       | 3724425  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=2700.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00275  |
|    ent_coef        | 3.65e-05 |
|    ent_coef_loss   | -27.2    |
|    learning_rate   | 0.000644 |
|    n_updates       | 3734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 712      |
|    fps             | 47       |
|    time_elapsed    | 74476    |
|    total_timesteps | 3560000  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=2696.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.000592 |
|    ent_coef        | 5.38e-05 |
|    ent_coef_loss   | 5.18     |
|    learning_rate   | 0.000643 |
|    n_updates       | 3744425  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=2699.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00241  |
|    ent_coef        | 4.41e-05 |
|    ent_coef_loss   | 7.26     |
|    learning_rate   | 0.000642 |
|    n_updates       | 3754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 716      |
|    fps             | 47       |
|    time_elapsed    | 74879    |
|    total_timesteps | 3580000  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=2700.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0029   |
|    ent_coef        | 5.83e-05 |
|    ent_coef_loss   | -19.2    |
|    learning_rate   | 0.000641 |
|    n_updates       | 3764425  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=2699.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3600000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00227  |
|    ent_coef        | 4.22e-05 |
|    ent_coef_loss   | -17.3    |
|    learning_rate   | 0.00064  |
|    n_updates       | 3774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 720      |
|    fps             | 47       |
|    time_elapsed    | 75283    |
|    total_timesteps | 3600000  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=2518.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3610000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00252  |
|    ent_coef        | 4.76e-05 |
|    ent_coef_loss   | 0.595    |
|    learning_rate   | 0.000639 |
|    n_updates       | 3784425  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=2516.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3620000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00132  |
|    ent_coef        | 9.78e-05 |
|    ent_coef_loss   | -10.5    |
|    learning_rate   | 0.000638 |
|    n_updates       | 3794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 724      |
|    fps             | 47       |
|    time_elapsed    | 75683    |
|    total_timesteps | 3620000  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=2686.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00796  |
|    ent_coef        | 0.000113 |
|    ent_coef_loss   | -6.94    |
|    learning_rate   | 0.000637 |
|    n_updates       | 3804425  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=2700.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00152  |
|    ent_coef        | 9e-05    |
|    ent_coef_loss   | 3.32     |
|    learning_rate   | 0.000636 |
|    n_updates       | 3814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 728      |
|    fps             | 47       |
|    time_elapsed    | 76089    |
|    total_timesteps | 3640000  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=2513.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0263   |
|    ent_coef        | 7.14e-05 |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.000635 |
|    n_updates       | 3824425  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=2700.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00153  |
|    ent_coef        | 5.27e-05 |
|    ent_coef_loss   | 55.1     |
|    learning_rate   | 0.000634 |
|    n_updates       | 3834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 732      |
|    fps             | 47       |
|    time_elapsed    | 76494    |
|    total_timesteps | 3660000  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=2700.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3670000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00181  |
|    ent_coef        | 7.41e-05 |
|    ent_coef_loss   | -17.5    |
|    learning_rate   | 0.000633 |
|    n_updates       | 3844425  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=2702.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3680000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000753 |
|    ent_coef        | 7.01e-05 |
|    ent_coef_loss   | -25.7    |
|    learning_rate   | 0.000632 |
|    n_updates       | 3854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 736      |
|    fps             | 47       |
|    time_elapsed    | 76896    |
|    total_timesteps | 3680000  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=2698.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3690000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000354 |
|    ent_coef        | 4.2e-05  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000631 |
|    n_updates       | 3864425  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=2706.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3700000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0592   |
|    ent_coef        | 4.46e-05 |
|    ent_coef_loss   | 1.36     |
|    learning_rate   | 0.00063  |
|    n_updates       | 3874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 740      |
|    fps             | 47       |
|    time_elapsed    | 77302    |
|    total_timesteps | 3700000  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=2706.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3710000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00184  |
|    ent_coef        | 4.99e-05 |
|    ent_coef_loss   | -7.53    |
|    learning_rate   | 0.000629 |
|    n_updates       | 3884425  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=2700.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3720000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00578  |
|    ent_coef        | 7.43e-05 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.000628 |
|    n_updates       | 3894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 744      |
|    fps             | 47       |
|    time_elapsed    | 77707    |
|    total_timesteps | 3720000  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=2700.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00172  |
|    ent_coef        | 3.24e-05 |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.000627 |
|    n_updates       | 3904425  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=2700.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00153  |
|    ent_coef        | 5.65e-05 |
|    ent_coef_loss   | 32.2     |
|    learning_rate   | 0.000626 |
|    n_updates       | 3914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 748      |
|    fps             | 47       |
|    time_elapsed    | 78111    |
|    total_timesteps | 3740000  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00103  |
|    ent_coef        | 3.09e-05 |
|    ent_coef_loss   | -17.9    |
|    learning_rate   | 0.000625 |
|    n_updates       | 3924425  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=2700.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3760000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00152  |
|    ent_coef        | 2.21e-05 |
|    ent_coef_loss   | -38.4    |
|    learning_rate   | 0.000624 |
|    n_updates       | 3934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 752      |
|    fps             | 47       |
|    time_elapsed    | 78515    |
|    total_timesteps | 3760000  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=2698.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000692 |
|    ent_coef        | 4.04e-05 |
|    ent_coef_loss   | 14.2     |
|    learning_rate   | 0.000623 |
|    n_updates       | 3944425  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=2706.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3780000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0109   |
|    ent_coef        | 8.62e-05 |
|    ent_coef_loss   | -8.34    |
|    learning_rate   | 0.000622 |
|    n_updates       | 3954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 756      |
|    fps             | 47       |
|    time_elapsed    | 78919    |
|    total_timesteps | 3780000  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=2697.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3790000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00169  |
|    ent_coef        | 4.08e-05 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.000621 |
|    n_updates       | 3964425  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=2700.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00265  |
|    ent_coef        | 5.76e-05 |
|    ent_coef_loss   | -38.7    |
|    learning_rate   | 0.00062  |
|    n_updates       | 3974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 760      |
|    fps             | 47       |
|    time_elapsed    | 79325    |
|    total_timesteps | 3800000  |
---------------------------------
Eval num_timesteps=3810000, episode_reward=2702.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3810000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00217  |
|    ent_coef        | 5.48e-05 |
|    ent_coef_loss   | -9.11    |
|    learning_rate   | 0.000619 |
|    n_updates       | 3984425  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3820000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0016   |
|    ent_coef        | 6.96e-05 |
|    ent_coef_loss   | 26.2     |
|    learning_rate   | 0.000618 |
|    n_updates       | 3994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 764      |
|    fps             | 47       |
|    time_elapsed    | 79730    |
|    total_timesteps | 3820000  |
---------------------------------
Eval num_timesteps=3830000, episode_reward=2698.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3830000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00833  |
|    ent_coef        | 4.18e-05 |
|    ent_coef_loss   | -33.8    |
|    learning_rate   | 0.000617 |
|    n_updates       | 4004425  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=2700.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3840000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00341  |
|    ent_coef        | 2.71e-05 |
|    ent_coef_loss   | 1.52     |
|    learning_rate   | 0.000616 |
|    n_updates       | 4014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 768      |
|    fps             | 47       |
|    time_elapsed    | 80137    |
|    total_timesteps | 3840000  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2700.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3850000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00547  |
|    ent_coef        | 0.000112 |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.000615 |
|    n_updates       | 4024425  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3860000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00104  |
|    ent_coef        | 5.33e-05 |
|    ent_coef_loss   | -32.1    |
|    learning_rate   | 0.000614 |
|    n_updates       | 4034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 772      |
|    fps             | 47       |
|    time_elapsed    | 80542    |
|    total_timesteps | 3860000  |
---------------------------------
Eval num_timesteps=3870000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3870000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000991 |
|    ent_coef        | 6.75e-05 |
|    ent_coef_loss   | -2.79    |
|    learning_rate   | 0.000613 |
|    n_updates       | 4044425  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=2698.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3880000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000772 |
|    ent_coef        | 5.6e-05  |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.000612 |
|    n_updates       | 4054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 776      |
|    fps             | 47       |
|    time_elapsed    | 80948    |
|    total_timesteps | 3880000  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=2703.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3890000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00185  |
|    ent_coef        | 4.75e-05 |
|    ent_coef_loss   | 7.78     |
|    learning_rate   | 0.000611 |
|    n_updates       | 4064425  |
---------------------------------
Eval num_timesteps=3900000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3900000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00156  |
|    ent_coef        | 5.87e-05 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00061  |
|    n_updates       | 4074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 780      |
|    fps             | 47       |
|    time_elapsed    | 81351    |
|    total_timesteps | 3900000  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=2698.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3910000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00136  |
|    ent_coef        | 4.23e-05 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.000609 |
|    n_updates       | 4084425  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=2701.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3920000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0172   |
|    ent_coef        | 4.5e-05  |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000608 |
|    n_updates       | 4094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 784      |
|    fps             | 47       |
|    time_elapsed    | 81755    |
|    total_timesteps | 3920000  |
---------------------------------
Eval num_timesteps=3930000, episode_reward=2698.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3930000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00116  |
|    ent_coef        | 5.9e-05  |
|    ent_coef_loss   | 5.9      |
|    learning_rate   | 0.000607 |
|    n_updates       | 4104425  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3940000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00311  |
|    ent_coef        | 4.41e-05 |
|    ent_coef_loss   | 42.2     |
|    learning_rate   | 0.000606 |
|    n_updates       | 4114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 788      |
|    fps             | 47       |
|    time_elapsed    | 82160    |
|    total_timesteps | 3940000  |
---------------------------------
Eval num_timesteps=3950000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3950000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0036   |
|    ent_coef        | 3.17e-05 |
|    ent_coef_loss   | 13.5     |
|    learning_rate   | 0.000605 |
|    n_updates       | 4124425  |
---------------------------------
Eval num_timesteps=3960000, episode_reward=2698.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3960000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00295  |
|    ent_coef        | 3.7e-05  |
|    ent_coef_loss   | -3.85    |
|    learning_rate   | 0.000604 |
|    n_updates       | 4134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 792      |
|    fps             | 47       |
|    time_elapsed    | 82564    |
|    total_timesteps | 3960000  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=2693.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3970000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00122  |
|    ent_coef        | 3.98e-05 |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.000603 |
|    n_updates       | 4144425  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=2700.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3980000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00291  |
|    ent_coef        | 8.15e-05 |
|    ent_coef_loss   | -4.34    |
|    learning_rate   | 0.000602 |
|    n_updates       | 4154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 796      |
|    fps             | 47       |
|    time_elapsed    | 82968    |
|    total_timesteps | 3980000  |
---------------------------------
Eval num_timesteps=3990000, episode_reward=2699.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3990000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.000647 |
|    ent_coef        | 4.61e-05 |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.000601 |
|    n_updates       | 4164425  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=2699.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4000000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00386  |
|    ent_coef        | 4.77e-05 |
|    ent_coef_loss   | 17.7     |
|    learning_rate   | 0.0006   |
|    n_updates       | 4174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 800      |
|    fps             | 47       |
|    time_elapsed    | 83373    |
|    total_timesteps | 4000000  |
---------------------------------
Eval num_timesteps=4010000, episode_reward=2700.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4010000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000719 |
|    ent_coef        | 4.61e-05 |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.000599 |
|    n_updates       | 4184425  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4020000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00138  |
|    ent_coef        | 5.04e-05 |
|    ent_coef_loss   | -43.1    |
|    learning_rate   | 0.000598 |
|    n_updates       | 4194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 804      |
|    fps             | 47       |
|    time_elapsed    | 83777    |
|    total_timesteps | 4020000  |
---------------------------------
Eval num_timesteps=4030000, episode_reward=2700.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4030000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00198  |
|    ent_coef        | 4.95e-05 |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.000597 |
|    n_updates       | 4204425  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=2698.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4040000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.000415 |
|    ent_coef        | 8.49e-05 |
|    ent_coef_loss   | -17.2    |
|    learning_rate   | 0.000596 |
|    n_updates       | 4214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 808      |
|    fps             | 47       |
|    time_elapsed    | 84183    |
|    total_timesteps | 4040000  |
---------------------------------
Eval num_timesteps=4050000, episode_reward=2702.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4050000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00422  |
|    ent_coef        | 8.57e-05 |
|    ent_coef_loss   | 28.7     |
|    learning_rate   | 0.000595 |
|    n_updates       | 4224425  |
---------------------------------
Eval num_timesteps=4060000, episode_reward=2700.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0024   |
|    ent_coef        | 3.52e-05 |
|    ent_coef_loss   | 48.8     |
|    learning_rate   | 0.000594 |
|    n_updates       | 4234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 812      |
|    fps             | 47       |
|    time_elapsed    | 84587    |
|    total_timesteps | 4060000  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=2699.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4070000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0262   |
|    ent_coef        | 5.61e-05 |
|    ent_coef_loss   | -58.9    |
|    learning_rate   | 0.000593 |
|    n_updates       | 4244425  |
---------------------------------
Eval num_timesteps=4080000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4080000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00356  |
|    ent_coef        | 0.00016  |
|    ent_coef_loss   | 8.66     |
|    learning_rate   | 0.000592 |
|    n_updates       | 4254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 816      |
|    fps             | 48       |
|    time_elapsed    | 84990    |
|    total_timesteps | 4080000  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00105  |
|    ent_coef        | 7.07e-05 |
|    ent_coef_loss   | 15.2     |
|    learning_rate   | 0.000591 |
|    n_updates       | 4264425  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=2703.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4100000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00604  |
|    ent_coef        | 6.63e-05 |
|    ent_coef_loss   | -9.58    |
|    learning_rate   | 0.00059  |
|    n_updates       | 4274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 820      |
|    fps             | 48       |
|    time_elapsed    | 85396    |
|    total_timesteps | 4100000  |
---------------------------------
Eval num_timesteps=4110000, episode_reward=2704.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4110000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00329  |
|    ent_coef        | 0.000104 |
|    ent_coef_loss   | -6.22    |
|    learning_rate   | 0.000589 |
|    n_updates       | 4284425  |
---------------------------------
Eval num_timesteps=4120000, episode_reward=2700.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4120000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00904  |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | 16.9     |
|    learning_rate   | 0.000588 |
|    n_updates       | 4294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 824      |
|    fps             | 48       |
|    time_elapsed    | 85802    |
|    total_timesteps | 4120000  |
---------------------------------
Eval num_timesteps=4130000, episode_reward=2698.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4130000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0695   |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 18.5     |
|    learning_rate   | 0.000587 |
|    n_updates       | 4304425  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4140000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00953  |
|    ent_coef        | 0.00053  |
|    ent_coef_loss   | 9.95     |
|    learning_rate   | 0.000586 |
|    n_updates       | 4314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 828      |
|    fps             | 48       |
|    time_elapsed    | 86206    |
|    total_timesteps | 4140000  |
---------------------------------
Eval num_timesteps=4150000, episode_reward=2700.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4150000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.00719  |
|    ent_coef        | 0.000365 |
|    ent_coef_loss   | 5.71     |
|    learning_rate   | 0.000585 |
|    n_updates       | 4324425  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4160000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0273   |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | 25.8     |
|    learning_rate   | 0.000584 |
|    n_updates       | 4334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 832      |
|    fps             | 48       |
|    time_elapsed    | 86612    |
|    total_timesteps | 4160000  |
---------------------------------
Eval num_timesteps=4170000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4170000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0479   |
|    ent_coef        | 0.000421 |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.000583 |
|    n_updates       | 4344425  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=2699.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4180000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0125   |
|    ent_coef        | 0.000308 |
|    ent_coef_loss   | 17.1     |
|    learning_rate   | 0.000582 |
|    n_updates       | 4354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 836      |
|    fps             | 48       |
|    time_elapsed    | 87017    |
|    total_timesteps | 4180000  |
---------------------------------
Eval num_timesteps=4190000, episode_reward=2699.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4190000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0031   |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | -5.76    |
|    learning_rate   | 0.000581 |
|    n_updates       | 4364425  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4200000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0122   |
|    ent_coef        | 0.000191 |
|    ent_coef_loss   | -0.178   |
|    learning_rate   | 0.00058  |
|    n_updates       | 4374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 840      |
|    fps             | 48       |
|    time_elapsed    | 87422    |
|    total_timesteps | 4200000  |
---------------------------------
Eval num_timesteps=4210000, episode_reward=2699.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4210000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0349   |
|    ent_coef        | 0.000226 |
|    ent_coef_loss   | 30.4     |
|    learning_rate   | 0.000579 |
|    n_updates       | 4384425  |
---------------------------------
Eval num_timesteps=4220000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4220000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.179    |
|    ent_coef        | 0.000264 |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000578 |
|    n_updates       | 4394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 844      |
|    fps             | 48       |
|    time_elapsed    | 87829    |
|    total_timesteps | 4220000  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=2699.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4230000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00223  |
|    ent_coef        | 0.000216 |
|    ent_coef_loss   | -17.3    |
|    learning_rate   | 0.000577 |
|    n_updates       | 4404425  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=2698.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4240000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0309   |
|    ent_coef        | 0.000185 |
|    ent_coef_loss   | -17.7    |
|    learning_rate   | 0.000576 |
|    n_updates       | 4414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 848      |
|    fps             | 48       |
|    time_elapsed    | 88234    |
|    total_timesteps | 4240000  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=2699.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4250000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0146   |
|    ent_coef        | 0.00018  |
|    ent_coef_loss   | 12.9     |
|    learning_rate   | 0.000575 |
|    n_updates       | 4424425  |
---------------------------------
Eval num_timesteps=4260000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4260000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0037   |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000574 |
|    n_updates       | 4434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 852      |
|    fps             | 48       |
|    time_elapsed    | 88642    |
|    total_timesteps | 4260000  |
---------------------------------
Eval num_timesteps=4270000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4270000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00165  |
|    ent_coef        | 0.000163 |
|    ent_coef_loss   | 14.6     |
|    learning_rate   | 0.000573 |
|    n_updates       | 4444425  |
---------------------------------
Eval num_timesteps=4280000, episode_reward=2699.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4280000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00762  |
|    ent_coef        | 0.000134 |
|    ent_coef_loss   | 6.88     |
|    learning_rate   | 0.000572 |
|    n_updates       | 4454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 856      |
|    fps             | 48       |
|    time_elapsed    | 89049    |
|    total_timesteps | 4280000  |
---------------------------------
Eval num_timesteps=4290000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4290000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00496  |
|    ent_coef        | 0.000114 |
|    ent_coef_loss   | -5.54    |
|    learning_rate   | 0.000571 |
|    n_updates       | 4464425  |
---------------------------------
Eval num_timesteps=4300000, episode_reward=2698.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4300000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00312  |
|    ent_coef        | 0.000124 |
|    ent_coef_loss   | -38.9    |
|    learning_rate   | 0.00057  |
|    n_updates       | 4474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 860      |
|    fps             | 48       |
|    time_elapsed    | 89454    |
|    total_timesteps | 4300000  |
---------------------------------
Eval num_timesteps=4310000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4310000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.000569 |
|    n_updates       | 4484425  |
---------------------------------
Eval num_timesteps=4320000, episode_reward=2699.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4320000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00198  |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | -4.48    |
|    learning_rate   | 0.000568 |
|    n_updates       | 4494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 864      |
|    fps             | 48       |
|    time_elapsed    | 89880    |
|    total_timesteps | 4320000  |
---------------------------------
Eval num_timesteps=4330000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4330000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00165  |
|    ent_coef        | 7.48e-05 |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.000567 |
|    n_updates       | 4504425  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=2699.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4340000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0456   |
|    ent_coef        | 9.88e-05 |
|    ent_coef_loss   | -25.6    |
|    learning_rate   | 0.000566 |
|    n_updates       | 4514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 868      |
|    fps             | 48       |
|    time_elapsed    | 90406    |
|    total_timesteps | 4340000  |
---------------------------------
Eval num_timesteps=4350000, episode_reward=2700.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4350000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00231  |
|    ent_coef        | 0.000103 |
|    ent_coef_loss   | 21.8     |
|    learning_rate   | 0.000565 |
|    n_updates       | 4524425  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4360000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.000302 |
|    ent_coef        | 0.000133 |
|    ent_coef_loss   | -6.19    |
|    learning_rate   | 0.000564 |
|    n_updates       | 4534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 872      |
|    fps             | 47       |
|    time_elapsed    | 90924    |
|    total_timesteps | 4360000  |
---------------------------------
Eval num_timesteps=4370000, episode_reward=2699.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4370000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0024   |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | 10.2     |
|    learning_rate   | 0.000563 |
|    n_updates       | 4544425  |
---------------------------------
Eval num_timesteps=4380000, episode_reward=2700.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4380000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.00249  |
|    ent_coef        | 0.000186 |
|    ent_coef_loss   | 0.0168   |
|    learning_rate   | 0.000562 |
|    n_updates       | 4554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 876      |
|    fps             | 47       |
|    time_elapsed    | 91429    |
|    total_timesteps | 4380000  |
---------------------------------
Eval num_timesteps=4390000, episode_reward=2512.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 4390000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0465   |
|    ent_coef        | 0.000176 |
|    ent_coef_loss   | -40.2    |
|    learning_rate   | 0.000561 |
|    n_updates       | 4564425  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=2698.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4400000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00491  |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -7.18    |
|    learning_rate   | 0.00056  |
|    n_updates       | 4574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 880      |
|    fps             | 47       |
|    time_elapsed    | 91923    |
|    total_timesteps | 4400000  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=2699.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4410000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0166   |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | 9.08     |
|    learning_rate   | 0.000559 |
|    n_updates       | 4584425  |
---------------------------------
Eval num_timesteps=4420000, episode_reward=2699.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4420000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0405   |
|    ent_coef        | 0.000423 |
|    ent_coef_loss   | -2.43    |
|    learning_rate   | 0.000558 |
|    n_updates       | 4594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 884      |
|    fps             | 47       |
|    time_elapsed    | 92418    |
|    total_timesteps | 4420000  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=2699.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4430000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000363 |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.000557 |
|    n_updates       | 4604425  |
---------------------------------
Eval num_timesteps=4440000, episode_reward=2698.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4440000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | 22.7     |
|    learning_rate   | 0.000556 |
|    n_updates       | 4614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 888      |
|    fps             | 47       |
|    time_elapsed    | 92915    |
|    total_timesteps | 4440000  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=2698.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4450000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.615    |
|    ent_coef        | 0.000393 |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.000555 |
|    n_updates       | 4624425  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=2698.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4460000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.02     |
|    ent_coef        | 0.000428 |
|    ent_coef_loss   | 44.6     |
|    learning_rate   | 0.000554 |
|    n_updates       | 4634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 892      |
|    fps             | 47       |
|    time_elapsed    | 93376    |
|    total_timesteps | 4460000  |
---------------------------------
Eval num_timesteps=4470000, episode_reward=2700.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4470000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.000319 |
|    ent_coef_loss   | 5.18     |
|    learning_rate   | 0.000553 |
|    n_updates       | 4644425  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=2699.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4480000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | -20.5    |
|    learning_rate   | 0.000552 |
|    n_updates       | 4654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 896      |
|    fps             | 47       |
|    time_elapsed    | 93779    |
|    total_timesteps | 4480000  |
---------------------------------
Eval num_timesteps=4490000, episode_reward=2698.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4490000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.00331  |
|    ent_coef        | 0.000297 |
|    ent_coef_loss   | -31      |
|    learning_rate   | 0.000551 |
|    n_updates       | 4664425  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=2698.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4500000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.0258   |
|    ent_coef        | 0.000336 |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.00055  |
|    n_updates       | 4674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 900      |
|    fps             | 47       |
|    time_elapsed    | 94181    |
|    total_timesteps | 4500000  |
---------------------------------
Eval num_timesteps=4510000, episode_reward=2698.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4510000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0611   |
|    ent_coef        | 0.000314 |
|    ent_coef_loss   | 21.7     |
|    learning_rate   | 0.000549 |
|    n_updates       | 4684425  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=2698.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4520000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0376   |
|    ent_coef        | 0.000733 |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.000548 |
|    n_updates       | 4694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 904      |
|    fps             | 47       |
|    time_elapsed    | 94584    |
|    total_timesteps | 4520000  |
---------------------------------
Eval num_timesteps=4530000, episode_reward=2698.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4530000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 12.3     |
|    ent_coef        | 0.000754 |
|    ent_coef_loss   | 13       |
|    learning_rate   | 0.000547 |
|    n_updates       | 4704425  |
---------------------------------
Eval num_timesteps=4540000, episode_reward=2699.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4540000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.0816   |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | 4.11     |
|    learning_rate   | 0.000546 |
|    n_updates       | 4714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 908      |
|    fps             | 47       |
|    time_elapsed    | 94988    |
|    total_timesteps | 4540000  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=2699.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4550000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.00153  |
|    ent_coef_loss   | -5.78    |
|    learning_rate   | 0.000545 |
|    n_updates       | 4724425  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=2699.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4560000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.234    |
|    ent_coef        | 0.00309  |
|    ent_coef_loss   | -6.61    |
|    learning_rate   | 0.000544 |
|    n_updates       | 4734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 912      |
|    fps             | 47       |
|    time_elapsed    | 95390    |
|    total_timesteps | 4560000  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=2512.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 4570000  |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 6.74     |
|    ent_coef        | 0.00381  |
|    ent_coef_loss   | 0.0906   |
|    learning_rate   | 0.000543 |
|    n_updates       | 4744425  |
---------------------------------
Eval num_timesteps=4580000, episode_reward=2699.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4580000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.601    |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | 2.88     |
|    learning_rate   | 0.000542 |
|    n_updates       | 4754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 916      |
|    fps             | 47       |
|    time_elapsed    | 95802    |
|    total_timesteps | 4580000  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=2698.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4590000  |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 0.438    |
|    ent_coef        | 0.00736  |
|    ent_coef_loss   | 3.5      |
|    learning_rate   | 0.000541 |
|    n_updates       | 4764425  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=2695.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4600000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.256    |
|    ent_coef        | 0.00599  |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00054  |
|    n_updates       | 4774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 920      |
|    fps             | 47       |
|    time_elapsed    | 96311    |
|    total_timesteps | 4600000  |
---------------------------------
Eval num_timesteps=4610000, episode_reward=2695.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4610000  |
| train/             |          |
|    actor_loss      | -48.4    |
|    critic_loss     | 1.27     |
|    ent_coef        | 0.00644  |
|    ent_coef_loss   | 7.72     |
|    learning_rate   | 0.000539 |
|    n_updates       | 4784425  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=2689.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4620000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00446  |
|    ent_coef_loss   | 16.7     |
|    learning_rate   | 0.000538 |
|    n_updates       | 4794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 924      |
|    fps             | 47       |
|    time_elapsed    | 96813    |
|    total_timesteps | 4620000  |
---------------------------------
Eval num_timesteps=4630000, episode_reward=2690.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4630000  |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 1.24     |
|    ent_coef        | 0.00759  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000537 |
|    n_updates       | 4804425  |
---------------------------------
Eval num_timesteps=4640000, episode_reward=2691.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4640000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 0.905    |
|    ent_coef        | 0.00955  |
|    ent_coef_loss   | -5.5     |
|    learning_rate   | 0.000536 |
|    n_updates       | 4814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 928      |
|    fps             | 47       |
|    time_elapsed    | 97304    |
|    total_timesteps | 4640000  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=2699.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4650000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 12.4     |
|    ent_coef        | 0.00809  |
|    ent_coef_loss   | -2.97    |
|    learning_rate   | 0.000535 |
|    n_updates       | 4824425  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=2690.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4660000  |
| train/             |          |
|    actor_loss      | -45.8    |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | 8.38     |
|    learning_rate   | 0.000534 |
|    n_updates       | 4834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 932      |
|    fps             | 47       |
|    time_elapsed    | 97794    |
|    total_timesteps | 4660000  |
---------------------------------
Eval num_timesteps=4670000, episode_reward=2690.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4670000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.545    |
|    ent_coef        | 0.00978  |
|    ent_coef_loss   | 0.361    |
|    learning_rate   | 0.000533 |
|    n_updates       | 4844425  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=2693.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4680000  |
| train/             |          |
|    actor_loss      | -43.8    |
|    critic_loss     | 4.17     |
|    ent_coef        | 0.00832  |
|    ent_coef_loss   | -1.44    |
|    learning_rate   | 0.000532 |
|    n_updates       | 4854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 936      |
|    fps             | 47       |
|    time_elapsed    | 98285    |
|    total_timesteps | 4680000  |
---------------------------------
Eval num_timesteps=4690000, episode_reward=2692.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4690000  |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 1.22     |
|    ent_coef        | 0.00893  |
|    ent_coef_loss   | -5.23    |
|    learning_rate   | 0.000531 |
|    n_updates       | 4864425  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2690.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4700000  |
| train/             |          |
|    actor_loss      | -46      |
|    critic_loss     | 3.39     |
|    ent_coef        | 0.00947  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.00053  |
|    n_updates       | 4874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 940      |
|    fps             | 47       |
|    time_elapsed    | 98777    |
|    total_timesteps | 4700000  |
---------------------------------
Eval num_timesteps=4710000, episode_reward=2692.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4710000  |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 2.72     |
|    ent_coef        | 0.00865  |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000529 |
|    n_updates       | 4884425  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=2696.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4720000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 21.2     |
|    ent_coef        | 0.00439  |
|    ent_coef_loss   | -5.51    |
|    learning_rate   | 0.000528 |
|    n_updates       | 4894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 944      |
|    fps             | 47       |
|    time_elapsed    | 99266    |
|    total_timesteps | 4720000  |
---------------------------------
Eval num_timesteps=4730000, episode_reward=2697.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4730000  |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00634  |
|    ent_coef_loss   | 6.08     |
|    learning_rate   | 0.000527 |
|    n_updates       | 4904425  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=2697.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4740000  |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 15.3     |
|    ent_coef        | 0.0084   |
|    ent_coef_loss   | -4.3     |
|    learning_rate   | 0.000526 |
|    n_updates       | 4914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 948      |
|    fps             | 47       |
|    time_elapsed    | 99759    |
|    total_timesteps | 4740000  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=2695.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4750000  |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 4.81     |
|    ent_coef        | 0.00532  |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.000525 |
|    n_updates       | 4924425  |
---------------------------------
Eval num_timesteps=4760000, episode_reward=2698.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4760000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.355    |
|    ent_coef        | 0.00496  |
|    ent_coef_loss   | -4.21    |
|    learning_rate   | 0.000524 |
|    n_updates       | 4934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 952      |
|    fps             | 47       |
|    time_elapsed    | 100251   |
|    total_timesteps | 4760000  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=2695.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4770000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.507    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | -5.67    |
|    learning_rate   | 0.000523 |
|    n_updates       | 4944425  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=2690.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4780000  |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 0.407    |
|    ent_coef        | 0.00405  |
|    ent_coef_loss   | -19.2    |
|    learning_rate   | 0.000522 |
|    n_updates       | 4954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 956      |
|    fps             | 47       |
|    time_elapsed    | 100744   |
|    total_timesteps | 4780000  |
---------------------------------
Eval num_timesteps=4790000, episode_reward=2699.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4790000  |
| train/             |          |
|    actor_loss      | -44      |
|    critic_loss     | 0.497    |
|    ent_coef        | 0.00786  |
|    ent_coef_loss   | -6.31    |
|    learning_rate   | 0.000521 |
|    n_updates       | 4964425  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=2694.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4800000  |
| train/             |          |
|    actor_loss      | -45.4    |
|    critic_loss     | 0.296    |
|    ent_coef        | 0.00587  |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.00052  |
|    n_updates       | 4974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 960      |
|    fps             | 47       |
|    time_elapsed    | 101233   |
|    total_timesteps | 4800000  |
---------------------------------
Eval num_timesteps=4810000, episode_reward=2697.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4810000  |
| train/             |          |
|    actor_loss      | -47.5    |
|    critic_loss     | 0.165    |
|    ent_coef        | 0.00514  |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000519 |
|    n_updates       | 4984425  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=2698.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4820000  |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 0.806    |
|    ent_coef        | 0.0036   |
|    ent_coef_loss   | -20.1    |
|    learning_rate   | 0.000518 |
|    n_updates       | 4994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 964      |
|    fps             | 47       |
|    time_elapsed    | 101724   |
|    total_timesteps | 4820000  |
---------------------------------
Eval num_timesteps=4830000, episode_reward=2698.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4830000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 0.273    |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | 2.75     |
|    learning_rate   | 0.000517 |
|    n_updates       | 5004425  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=2692.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4840000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.525    |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -4.09    |
|    learning_rate   | 0.000516 |
|    n_updates       | 5014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 968      |
|    fps             | 47       |
|    time_elapsed    | 102215   |
|    total_timesteps | 4840000  |
---------------------------------
Eval num_timesteps=4850000, episode_reward=2695.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4850000  |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 25.4     |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | -0.75    |
|    learning_rate   | 0.000515 |
|    n_updates       | 5024425  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=2700.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4860000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.321    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.000514 |
|    n_updates       | 5034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 972      |
|    fps             | 47       |
|    time_elapsed    | 102705   |
|    total_timesteps | 4860000  |
---------------------------------
Eval num_timesteps=4870000, episode_reward=2700.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4870000  |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 4.2      |
|    ent_coef        | 0.00597  |
|    ent_coef_loss   | 8.47     |
|    learning_rate   | 0.000513 |
|    n_updates       | 5044425  |
---------------------------------
Eval num_timesteps=4880000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4880000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 1.65     |
|    ent_coef        | 0.00467  |
|    ent_coef_loss   | 8.38     |
|    learning_rate   | 0.000512 |
|    n_updates       | 5054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 976      |
|    fps             | 47       |
|    time_elapsed    | 103196   |
|    total_timesteps | 4880000  |
---------------------------------
Eval num_timesteps=4890000, episode_reward=2705.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4890000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | 5.54     |
|    learning_rate   | 0.000511 |
|    n_updates       | 5064425  |
---------------------------------
Eval num_timesteps=4900000, episode_reward=2693.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4900000  |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.00051  |
|    n_updates       | 5074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 980      |
|    fps             | 47       |
|    time_elapsed    | 103688   |
|    total_timesteps | 4900000  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=2696.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4910000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.632    |
|    ent_coef        | 0.00379  |
|    ent_coef_loss   | 0.33     |
|    learning_rate   | 0.000509 |
|    n_updates       | 5084425  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=2694.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4920000  |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.00893  |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.000508 |
|    n_updates       | 5094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 984      |
|    fps             | 47       |
|    time_elapsed    | 104182   |
|    total_timesteps | 4920000  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=2693.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4930000  |
| train/             |          |
|    actor_loss      | -41.3    |
|    critic_loss     | 2.23     |
|    ent_coef        | 0.00659  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000507 |
|    n_updates       | 5104425  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=2697.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    actor_loss      | -35.5    |
|    critic_loss     | 1.83     |
|    ent_coef        | 0.00934  |
|    ent_coef_loss   | 6.84     |
|    learning_rate   | 0.000506 |
|    n_updates       | 5114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 988      |
|    fps             | 47       |
|    time_elapsed    | 104674   |
|    total_timesteps | 4940000  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=2697.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4950000  |
| train/             |          |
|    actor_loss      | -36.9    |
|    critic_loss     | 3.29     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -9.05    |
|    learning_rate   | 0.000505 |
|    n_updates       | 5124425  |
---------------------------------
Eval num_timesteps=4960000, episode_reward=2697.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4960000  |
| train/             |          |
|    actor_loss      | -39.1    |
|    critic_loss     | 0.508    |
|    ent_coef        | 0.00763  |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.000504 |
|    n_updates       | 5134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 992      |
|    fps             | 47       |
|    time_elapsed    | 105178   |
|    total_timesteps | 4960000  |
---------------------------------
Eval num_timesteps=4970000, episode_reward=2700.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4970000  |
| train/             |          |
|    actor_loss      | -38.7    |
|    critic_loss     | 0.624    |
|    ent_coef        | 0.00593  |
|    ent_coef_loss   | 5.26     |
|    learning_rate   | 0.000503 |
|    n_updates       | 5144425  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=2698.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4980000  |
| train/             |          |
|    actor_loss      | -43      |
|    critic_loss     | 1.18     |
|    ent_coef        | 0.00493  |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.000502 |
|    n_updates       | 5154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 996      |
|    fps             | 47       |
|    time_elapsed    | 105696   |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4990000, episode_reward=2699.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4990000  |
| train/             |          |
|    actor_loss      | -47.5    |
|    critic_loss     | 0.347    |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.000501 |
|    n_updates       | 5164425  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=2699.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5000000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 0.344    |
|    ent_coef        | 0.0037   |
|    ent_coef_loss   | -6.38    |
|    learning_rate   | 0.0005   |
|    n_updates       | 5174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1000     |
|    fps             | 47       |
|    time_elapsed    | 106214   |
|    total_timesteps | 5000000  |
---------------------------------
Eval num_timesteps=5010000, episode_reward=2699.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5010000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 2.65     |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000499 |
|    n_updates       | 5184425  |
---------------------------------
Eval num_timesteps=5020000, episode_reward=2701.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5020000  |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 0.242    |
|    ent_coef        | 0.00284  |
|    ent_coef_loss   | 5.63     |
|    learning_rate   | 0.000498 |
|    n_updates       | 5194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1004     |
|    fps             | 47       |
|    time_elapsed    | 106726   |
|    total_timesteps | 5020000  |
---------------------------------
Eval num_timesteps=5030000, episode_reward=2700.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5030000  |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 0.449    |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 0.258    |
|    learning_rate   | 0.000497 |
|    n_updates       | 5204425  |
---------------------------------
Eval num_timesteps=5040000, episode_reward=2699.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5040000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.096    |
|    ent_coef        | 0.00193  |
|    ent_coef_loss   | -6.05    |
|    learning_rate   | 0.000496 |
|    n_updates       | 5214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1008     |
|    fps             | 46       |
|    time_elapsed    | 107241   |
|    total_timesteps | 5040000  |
---------------------------------
Eval num_timesteps=5050000, episode_reward=2698.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5050000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.000989 |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000495 |
|    n_updates       | 5224425  |
---------------------------------
Eval num_timesteps=5060000, episode_reward=2698.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5060000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0721   |
|    ent_coef        | 0.000875 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.000494 |
|    n_updates       | 5234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1012     |
|    fps             | 46       |
|    time_elapsed    | 107735   |
|    total_timesteps | 5060000  |
---------------------------------
Eval num_timesteps=5070000, episode_reward=2705.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5070000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.278    |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 0.225    |
|    learning_rate   | 0.000493 |
|    n_updates       | 5244425  |
---------------------------------
Eval num_timesteps=5080000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5080000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 2.62     |
|    learning_rate   | 0.000492 |
|    n_updates       | 5254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1016     |
|    fps             | 46       |
|    time_elapsed    | 108229   |
|    total_timesteps | 5080000  |
---------------------------------
Eval num_timesteps=5090000, episode_reward=2700.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5090000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.247    |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | 2.6      |
|    learning_rate   | 0.000491 |
|    n_updates       | 5264425  |
---------------------------------
Eval num_timesteps=5100000, episode_reward=2700.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5100000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.398    |
|    ent_coef        | 0.000984 |
|    ent_coef_loss   | 6.23     |
|    learning_rate   | 0.00049  |
|    n_updates       | 5274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1020     |
|    fps             | 46       |
|    time_elapsed    | 108724   |
|    total_timesteps | 5100000  |
---------------------------------
Eval num_timesteps=5110000, episode_reward=2699.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5110000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.233    |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | 0.658    |
|    learning_rate   | 0.000489 |
|    n_updates       | 5284425  |
---------------------------------
Eval num_timesteps=5120000, episode_reward=2699.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5120000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.06     |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.000488 |
|    n_updates       | 5294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1024     |
|    fps             | 46       |
|    time_elapsed    | 109218   |
|    total_timesteps | 5120000  |
---------------------------------
Eval num_timesteps=5130000, episode_reward=2700.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5130000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.371    |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -3.71    |
|    learning_rate   | 0.000487 |
|    n_updates       | 5304425  |
---------------------------------
Eval num_timesteps=5140000, episode_reward=2704.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5140000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.000863 |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.000486 |
|    n_updates       | 5314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1028     |
|    fps             | 46       |
|    time_elapsed    | 109710   |
|    total_timesteps | 5140000  |
---------------------------------
Eval num_timesteps=5150000, episode_reward=2704.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5150000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.122    |
|    ent_coef        | 0.000933 |
|    ent_coef_loss   | 21       |
|    learning_rate   | 0.000485 |
|    n_updates       | 5324425  |
---------------------------------
Eval num_timesteps=5160000, episode_reward=2706.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5160000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0876   |
|    ent_coef        | 0.00133  |
|    ent_coef_loss   | -3.88    |
|    learning_rate   | 0.000484 |
|    n_updates       | 5334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1032     |
|    fps             | 46       |
|    time_elapsed    | 110203   |
|    total_timesteps | 5160000  |
---------------------------------
Eval num_timesteps=5170000, episode_reward=2707.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5170000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0724   |
|    ent_coef        | 0.00108  |
|    ent_coef_loss   | 1.6      |
|    learning_rate   | 0.000483 |
|    n_updates       | 5344425  |
---------------------------------
Eval num_timesteps=5180000, episode_reward=2707.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5180000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0393   |
|    ent_coef        | 0.000605 |
|    ent_coef_loss   | -11.1    |
|    learning_rate   | 0.000482 |
|    n_updates       | 5354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1036     |
|    fps             | 46       |
|    time_elapsed    | 110696   |
|    total_timesteps | 5180000  |
---------------------------------
Eval num_timesteps=5190000, episode_reward=2707.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5190000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0403   |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | 4.12     |
|    learning_rate   | 0.000481 |
|    n_updates       | 5364425  |
---------------------------------
Eval num_timesteps=5200000, episode_reward=2706.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5200000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.16     |
|    ent_coef        | 0.000322 |
|    ent_coef_loss   | 6.8      |
|    learning_rate   | 0.00048  |
|    n_updates       | 5374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1040     |
|    fps             | 46       |
|    time_elapsed    | 111188   |
|    total_timesteps | 5200000  |
---------------------------------
Eval num_timesteps=5210000, episode_reward=2699.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5210000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.309    |
|    ent_coef        | 0.000615 |
|    ent_coef_loss   | 6.94     |
|    learning_rate   | 0.000479 |
|    n_updates       | 5384425  |
---------------------------------
Eval num_timesteps=5220000, episode_reward=2698.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5220000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.646    |
|    ent_coef        | 0.000949 |
|    ent_coef_loss   | 15.2     |
|    learning_rate   | 0.000478 |
|    n_updates       | 5394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1044     |
|    fps             | 46       |
|    time_elapsed    | 111681   |
|    total_timesteps | 5220000  |
---------------------------------
Eval num_timesteps=5230000, episode_reward=2699.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5230000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0576   |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 6.27     |
|    learning_rate   | 0.000477 |
|    n_updates       | 5404425  |
---------------------------------
Eval num_timesteps=5240000, episode_reward=2699.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5240000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0307   |
|    ent_coef        | 0.000968 |
|    ent_coef_loss   | 12.9     |
|    learning_rate   | 0.000476 |
|    n_updates       | 5414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1048     |
|    fps             | 46       |
|    time_elapsed    | 112175   |
|    total_timesteps | 5240000  |
---------------------------------
Eval num_timesteps=5250000, episode_reward=2700.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5250000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0378   |
|    ent_coef        | 0.000806 |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000475 |
|    n_updates       | 5424425  |
---------------------------------
Eval num_timesteps=5260000, episode_reward=2697.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5260000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.000861 |
|    ent_coef_loss   | 7.15     |
|    learning_rate   | 0.000474 |
|    n_updates       | 5434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1052     |
|    fps             | 46       |
|    time_elapsed    | 112668   |
|    total_timesteps | 5260000  |
---------------------------------
Eval num_timesteps=5270000, episode_reward=2697.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5270000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.206    |
|    ent_coef        | 0.000706 |
|    ent_coef_loss   | -0.382   |
|    learning_rate   | 0.000473 |
|    n_updates       | 5444425  |
---------------------------------
Eval num_timesteps=5280000, episode_reward=2695.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5280000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0274   |
|    ent_coef        | 0.000566 |
|    ent_coef_loss   | 9.54     |
|    learning_rate   | 0.000472 |
|    n_updates       | 5454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1056     |
|    fps             | 46       |
|    time_elapsed    | 113163   |
|    total_timesteps | 5280000  |
---------------------------------
Eval num_timesteps=5290000, episode_reward=2698.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5290000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.00067  |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.000471 |
|    n_updates       | 5464425  |
---------------------------------
Eval num_timesteps=5300000, episode_reward=2698.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5300000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.0234   |
|    ent_coef        | 0.000891 |
|    ent_coef_loss   | -0.939   |
|    learning_rate   | 0.00047  |
|    n_updates       | 5474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1060     |
|    fps             | 46       |
|    time_elapsed    | 113655   |
|    total_timesteps | 5300000  |
---------------------------------
Eval num_timesteps=5310000, episode_reward=2699.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5310000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | 18.9     |
|    learning_rate   | 0.000469 |
|    n_updates       | 5484425  |
---------------------------------
Eval num_timesteps=5320000, episode_reward=2697.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5320000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0727   |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000468 |
|    n_updates       | 5494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1064     |
|    fps             | 46       |
|    time_elapsed    | 114149   |
|    total_timesteps | 5320000  |
---------------------------------
Eval num_timesteps=5330000, episode_reward=2696.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5330000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.0719   |
|    ent_coef        | 0.00112  |
|    ent_coef_loss   | 17.1     |
|    learning_rate   | 0.000467 |
|    n_updates       | 5504425  |
---------------------------------
Eval num_timesteps=5340000, episode_reward=2699.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5340000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0758   |
|    ent_coef        | 0.0013   |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.000466 |
|    n_updates       | 5514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1068     |
|    fps             | 46       |
|    time_elapsed    | 114643   |
|    total_timesteps | 5340000  |
---------------------------------
Eval num_timesteps=5350000, episode_reward=2698.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5350000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0707   |
|    ent_coef        | 0.00112  |
|    ent_coef_loss   | -3.3     |
|    learning_rate   | 0.000465 |
|    n_updates       | 5524425  |
---------------------------------
Eval num_timesteps=5360000, episode_reward=2698.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5360000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.135    |
|    ent_coef        | 0.00111  |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.000464 |
|    n_updates       | 5534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1072     |
|    fps             | 46       |
|    time_elapsed    | 115134   |
|    total_timesteps | 5360000  |
---------------------------------
Eval num_timesteps=5370000, episode_reward=2697.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5370000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0873   |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | 4.27     |
|    learning_rate   | 0.000463 |
|    n_updates       | 5544425  |
---------------------------------
Eval num_timesteps=5380000, episode_reward=2699.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5380000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.017    |
|    ent_coef        | 0.000737 |
|    ent_coef_loss   | 2.96     |
|    learning_rate   | 0.000462 |
|    n_updates       | 5554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1076     |
|    fps             | 46       |
|    time_elapsed    | 115626   |
|    total_timesteps | 5380000  |
---------------------------------
Eval num_timesteps=5390000, episode_reward=2695.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5390000  |
| train/             |          |
|    actor_loss      | -54.7    |
|    critic_loss     | 0.59     |
|    ent_coef        | 0.000935 |
|    ent_coef_loss   | 9.36     |
|    learning_rate   | 0.000461 |
|    n_updates       | 5564425  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=2695.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5400000  |
| train/             |          |
|    actor_loss      | -54.7    |
|    critic_loss     | 0.0585   |
|    ent_coef        | 0.000826 |
|    ent_coef_loss   | -12.9    |
|    learning_rate   | 0.00046  |
|    n_updates       | 5574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1080     |
|    fps             | 46       |
|    time_elapsed    | 116117   |
|    total_timesteps | 5400000  |
---------------------------------
Eval num_timesteps=5410000, episode_reward=2695.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5410000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 0.0261   |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | 0.22     |
|    learning_rate   | 0.000459 |
|    n_updates       | 5584425  |
---------------------------------
Eval num_timesteps=5420000, episode_reward=2696.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5420000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.0264   |
|    ent_coef        | 0.000675 |
|    ent_coef_loss   | 4.28     |
|    learning_rate   | 0.000458 |
|    n_updates       | 5594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1084     |
|    fps             | 46       |
|    time_elapsed    | 116609   |
|    total_timesteps | 5420000  |
---------------------------------
Eval num_timesteps=5430000, episode_reward=2695.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5430000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.0175   |
|    ent_coef        | 0.000452 |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000457 |
|    n_updates       | 5604425  |
---------------------------------
Eval num_timesteps=5440000, episode_reward=2696.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5440000  |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 0.0532   |
|    ent_coef        | 0.000412 |
|    ent_coef_loss   | -5.26    |
|    learning_rate   | 0.000456 |
|    n_updates       | 5614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1088     |
|    fps             | 46       |
|    time_elapsed    | 117100   |
|    total_timesteps | 5440000  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=2695.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5450000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.0331   |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.000455 |
|    n_updates       | 5624425  |
---------------------------------
Eval num_timesteps=5460000, episode_reward=2697.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5460000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0519   |
|    ent_coef        | 0.000426 |
|    ent_coef_loss   | 6.14     |
|    learning_rate   | 0.000454 |
|    n_updates       | 5634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1092     |
|    fps             | 46       |
|    time_elapsed    | 117594   |
|    total_timesteps | 5460000  |
---------------------------------
Eval num_timesteps=5470000, episode_reward=2700.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5470000  |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 0.0229   |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | -5.77    |
|    learning_rate   | 0.000453 |
|    n_updates       | 5644425  |
---------------------------------
Eval num_timesteps=5480000, episode_reward=2699.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5480000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.000542 |
|    ent_coef_loss   | -2.12    |
|    learning_rate   | 0.000452 |
|    n_updates       | 5654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1096     |
|    fps             | 46       |
|    time_elapsed    | 118088   |
|    total_timesteps | 5480000  |
---------------------------------
Eval num_timesteps=5490000, episode_reward=2699.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5490000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.0579   |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.000451 |
|    n_updates       | 5664425  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=2698.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5500000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0219   |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | -12.3    |
|    learning_rate   | 0.00045  |
|    n_updates       | 5674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1100     |
|    fps             | 46       |
|    time_elapsed    | 118580   |
|    total_timesteps | 5500000  |
---------------------------------
Eval num_timesteps=5510000, episode_reward=2699.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5510000  |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | -33.2    |
|    learning_rate   | 0.000449 |
|    n_updates       | 5684425  |
---------------------------------
Eval num_timesteps=5520000, episode_reward=2698.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5520000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0697   |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | 0.28     |
|    learning_rate   | 0.000448 |
|    n_updates       | 5694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1104     |
|    fps             | 46       |
|    time_elapsed    | 119072   |
|    total_timesteps | 5520000  |
---------------------------------
Eval num_timesteps=5530000, episode_reward=2696.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5530000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.0383   |
|    ent_coef        | 0.000708 |
|    ent_coef_loss   | 5.35     |
|    learning_rate   | 0.000447 |
|    n_updates       | 5704425  |
---------------------------------
Eval num_timesteps=5540000, episode_reward=2695.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5540000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0322   |
|    ent_coef        | 0.000551 |
|    ent_coef_loss   | 1.79     |
|    learning_rate   | 0.000446 |
|    n_updates       | 5714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1108     |
|    fps             | 46       |
|    time_elapsed    | 119564   |
|    total_timesteps | 5540000  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=2695.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5550000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0373   |
|    ent_coef        | 0.000367 |
|    ent_coef_loss   | 28.3     |
|    learning_rate   | 0.000445 |
|    n_updates       | 5724425  |
---------------------------------
Eval num_timesteps=5560000, episode_reward=2697.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5560000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0166   |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | 9.17     |
|    learning_rate   | 0.000444 |
|    n_updates       | 5734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1112     |
|    fps             | 46       |
|    time_elapsed    | 120055   |
|    total_timesteps | 5560000  |
---------------------------------
Eval num_timesteps=5570000, episode_reward=2696.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5570000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.025    |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 10.1     |
|    learning_rate   | 0.000443 |
|    n_updates       | 5744425  |
---------------------------------
Eval num_timesteps=5580000, episode_reward=2696.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5580000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0465   |
|    ent_coef        | 0.000267 |
|    ent_coef_loss   | 2.14     |
|    learning_rate   | 0.000442 |
|    n_updates       | 5754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1116     |
|    fps             | 46       |
|    time_elapsed    | 120544   |
|    total_timesteps | 5580000  |
---------------------------------
Eval num_timesteps=5590000, episode_reward=2695.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5590000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.0178   |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | 13.5     |
|    learning_rate   | 0.000441 |
|    n_updates       | 5764425  |
---------------------------------
Eval num_timesteps=5600000, episode_reward=2698.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5600000  |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 0.021    |
|    ent_coef        | 0.00113  |
|    ent_coef_loss   | 36.4     |
|    learning_rate   | 0.00044  |
|    n_updates       | 5774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1120     |
|    fps             | 46       |
|    time_elapsed    | 121033   |
|    total_timesteps | 5600000  |
---------------------------------
Eval num_timesteps=5610000, episode_reward=2695.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5610000  |
| train/             |          |
|    actor_loss      | -54.7    |
|    critic_loss     | 0.0144   |
|    ent_coef        | 0.000564 |
|    ent_coef_loss   | -2.8     |
|    learning_rate   | 0.000439 |
|    n_updates       | 5784425  |
---------------------------------
Eval num_timesteps=5620000, episode_reward=2697.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5620000  |
| train/             |          |
|    actor_loss      | -55.1    |
|    critic_loss     | 0.0218   |
|    ent_coef        | 0.00107  |
|    ent_coef_loss   | -35.9    |
|    learning_rate   | 0.000438 |
|    n_updates       | 5794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1124     |
|    fps             | 46       |
|    time_elapsed    | 121523   |
|    total_timesteps | 5620000  |
---------------------------------
Eval num_timesteps=5630000, episode_reward=2699.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5630000  |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 0.0216   |
|    ent_coef        | 0.000264 |
|    ent_coef_loss   | -3.82    |
|    learning_rate   | 0.000437 |
|    n_updates       | 5804425  |
---------------------------------
Eval num_timesteps=5640000, episode_reward=2697.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5640000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.0676   |
|    ent_coef        | 0.00027  |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.000436 |
|    n_updates       | 5814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1128     |
|    fps             | 46       |
|    time_elapsed    | 122013   |
|    total_timesteps | 5640000  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=2689.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 5650000  |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 0.0317   |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | 4.86     |
|    learning_rate   | 0.000435 |
|    n_updates       | 5824425  |
---------------------------------
Eval num_timesteps=5660000, episode_reward=2698.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5660000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.00534  |
|    ent_coef        | 0.000305 |
|    ent_coef_loss   | 6.98     |
|    learning_rate   | 0.000434 |
|    n_updates       | 5834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1132     |
|    fps             | 46       |
|    time_elapsed    | 122501   |
|    total_timesteps | 5660000  |
---------------------------------
Eval num_timesteps=5670000, episode_reward=2695.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5670000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.00537  |
|    ent_coef        | 0.00037  |
|    ent_coef_loss   | -6.73    |
|    learning_rate   | 0.000433 |
|    n_updates       | 5844425  |
---------------------------------
Eval num_timesteps=5680000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5680000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.00978  |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 7.85     |
|    learning_rate   | 0.000432 |
|    n_updates       | 5854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1136     |
|    fps             | 46       |
|    time_elapsed    | 122990   |
|    total_timesteps | 5680000  |
---------------------------------
Eval num_timesteps=5690000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5690000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.0423   |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -5.5     |
|    learning_rate   | 0.000431 |
|    n_updates       | 5864425  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5700000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.276    |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | -5.99    |
|    learning_rate   | 0.00043  |
|    n_updates       | 5874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1140     |
|    fps             | 46       |
|    time_elapsed    | 123480   |
|    total_timesteps | 5700000  |
---------------------------------
Eval num_timesteps=5710000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5710000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.028    |
|    ent_coef        | 0.00092  |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000429 |
|    n_updates       | 5884425  |
---------------------------------
Eval num_timesteps=5720000, episode_reward=2697.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5720000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.00801  |
|    ent_coef        | 0.000834 |
|    ent_coef_loss   | -0.593   |
|    learning_rate   | 0.000428 |
|    n_updates       | 5894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1144     |
|    fps             | 46       |
|    time_elapsed    | 123970   |
|    total_timesteps | 5720000  |
---------------------------------
Eval num_timesteps=5730000, episode_reward=2700.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5730000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.0107   |
|    ent_coef        | 0.000489 |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.000427 |
|    n_updates       | 5904425  |
---------------------------------
Eval num_timesteps=5740000, episode_reward=2694.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 5740000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0245   |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | -8.71    |
|    learning_rate   | 0.000426 |
|    n_updates       | 5914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1148     |
|    fps             | 46       |
|    time_elapsed    | 124462   |
|    total_timesteps | 5740000  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=2694.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 5750000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.00726  |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | -14      |
|    learning_rate   | 0.000425 |
|    n_updates       | 5924425  |
---------------------------------
Eval num_timesteps=5760000, episode_reward=2696.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5760000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.013    |
|    ent_coef        | 0.000474 |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.000424 |
|    n_updates       | 5934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1152     |
|    fps             | 46       |
|    time_elapsed    | 124954   |
|    total_timesteps | 5760000  |
---------------------------------
Eval num_timesteps=5770000, episode_reward=2698.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5770000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0461   |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 21.2     |
|    learning_rate   | 0.000423 |
|    n_updates       | 5944425  |
---------------------------------
Eval num_timesteps=5780000, episode_reward=2700.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5780000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00129  |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.000422 |
|    n_updates       | 5954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1156     |
|    fps             | 46       |
|    time_elapsed    | 125453   |
|    total_timesteps | 5780000  |
---------------------------------
Eval num_timesteps=5790000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5790000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0317   |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 10.2     |
|    learning_rate   | 0.000421 |
|    n_updates       | 5964425  |
---------------------------------
Eval num_timesteps=5800000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5800000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0357   |
|    ent_coef        | 0.000642 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00042  |
|    n_updates       | 5974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1160     |
|    fps             | 46       |
|    time_elapsed    | 125941   |
|    total_timesteps | 5800000  |
---------------------------------
Eval num_timesteps=5810000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5810000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0559   |
|    ent_coef        | 0.000702 |
|    ent_coef_loss   | 65.6     |
|    learning_rate   | 0.000419 |
|    n_updates       | 5984425  |
---------------------------------
Eval num_timesteps=5820000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5820000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0344   |
|    ent_coef        | 0.000635 |
|    ent_coef_loss   | 1.26     |
|    learning_rate   | 0.000418 |
|    n_updates       | 5994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 1164     |
|    fps             | 46       |
|    time_elapsed    | 126431   |
|    total_timesteps | 5820000  |
---------------------------------
Eval num_timesteps=5830000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5830000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.00787  |
|    ent_coef        | 0.000854 |
|    ent_coef_loss   | 7.2      |
|    learning_rate   | 0.000417 |
|    n_updates       | 6004425  |
---------------------------------
Eval num_timesteps=5840000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5840000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0184   |
|    ent_coef        | 0.00107  |
|    ent_coef_loss   | 6.61     |
|    learning_rate   | 0.000416 |
|    n_updates       | 6014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 1168     |
|    fps             | 46       |
|    time_elapsed    | 126922   |
|    total_timesteps | 5840000  |
---------------------------------
Eval num_timesteps=5850000, episode_reward=2518.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 5850000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.00942  |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | -6.17    |
|    learning_rate   | 0.000415 |
|    n_updates       | 6024425  |
---------------------------------
Eval num_timesteps=5860000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5860000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0272   |
|    ent_coef        | 0.00158  |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.000414 |
|    n_updates       | 6034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 1172     |
|    fps             | 45       |
|    time_elapsed    | 127413   |
|    total_timesteps | 5860000  |
---------------------------------
Eval num_timesteps=5870000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5870000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.00842  |
|    ent_coef        | 0.0012   |
|    ent_coef_loss   | -15.4    |
|    learning_rate   | 0.000413 |
|    n_updates       | 6044425  |
---------------------------------
Eval num_timesteps=5880000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5880000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.00824  |
|    ent_coef        | 0.00129  |
|    ent_coef_loss   | 17.2     |
|    learning_rate   | 0.000412 |
|    n_updates       | 6054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 1176     |
|    fps             | 45       |
|    time_elapsed    | 127902   |
|    total_timesteps | 5880000  |
---------------------------------
Eval num_timesteps=5890000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5890000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0198   |
|    ent_coef        | 0.00117  |
|    ent_coef_loss   | 12.5     |
|    learning_rate   | 0.000411 |
|    n_updates       | 6064425  |
---------------------------------
Eval num_timesteps=5900000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5900000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.00528  |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | -3.12    |
|    learning_rate   | 0.00041  |
|    n_updates       | 6074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1180     |
|    fps             | 45       |
|    time_elapsed    | 128394   |
|    total_timesteps | 5900000  |
---------------------------------
Eval num_timesteps=5910000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5910000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00382  |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | -5.64    |
|    learning_rate   | 0.000409 |
|    n_updates       | 6084425  |
---------------------------------
Eval num_timesteps=5920000, episode_reward=2518.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 5920000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.0766   |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | 8.4      |
|    learning_rate   | 0.000408 |
|    n_updates       | 6094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1184     |
|    fps             | 45       |
|    time_elapsed    | 128885   |
|    total_timesteps | 5920000  |
---------------------------------
Eval num_timesteps=5930000, episode_reward=2511.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 5930000  |
| train/             |          |
|    actor_loss      | -49.9    |
|    critic_loss     | 0.00616  |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | 8.37     |
|    learning_rate   | 0.000407 |
|    n_updates       | 6104425  |
---------------------------------
Eval num_timesteps=5940000, episode_reward=2704.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5940000  |
| train/             |          |
|    actor_loss      | -49.9    |
|    critic_loss     | 0.00978  |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | 10.5     |
|    learning_rate   | 0.000406 |
|    n_updates       | 6114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1188     |
|    fps             | 45       |
|    time_elapsed    | 129377   |
|    total_timesteps | 5940000  |
---------------------------------
Eval num_timesteps=5950000, episode_reward=2704.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5950000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.00474  |
|    ent_coef        | 0.00264  |
|    ent_coef_loss   | 0.368    |
|    learning_rate   | 0.000405 |
|    n_updates       | 6124425  |
---------------------------------
Eval num_timesteps=5960000, episode_reward=2703.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5960000  |
| train/             |          |
|    actor_loss      | -48.6    |
|    critic_loss     | 0.00837  |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | 6.95     |
|    learning_rate   | 0.000404 |
|    n_updates       | 6134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1192     |
|    fps             | 45       |
|    time_elapsed    | 129871   |
|    total_timesteps | 5960000  |
---------------------------------
Eval num_timesteps=5970000, episode_reward=2696.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5970000  |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 0.0087   |
|    ent_coef        | 0.00259  |
|    ent_coef_loss   | 11.4     |
|    learning_rate   | 0.000403 |
|    n_updates       | 6144425  |
---------------------------------
Eval num_timesteps=5980000, episode_reward=2699.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 5980000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 0.0069   |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -5.74    |
|    learning_rate   | 0.000402 |
|    n_updates       | 6154425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1196     |
|    fps             | 45       |
|    time_elapsed    | 130363   |
|    total_timesteps | 5980000  |
---------------------------------
Eval num_timesteps=5990000, episode_reward=2705.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 5990000  |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 0.00705  |
|    ent_coef        | 0.0019   |
|    ent_coef_loss   | 6.95     |
|    learning_rate   | 0.000401 |
|    n_updates       | 6164425  |
---------------------------------
Eval num_timesteps=6000000, episode_reward=2700.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6000000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.012    |
|    ent_coef        | 0.00169  |
|    ent_coef_loss   | -0.862   |
|    learning_rate   | 0.0004   |
|    n_updates       | 6174425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1200     |
|    fps             | 45       |
|    time_elapsed    | 130856   |
|    total_timesteps | 6000000  |
---------------------------------
Eval num_timesteps=6010000, episode_reward=2703.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6010000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.00984  |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | -3.53    |
|    learning_rate   | 0.000399 |
|    n_updates       | 6184425  |
---------------------------------
Eval num_timesteps=6020000, episode_reward=2701.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6020000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.00307  |
|    ent_coef        | 0.000304 |
|    ent_coef_loss   | 2.37     |
|    learning_rate   | 0.000398 |
|    n_updates       | 6194425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1204     |
|    fps             | 45       |
|    time_elapsed    | 131346   |
|    total_timesteps | 6020000  |
---------------------------------
Eval num_timesteps=6030000, episode_reward=2698.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6030000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.000308 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.000397 |
|    n_updates       | 6204425  |
---------------------------------
Eval num_timesteps=6040000, episode_reward=2700.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6040000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0133   |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | 34.5     |
|    learning_rate   | 0.000396 |
|    n_updates       | 6214425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1208     |
|    fps             | 45       |
|    time_elapsed    | 131838   |
|    total_timesteps | 6040000  |
---------------------------------
Eval num_timesteps=6050000, episode_reward=2704.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6050000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.0104   |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.000395 |
|    n_updates       | 6224425  |
---------------------------------
Eval num_timesteps=6060000, episode_reward=2705.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 6060000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.00859  |
|    ent_coef        | 0.00221  |
|    ent_coef_loss   | 4.75     |
|    learning_rate   | 0.000394 |
|    n_updates       | 6234425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1212     |
|    fps             | 45       |
|    time_elapsed    | 132332   |
|    total_timesteps | 6060000  |
---------------------------------
Eval num_timesteps=6070000, episode_reward=2704.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6070000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.0116   |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | -2.45    |
|    learning_rate   | 0.000393 |
|    n_updates       | 6244425  |
---------------------------------
Eval num_timesteps=6080000, episode_reward=2695.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6080000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.00485  |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | 0.313    |
|    learning_rate   | 0.000392 |
|    n_updates       | 6254425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1216     |
|    fps             | 45       |
|    time_elapsed    | 132825   |
|    total_timesteps | 6080000  |
---------------------------------
Eval num_timesteps=6090000, episode_reward=2698.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6090000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 0.000288 |
|    ent_coef_loss   | -3.39    |
|    learning_rate   | 0.000391 |
|    n_updates       | 6264425  |
---------------------------------
Eval num_timesteps=6100000, episode_reward=2699.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6100000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00466  |
|    ent_coef        | 0.0025   |
|    ent_coef_loss   | 9.45     |
|    learning_rate   | 0.00039  |
|    n_updates       | 6274425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1220     |
|    fps             | 45       |
|    time_elapsed    | 133317   |
|    total_timesteps | 6100000  |
---------------------------------
Eval num_timesteps=6110000, episode_reward=2702.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6110000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0324   |
|    ent_coef        | 0.00231  |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000389 |
|    n_updates       | 6284425  |
---------------------------------
Eval num_timesteps=6120000, episode_reward=2701.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6120000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.0244   |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -2.12    |
|    learning_rate   | 0.000388 |
|    n_updates       | 6294425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1224     |
|    fps             | 45       |
|    time_elapsed    | 133809   |
|    total_timesteps | 6120000  |
---------------------------------
Eval num_timesteps=6130000, episode_reward=2698.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6130000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.0145   |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | 3.78     |
|    learning_rate   | 0.000387 |
|    n_updates       | 6304425  |
---------------------------------
Eval num_timesteps=6140000, episode_reward=2697.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6140000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00627  |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | -22.3    |
|    learning_rate   | 0.000386 |
|    n_updates       | 6314425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1228     |
|    fps             | 45       |
|    time_elapsed    | 134300   |
|    total_timesteps | 6140000  |
---------------------------------
Eval num_timesteps=6150000, episode_reward=2700.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6150000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 0.00509  |
|    ent_coef        | 0.00209  |
|    ent_coef_loss   | 10.3     |
|    learning_rate   | 0.000385 |
|    n_updates       | 6324425  |
---------------------------------
Eval num_timesteps=6160000, episode_reward=2698.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6160000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00321  |
|    ent_coef        | 0.00226  |
|    ent_coef_loss   | -14.3    |
|    learning_rate   | 0.000384 |
|    n_updates       | 6334425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1232     |
|    fps             | 45       |
|    time_elapsed    | 134792   |
|    total_timesteps | 6160000  |
---------------------------------
Eval num_timesteps=6170000, episode_reward=2699.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6170000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.0137   |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | 7.1      |
|    learning_rate   | 0.000383 |
|    n_updates       | 6344425  |
---------------------------------
Eval num_timesteps=6180000, episode_reward=2698.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6180000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.00687  |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -3.74    |
|    learning_rate   | 0.000382 |
|    n_updates       | 6354425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 1236     |
|    fps             | 45       |
|    time_elapsed    | 135285   |
|    total_timesteps | 6180000  |
---------------------------------
Eval num_timesteps=6190000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6190000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0223   |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | -5.45    |
|    learning_rate   | 0.000381 |
|    n_updates       | 6364425  |
---------------------------------
Eval num_timesteps=6200000, episode_reward=2697.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6200000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.0124   |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | -17.5    |
|    learning_rate   | 0.00038  |
|    n_updates       | 6374425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1240     |
|    fps             | 45       |
|    time_elapsed    | 135778   |
|    total_timesteps | 6200000  |
---------------------------------
Eval num_timesteps=6210000, episode_reward=2699.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6210000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.0177   |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.000379 |
|    n_updates       | 6384425  |
---------------------------------
Eval num_timesteps=6220000, episode_reward=2699.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6220000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00163  |
|    ent_coef        | 0.00158  |
|    ent_coef_loss   | -9.96    |
|    learning_rate   | 0.000378 |
|    n_updates       | 6394425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1244     |
|    fps             | 45       |
|    time_elapsed    | 136272   |
|    total_timesteps | 6220000  |
---------------------------------
Eval num_timesteps=6230000, episode_reward=2701.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6230000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0015   |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | -6.88    |
|    learning_rate   | 0.000377 |
|    n_updates       | 6404425  |
---------------------------------
Eval num_timesteps=6240000, episode_reward=2698.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6240000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.00361  |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | -0.208   |
|    learning_rate   | 0.000376 |
|    n_updates       | 6414425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1248     |
|    fps             | 45       |
|    time_elapsed    | 136764   |
|    total_timesteps | 6240000  |
---------------------------------
Eval num_timesteps=6250000, episode_reward=2700.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6250000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0109   |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | -5.41    |
|    learning_rate   | 0.000375 |
|    n_updates       | 6424425  |
---------------------------------
Eval num_timesteps=6260000, episode_reward=2699.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6260000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.0168   |
|    ent_coef        | 0.00212  |
|    ent_coef_loss   | 17.5     |
|    learning_rate   | 0.000374 |
|    n_updates       | 6434425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1252     |
|    fps             | 45       |
|    time_elapsed    | 137257   |
|    total_timesteps | 6260000  |
---------------------------------
Eval num_timesteps=6270000, episode_reward=2699.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6270000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0561   |
|    ent_coef        | 0.00214  |
|    ent_coef_loss   | -14.6    |
|    learning_rate   | 0.000373 |
|    n_updates       | 6444425  |
---------------------------------
Eval num_timesteps=6280000, episode_reward=2699.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6280000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | 4.89     |
|    learning_rate   | 0.000372 |
|    n_updates       | 6454425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 1256     |
|    fps             | 45       |
|    time_elapsed    | 137753   |
|    total_timesteps | 6280000  |
---------------------------------
Eval num_timesteps=6290000, episode_reward=2699.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6290000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.0514   |
|    ent_coef        | 0.00151  |
|    ent_coef_loss   | -4.08    |
|    learning_rate   | 0.000371 |
|    n_updates       | 6464425  |
---------------------------------
Eval num_timesteps=6300000, episode_reward=2696.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6300000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.0053   |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | 11.7     |
|    learning_rate   | 0.00037  |
|    n_updates       | 6474425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 1260     |
|    fps             | 45       |
|    time_elapsed    | 138247   |
|    total_timesteps | 6300000  |
---------------------------------
Eval num_timesteps=6310000, episode_reward=2695.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6310000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.0552   |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | 5.27     |
|    learning_rate   | 0.000369 |
|    n_updates       | 6484425  |
---------------------------------
Eval num_timesteps=6320000, episode_reward=2697.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6320000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.00409  |
|    ent_coef        | 0.00135  |
|    ent_coef_loss   | 13.6     |
|    learning_rate   | 0.000368 |
|    n_updates       | 6494425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 1264     |
|    fps             | 45       |
|    time_elapsed    | 138741   |
|    total_timesteps | 6320000  |
---------------------------------
Eval num_timesteps=6330000, episode_reward=2694.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6330000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.00228  |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | -0.375   |
|    learning_rate   | 0.000367 |
|    n_updates       | 6504425  |
---------------------------------
Eval num_timesteps=6340000, episode_reward=2699.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6340000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.00927  |
|    ent_coef        | 0.00137  |
|    ent_coef_loss   | 8.32     |
|    learning_rate   | 0.000366 |
|    n_updates       | 6514425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 1268     |
|    fps             | 45       |
|    time_elapsed    | 139235   |
|    total_timesteps | 6340000  |
---------------------------------
Eval num_timesteps=6350000, episode_reward=2513.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 6350000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.0181   |
|    ent_coef        | 0.00119  |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.000365 |
|    n_updates       | 6524425  |
---------------------------------
Eval num_timesteps=6360000, episode_reward=2697.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6360000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.007    |
|    ent_coef        | 0.00164  |
|    ent_coef_loss   | -6.85    |
|    learning_rate   | 0.000364 |
|    n_updates       | 6534425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 1272     |
|    fps             | 45       |
|    time_elapsed    | 139726   |
|    total_timesteps | 6360000  |
---------------------------------
Eval num_timesteps=6370000, episode_reward=2697.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6370000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.0261   |
|    ent_coef        | 0.00136  |
|    ent_coef_loss   | 2.71     |
|    learning_rate   | 0.000363 |
|    n_updates       | 6544425  |
---------------------------------
Eval num_timesteps=6380000, episode_reward=2698.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6380000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.00984  |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -9       |
|    learning_rate   | 0.000362 |
|    n_updates       | 6554425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1276     |
|    fps             | 45       |
|    time_elapsed    | 140219   |
|    total_timesteps | 6380000  |
---------------------------------
Eval num_timesteps=6390000, episode_reward=2696.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6390000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.0133   |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | 12.9     |
|    learning_rate   | 0.000361 |
|    n_updates       | 6564425  |
---------------------------------
Eval num_timesteps=6400000, episode_reward=2691.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6400000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.007    |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -0.767   |
|    learning_rate   | 0.00036  |
|    n_updates       | 6574425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1280     |
|    fps             | 45       |
|    time_elapsed    | 140712   |
|    total_timesteps | 6400000  |
---------------------------------
Eval num_timesteps=6410000, episode_reward=2693.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6410000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.00708  |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | -22.6    |
|    learning_rate   | 0.000359 |
|    n_updates       | 6584425  |
---------------------------------
Eval num_timesteps=6420000, episode_reward=2694.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6420000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.00125  |
|    ent_coef_loss   | 4.05     |
|    learning_rate   | 0.000358 |
|    n_updates       | 6594425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1284     |
|    fps             | 45       |
|    time_elapsed    | 141206   |
|    total_timesteps | 6420000  |
---------------------------------
Eval num_timesteps=6430000, episode_reward=2695.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6430000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.0427   |
|    ent_coef        | 0.00147  |
|    ent_coef_loss   | -4.46    |
|    learning_rate   | 0.000357 |
|    n_updates       | 6604425  |
---------------------------------
Eval num_timesteps=6440000, episode_reward=2694.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6440000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00713  |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | 5.11     |
|    learning_rate   | 0.000356 |
|    n_updates       | 6614425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1288     |
|    fps             | 45       |
|    time_elapsed    | 141701   |
|    total_timesteps | 6440000  |
---------------------------------
Eval num_timesteps=6450000, episode_reward=2695.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6450000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.0128   |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.000355 |
|    n_updates       | 6624425  |
---------------------------------
Eval num_timesteps=6460000, episode_reward=2692.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6460000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.00213  |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000354 |
|    n_updates       | 6634425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1292     |
|    fps             | 45       |
|    time_elapsed    | 142196   |
|    total_timesteps | 6460000  |
---------------------------------
Eval num_timesteps=6470000, episode_reward=2513.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 6470000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.00528  |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000353 |
|    n_updates       | 6644425  |
---------------------------------
Eval num_timesteps=6480000, episode_reward=2696.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6480000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.00173  |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | 4.33     |
|    learning_rate   | 0.000352 |
|    n_updates       | 6654425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1296     |
|    fps             | 45       |
|    time_elapsed    | 142688   |
|    total_timesteps | 6480000  |
---------------------------------
Eval num_timesteps=6490000, episode_reward=2696.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6490000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.0192   |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -8.52    |
|    learning_rate   | 0.000351 |
|    n_updates       | 6664425  |
---------------------------------
Eval num_timesteps=6500000, episode_reward=2694.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6500000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.00155  |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.00035  |
|    n_updates       | 6674425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 1300     |
|    fps             | 45       |
|    time_elapsed    | 143182   |
|    total_timesteps | 6500000  |
---------------------------------
Eval num_timesteps=6510000, episode_reward=2710.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 6510000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.00271  |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -13      |
|    learning_rate   | 0.000349 |
|    n_updates       | 6684425  |
---------------------------------
Eval num_timesteps=6520000, episode_reward=2517.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 6520000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.00607  |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000348 |
|    n_updates       | 6694425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1304     |
|    fps             | 45       |
|    time_elapsed    | 143674   |
|    total_timesteps | 6520000  |
---------------------------------
Eval num_timesteps=6530000, episode_reward=2699.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6530000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.0073   |
|    ent_coef        | 0.00228  |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.000347 |
|    n_updates       | 6704425  |
---------------------------------
Eval num_timesteps=6540000, episode_reward=2514.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 6540000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.00718  |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | -1.83    |
|    learning_rate   | 0.000346 |
|    n_updates       | 6714425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1308     |
|    fps             | 45       |
|    time_elapsed    | 144168   |
|    total_timesteps | 6540000  |
---------------------------------
Eval num_timesteps=6550000, episode_reward=2692.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6550000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.00288  |
|    ent_coef        | 0.00206  |
|    ent_coef_loss   | -14.3    |
|    learning_rate   | 0.000345 |
|    n_updates       | 6724425  |
---------------------------------
Eval num_timesteps=6560000, episode_reward=2699.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6560000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.00192  |
|    ent_coef        | 0.00199  |
|    ent_coef_loss   | -5.52    |
|    learning_rate   | 0.000344 |
|    n_updates       | 6734425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1312     |
|    fps             | 45       |
|    time_elapsed    | 144662   |
|    total_timesteps | 6560000  |
---------------------------------
Eval num_timesteps=6570000, episode_reward=2697.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6570000  |
| train/             |          |
|    actor_loss      | -49.9    |
|    critic_loss     | 0.011    |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000343 |
|    n_updates       | 6744425  |
---------------------------------
Eval num_timesteps=6580000, episode_reward=2699.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6580000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.00277  |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | 3.76     |
|    learning_rate   | 0.000342 |
|    n_updates       | 6754425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1316     |
|    fps             | 45       |
|    time_elapsed    | 145154   |
|    total_timesteps | 6580000  |
---------------------------------
Eval num_timesteps=6590000, episode_reward=2697.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6590000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.00693  |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | 5.2      |
|    learning_rate   | 0.000341 |
|    n_updates       | 6764425  |
---------------------------------
Eval num_timesteps=6600000, episode_reward=2697.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6600000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.00483  |
|    ent_coef        | 0.000636 |
|    ent_coef_loss   | -41.8    |
|    learning_rate   | 0.00034  |
|    n_updates       | 6774425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1320     |
|    fps             | 45       |
|    time_elapsed    | 145647   |
|    total_timesteps | 6600000  |
---------------------------------
Eval num_timesteps=6610000, episode_reward=2695.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6610000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0215   |
|    ent_coef        | 0.000366 |
|    ent_coef_loss   | 0.211    |
|    learning_rate   | 0.000339 |
|    n_updates       | 6784425  |
---------------------------------
Eval num_timesteps=6620000, episode_reward=2695.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6620000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.00629  |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | -0.953   |
|    learning_rate   | 0.000338 |
|    n_updates       | 6794425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1324     |
|    fps             | 45       |
|    time_elapsed    | 146138   |
|    total_timesteps | 6620000  |
---------------------------------
Eval num_timesteps=6630000, episode_reward=2696.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6630000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0187   |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | 25.9     |
|    learning_rate   | 0.000337 |
|    n_updates       | 6804425  |
---------------------------------
Eval num_timesteps=6640000, episode_reward=2695.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6640000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00252  |
|    ent_coef        | 0.000234 |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.000336 |
|    n_updates       | 6814425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1328     |
|    fps             | 45       |
|    time_elapsed    | 146627   |
|    total_timesteps | 6640000  |
---------------------------------
Eval num_timesteps=6650000, episode_reward=2696.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6650000  |
| train/             |          |
|    actor_loss      | -52.9    |
|    critic_loss     | 0.00163  |
|    ent_coef        | 0.000195 |
|    ent_coef_loss   | -9.36    |
|    learning_rate   | 0.000335 |
|    n_updates       | 6824425  |
---------------------------------
Eval num_timesteps=6660000, episode_reward=2694.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6660000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00751  |
|    ent_coef        | 0.000277 |
|    ent_coef_loss   | 21.5     |
|    learning_rate   | 0.000334 |
|    n_updates       | 6834425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1332     |
|    fps             | 45       |
|    time_elapsed    | 147120   |
|    total_timesteps | 6660000  |
---------------------------------
Eval num_timesteps=6670000, episode_reward=2696.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6670000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.00288  |
|    ent_coef        | 0.000156 |
|    ent_coef_loss   | -4.2     |
|    learning_rate   | 0.000333 |
|    n_updates       | 6844425  |
---------------------------------
Eval num_timesteps=6680000, episode_reward=2696.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6680000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0014   |
|    ent_coef        | 0.000156 |
|    ent_coef_loss   | 1.52     |
|    learning_rate   | 0.000332 |
|    n_updates       | 6854425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1336     |
|    fps             | 45       |
|    time_elapsed    | 147612   |
|    total_timesteps | 6680000  |
---------------------------------
Eval num_timesteps=6690000, episode_reward=2696.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6690000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0124   |
|    ent_coef        | 0.000141 |
|    ent_coef_loss   | 0.795    |
|    learning_rate   | 0.000331 |
|    n_updates       | 6864425  |
---------------------------------
Eval num_timesteps=6700000, episode_reward=2696.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6700000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.000467 |
|    ent_coef        | 0.000162 |
|    ent_coef_loss   | 0.365    |
|    learning_rate   | 0.00033  |
|    n_updates       | 6874425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1340     |
|    fps             | 45       |
|    time_elapsed    | 148102   |
|    total_timesteps | 6700000  |
---------------------------------
Eval num_timesteps=6710000, episode_reward=2695.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6710000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00356  |
|    ent_coef        | 0.000137 |
|    ent_coef_loss   | -23      |
|    learning_rate   | 0.000329 |
|    n_updates       | 6884425  |
---------------------------------
Eval num_timesteps=6720000, episode_reward=2696.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6720000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.00298  |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | -25.8    |
|    learning_rate   | 0.000328 |
|    n_updates       | 6894425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1344     |
|    fps             | 45       |
|    time_elapsed    | 148595   |
|    total_timesteps | 6720000  |
---------------------------------
Eval num_timesteps=6730000, episode_reward=2693.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6730000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0176   |
|    ent_coef        | 0.00014  |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.000327 |
|    n_updates       | 6904425  |
---------------------------------
Eval num_timesteps=6740000, episode_reward=2695.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6740000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.012    |
|    ent_coef        | 0.000115 |
|    ent_coef_loss   | -25.3    |
|    learning_rate   | 0.000326 |
|    n_updates       | 6914425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1348     |
|    fps             | 45       |
|    time_elapsed    | 149087   |
|    total_timesteps | 6740000  |
---------------------------------
Eval num_timesteps=6750000, episode_reward=2697.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6750000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00471  |
|    ent_coef        | 0.000112 |
|    ent_coef_loss   | 17.4     |
|    learning_rate   | 0.000325 |
|    n_updates       | 6924425  |
---------------------------------
Eval num_timesteps=6760000, episode_reward=2696.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6760000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.0464   |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | 9.1      |
|    learning_rate   | 0.000324 |
|    n_updates       | 6934425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1352     |
|    fps             | 45       |
|    time_elapsed    | 149579   |
|    total_timesteps | 6760000  |
---------------------------------
Eval num_timesteps=6770000, episode_reward=2694.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6770000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.00405  |
|    ent_coef        | 0.00024  |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.000323 |
|    n_updates       | 6944425  |
---------------------------------
Eval num_timesteps=6780000, episode_reward=2699.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6780000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.00491  |
|    ent_coef        | 0.000178 |
|    ent_coef_loss   | -6.15    |
|    learning_rate   | 0.000322 |
|    n_updates       | 6954425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1356     |
|    fps             | 45       |
|    time_elapsed    | 150072   |
|    total_timesteps | 6780000  |
---------------------------------
Eval num_timesteps=6790000, episode_reward=2695.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6790000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.00346  |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.000321 |
|    n_updates       | 6964425  |
---------------------------------
Eval num_timesteps=6800000, episode_reward=2532.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 6800000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.00503  |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00032  |
|    n_updates       | 6974425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1360     |
|    fps             | 45       |
|    time_elapsed    | 150561   |
|    total_timesteps | 6800000  |
---------------------------------
Eval num_timesteps=6810000, episode_reward=2693.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6810000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.00385  |
|    ent_coef        | 0.000311 |
|    ent_coef_loss   | 15       |
|    learning_rate   | 0.000319 |
|    n_updates       | 6984425  |
---------------------------------
Eval num_timesteps=6820000, episode_reward=2693.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6820000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0208   |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | 17.9     |
|    learning_rate   | 0.000318 |
|    n_updates       | 6994425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1364     |
|    fps             | 45       |
|    time_elapsed    | 151051   |
|    total_timesteps | 6820000  |
---------------------------------
Eval num_timesteps=6830000, episode_reward=2693.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6830000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00887  |
|    ent_coef        | 0.000249 |
|    ent_coef_loss   | 1.73     |
|    learning_rate   | 0.000317 |
|    n_updates       | 7004425  |
---------------------------------
Eval num_timesteps=6840000, episode_reward=2694.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6840000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.00202  |
|    ent_coef        | 0.000248 |
|    ent_coef_loss   | -19.1    |
|    learning_rate   | 0.000316 |
|    n_updates       | 7014425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 1368     |
|    fps             | 45       |
|    time_elapsed    | 151543   |
|    total_timesteps | 6840000  |
---------------------------------
Eval num_timesteps=6850000, episode_reward=2694.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6850000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 0.00436  |
|    ent_coef        | 0.000247 |
|    ent_coef_loss   | -7.54    |
|    learning_rate   | 0.000315 |
|    n_updates       | 7024425  |
---------------------------------
Eval num_timesteps=6860000, episode_reward=2694.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6860000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00927  |
|    ent_coef        | 0.000234 |
|    ent_coef_loss   | 23.1     |
|    learning_rate   | 0.000314 |
|    n_updates       | 7034425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1372     |
|    fps             | 45       |
|    time_elapsed    | 152033   |
|    total_timesteps | 6860000  |
---------------------------------
Eval num_timesteps=6870000, episode_reward=2694.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6870000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0109   |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | 20.6     |
|    learning_rate   | 0.000313 |
|    n_updates       | 7044425  |
---------------------------------
Eval num_timesteps=6880000, episode_reward=2695.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 6880000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0039   |
|    ent_coef        | 0.000191 |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.000312 |
|    n_updates       | 7054425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1376     |
|    fps             | 45       |
|    time_elapsed    | 152524   |
|    total_timesteps | 6880000  |
---------------------------------
Eval num_timesteps=6890000, episode_reward=2693.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6890000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00475  |
|    ent_coef        | 0.000184 |
|    ent_coef_loss   | -8.05    |
|    learning_rate   | 0.000311 |
|    n_updates       | 7064425  |
---------------------------------
Eval num_timesteps=6900000, episode_reward=2693.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6900000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00821  |
|    ent_coef        | 9.79e-05 |
|    ent_coef_loss   | 24.1     |
|    learning_rate   | 0.00031  |
|    n_updates       | 7074425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1380     |
|    fps             | 45       |
|    time_elapsed    | 153019   |
|    total_timesteps | 6900000  |
---------------------------------
Eval num_timesteps=6910000, episode_reward=2693.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6910000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0137   |
|    ent_coef        | 0.000194 |
|    ent_coef_loss   | 6.64     |
|    learning_rate   | 0.000309 |
|    n_updates       | 7084425  |
---------------------------------
Eval num_timesteps=6920000, episode_reward=2690.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6920000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00309  |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | -26.3    |
|    learning_rate   | 0.000308 |
|    n_updates       | 7094425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1384     |
|    fps             | 45       |
|    time_elapsed    | 153508   |
|    total_timesteps | 6920000  |
---------------------------------
Eval num_timesteps=6930000, episode_reward=2691.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6930000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000111 |
|    ent_coef_loss   | 34.8     |
|    learning_rate   | 0.000307 |
|    n_updates       | 7104425  |
---------------------------------
Eval num_timesteps=6940000, episode_reward=2695.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6940000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0155   |
|    ent_coef        | 8.83e-05 |
|    ent_coef_loss   | 29.1     |
|    learning_rate   | 0.000306 |
|    n_updates       | 7114425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1388     |
|    fps             | 45       |
|    time_elapsed    | 153998   |
|    total_timesteps | 6940000  |
---------------------------------
Eval num_timesteps=6950000, episode_reward=2687.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6950000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.0152   |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | 6.86     |
|    learning_rate   | 0.000305 |
|    n_updates       | 7124425  |
---------------------------------
Eval num_timesteps=6960000, episode_reward=2687.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 6960000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.00328  |
|    ent_coef        | 0.00015  |
|    ent_coef_loss   | -3.93    |
|    learning_rate   | 0.000304 |
|    n_updates       | 7134425  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 1392     |
|    fps             | 45       |
|    time_elapsed    | 154491   |
|    total_timesteps | 6960000  |
---------------------------------
Eval num_timesteps=6970000, episode_reward=2684.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 6970000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.00176  |
|    ent_coef        | 0.000131 |
|    ent_coef_loss   | 13.7     |
|    learning_rate   | 0.000303 |
|    n_updates       | 7144425  |
