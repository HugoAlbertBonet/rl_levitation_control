Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a DummyVecEnv.
Logging to 5gdl/SAC_49
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Eval num_timesteps=10000, episode_reward=2105.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.11e+03 |
| time/              |          |
|    total_timesteps | 10000    |
| train/             |          |
|    actor_loss      | -31.4    |
|    critic_loss     | 0.172    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -4.89    |
|    learning_rate   | 0.000999 |
|    n_updates       | 1354324  |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=2234.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -36.1    |
|    critic_loss     | 0.103    |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | 5.32     |
|    learning_rate   | 0.000998 |
|    n_updates       | 1364324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 4        |
|    fps             | 38       |
|    time_elapsed    | 521      |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=2220.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | -39.6    |
|    critic_loss     | 0.158    |
|    ent_coef        | 0.0017   |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.000997 |
|    n_updates       | 1374324  |
---------------------------------
Eval num_timesteps=40000, episode_reward=2258.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | -42      |
|    critic_loss     | 0.0847   |
|    ent_coef        | 0.00204  |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.000996 |
|    n_updates       | 1384324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 8        |
|    fps             | 38       |
|    time_elapsed    | 1034     |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1783.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -43.2    |
|    critic_loss     | 0.362    |
|    ent_coef        | 0.0014   |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.000995 |
|    n_updates       | 1394324  |
---------------------------------
Eval num_timesteps=60000, episode_reward=2923.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -44.3    |
|    critic_loss     | 0.0753   |
|    ent_coef        | 0.00106  |
|    ent_coef_loss   | 5.42     |
|    learning_rate   | 0.000994 |
|    n_updates       | 1404324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 12       |
|    fps             | 38       |
|    time_elapsed    | 1548     |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=2250.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -41.8    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.000993 |
|    n_updates       | 1414324  |
---------------------------------
Eval num_timesteps=80000, episode_reward=2265.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -44.1    |
|    critic_loss     | 0.0583   |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.000992 |
|    n_updates       | 1424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 16       |
|    fps             | 39       |
|    time_elapsed    | 2043     |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=90000, episode_reward=2265.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | -45      |
|    critic_loss     | 0.183    |
|    ent_coef        | 0.00148  |
|    ent_coef_loss   | 7.08     |
|    learning_rate   | 0.000991 |
|    n_updates       | 1434324  |
---------------------------------
Eval num_timesteps=100000, episode_reward=2121.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | -45.1    |
|    critic_loss     | 0.0902   |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.00099  |
|    n_updates       | 1444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 20       |
|    fps             | 39       |
|    time_elapsed    | 2539     |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=110000, episode_reward=2251.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00226  |
|    ent_coef_loss   | -3.03    |
|    learning_rate   | 0.000989 |
|    n_updates       | 1454324  |
---------------------------------
Eval num_timesteps=120000, episode_reward=1976.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | -44.8    |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000988 |
|    n_updates       | 1464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 24       |
|    fps             | 39       |
|    time_elapsed    | 3035     |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1729.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -44.2    |
|    critic_loss     | 0.114    |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | -5.24    |
|    learning_rate   | 0.000987 |
|    n_updates       | 1474324  |
---------------------------------
Eval num_timesteps=140000, episode_reward=2578.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -43.1    |
|    critic_loss     | 0.0557   |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -3.29    |
|    learning_rate   | 0.000986 |
|    n_updates       | 1484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 28       |
|    fps             | 39       |
|    time_elapsed    | 3529     |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=3007.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -43.6    |
|    critic_loss     | 0.0696   |
|    ent_coef        | 0.00227  |
|    ent_coef_loss   | 5.67     |
|    learning_rate   | 0.000985 |
|    n_updates       | 1494324  |
---------------------------------
New best mean reward!
Eval num_timesteps=160000, episode_reward=2291.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 0.0775   |
|    ent_coef        | 0.00202  |
|    ent_coef_loss   | -0.915   |
|    learning_rate   | 0.000984 |
|    n_updates       | 1504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 32       |
|    fps             | 39       |
|    time_elapsed    | 4021     |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=2742.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -45.1    |
|    critic_loss     | 0.0506   |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.000983 |
|    n_updates       | 1514324  |
---------------------------------
Eval num_timesteps=180000, episode_reward=2001.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 0.535    |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | 5.4      |
|    learning_rate   | 0.000982 |
|    n_updates       | 1524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 36       |
|    fps             | 39       |
|    time_elapsed    | 4513     |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1998.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 0.17     |
|    ent_coef        | 0.00222  |
|    ent_coef_loss   | -4.22    |
|    learning_rate   | 0.000981 |
|    n_updates       | 1534324  |
---------------------------------
Eval num_timesteps=200000, episode_reward=2158.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -45      |
|    critic_loss     | 0.162    |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.00098  |
|    n_updates       | 1544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 40       |
|    fps             | 39       |
|    time_elapsed    | 5009     |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=2309.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -43.6    |
|    critic_loss     | 0.0484   |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | 5.79     |
|    learning_rate   | 0.000979 |
|    n_updates       | 1554324  |
---------------------------------
Eval num_timesteps=220000, episode_reward=2006.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -43.4    |
|    critic_loss     | 0.0665   |
|    ent_coef        | 0.00231  |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.000978 |
|    n_updates       | 1564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 44       |
|    fps             | 39       |
|    time_elapsed    | 5501     |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=2446.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -43.3    |
|    critic_loss     | 0.0947   |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | 3.69     |
|    learning_rate   | 0.000977 |
|    n_updates       | 1574324  |
---------------------------------
Eval num_timesteps=240000, episode_reward=3024.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -43.2    |
|    critic_loss     | 0.0617   |
|    ent_coef        | 0.00229  |
|    ent_coef_loss   | -5.11    |
|    learning_rate   | 0.000976 |
|    n_updates       | 1584324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.17e+03 |
| time/              |          |
|    episodes        | 48       |
|    fps             | 40       |
|    time_elapsed    | 5993     |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=2389.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -43.5    |
|    critic_loss     | 0.0427   |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | -0.964   |
|    learning_rate   | 0.000975 |
|    n_updates       | 1594324  |
---------------------------------
Eval num_timesteps=260000, episode_reward=2238.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000974 |
|    n_updates       | 1604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 52       |
|    fps             | 40       |
|    time_elapsed    | 6488     |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=2152.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -43.9    |
|    critic_loss     | 0.0437   |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | -1.08    |
|    learning_rate   | 0.000973 |
|    n_updates       | 1614324  |
---------------------------------
Eval num_timesteps=280000, episode_reward=3141.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -44.5    |
|    critic_loss     | 0.0307   |
|    ent_coef        | 0.00229  |
|    ent_coef_loss   | 0.76     |
|    learning_rate   | 0.000972 |
|    n_updates       | 1624324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    episodes        | 56       |
|    fps             | 40       |
|    time_elapsed    | 6984     |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=290000, episode_reward=3130.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -44.6    |
|    critic_loss     | 0.0776   |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000971 |
|    n_updates       | 1634324  |
---------------------------------
Eval num_timesteps=300000, episode_reward=3115.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -45.7    |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | 1.19     |
|    learning_rate   | 0.00097  |
|    n_updates       | 1644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 60       |
|    fps             | 40       |
|    time_elapsed    | 7482     |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=3136.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 0.23     |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -0.016   |
|    learning_rate   | 0.000969 |
|    n_updates       | 1654324  |
---------------------------------
Eval num_timesteps=320000, episode_reward=3139.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 0.049    |
|    ent_coef        | 0.00239  |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.000968 |
|    n_updates       | 1664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.33e+03 |
| time/              |          |
|    episodes        | 64       |
|    fps             | 40       |
|    time_elapsed    | 7975     |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=3135.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 0.0917   |
|    ent_coef        | 0.00247  |
|    ent_coef_loss   | 2.9      |
|    learning_rate   | 0.000967 |
|    n_updates       | 1674324  |
---------------------------------
Eval num_timesteps=340000, episode_reward=3132.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 0.0621   |
|    ent_coef        | 0.00261  |
|    ent_coef_loss   | 3.57     |
|    learning_rate   | 0.000966 |
|    n_updates       | 1684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 68       |
|    fps             | 40       |
|    time_elapsed    | 8469     |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=350000, episode_reward=3138.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.0396   |
|    ent_coef        | 0.00287  |
|    ent_coef_loss   | 1.77     |
|    learning_rate   | 0.000965 |
|    n_updates       | 1694324  |
---------------------------------
Eval num_timesteps=360000, episode_reward=3147.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 0.0537   |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | 3.36     |
|    learning_rate   | 0.000964 |
|    n_updates       | 1704324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 72       |
|    fps             | 40       |
|    time_elapsed    | 8964     |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=370000, episode_reward=3148.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.0514   |
|    ent_coef        | 0.00233  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.000963 |
|    n_updates       | 1714324  |
---------------------------------
New best mean reward!
Eval num_timesteps=380000, episode_reward=3145.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -47.4    |
|    critic_loss     | 0.033    |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -4.07    |
|    learning_rate   | 0.000962 |
|    n_updates       | 1724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    episodes        | 76       |
|    fps             | 40       |
|    time_elapsed    | 9457     |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=3144.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.0607   |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | -4.16    |
|    learning_rate   | 0.000961 |
|    n_updates       | 1734324  |
---------------------------------
Eval num_timesteps=400000, episode_reward=2176.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 0.0329   |
|    ent_coef        | 0.00234  |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.00096  |
|    n_updates       | 1744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    episodes        | 80       |
|    fps             | 40       |
|    time_elapsed    | 9949     |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=2195.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 0.0904   |
|    ent_coef        | 0.00249  |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.000959 |
|    n_updates       | 1754324  |
---------------------------------
Eval num_timesteps=420000, episode_reward=3136.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 0.044    |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | 0.651    |
|    learning_rate   | 0.000958 |
|    n_updates       | 1764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    episodes        | 84       |
|    fps             | 40       |
|    time_elapsed    | 10442    |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=430000, episode_reward=3135.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 430000   |
| train/             |          |
|    actor_loss      | -48.6    |
|    critic_loss     | 0.0716   |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000957 |
|    n_updates       | 1774324  |
---------------------------------
Eval num_timesteps=440000, episode_reward=3135.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 440000   |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 0.0487   |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | 0.963    |
|    learning_rate   | 0.000956 |
|    n_updates       | 1784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 88       |
|    fps             | 40       |
|    time_elapsed    | 10935    |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=3138.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 450000   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 0.0265   |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | -6.11    |
|    learning_rate   | 0.000955 |
|    n_updates       | 1794324  |
---------------------------------
Eval num_timesteps=460000, episode_reward=3151.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 460000   |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 0.0236   |
|    ent_coef        | 0.00271  |
|    ent_coef_loss   | 0.951    |
|    learning_rate   | 0.000954 |
|    n_updates       | 1804324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 92       |
|    fps             | 40       |
|    time_elapsed    | 11427    |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=470000, episode_reward=2156.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 470000   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 0.0289   |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | 0.642    |
|    learning_rate   | 0.000953 |
|    n_updates       | 1814324  |
---------------------------------
Eval num_timesteps=480000, episode_reward=3010.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 0.0381   |
|    ent_coef        | 0.00259  |
|    ent_coef_loss   | 4.49     |
|    learning_rate   | 0.000952 |
|    n_updates       | 1824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 96       |
|    fps             | 40       |
|    time_elapsed    | 11920    |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=2998.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 0.0227   |
|    ent_coef        | 0.00271  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000951 |
|    n_updates       | 1834324  |
---------------------------------
Eval num_timesteps=500000, episode_reward=3002.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 0.0602   |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -2.18    |
|    learning_rate   | 0.00095  |
|    n_updates       | 1844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 100      |
|    fps             | 40       |
|    time_elapsed    | 12416    |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=3148.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 0.034    |
|    ent_coef        | 0.00276  |
|    ent_coef_loss   | -0.349   |
|    learning_rate   | 0.000949 |
|    n_updates       | 1854324  |
---------------------------------
Eval num_timesteps=520000, episode_reward=3140.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 520000   |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 0.0916   |
|    ent_coef        | 0.00275  |
|    ent_coef_loss   | -3.15    |
|    learning_rate   | 0.000948 |
|    n_updates       | 1864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 104      |
|    fps             | 40       |
|    time_elapsed    | 12909    |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=530000, episode_reward=3141.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 530000   |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 0.0643   |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | 5.62     |
|    learning_rate   | 0.000947 |
|    n_updates       | 1874324  |
---------------------------------
Eval num_timesteps=540000, episode_reward=3132.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 540000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.00292  |
|    ent_coef_loss   | -4.25    |
|    learning_rate   | 0.000946 |
|    n_updates       | 1884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 108      |
|    fps             | 40       |
|    time_elapsed    | 13400    |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=550000, episode_reward=3151.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 550000   |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.048    |
|    ent_coef        | 0.00289  |
|    ent_coef_loss   | -0.756   |
|    learning_rate   | 0.000945 |
|    n_updates       | 1894324  |
---------------------------------
Eval num_timesteps=560000, episode_reward=3149.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.0287   |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | -0.596   |
|    learning_rate   | 0.000944 |
|    n_updates       | 1904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 112      |
|    fps             | 40       |
|    time_elapsed    | 13894    |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=3141.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 0.0346   |
|    ent_coef        | 0.00272  |
|    ent_coef_loss   | -3.43    |
|    learning_rate   | 0.000943 |
|    n_updates       | 1914324  |
---------------------------------
Eval num_timesteps=580000, episode_reward=3139.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.0286   |
|    ent_coef        | 0.00255  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.000942 |
|    n_updates       | 1924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 116      |
|    fps             | 40       |
|    time_elapsed    | 14385    |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=3152.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.0782   |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | -3.96    |
|    learning_rate   | 0.000941 |
|    n_updates       | 1934324  |
---------------------------------
New best mean reward!
Eval num_timesteps=600000, episode_reward=2143.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 600000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.0629   |
|    ent_coef        | 0.00245  |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.00094  |
|    n_updates       | 1944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 120      |
|    fps             | 40       |
|    time_elapsed    | 14876    |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=610000, episode_reward=2991.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 610000   |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 0.0947   |
|    ent_coef        | 0.00269  |
|    ent_coef_loss   | -0.693   |
|    learning_rate   | 0.000939 |
|    n_updates       | 1954324  |
---------------------------------
Eval num_timesteps=620000, episode_reward=3161.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 620000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0185   |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | -0.325   |
|    learning_rate   | 0.000938 |
|    n_updates       | 1964324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    episodes        | 124      |
|    fps             | 40       |
|    time_elapsed    | 15368    |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=3144.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 630000   |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.0342   |
|    ent_coef        | 0.00256  |
|    ent_coef_loss   | 3.51     |
|    learning_rate   | 0.000937 |
|    n_updates       | 1974324  |
---------------------------------
Eval num_timesteps=640000, episode_reward=2990.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 640000   |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.0708   |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | 0.733    |
|    learning_rate   | 0.000936 |
|    n_updates       | 1984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 128      |
|    fps             | 40       |
|    time_elapsed    | 15858    |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=3144.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0575   |
|    ent_coef        | 0.00246  |
|    ent_coef_loss   | 1.7      |
|    learning_rate   | 0.000935 |
|    n_updates       | 1994324  |
---------------------------------
Eval num_timesteps=660000, episode_reward=3032.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 0.0256   |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -3.48    |
|    learning_rate   | 0.000934 |
|    n_updates       | 2004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 132      |
|    fps             | 40       |
|    time_elapsed    | 16349    |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=3143.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0369   |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 0.556    |
|    learning_rate   | 0.000933 |
|    n_updates       | 2014324  |
---------------------------------
Eval num_timesteps=680000, episode_reward=3161.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0158   |
|    ent_coef        | 0.00288  |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.000932 |
|    n_updates       | 2024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 136      |
|    fps             | 40       |
|    time_elapsed    | 16839    |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=3154.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 690000   |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0294   |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | 1.63     |
|    learning_rate   | 0.000931 |
|    n_updates       | 2034324  |
---------------------------------
Eval num_timesteps=700000, episode_reward=3173.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 700000   |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0936   |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | 5.32     |
|    learning_rate   | 0.00093  |
|    n_updates       | 2044324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 140      |
|    fps             | 40       |
|    time_elapsed    | 17329    |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=710000, episode_reward=3153.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 710000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0294   |
|    ent_coef        | 0.00282  |
|    ent_coef_loss   | 4.01     |
|    learning_rate   | 0.000929 |
|    n_updates       | 2054324  |
---------------------------------
Eval num_timesteps=720000, episode_reward=3187.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 720000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0274   |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | 5.55     |
|    learning_rate   | 0.000928 |
|    n_updates       | 2064324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 144      |
|    fps             | 40       |
|    time_elapsed    | 17821    |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=3045.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.0321   |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.000927 |
|    n_updates       | 2074324  |
---------------------------------
Eval num_timesteps=740000, episode_reward=3173.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0393   |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | 3.98     |
|    learning_rate   | 0.000926 |
|    n_updates       | 2084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 148      |
|    fps             | 40       |
|    time_elapsed    | 18312    |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=3189.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0346   |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.000925 |
|    n_updates       | 2094324  |
---------------------------------
New best mean reward!
Eval num_timesteps=760000, episode_reward=3179.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0468   |
|    ent_coef        | 0.00305  |
|    ent_coef_loss   | -2.02    |
|    learning_rate   | 0.000924 |
|    n_updates       | 2104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 152      |
|    fps             | 40       |
|    time_elapsed    | 18804    |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=770000, episode_reward=3165.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 770000   |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 0.0359   |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.000923 |
|    n_updates       | 2114324  |
---------------------------------
Eval num_timesteps=780000, episode_reward=3031.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 780000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.0729   |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | 0.108    |
|    learning_rate   | 0.000922 |
|    n_updates       | 2124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 156      |
|    fps             | 40       |
|    time_elapsed    | 19297    |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=790000, episode_reward=3051.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 790000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.0835   |
|    ent_coef        | 0.00278  |
|    ent_coef_loss   | 2.04     |
|    learning_rate   | 0.000921 |
|    n_updates       | 2134324  |
---------------------------------
Eval num_timesteps=800000, episode_reward=3029.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 800000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.00313  |
|    ent_coef_loss   | -1.61    |
|    learning_rate   | 0.00092  |
|    n_updates       | 2144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 40       |
|    time_elapsed    | 19797    |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=3162.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 810000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0427   |
|    ent_coef        | 0.00309  |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000919 |
|    n_updates       | 2154324  |
---------------------------------
Eval num_timesteps=820000, episode_reward=3163.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0412   |
|    ent_coef        | 0.00306  |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.000918 |
|    n_updates       | 2164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 164      |
|    fps             | 40       |
|    time_elapsed    | 20287    |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=3081.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 0.0333   |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 0.435    |
|    learning_rate   | 0.000917 |
|    n_updates       | 2174324  |
---------------------------------
Eval num_timesteps=840000, episode_reward=3179.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.035    |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | -1.29    |
|    learning_rate   | 0.000916 |
|    n_updates       | 2184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 168      |
|    fps             | 40       |
|    time_elapsed    | 20779    |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=2124.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0342   |
|    ent_coef        | 0.00322  |
|    ent_coef_loss   | 0.000923 |
|    learning_rate   | 0.000915 |
|    n_updates       | 2194324  |
---------------------------------
Eval num_timesteps=860000, episode_reward=3038.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 860000   |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 0.0349   |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | 3.35     |
|    learning_rate   | 0.000914 |
|    n_updates       | 2204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 172      |
|    fps             | 40       |
|    time_elapsed    | 21272    |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=3155.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 870000   |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0328   |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | -2.91    |
|    learning_rate   | 0.000913 |
|    n_updates       | 2214324  |
---------------------------------
Eval num_timesteps=880000, episode_reward=2154.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 880000   |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 0.0409   |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -0.713   |
|    learning_rate   | 0.000912 |
|    n_updates       | 2224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 176      |
|    fps             | 40       |
|    time_elapsed    | 21763    |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=890000, episode_reward=2299.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 890000   |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 0.0409   |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.000911 |
|    n_updates       | 2234324  |
---------------------------------
Eval num_timesteps=900000, episode_reward=2104.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00325  |
|    ent_coef_loss   | 0.0626   |
|    learning_rate   | 0.00091  |
|    n_updates       | 2244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 180      |
|    fps             | 40       |
|    time_elapsed    | 22254    |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=3013.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.0786   |
|    ent_coef        | 0.00354  |
|    ent_coef_loss   | 0.308    |
|    learning_rate   | 0.000909 |
|    n_updates       | 2254324  |
---------------------------------
Eval num_timesteps=920000, episode_reward=3007.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.0613   |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -0.591   |
|    learning_rate   | 0.000908 |
|    n_updates       | 2264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 184      |
|    fps             | 40       |
|    time_elapsed    | 22747    |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=3004.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.046    |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000907 |
|    n_updates       | 2274324  |
---------------------------------
Eval num_timesteps=940000, episode_reward=2979.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 940000   |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.16     |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | 0.816    |
|    learning_rate   | 0.000906 |
|    n_updates       | 2284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 188      |
|    fps             | 40       |
|    time_elapsed    | 23238    |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=950000, episode_reward=2139.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 950000   |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0339   |
|    ent_coef        | 0.00398  |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.000905 |
|    n_updates       | 2294324  |
---------------------------------
Eval num_timesteps=960000, episode_reward=3025.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 960000   |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.195    |
|    ent_coef        | 0.00407  |
|    ent_coef_loss   | -1.89    |
|    learning_rate   | 0.000904 |
|    n_updates       | 2304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 192      |
|    fps             | 40       |
|    time_elapsed    | 23729    |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=970000, episode_reward=3172.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 970000   |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.029    |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | -0.485   |
|    learning_rate   | 0.000903 |
|    n_updates       | 2314324  |
---------------------------------
Eval num_timesteps=980000, episode_reward=2907.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 980000   |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.0781   |
|    ent_coef        | 0.00383  |
|    ent_coef_loss   | -0.606   |
|    learning_rate   | 0.000902 |
|    n_updates       | 2324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 196      |
|    fps             | 40       |
|    time_elapsed    | 24220    |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=3051.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.0852   |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | -2.73    |
|    learning_rate   | 0.000901 |
|    n_updates       | 2334324  |
---------------------------------
Eval num_timesteps=1000000, episode_reward=3148.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.0472   |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | 4.52     |
|    learning_rate   | 0.0009   |
|    n_updates       | 2344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 200      |
|    fps             | 40       |
|    time_elapsed    | 24710    |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=3149.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.0942   |
|    ent_coef        | 0.00408  |
|    ent_coef_loss   | 1.77     |
|    learning_rate   | 0.000899 |
|    n_updates       | 2354324  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=3195.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.041    |
|    ent_coef        | 0.00417  |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.000898 |
|    n_updates       | 2364324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 204      |
|    fps             | 40       |
|    time_elapsed    | 25200    |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=3035.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 1030000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.148    |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | 0.394    |
|    learning_rate   | 0.000897 |
|    n_updates       | 2374324  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=3060.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 1040000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.157    |
|    ent_coef        | 0.00429  |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000896 |
|    n_updates       | 2384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 208      |
|    fps             | 40       |
|    time_elapsed    | 25690    |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=3226.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 1050000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.0634   |
|    ent_coef        | 0.00497  |
|    ent_coef_loss   | 3.16     |
|    learning_rate   | 0.000895 |
|    n_updates       | 2394324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1060000, episode_reward=3195.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1060000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00514  |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000894 |
|    n_updates       | 2404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 212      |
|    fps             | 40       |
|    time_elapsed    | 26182    |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=3200.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0767   |
|    ent_coef        | 0.00483  |
|    ent_coef_loss   | -0.0651  |
|    learning_rate   | 0.000893 |
|    n_updates       | 2414324  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=3215.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.0594   |
|    ent_coef        | 0.00458  |
|    ent_coef_loss   | -2.74    |
|    learning_rate   | 0.000892 |
|    n_updates       | 2424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 216      |
|    fps             | 40       |
|    time_elapsed    | 26673    |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=3143.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 0.228    |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000891 |
|    n_updates       | 2434324  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=3216.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.0657   |
|    ent_coef        | 0.00427  |
|    ent_coef_loss   | 0.526    |
|    learning_rate   | 0.00089  |
|    n_updates       | 2444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 220      |
|    fps             | 40       |
|    time_elapsed    | 27163    |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=3065.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 1110000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.0398   |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000889 |
|    n_updates       | 2454324  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=3203.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1120000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 2.48     |
|    ent_coef        | 0.00358  |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.000888 |
|    n_updates       | 2464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 40       |
|    time_elapsed    | 27653    |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=2265.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 1130000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.0677   |
|    ent_coef        | 0.00362  |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.000887 |
|    n_updates       | 2474324  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=3090.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 1140000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.0816   |
|    ent_coef        | 0.0037   |
|    ent_coef_loss   | 0.662    |
|    learning_rate   | 0.000886 |
|    n_updates       | 2484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 228      |
|    fps             | 40       |
|    time_elapsed    | 28143    |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=3217.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1150000  |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 0.0651   |
|    ent_coef        | 0.0041   |
|    ent_coef_loss   | -0.89    |
|    learning_rate   | 0.000885 |
|    n_updates       | 2494324  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=3140.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -55.7    |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.000884 |
|    n_updates       | 2504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 232      |
|    fps             | 40       |
|    time_elapsed    | 28633    |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=3034.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -56      |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00433  |
|    ent_coef_loss   | -0.327   |
|    learning_rate   | 0.000883 |
|    n_updates       | 2514324  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=3033.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 0.0382   |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | -2.66    |
|    learning_rate   | 0.000882 |
|    n_updates       | 2524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 236      |
|    fps             | 40       |
|    time_elapsed    | 29123    |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=3189.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 0.151    |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.000881 |
|    n_updates       | 2534324  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=3057.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 1200000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.0736   |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | 0.489    |
|    learning_rate   | 0.00088  |
|    n_updates       | 2544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 40       |
|    time_elapsed    | 29614    |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=3125.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 1210000  |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 0.143    |
|    ent_coef        | 0.00371  |
|    ent_coef_loss   | 0.923    |
|    learning_rate   | 0.000879 |
|    n_updates       | 2554324  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=3180.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 1220000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 0.21     |
|    ent_coef        | 0.00378  |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.000878 |
|    n_updates       | 2564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 40       |
|    time_elapsed    | 30106    |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=3084.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 1230000  |
| train/             |          |
|    actor_loss      | -55.7    |
|    critic_loss     | 0.103    |
|    ent_coef        | 0.00384  |
|    ent_coef_loss   | 0.224    |
|    learning_rate   | 0.000877 |
|    n_updates       | 2574324  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=3120.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -56.3    |
|    critic_loss     | 0.127    |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | -1.22    |
|    learning_rate   | 0.000876 |
|    n_updates       | 2584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 40       |
|    time_elapsed    | 30597    |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=3060.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -56.8    |
|    critic_loss     | 0.134    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.000875 |
|    n_updates       | 2594324  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=3025.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -56.8    |
|    critic_loss     | 0.0692   |
|    ent_coef        | 0.00386  |
|    ent_coef_loss   | 0.746    |
|    learning_rate   | 0.000874 |
|    n_updates       | 2604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 40       |
|    time_elapsed    | 31089    |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=2538.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -57.5    |
|    critic_loss     | 0.0726   |
|    ent_coef        | 0.00301  |
|    ent_coef_loss   | -2       |
|    learning_rate   | 0.000873 |
|    n_updates       | 2614324  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=3094.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 1280000  |
| train/             |          |
|    actor_loss      | -57.4    |
|    critic_loss     | 0.056    |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000872 |
|    n_updates       | 2624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 40       |
|    time_elapsed    | 31581    |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=3299.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 1290000  |
| train/             |          |
|    actor_loss      | -57.9    |
|    critic_loss     | 0.243    |
|    ent_coef        | 0.0042   |
|    ent_coef_loss   | 3.53     |
|    learning_rate   | 0.000871 |
|    n_updates       | 2634324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1300000, episode_reward=3189.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1300000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 0.0563   |
|    ent_coef        | 0.0041   |
|    ent_coef_loss   | -0.315   |
|    learning_rate   | 0.00087  |
|    n_updates       | 2644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 260      |
|    fps             | 40       |
|    time_elapsed    | 32076    |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=3202.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 1310000  |
| train/             |          |
|    actor_loss      | -56.9    |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00399  |
|    ent_coef_loss   | 5.02     |
|    learning_rate   | 0.000869 |
|    n_updates       | 2654324  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=4047.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.05e+03 |
| time/              |          |
|    total_timesteps | 1320000  |
| train/             |          |
|    actor_loss      | -56.2    |
|    critic_loss     | 0.0378   |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | -2.35    |
|    learning_rate   | 0.000868 |
|    n_updates       | 2664324  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 264      |
|    fps             | 40       |
|    time_elapsed    | 32568    |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=4741.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.74e+03 |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -55.7    |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.00409  |
|    ent_coef_loss   | 0.637    |
|    learning_rate   | 0.000867 |
|    n_updates       | 2674324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1340000, episode_reward=3238.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.169    |
|    ent_coef        | 0.00431  |
|    ent_coef_loss   | 0.945    |
|    learning_rate   | 0.000866 |
|    n_updates       | 2684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 268      |
|    fps             | 40       |
|    time_elapsed    | 33060    |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=4851.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.85e+03 |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -56.6    |
|    critic_loss     | 0.131    |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | 0.089    |
|    learning_rate   | 0.000865 |
|    n_updates       | 2694324  |
---------------------------------
New best mean reward!
Eval num_timesteps=1360000, episode_reward=3193.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 0.067    |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | -0.752   |
|    learning_rate   | 0.000864 |
|    n_updates       | 2704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 272      |
|    fps             | 40       |
|    time_elapsed    | 33551    |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=3578.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.58e+03 |
| time/              |          |
|    total_timesteps | 1370000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 0.0664   |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.000863 |
|    n_updates       | 2714324  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=4797.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.8e+03  |
| time/              |          |
|    total_timesteps | 1380000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 0.0985   |
|    ent_coef        | 0.00402  |
|    ent_coef_loss   | 0.402    |
|    learning_rate   | 0.000862 |
|    n_updates       | 2724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 276      |
|    fps             | 40       |
|    time_elapsed    | 34044    |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=3120.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 1390000  |
| train/             |          |
|    actor_loss      | -57      |
|    critic_loss     | 0.0662   |
|    ent_coef        | 0.00404  |
|    ent_coef_loss   | 1        |
|    learning_rate   | 0.000861 |
|    n_updates       | 2734324  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=3086.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 1400000  |
| train/             |          |
|    actor_loss      | -57.5    |
|    critic_loss     | 0.123    |
|    ent_coef        | 0.00411  |
|    ent_coef_loss   | -5.07    |
|    learning_rate   | 0.00086  |
|    n_updates       | 2744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.04e+03 |
| time/              |          |
|    episodes        | 280      |
|    fps             | 40       |
|    time_elapsed    | 34535    |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=4565.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 4.57e+03 |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -57.3    |
|    critic_loss     | 0.0924   |
|    ent_coef        | 0.00397  |
|    ent_coef_loss   | 0.0676   |
|    learning_rate   | 0.000859 |
|    n_updates       | 2754324  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=3071.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -57.2    |
|    critic_loss     | 0.132    |
|    ent_coef        | 0.00419  |
|    ent_coef_loss   | -1.73    |
|    learning_rate   | 0.000858 |
|    n_updates       | 2764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.05e+03 |
| time/              |          |
|    episodes        | 284      |
|    fps             | 40       |
|    time_elapsed    | 35027    |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=3015.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -57.3    |
|    critic_loss     | 0.675    |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.000857 |
|    n_updates       | 2774324  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=3012.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -58.2    |
|    critic_loss     | 0.196    |
|    ent_coef        | 0.00373  |
|    ent_coef_loss   | -2.07    |
|    learning_rate   | 0.000856 |
|    n_updates       | 2784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 288      |
|    fps             | 40       |
|    time_elapsed    | 35522    |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=2932.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -57.7    |
|    critic_loss     | 0.407    |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.000855 |
|    n_updates       | 2794324  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=2908.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 1460000  |
| train/             |          |
|    actor_loss      | -56.9    |
|    critic_loss     | 0.676    |
|    ent_coef        | 0.00396  |
|    ent_coef_loss   | 0.467    |
|    learning_rate   | 0.000854 |
|    n_updates       | 2804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 292      |
|    fps             | 40       |
|    time_elapsed    | 36017    |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=2421.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.42e+03 |
| time/              |          |
|    total_timesteps | 1470000  |
| train/             |          |
|    actor_loss      | -56.9    |
|    critic_loss     | 0.208    |
|    ent_coef        | 0.00415  |
|    ent_coef_loss   | 0.26     |
|    learning_rate   | 0.000853 |
|    n_updates       | 2814324  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=3229.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 1480000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.0678   |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000852 |
|    n_updates       | 2824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.06e+03 |
| time/              |          |
|    episodes        | 296      |
|    fps             | 40       |
|    time_elapsed    | 36511    |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=2083.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.08e+03 |
| time/              |          |
|    total_timesteps | 1490000  |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 0.262    |
|    ent_coef        | 0.00386  |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.000851 |
|    n_updates       | 2834324  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=3221.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -56.3    |
|    critic_loss     | 0.1      |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | -0.222   |
|    learning_rate   | 0.00085  |
|    n_updates       | 2844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 300      |
|    fps             | 40       |
|    time_elapsed    | 37004    |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=3142.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -57.7    |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | 0.539    |
|    learning_rate   | 0.000849 |
|    n_updates       | 2854324  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=1892.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -57.1    |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | 0.993    |
|    learning_rate   | 0.000848 |
|    n_updates       | 2864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.07e+03 |
| time/              |          |
|    episodes        | 304      |
|    fps             | 40       |
|    time_elapsed    | 37498    |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=2930.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -57.8    |
|    critic_loss     | 0.0832   |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.000847 |
|    n_updates       | 2874324  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=3183.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 1540000  |
| train/             |          |
|    actor_loss      | -57.1    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00368  |
|    ent_coef_loss   | -0.04    |
|    learning_rate   | 0.000846 |
|    n_updates       | 2884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.05e+03 |
| time/              |          |
|    episodes        | 308      |
|    fps             | 40       |
|    time_elapsed    | 37991    |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=3155.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 1550000  |
| train/             |          |
|    actor_loss      | -56.6    |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000845 |
|    n_updates       | 2894324  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=3105.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 1560000  |
| train/             |          |
|    actor_loss      | -56.4    |
|    critic_loss     | 0.127    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000844 |
|    n_updates       | 2904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.04e+03 |
| time/              |          |
|    episodes        | 312      |
|    fps             | 40       |
|    time_elapsed    | 38485    |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=2534.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 1570000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.989    |
|    ent_coef        | 0.00286  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000843 |
|    n_updates       | 2914324  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=1829.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -57      |
|    critic_loss     | 0.188    |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | -2.27    |
|    learning_rate   | 0.000842 |
|    n_updates       | 2924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 316      |
|    fps             | 40       |
|    time_elapsed    | 38982    |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=3144.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -56.6    |
|    critic_loss     | 0.173    |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | -3.8     |
|    learning_rate   | 0.000841 |
|    n_updates       | 2934324  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=3145.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00342  |
|    ent_coef_loss   | -3.19    |
|    learning_rate   | 0.00084  |
|    n_updates       | 2944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 320      |
|    fps             | 40       |
|    time_elapsed    | 39477    |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=2213.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -56.1    |
|    critic_loss     | 0.483    |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | -4.06    |
|    learning_rate   | 0.000839 |
|    n_updates       | 2954324  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=3013.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -55.6    |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00443  |
|    ent_coef_loss   | -1.4     |
|    learning_rate   | 0.000838 |
|    n_updates       | 2964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 324      |
|    fps             | 40       |
|    time_elapsed    | 39971    |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=2219.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 1630000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 0.208    |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.000837 |
|    n_updates       | 2974324  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=3034.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1640000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.323    |
|    ent_coef        | 0.00369  |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.000836 |
|    n_updates       | 2984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    episodes        | 328      |
|    fps             | 40       |
|    time_elapsed    | 40463    |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=3293.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 1650000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.327    |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | -0.0506  |
|    learning_rate   | 0.000835 |
|    n_updates       | 2994324  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=3073.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 1660000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 0.31     |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.000834 |
|    n_updates       | 3004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    episodes        | 332      |
|    fps             | 40       |
|    time_elapsed    | 40956    |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=3110.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 0.385    |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | 0.617    |
|    learning_rate   | 0.000833 |
|    n_updates       | 3014324  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=2062.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -55.3    |
|    critic_loss     | 0.15     |
|    ent_coef        | 0.00367  |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.000832 |
|    n_updates       | 3024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 336      |
|    fps             | 40       |
|    time_elapsed    | 41449    |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=3037.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -55.9    |
|    critic_loss     | 0.867    |
|    ent_coef        | 0.00418  |
|    ent_coef_loss   | 4.22     |
|    learning_rate   | 0.000831 |
|    n_updates       | 3034324  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=1832.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 0.438    |
|    ent_coef        | 0.00483  |
|    ent_coef_loss   | -2.71    |
|    learning_rate   | 0.00083  |
|    n_updates       | 3044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    episodes        | 340      |
|    fps             | 40       |
|    time_elapsed    | 41942    |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=3184.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 1710000  |
| train/             |          |
|    actor_loss      | -54.8    |
|    critic_loss     | 2.24     |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | 5.73     |
|    learning_rate   | 0.000829 |
|    n_updates       | 3054324  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=3191.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1720000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.706    |
|    ent_coef        | 0.0062   |
|    ent_coef_loss   | 0.736    |
|    learning_rate   | 0.000828 |
|    n_updates       | 3064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 344      |
|    fps             | 40       |
|    time_elapsed    | 42436    |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=3027.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1730000  |
| train/             |          |
|    actor_loss      | -55.4    |
|    critic_loss     | 0.528    |
|    ent_coef        | 0.00674  |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.000827 |
|    n_updates       | 3074324  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=3165.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 1740000  |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 0.706    |
|    ent_coef        | 0.00599  |
|    ent_coef_loss   | -0.402   |
|    learning_rate   | 0.000826 |
|    n_updates       | 3084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 348      |
|    fps             | 40       |
|    time_elapsed    | 42932    |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=3163.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 1750000  |
| train/             |          |
|    actor_loss      | -53.6    |
|    critic_loss     | 0.574    |
|    ent_coef        | 0.00589  |
|    ent_coef_loss   | -0.124   |
|    learning_rate   | 0.000825 |
|    n_updates       | 3094324  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=3173.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.697    |
|    ent_coef        | 0.00584  |
|    ent_coef_loss   | 0.652    |
|    learning_rate   | 0.000824 |
|    n_updates       | 3104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 352      |
|    fps             | 40       |
|    time_elapsed    | 43428    |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=3186.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.355    |
|    ent_coef        | 0.00514  |
|    ent_coef_loss   | -3.33    |
|    learning_rate   | 0.000823 |
|    n_updates       | 3114324  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=2455.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.586    |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | 0.529    |
|    learning_rate   | 0.000822 |
|    n_updates       | 3124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 356      |
|    fps             | 40       |
|    time_elapsed    | 43922    |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=3194.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.585    |
|    ent_coef        | 0.00467  |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.000821 |
|    n_updates       | 3134324  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=2604.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 1800000  |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 1.08     |
|    ent_coef        | 0.00459  |
|    ent_coef_loss   | 7.91     |
|    learning_rate   | 0.00082  |
|    n_updates       | 3144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 360      |
|    fps             | 40       |
|    time_elapsed    | 44418    |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=2868.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 1810000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.267    |
|    ent_coef        | 0.00469  |
|    ent_coef_loss   | -1.43    |
|    learning_rate   | 0.000819 |
|    n_updates       | 3154324  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=3210.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 1820000  |
| train/             |          |
|    actor_loss      | -52      |
|    critic_loss     | 0.299    |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | -4.51    |
|    learning_rate   | 0.000818 |
|    n_updates       | 3164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 364      |
|    fps             | 40       |
|    time_elapsed    | 44913    |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=3215.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 1830000  |
| train/             |          |
|    actor_loss      | -53.2    |
|    critic_loss     | 0.275    |
|    ent_coef        | 0.00472  |
|    ent_coef_loss   | -0.126   |
|    learning_rate   | 0.000817 |
|    n_updates       | 3174324  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=3064.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.00445  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.000816 |
|    n_updates       | 3184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 368      |
|    fps             | 40       |
|    time_elapsed    | 45407    |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=3027.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -52.6    |
|    critic_loss     | 0.622    |
|    ent_coef        | 0.00494  |
|    ent_coef_loss   | 0.0459   |
|    learning_rate   | 0.000815 |
|    n_updates       | 3194324  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=3183.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 0.396    |
|    ent_coef        | 0.00502  |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000814 |
|    n_updates       | 3204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 372      |
|    fps             | 40       |
|    time_elapsed    | 45900    |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=3047.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.463    |
|    ent_coef        | 0.00566  |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.000813 |
|    n_updates       | 3214324  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=3042.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 1880000  |
| train/             |          |
|    actor_loss      | -52.8    |
|    critic_loss     | 0.625    |
|    ent_coef        | 0.00478  |
|    ent_coef_loss   | 3.31     |
|    learning_rate   | 0.000812 |
|    n_updates       | 3224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 376      |
|    fps             | 40       |
|    time_elapsed    | 46392    |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=3050.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 1890000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 0.19     |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | -4.89    |
|    learning_rate   | 0.000811 |
|    n_updates       | 3234324  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=3001.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 1900000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.846    |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.00081  |
|    n_updates       | 3244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 380      |
|    fps             | 40       |
|    time_elapsed    | 46883    |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=1747.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 1910000  |
| train/             |          |
|    actor_loss      | -51.9    |
|    critic_loss     | 0.383    |
|    ent_coef        | 0.00489  |
|    ent_coef_loss   | -4.45    |
|    learning_rate   | 0.000809 |
|    n_updates       | 3254324  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=3044.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 1920000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.742    |
|    ent_coef        | 0.00561  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000808 |
|    n_updates       | 3264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 384      |
|    fps             | 40       |
|    time_elapsed    | 47377    |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=3165.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.704    |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | -2.67    |
|    learning_rate   | 0.000807 |
|    n_updates       | 3274324  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=3037.67 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 0.329    |
|    ent_coef        | 0.00508  |
|    ent_coef_loss   | 0.122    |
|    learning_rate   | 0.000806 |
|    n_updates       | 3284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 388      |
|    fps             | 40       |
|    time_elapsed    | 47871    |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=3147.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -55.1    |
|    critic_loss     | 0.254    |
|    ent_coef        | 0.00527  |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.000805 |
|    n_updates       | 3294324  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=2994.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -53.1    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.00584  |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.000804 |
|    n_updates       | 3304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 392      |
|    fps             | 40       |
|    time_elapsed    | 48364    |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=1990.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 1970000  |
| train/             |          |
|    actor_loss      | -54.4    |
|    critic_loss     | 0.284    |
|    ent_coef        | 0.00611  |
|    ent_coef_loss   | -0.0215  |
|    learning_rate   | 0.000803 |
|    n_updates       | 3314324  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=2002.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 1980000  |
| train/             |          |
|    actor_loss      | -53.8    |
|    critic_loss     | 3.15     |
|    ent_coef        | 0.00606  |
|    ent_coef_loss   | 2.57     |
|    learning_rate   | 0.000802 |
|    n_updates       | 3324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 396      |
|    fps             | 40       |
|    time_elapsed    | 48857    |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=1943.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 1990000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 1.82     |
|    ent_coef        | 0.00668  |
|    ent_coef_loss   | 5.78     |
|    learning_rate   | 0.000801 |
|    n_updates       | 3334324  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=1873.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 2000000  |
| train/             |          |
|    actor_loss      | -53.9    |
|    critic_loss     | 0.238    |
|    ent_coef        | 0.00682  |
|    ent_coef_loss   | -2.48    |
|    learning_rate   | 0.0008   |
|    n_updates       | 3344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 400      |
|    fps             | 40       |
|    time_elapsed    | 49295    |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=1541.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -52.3    |
|    critic_loss     | 0.762    |
|    ent_coef        | 0.00962  |
|    ent_coef_loss   | 1.35     |
|    learning_rate   | 0.000799 |
|    n_updates       | 3354324  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=3025.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -52.5    |
|    critic_loss     | 0.84     |
|    ent_coef        | 0.00839  |
|    ent_coef_loss   | -2.97    |
|    learning_rate   | 0.000798 |
|    n_updates       | 3364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 404      |
|    fps             | 40       |
|    time_elapsed    | 49716    |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=2673.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -51.1    |
|    critic_loss     | 0.798    |
|    ent_coef        | 0.00772  |
|    ent_coef_loss   | -3.53    |
|    learning_rate   | 0.000797 |
|    n_updates       | 3374324  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=3139.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -51.8    |
|    critic_loss     | 0.694    |
|    ent_coef        | 0.00784  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.000796 |
|    n_updates       | 3384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 408      |
|    fps             | 40       |
|    time_elapsed    | 50156    |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=2995.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2050000  |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 1.1      |
|    ent_coef        | 0.00797  |
|    ent_coef_loss   | -1.6     |
|    learning_rate   | 0.000795 |
|    n_updates       | 3394324  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=3012.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2060000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.00776  |
|    ent_coef_loss   | -4.27    |
|    learning_rate   | 0.000794 |
|    n_updates       | 3404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 412      |
|    fps             | 40       |
|    time_elapsed    | 50587    |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=1814.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 2070000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 1.17     |
|    ent_coef        | 0.00795  |
|    ent_coef_loss   | -0.528   |
|    learning_rate   | 0.000793 |
|    n_updates       | 3414324  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=3017.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2080000  |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 4.44     |
|    ent_coef        | 0.00825  |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000792 |
|    n_updates       | 3424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 416      |
|    fps             | 40       |
|    time_elapsed    | 51018    |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=3014.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2090000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 1.67     |
|    ent_coef        | 0.00853  |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000791 |
|    n_updates       | 3434324  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=3002.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -49      |
|    critic_loss     | 1.9      |
|    ent_coef        | 0.00711  |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.00079  |
|    n_updates       | 3444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 420      |
|    fps             | 40       |
|    time_elapsed    | 51453    |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=2994.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -51.3    |
|    critic_loss     | 0.551    |
|    ent_coef        | 0.00627  |
|    ent_coef_loss   | -2.5     |
|    learning_rate   | 0.000789 |
|    n_updates       | 3454324  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=3002.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 0.984    |
|    ent_coef        | 0.0062   |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.000788 |
|    n_updates       | 3464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 424      |
|    fps             | 40       |
|    time_elapsed    | 51871    |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=3139.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -48.6    |
|    critic_loss     | 2.83     |
|    ent_coef        | 0.00676  |
|    ent_coef_loss   | -4.29    |
|    learning_rate   | 0.000787 |
|    n_updates       | 3474324  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=3146.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2140000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 3.37     |
|    ent_coef        | 0.00631  |
|    ent_coef_loss   | -2.5     |
|    learning_rate   | 0.000786 |
|    n_updates       | 3484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 428      |
|    fps             | 40       |
|    time_elapsed    | 52289    |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=2406.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2150000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 0.516    |
|    ent_coef        | 0.00646  |
|    ent_coef_loss   | -3.65    |
|    learning_rate   | 0.000785 |
|    n_updates       | 3494324  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=2994.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2160000  |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 0.82     |
|    ent_coef        | 0.00665  |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 0.000784 |
|    n_updates       | 3504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 432      |
|    fps             | 40       |
|    time_elapsed    | 52706    |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=3002.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2170000  |
| train/             |          |
|    actor_loss      | -46.5    |
|    critic_loss     | 0.492    |
|    ent_coef        | 0.00605  |
|    ent_coef_loss   | -2.43    |
|    learning_rate   | 0.000783 |
|    n_updates       | 3514324  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=3018.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 0.918    |
|    ent_coef        | 0.00699  |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000782 |
|    n_updates       | 3524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 436      |
|    fps             | 41       |
|    time_elapsed    | 53124    |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=3147.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 0.793    |
|    ent_coef        | 0.00688  |
|    ent_coef_loss   | -0.496   |
|    learning_rate   | 0.000781 |
|    n_updates       | 3534324  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=3006.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 2.66     |
|    ent_coef        | 0.00728  |
|    ent_coef_loss   | -5.26    |
|    learning_rate   | 0.00078  |
|    n_updates       | 3544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 440      |
|    fps             | 41       |
|    time_elapsed    | 53540    |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=2969.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 0.422    |
|    ent_coef        | 0.00775  |
|    ent_coef_loss   | -2.3     |
|    learning_rate   | 0.000779 |
|    n_updates       | 3554324  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=2595.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 2220000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.00795  |
|    ent_coef_loss   | 1.32     |
|    learning_rate   | 0.000778 |
|    n_updates       | 3564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 444      |
|    fps             | 41       |
|    time_elapsed    | 53955    |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=2503.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 2230000  |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 1.01     |
|    ent_coef        | 0.00686  |
|    ent_coef_loss   | 0.444    |
|    learning_rate   | 0.000777 |
|    n_updates       | 3574324  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=2996.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2240000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.00659  |
|    ent_coef_loss   | 7.33     |
|    learning_rate   | 0.000776 |
|    n_updates       | 3584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 448      |
|    fps             | 41       |
|    time_elapsed    | 54372    |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=2976.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 2250000  |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 0.435    |
|    ent_coef        | 0.00682  |
|    ent_coef_loss   | -0.241   |
|    learning_rate   | 0.000775 |
|    n_updates       | 3594324  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=3139.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2260000  |
| train/             |          |
|    actor_loss      | -49.9    |
|    critic_loss     | 3.23     |
|    ent_coef        | 0.00742  |
|    ent_coef_loss   | -0.08    |
|    learning_rate   | 0.000774 |
|    n_updates       | 3604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 452      |
|    fps             | 41       |
|    time_elapsed    | 54789    |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=1638.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 1.89     |
|    ent_coef        | 0.00682  |
|    ent_coef_loss   | -3.21    |
|    learning_rate   | 0.000773 |
|    n_updates       | 3614324  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=3127.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 1.14     |
|    ent_coef        | 0.00739  |
|    ent_coef_loss   | 0.939    |
|    learning_rate   | 0.000772 |
|    n_updates       | 3624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 456      |
|    fps             | 41       |
|    time_elapsed    | 55205    |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=3142.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 0.395    |
|    ent_coef        | 0.00667  |
|    ent_coef_loss   | 0.0394   |
|    learning_rate   | 0.000771 |
|    n_updates       | 3634324  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=3138.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 0.549    |
|    ent_coef        | 0.00583  |
|    ent_coef_loss   | -0.404   |
|    learning_rate   | 0.00077  |
|    n_updates       | 3644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 460      |
|    fps             | 41       |
|    time_elapsed    | 55620    |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=1989.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 2310000  |
| train/             |          |
|    actor_loss      | -49.7    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.00449  |
|    ent_coef_loss   | 0.673    |
|    learning_rate   | 0.000769 |
|    n_updates       | 3654324  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=3153.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2320000  |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.00477  |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.000768 |
|    n_updates       | 3664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 464      |
|    fps             | 41       |
|    time_elapsed    | 56035    |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=3151.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2330000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 4.45     |
|    ent_coef        | 0.00501  |
|    ent_coef_loss   | -0.0719  |
|    learning_rate   | 0.000767 |
|    n_updates       | 3674324  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=3146.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2340000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 4.67     |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | 4.55     |
|    learning_rate   | 0.000766 |
|    n_updates       | 3684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 468      |
|    fps             | 41       |
|    time_elapsed    | 56450    |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=3265.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.275    |
|    ent_coef        | 0.00586  |
|    ent_coef_loss   | -2.45    |
|    learning_rate   | 0.000765 |
|    n_updates       | 3694324  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=3139.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00626  |
|    ent_coef_loss   | -1.85    |
|    learning_rate   | 0.000764 |
|    n_updates       | 3704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 472      |
|    fps             | 41       |
|    time_elapsed    | 56867    |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=3145.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 0.537    |
|    ent_coef        | 0.00608  |
|    ent_coef_loss   | 0.492    |
|    learning_rate   | 0.000763 |
|    n_updates       | 3714324  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=1750.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 0.529    |
|    ent_coef        | 0.00559  |
|    ent_coef_loss   | 0.00253  |
|    learning_rate   | 0.000762 |
|    n_updates       | 3724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 476      |
|    fps             | 41       |
|    time_elapsed    | 57282    |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=1662.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 2390000  |
| train/             |          |
|    actor_loss      | -51.6    |
|    critic_loss     | 0.658    |
|    ent_coef        | 0.00565  |
|    ent_coef_loss   | -0.654   |
|    learning_rate   | 0.000761 |
|    n_updates       | 3734324  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=2278.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 2400000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.279    |
|    ent_coef        | 0.00552  |
|    ent_coef_loss   | 0.426    |
|    learning_rate   | 0.00076  |
|    n_updates       | 3744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 480      |
|    fps             | 41       |
|    time_elapsed    | 57697    |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=1957.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 2410000  |
| train/             |          |
|    actor_loss      | -50.3    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.000759 |
|    n_updates       | 3754324  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=2854.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 2420000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.501    |
|    ent_coef        | 0.00686  |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.000758 |
|    n_updates       | 3764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.79e+03 |
| time/              |          |
|    episodes        | 484      |
|    fps             | 41       |
|    time_elapsed    | 58114    |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=2263.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2430000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 1.32     |
|    ent_coef        | 0.00752  |
|    ent_coef_loss   | -5.89    |
|    learning_rate   | 0.000757 |
|    n_updates       | 3774324  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=2265.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.00679  |
|    ent_coef_loss   | 6.46     |
|    learning_rate   | 0.000756 |
|    n_updates       | 3784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    episodes        | 488      |
|    fps             | 41       |
|    time_elapsed    | 58530    |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=2234.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 0.375    |
|    ent_coef        | 0.00612  |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.000755 |
|    n_updates       | 3794324  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=1691.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.614    |
|    ent_coef        | 0.00597  |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.000754 |
|    n_updates       | 3804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 492      |
|    fps             | 41       |
|    time_elapsed    | 58946    |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=2746.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 1.28     |
|    ent_coef        | 0.00721  |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.000753 |
|    n_updates       | 3814324  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=2095.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2480000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00694  |
|    ent_coef_loss   | -4.75    |
|    learning_rate   | 0.000752 |
|    n_updates       | 3824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 496      |
|    fps             | 41       |
|    time_elapsed    | 59361    |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=1718.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2490000  |
| train/             |          |
|    actor_loss      | -48.7    |
|    critic_loss     | 0.924    |
|    ent_coef        | 0.00658  |
|    ent_coef_loss   | 3.43     |
|    learning_rate   | 0.000751 |
|    n_updates       | 3834324  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=2102.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 2500000  |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 0.961    |
|    ent_coef        | 0.00656  |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.00075  |
|    n_updates       | 3844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 500      |
|    fps             | 41       |
|    time_elapsed    | 59778    |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=3120.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 2510000  |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 0.678    |
|    ent_coef        | 0.00671  |
|    ent_coef_loss   | -2.85    |
|    learning_rate   | 0.000749 |
|    n_updates       | 3854324  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=3009.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -48.4    |
|    critic_loss     | 0.52     |
|    ent_coef        | 0.00737  |
|    ent_coef_loss   | -3.22    |
|    learning_rate   | 0.000748 |
|    n_updates       | 3864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 504      |
|    fps             | 41       |
|    time_elapsed    | 60194    |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=3002.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -45.9    |
|    critic_loss     | 0.73     |
|    ent_coef        | 0.00772  |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.000747 |
|    n_updates       | 3874324  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=3115.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 1.36     |
|    ent_coef        | 0.00724  |
|    ent_coef_loss   | -0.582   |
|    learning_rate   | 0.000746 |
|    n_updates       | 3884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 508      |
|    fps             | 41       |
|    time_elapsed    | 60610    |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=3028.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 1.38     |
|    ent_coef        | 0.00773  |
|    ent_coef_loss   | 2.83     |
|    learning_rate   | 0.000745 |
|    n_updates       | 3894324  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=1851.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 2560000  |
| train/             |          |
|    actor_loss      | -45.9    |
|    critic_loss     | 1.63     |
|    ent_coef        | 0.00754  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000744 |
|    n_updates       | 3904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 512      |
|    fps             | 41       |
|    time_elapsed    | 61026    |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=1636.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 2570000  |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 0.984    |
|    ent_coef        | 0.00864  |
|    ent_coef_loss   | -0.103   |
|    learning_rate   | 0.000743 |
|    n_updates       | 3914324  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=2972.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 2580000  |
| train/             |          |
|    actor_loss      | -46.8    |
|    critic_loss     | 0.793    |
|    ent_coef        | 0.00909  |
|    ent_coef_loss   | -0.674   |
|    learning_rate   | 0.000742 |
|    n_updates       | 3924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 516      |
|    fps             | 41       |
|    time_elapsed    | 61442    |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=3139.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2590000  |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 0.447    |
|    ent_coef        | 0.00938  |
|    ent_coef_loss   | -1.63    |
|    learning_rate   | 0.000741 |
|    n_updates       | 3934324  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=2998.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2600000  |
| train/             |          |
|    actor_loss      | -46.4    |
|    critic_loss     | 0.979    |
|    ent_coef        | 0.00842  |
|    ent_coef_loss   | -0.309   |
|    learning_rate   | 0.00074  |
|    n_updates       | 3944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 520      |
|    fps             | 42       |
|    time_elapsed    | 61858    |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=3151.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -46.7    |
|    critic_loss     | 0.969    |
|    ent_coef        | 0.00763  |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.000739 |
|    n_updates       | 3954324  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=3000.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 0.635    |
|    ent_coef        | 0.00744  |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.000738 |
|    n_updates       | 3964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 524      |
|    fps             | 42       |
|    time_elapsed    | 62274    |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=1905.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 0.817    |
|    ent_coef        | 0.00695  |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.000737 |
|    n_updates       | 3974324  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=3162.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 2.67     |
|    ent_coef        | 0.00613  |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.000736 |
|    n_updates       | 3984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 528      |
|    fps             | 42       |
|    time_elapsed    | 62690    |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=2244.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 2650000  |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 0.949    |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | 0.46     |
|    learning_rate   | 0.000735 |
|    n_updates       | 3994324  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=2014.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 2660000  |
| train/             |          |
|    actor_loss      | -50      |
|    critic_loss     | 0.403    |
|    ent_coef        | 0.00551  |
|    ent_coef_loss   | -4.65    |
|    learning_rate   | 0.000734 |
|    n_updates       | 4004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 532      |
|    fps             | 42       |
|    time_elapsed    | 63106    |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=1744.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 2.75     |
|    ent_coef        | 0.00731  |
|    ent_coef_loss   | -4.12    |
|    learning_rate   | 0.000733 |
|    n_updates       | 4014324  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=1663.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 2680000  |
| train/             |          |
|    actor_loss      | -46.4    |
|    critic_loss     | 7.33     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.000732 |
|    n_updates       | 4024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 536      |
|    fps             | 42       |
|    time_elapsed    | 63522    |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=2307.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 1.69     |
|    ent_coef        | 0.00964  |
|    ent_coef_loss   | -0.247   |
|    learning_rate   | 0.000731 |
|    n_updates       | 4034324  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=1716.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 0.775    |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | -3.81    |
|    learning_rate   | 0.00073  |
|    n_updates       | 4044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 540      |
|    fps             | 42       |
|    time_elapsed    | 63937    |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=3130.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -48.8    |
|    critic_loss     | 0.451    |
|    ent_coef        | 0.00968  |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.000729 |
|    n_updates       | 4054324  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=3130.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.0108   |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.000728 |
|    n_updates       | 4064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 544      |
|    fps             | 42       |
|    time_elapsed    | 64353    |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=2385.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.683    |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | 0.371    |
|    learning_rate   | 0.000727 |
|    n_updates       | 4074324  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=3136.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2740000  |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 5.16     |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.000726 |
|    n_updates       | 4084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 548      |
|    fps             | 42       |
|    time_elapsed    | 64768    |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=3142.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2750000  |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 0.734    |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.000725 |
|    n_updates       | 4094324  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=3139.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
| train/             |          |
|    actor_loss      | -48      |
|    critic_loss     | 0.778    |
|    ent_coef        | 0.0126   |
|    ent_coef_loss   | 0.774    |
|    learning_rate   | 0.000724 |
|    n_updates       | 4104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 552      |
|    fps             | 42       |
|    time_elapsed    | 65184    |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=3136.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2770000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 1.05     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -3.11    |
|    learning_rate   | 0.000723 |
|    n_updates       | 4114324  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=2992.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -48.6    |
|    critic_loss     | 1.99     |
|    ent_coef        | 0.0122   |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000722 |
|    n_updates       | 4124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 556      |
|    fps             | 42       |
|    time_elapsed    | 65599    |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=2993.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -48.2    |
|    critic_loss     | 0.864    |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | 0.155    |
|    learning_rate   | 0.000721 |
|    n_updates       | 4134324  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=2947.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -47.2    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.0107   |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.00072  |
|    n_updates       | 4144324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 560      |
|    fps             | 42       |
|    time_elapsed    | 66015    |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=2991.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 7.26     |
|    ent_coef        | 0.0105   |
|    ent_coef_loss   | 4.17     |
|    learning_rate   | 0.000719 |
|    n_updates       | 4154324  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=2001.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 2820000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 3.59     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | -3.58    |
|    learning_rate   | 0.000718 |
|    n_updates       | 4164324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 564      |
|    fps             | 42       |
|    time_elapsed    | 66431    |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=2225.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 2830000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 2.76     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -0.735   |
|    learning_rate   | 0.000717 |
|    n_updates       | 4174324  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=1563.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 6.35     |
|    ent_coef        | 0.0119   |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.000716 |
|    n_updates       | 4184324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 568      |
|    fps             | 42       |
|    time_elapsed    | 66846    |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=1534.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 2.01     |
|    ent_coef        | 0.0131   |
|    ent_coef_loss   | 0.677    |
|    learning_rate   | 0.000715 |
|    n_updates       | 4194324  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=1599.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 3.03     |
|    ent_coef        | 0.017    |
|    ent_coef_loss   | 1.19     |
|    learning_rate   | 0.000714 |
|    n_updates       | 4204324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 572      |
|    fps             | 42       |
|    time_elapsed    | 67262    |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=1720.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 5.9      |
|    ent_coef        | 0.0165   |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000713 |
|    n_updates       | 4214324  |
---------------------------------
Eval num_timesteps=2880000, episode_reward=2257.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -49.3    |
|    critic_loss     | 2.74     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | -0.922   |
|    learning_rate   | 0.000712 |
|    n_updates       | 4224324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 576      |
|    fps             | 42       |
|    time_elapsed    | 67679    |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=1728.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 11.3     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | 4.67     |
|    learning_rate   | 0.000711 |
|    n_updates       | 4234324  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=1879.00 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -47.7    |
|    critic_loss     | 3.13     |
|    ent_coef        | 0.0144   |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.00071  |
|    n_updates       | 4244324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 580      |
|    fps             | 42       |
|    time_elapsed    | 68096    |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=3131.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 1.57     |
|    ent_coef        | 0.0116   |
|    ent_coef_loss   | 0.176    |
|    learning_rate   | 0.000709 |
|    n_updates       | 4254324  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=3135.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 2920000  |
| train/             |          |
|    actor_loss      | -45.8    |
|    critic_loss     | 0.946    |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000708 |
|    n_updates       | 4264324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 584      |
|    fps             | 42       |
|    time_elapsed    | 68512    |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=3132.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2930000  |
| train/             |          |
|    actor_loss      | -46.3    |
|    critic_loss     | 1.17     |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -3.19    |
|    learning_rate   | 0.000707 |
|    n_updates       | 4274324  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=2406.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 2940000  |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 3.72     |
|    ent_coef        | 0.0118   |
|    ent_coef_loss   | -2.9     |
|    learning_rate   | 0.000706 |
|    n_updates       | 4284324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 588      |
|    fps             | 42       |
|    time_elapsed    | 68928    |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=3131.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -43.3    |
|    critic_loss     | 0.957    |
|    ent_coef        | 0.0126   |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.000705 |
|    n_updates       | 4294324  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=3132.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -47      |
|    critic_loss     | 1.96     |
|    ent_coef        | 0.0133   |
|    ent_coef_loss   | -1.95    |
|    learning_rate   | 0.000704 |
|    n_updates       | 4304324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 592      |
|    fps             | 42       |
|    time_elapsed    | 69346    |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=1782.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -52.2    |
|    critic_loss     | 3.05     |
|    ent_coef        | 0.0123   |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.000703 |
|    n_updates       | 4314324  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=2259.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -49.4    |
|    critic_loss     | 1.53     |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.000702 |
|    n_updates       | 4324324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 596      |
|    fps             | 42       |
|    time_elapsed    | 69763    |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=1765.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 2990000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 4.87     |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000701 |
|    n_updates       | 4334324  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=1728.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3000000  |
| train/             |          |
|    actor_loss      | -47.3    |
|    critic_loss     | 2.54     |
|    ent_coef        | 0.013    |
|    ent_coef_loss   | 3.2      |
|    learning_rate   | 0.0007   |
|    n_updates       | 4344324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 600      |
|    fps             | 42       |
|    time_elapsed    | 70177    |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=2991.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | -46.4    |
|    critic_loss     | 1.78     |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | 1.04     |
|    learning_rate   | 0.000699 |
|    n_updates       | 4354324  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=2227.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3020000  |
| train/             |          |
|    actor_loss      | -47.6    |
|    critic_loss     | 0.632    |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | 0.411    |
|    learning_rate   | 0.000698 |
|    n_updates       | 4364324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 604      |
|    fps             | 42       |
|    time_elapsed    | 70601    |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=3131.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3030000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 0.656    |
|    ent_coef        | 0.0114   |
|    ent_coef_loss   | -0.323   |
|    learning_rate   | 0.000697 |
|    n_updates       | 4374324  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=3139.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -47.8    |
|    critic_loss     | 1.73     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 3.57     |
|    learning_rate   | 0.000696 |
|    n_updates       | 4384324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 608      |
|    fps             | 42       |
|    time_elapsed    | 71013    |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=2511.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -49.2    |
|    critic_loss     | 0.96     |
|    ent_coef        | 0.00756  |
|    ent_coef_loss   | -0.932   |
|    learning_rate   | 0.000695 |
|    n_updates       | 4394324  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=3139.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -49.6    |
|    critic_loss     | 1.51     |
|    ent_coef        | 0.00693  |
|    ent_coef_loss   | 1.66     |
|    learning_rate   | 0.000694 |
|    n_updates       | 4404324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 612      |
|    fps             | 42       |
|    time_elapsed    | 71427    |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=3141.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.83     |
|    ent_coef        | 0.00741  |
|    ent_coef_loss   | -1.42    |
|    learning_rate   | 0.000693 |
|    n_updates       | 4414324  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=3090.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 3080000  |
| train/             |          |
|    actor_loss      | -51      |
|    critic_loss     | 0.809    |
|    ent_coef        | 0.00697  |
|    ent_coef_loss   | 0.197    |
|    learning_rate   | 0.000692 |
|    n_updates       | 4424324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 616      |
|    fps             | 42       |
|    time_elapsed    | 71838    |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=3130.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3090000  |
| train/             |          |
|    actor_loss      | -53.3    |
|    critic_loss     | 1.52     |
|    ent_coef        | 0.00822  |
|    ent_coef_loss   | 0.518    |
|    learning_rate   | 0.000691 |
|    n_updates       | 4434324  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=3129.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3100000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 0.512    |
|    ent_coef        | 0.00774  |
|    ent_coef_loss   | -1.79    |
|    learning_rate   | 0.00069  |
|    n_updates       | 4444324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 620      |
|    fps             | 42       |
|    time_elapsed    | 72246    |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=2823.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 3110000  |
| train/             |          |
|    actor_loss      | -56.3    |
|    critic_loss     | 0.863    |
|    ent_coef        | 0.00859  |
|    ent_coef_loss   | -3.7     |
|    learning_rate   | 0.000689 |
|    n_updates       | 4454324  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=3132.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -54.7    |
|    critic_loss     | 10.3     |
|    ent_coef        | 0.00842  |
|    ent_coef_loss   | 2.3      |
|    learning_rate   | 0.000688 |
|    n_updates       | 4464324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 624      |
|    fps             | 42       |
|    time_elapsed    | 72655    |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=2813.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -55.5    |
|    critic_loss     | 4.07     |
|    ent_coef        | 0.00759  |
|    ent_coef_loss   | 0.328    |
|    learning_rate   | 0.000687 |
|    n_updates       | 4474324  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=2238.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -58.2    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00765  |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000686 |
|    n_updates       | 4484324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 628      |
|    fps             | 42       |
|    time_elapsed    | 73064    |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=3150.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 4.88     |
|    ent_coef        | 0.00677  |
|    ent_coef_loss   | -1.94    |
|    learning_rate   | 0.000685 |
|    n_updates       | 4494324  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=1571.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 3160000  |
| train/             |          |
|    actor_loss      | -54      |
|    critic_loss     | 1.06     |
|    ent_coef        | 0.00665  |
|    ent_coef_loss   | -0.957   |
|    learning_rate   | 0.000684 |
|    n_updates       | 4504324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 632      |
|    fps             | 43       |
|    time_elapsed    | 73474    |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=1632.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3170000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 0.244    |
|    ent_coef        | 0.0066   |
|    ent_coef_loss   | -5.79    |
|    learning_rate   | 0.000683 |
|    n_updates       | 4514324  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=3134.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
| train/             |          |
|    actor_loss      | -53.5    |
|    critic_loss     | 0.887    |
|    ent_coef        | 0.0067   |
|    ent_coef_loss   | 0.0485   |
|    learning_rate   | 0.000682 |
|    n_updates       | 4524324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 636      |
|    fps             | 43       |
|    time_elapsed    | 73883    |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=3158.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 3190000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 1.08     |
|    ent_coef        | 0.00634  |
|    ent_coef_loss   | 5.04     |
|    learning_rate   | 0.000681 |
|    n_updates       | 4534324  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=3136.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3200000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.326    |
|    ent_coef        | 0.00686  |
|    ent_coef_loss   | -3.43    |
|    learning_rate   | 0.00068  |
|    n_updates       | 4544324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 640      |
|    fps             | 43       |
|    time_elapsed    | 74291    |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=3144.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 0.833    |
|    ent_coef        | 0.00697  |
|    ent_coef_loss   | 2.18     |
|    learning_rate   | 0.000679 |
|    n_updates       | 4554324  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=3099.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -51.4    |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00769  |
|    ent_coef_loss   | 0.0509   |
|    learning_rate   | 0.000678 |
|    n_updates       | 4564324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 644      |
|    fps             | 43       |
|    time_elapsed    | 74700    |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=2435.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 0.655    |
|    ent_coef        | 0.00884  |
|    ent_coef_loss   | -4.88    |
|    learning_rate   | 0.000677 |
|    n_updates       | 4574324  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=2712.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -51.7    |
|    critic_loss     | 2.75     |
|    ent_coef        | 0.0084   |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000676 |
|    n_updates       | 4584324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 648      |
|    fps             | 43       |
|    time_elapsed    | 75109    |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=1924.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3250000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 1.37     |
|    ent_coef        | 0.00815  |
|    ent_coef_loss   | -0.814   |
|    learning_rate   | 0.000675 |
|    n_updates       | 4594324  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=1558.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3260000  |
| train/             |          |
|    actor_loss      | -52.7    |
|    critic_loss     | 4.36     |
|    ent_coef        | 0.00912  |
|    ent_coef_loss   | 5.12     |
|    learning_rate   | 0.000674 |
|    n_updates       | 4604324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 652      |
|    fps             | 43       |
|    time_elapsed    | 75520    |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=1546.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3270000  |
| train/             |          |
|    actor_loss      | -52.1    |
|    critic_loss     | 4.97     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | -0.983   |
|    learning_rate   | 0.000673 |
|    n_updates       | 4614324  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=2216.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3280000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 0.996    |
|    ent_coef        | 0.0121   |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.000672 |
|    n_updates       | 4624324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 656      |
|    fps             | 43       |
|    time_elapsed    | 75929    |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=1549.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.000671 |
|    n_updates       | 4634324  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=3148.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 2.82     |
|    ent_coef        | 0.0125   |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.00067  |
|    n_updates       | 4644324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    episodes        | 660      |
|    fps             | 43       |
|    time_elapsed    | 76339    |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=2998.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -47.1    |
|    critic_loss     | 4.14     |
|    ent_coef        | 0.0126   |
|    ent_coef_loss   | -2.8     |
|    learning_rate   | 0.000669 |
|    n_updates       | 4654324  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=1763.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 10.2     |
|    ent_coef        | 0.014    |
|    ent_coef_loss   | 2        |
|    learning_rate   | 0.000668 |
|    n_updates       | 4664324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    episodes        | 664      |
|    fps             | 43       |
|    time_elapsed    | 76748    |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=2955.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 3330000  |
| train/             |          |
|    actor_loss      | -52.4    |
|    critic_loss     | 17.5     |
|    ent_coef        | 0.0202   |
|    ent_coef_loss   | -0.68    |
|    learning_rate   | 0.000667 |
|    n_updates       | 4674324  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=1728.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 3340000  |
| train/             |          |
|    actor_loss      | -57.2    |
|    critic_loss     | 2.3      |
|    ent_coef        | 0.0243   |
|    ent_coef_loss   | 0.0292   |
|    learning_rate   | 0.000666 |
|    n_updates       | 4684324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    episodes        | 668      |
|    fps             | 43       |
|    time_elapsed    | 77156    |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=2993.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3350000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 4.18     |
|    ent_coef        | 0.0197   |
|    ent_coef_loss   | -0.389   |
|    learning_rate   | 0.000665 |
|    n_updates       | 4694324  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=2989.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3360000  |
| train/             |          |
|    actor_loss      | -43.3    |
|    critic_loss     | 4.62     |
|    ent_coef        | 0.0174   |
|    ent_coef_loss   | -0.0165  |
|    learning_rate   | 0.000664 |
|    n_updates       | 4704324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    episodes        | 672      |
|    fps             | 43       |
|    time_elapsed    | 77565    |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=3138.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3370000  |
| train/             |          |
|    actor_loss      | -45.6    |
|    critic_loss     | 1.56     |
|    ent_coef        | 0.0164   |
|    ent_coef_loss   | -1.51    |
|    learning_rate   | 0.000663 |
|    n_updates       | 4714324  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=1744.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -46.9    |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.0156   |
|    ent_coef_loss   | 0.911    |
|    learning_rate   | 0.000662 |
|    n_updates       | 4724324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 676      |
|    fps             | 43       |
|    time_elapsed    | 77973    |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=3121.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -45.5    |
|    critic_loss     | 1.8      |
|    ent_coef        | 0.0143   |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000661 |
|    n_updates       | 4734324  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=3147.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -45.3    |
|    critic_loss     | 2.1      |
|    ent_coef        | 0.012    |
|    ent_coef_loss   | -0.48    |
|    learning_rate   | 0.00066  |
|    n_updates       | 4744324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 680      |
|    fps             | 43       |
|    time_elapsed    | 78381    |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=3139.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -49.5    |
|    critic_loss     | 0.782    |
|    ent_coef        | 0.0131   |
|    ent_coef_loss   | -2.42    |
|    learning_rate   | 0.000659 |
|    n_updates       | 4754324  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=3143.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3420000  |
| train/             |          |
|    actor_loss      | -50.1    |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.0136   |
|    ent_coef_loss   | -1.8     |
|    learning_rate   | 0.000658 |
|    n_updates       | 4764324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 684      |
|    fps             | 43       |
|    time_elapsed    | 78791    |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=3133.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3430000  |
| train/             |          |
|    actor_loss      | -54.6    |
|    critic_loss     | 4.75     |
|    ent_coef        | 0.0135   |
|    ent_coef_loss   | -0.474   |
|    learning_rate   | 0.000657 |
|    n_updates       | 4774324  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=3057.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 3440000  |
| train/             |          |
|    actor_loss      | -55.6    |
|    critic_loss     | 2.19     |
|    ent_coef        | 0.0162   |
|    ent_coef_loss   | -1.03    |
|    learning_rate   | 0.000656 |
|    n_updates       | 4784324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 688      |
|    fps             | 43       |
|    time_elapsed    | 79199    |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=3150.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3450000  |
| train/             |          |
|    actor_loss      | -54.3    |
|    critic_loss     | 3.32     |
|    ent_coef        | 0.0158   |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.000655 |
|    n_updates       | 4794324  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=3134.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -53.7    |
|    critic_loss     | 1.29     |
|    ent_coef        | 0.0135   |
|    ent_coef_loss   | 1.53     |
|    learning_rate   | 0.000654 |
|    n_updates       | 4804324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 692      |
|    fps             | 43       |
|    time_elapsed    | 79608    |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=3150.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 1.54     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | 0.308    |
|    learning_rate   | 0.000653 |
|    n_updates       | 4814324  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=3139.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -49.8    |
|    critic_loss     | 2.86     |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | -1.04    |
|    learning_rate   | 0.000652 |
|    n_updates       | 4824324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 696      |
|    fps             | 43       |
|    time_elapsed    | 80036    |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=3142.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 1.26     |
|    ent_coef        | 0.0126   |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.000651 |
|    n_updates       | 4834324  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=3130.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3500000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 4.9      |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -0.163   |
|    learning_rate   | 0.00065  |
|    n_updates       | 4844324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 700      |
|    fps             | 43       |
|    time_elapsed    | 80593    |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=3135.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3510000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 0.785    |
|    ent_coef        | 0.0104   |
|    ent_coef_loss   | -1.95    |
|    learning_rate   | 0.000649 |
|    n_updates       | 4854324  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=3142.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3520000  |
| train/             |          |
|    actor_loss      | -50.9    |
|    critic_loss     | 2.03     |
|    ent_coef        | 0.00973  |
|    ent_coef_loss   | 0.477    |
|    learning_rate   | 0.000648 |
|    n_updates       | 4864324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 704      |
|    fps             | 43       |
|    time_elapsed    | 81230    |
|    total_timesteps | 3520000  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=3141.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3530000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.77     |
|    ent_coef        | 0.00964  |
|    ent_coef_loss   | 0.919    |
|    learning_rate   | 0.000647 |
|    n_updates       | 4874324  |
---------------------------------
Eval num_timesteps=3540000, episode_reward=3138.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3540000  |
| train/             |          |
|    actor_loss      | -48.5    |
|    critic_loss     | 1.6      |
|    ent_coef        | 0.00945  |
|    ent_coef_loss   | -0.0226  |
|    learning_rate   | 0.000646 |
|    n_updates       | 4884324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 708      |
|    fps             | 43       |
|    time_elapsed    | 81868    |
|    total_timesteps | 3540000  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=3142.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | -53      |
|    critic_loss     | 1.04     |
|    ent_coef        | 0.0103   |
|    ent_coef_loss   | -0.4     |
|    learning_rate   | 0.000645 |
|    n_updates       | 4894324  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=3138.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -48.3    |
|    critic_loss     | 1.91     |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | 0.286    |
|    learning_rate   | 0.000644 |
|    n_updates       | 4904324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 712      |
|    fps             | 43       |
|    time_elapsed    | 82475    |
|    total_timesteps | 3560000  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=3141.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 1.31     |
|    ent_coef        | 0.01     |
|    ent_coef_loss   | -0.655   |
|    learning_rate   | 0.000643 |
|    n_updates       | 4914324  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=3138.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | -50.4    |
|    critic_loss     | 0.848    |
|    ent_coef        | 0.0099   |
|    ent_coef_loss   | 3.08     |
|    learning_rate   | 0.000642 |
|    n_updates       | 4924324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 716      |
|    fps             | 43       |
|    time_elapsed    | 83078    |
|    total_timesteps | 3580000  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=3135.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | -48.9    |
|    critic_loss     | 1.11     |
|    ent_coef        | 0.00911  |
|    ent_coef_loss   | 1.8      |
|    learning_rate   | 0.000641 |
|    n_updates       | 4934324  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=3134.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3600000  |
| train/             |          |
|    actor_loss      | -49.1    |
|    critic_loss     | 6.07     |
|    ent_coef        | 0.00886  |
|    ent_coef_loss   | -0.349   |
|    learning_rate   | 0.00064  |
|    n_updates       | 4944324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 720      |
|    fps             | 43       |
|    time_elapsed    | 83680    |
|    total_timesteps | 3600000  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=3134.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 3610000  |
| train/             |          |
|    actor_loss      | -50.6    |
|    critic_loss     | 3        |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | -3.55    |
|    learning_rate   | 0.000639 |
|    n_updates       | 4954324  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=3140.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3620000  |
| train/             |          |
|    actor_loss      | -50.5    |
|    critic_loss     | 1.53     |
|    ent_coef        | 0.0104   |
|    ent_coef_loss   | -0.58    |
|    learning_rate   | 0.000638 |
|    n_updates       | 4964324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 724      |
|    fps             | 42       |
|    time_elapsed    | 84286    |
|    total_timesteps | 3620000  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=3141.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | -54.5    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.0101   |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.000637 |
|    n_updates       | 4974324  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=3136.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 2.38     |
|    ent_coef        | 0.00843  |
|    ent_coef_loss   | -2.77    |
|    learning_rate   | 0.000636 |
|    n_updates       | 4984324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    episodes        | 728      |
|    fps             | 42       |
|    time_elapsed    | 84867    |
|    total_timesteps | 3640000  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=1769.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | -51.2    |
|    critic_loss     | 2.57     |
|    ent_coef        | 0.0168   |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.000635 |
|    n_updates       | 4994324  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=3136.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | -48.1    |
|    critic_loss     | 1.09     |
|    ent_coef        | 0.0155   |
|    ent_coef_loss   | -4.23    |
|    learning_rate   | 0.000634 |
|    n_updates       | 5004324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 732      |
|    fps             | 42       |
|    time_elapsed    | 85418    |
|    total_timesteps | 3660000  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=3137.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3670000  |
| train/             |          |
|    actor_loss      | -50.8    |
|    critic_loss     | 1.77     |
|    ent_coef        | 0.0155   |
|    ent_coef_loss   | 0.856    |
|    learning_rate   | 0.000633 |
|    n_updates       | 5014324  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=3142.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3680000  |
| train/             |          |
|    actor_loss      | -47.9    |
|    critic_loss     | 1.78     |
|    ent_coef        | 0.0131   |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000632 |
|    n_updates       | 5024324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 736      |
|    fps             | 42       |
|    time_elapsed    | 85989    |
|    total_timesteps | 3680000  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=2991.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3690000  |
| train/             |          |
|    actor_loss      | -50.2    |
|    critic_loss     | 4.72     |
|    ent_coef        | 0.0136   |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.000631 |
|    n_updates       | 5034324  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=3138.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3700000  |
| train/             |          |
|    actor_loss      | -50.7    |
|    critic_loss     | 1.92     |
|    ent_coef        | 0.0128   |
|    ent_coef_loss   | 3.87     |
|    learning_rate   | 0.00063  |
|    n_updates       | 5044324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 740      |
|    fps             | 42       |
|    time_elapsed    | 86574    |
|    total_timesteps | 3700000  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=2856.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 3710000  |
| train/             |          |
|    actor_loss      | -51.5    |
|    critic_loss     | 0.794    |
|    ent_coef        | 0.0117   |
|    ent_coef_loss   | -1.3     |
|    learning_rate   | 0.000629 |
|    n_updates       | 5054324  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=3138.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3720000  |
| train/             |          |
|    actor_loss      | -53.4    |
|    critic_loss     | 3.16     |
|    ent_coef        | 0.014    |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000628 |
|    n_updates       | 5064324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    episodes        | 744      |
|    fps             | 42       |
|    time_elapsed    | 87158    |
|    total_timesteps | 3720000  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=1590.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | -55.2    |
|    critic_loss     | 1.47     |
|    ent_coef        | 0.0157   |
|    ent_coef_loss   | -0.929   |
|    learning_rate   | 0.000627 |
|    n_updates       | 5074324  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=3150.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | -54.1    |
|    critic_loss     | 2.97     |
|    ent_coef        | 0.0145   |
|    ent_coef_loss   | -0.0915  |
|    learning_rate   | 0.000626 |
|    n_updates       | 5084324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    episodes        | 748      |
|    fps             | 42       |
|    time_elapsed    | 87752    |
|    total_timesteps | 3740000  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=1563.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | -55      |
|    critic_loss     | 0.606    |
|    ent_coef        | 0.0127   |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000625 |
|    n_updates       | 5094324  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=3141.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3760000  |
| train/             |          |
|    actor_loss      | -54.2    |
|    critic_loss     | 1.33     |
|    ent_coef        | 0.0113   |
|    ent_coef_loss   | 0.474    |
|    learning_rate   | 0.000624 |
|    n_updates       | 5104324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.75e+03 |
| time/              |          |
|    episodes        | 752      |
|    fps             | 42       |
|    time_elapsed    | 88342    |
|    total_timesteps | 3760000  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=1707.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 3770000  |
| train/             |          |
|    actor_loss      | -56.5    |
|    critic_loss     | 6.41     |
|    ent_coef        | 0.0135   |
|    ent_coef_loss   | -0.491   |
|    learning_rate   | 0.000623 |
|    n_updates       | 5114324  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=3143.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 3780000  |
| train/             |          |
|    actor_loss      | -59      |
|    critic_loss     | 2.85     |
|    ent_coef        | 0.0137   |
|    ent_coef_loss   | 0.712    |
|    learning_rate   | 0.000622 |
|    n_updates       | 5124324  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 756      |
|    fps             | 42       |
|    time_elapsed    | 88933    |
|    total_timesteps | 3780000  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=2993.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 3790000  |
| train/             |          |
|    actor_loss      | -63.7    |
|    critic_loss     | 7.28     |
|    ent_coef        | 0.0144   |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.000621 |
|    n_updates       | 5134324  |
