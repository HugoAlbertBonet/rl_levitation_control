Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to 2gdl/SAC_53
Eval num_timesteps=10000, episode_reward=-122670.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 10000     |
| train/             |           |
|    actor_loss      | 1.89e+03  |
|    critic_loss     | 1.62      |
|    ent_coef        | 0.0156    |
|    ent_coef_loss   | 0.0923    |
|    learning_rate   | 9.99e-05  |
|    n_updates       | 3689798   |
----------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=-122713.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 20000     |
| train/             |           |
|    actor_loss      | 2.11e+03  |
|    critic_loss     | 0.638     |
|    ent_coef        | 0.0122    |
|    ent_coef_loss   | 0.421     |
|    learning_rate   | 9.98e-05  |
|    n_updates       | 3699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 4         |
|    fps             | 14        |
|    time_elapsed    | 1383      |
|    total_timesteps | 20000     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-122710.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 30000     |
| train/             |           |
|    actor_loss      | 2.25e+03  |
|    critic_loss     | 1.43      |
|    ent_coef        | 0.013     |
|    ent_coef_loss   | 0.662     |
|    learning_rate   | 9.97e-05  |
|    n_updates       | 3709798   |
----------------------------------
Eval num_timesteps=40000, episode_reward=-122695.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 40000     |
| train/             |           |
|    actor_loss      | 2.33e+03  |
|    critic_loss     | 2.95      |
|    ent_coef        | 0.0213    |
|    ent_coef_loss   | -1.51     |
|    learning_rate   | 9.96e-05  |
|    n_updates       | 3719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 8         |
|    fps             | 14        |
|    time_elapsed    | 2772      |
|    total_timesteps | 40000     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-122686.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 50000     |
| train/             |           |
|    actor_loss      | 2.38e+03  |
|    critic_loss     | 4.67      |
|    ent_coef        | 0.0113    |
|    ent_coef_loss   | -2.13     |
|    learning_rate   | 9.95e-05  |
|    n_updates       | 3729798   |
----------------------------------
Eval num_timesteps=60000, episode_reward=-122689.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 60000     |
| train/             |           |
|    actor_loss      | 2.41e+03  |
|    critic_loss     | 0.592     |
|    ent_coef        | 0.0115    |
|    ent_coef_loss   | -1.25     |
|    learning_rate   | 9.94e-05  |
|    n_updates       | 3739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 12        |
|    fps             | 14        |
|    time_elapsed    | 4120      |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-122702.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 70000     |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 1.13      |
|    ent_coef        | 0.0126    |
|    ent_coef_loss   | 1.38      |
|    learning_rate   | 9.93e-05  |
|    n_updates       | 3749798   |
----------------------------------
Eval num_timesteps=80000, episode_reward=-122711.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 80000     |
| train/             |           |
|    actor_loss      | 2.44e+03  |
|    critic_loss     | 1.06      |
|    ent_coef        | 0.0217    |
|    ent_coef_loss   | 0.0115    |
|    learning_rate   | 9.92e-05  |
|    n_updates       | 3759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 16        |
|    fps             | 14        |
|    time_elapsed    | 5453      |
|    total_timesteps | 80000     |
----------------------------------
Eval num_timesteps=90000, episode_reward=-122751.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 90000     |
| train/             |           |
|    actor_loss      | 2.44e+03  |
|    critic_loss     | 3.03      |
|    ent_coef        | 0.0317    |
|    ent_coef_loss   | 0.267     |
|    learning_rate   | 9.91e-05  |
|    n_updates       | 3769798   |
----------------------------------
Eval num_timesteps=100000, episode_reward=-122734.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 100000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 10.3      |
|    ent_coef        | 0.0193    |
|    ent_coef_loss   | -0.848    |
|    learning_rate   | 9.9e-05   |
|    n_updates       | 3779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 20        |
|    fps             | 14        |
|    time_elapsed    | 6780      |
|    total_timesteps | 100000    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-122736.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 110000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.91      |
|    ent_coef        | 0.0259    |
|    ent_coef_loss   | -1.78     |
|    learning_rate   | 9.89e-05  |
|    n_updates       | 3789798   |
----------------------------------
Eval num_timesteps=120000, episode_reward=-122746.16 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 120000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.969     |
|    ent_coef        | 0.0153    |
|    ent_coef_loss   | -0.112    |
|    learning_rate   | 9.88e-05  |
|    n_updates       | 3799798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 24        |
|    fps             | 14        |
|    time_elapsed    | 8103      |
|    total_timesteps | 120000    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-122744.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 130000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.559     |
|    ent_coef        | 0.0266    |
|    ent_coef_loss   | 4.89      |
|    learning_rate   | 9.87e-05  |
|    n_updates       | 3809798   |
----------------------------------
Eval num_timesteps=140000, episode_reward=-122744.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 140000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.762     |
|    ent_coef        | 0.033     |
|    ent_coef_loss   | 0.531     |
|    learning_rate   | 9.86e-05  |
|    n_updates       | 3819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 28        |
|    fps             | 14        |
|    time_elapsed    | 9422      |
|    total_timesteps | 140000    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-122733.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 150000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.25      |
|    ent_coef        | 0.0249    |
|    ent_coef_loss   | 0.375     |
|    learning_rate   | 9.85e-05  |
|    n_updates       | 3829798   |
----------------------------------
Eval num_timesteps=160000, episode_reward=-122738.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 160000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.05      |
|    ent_coef        | 0.0291    |
|    ent_coef_loss   | -1.98     |
|    learning_rate   | 9.84e-05  |
|    n_updates       | 3839798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 32        |
|    fps             | 14        |
|    time_elapsed    | 10747     |
|    total_timesteps | 160000    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-122740.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 170000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.25      |
|    ent_coef        | 0.0233    |
|    ent_coef_loss   | 0.652     |
|    learning_rate   | 9.83e-05  |
|    n_updates       | 3849798   |
----------------------------------
Eval num_timesteps=180000, episode_reward=-122741.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 180000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 3.36      |
|    ent_coef        | 0.0374    |
|    ent_coef_loss   | -1.39     |
|    learning_rate   | 9.82e-05  |
|    n_updates       | 3859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 36        |
|    fps             | 14        |
|    time_elapsed    | 12078     |
|    total_timesteps | 180000    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-122692.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 190000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.1       |
|    ent_coef        | 0.0405    |
|    ent_coef_loss   | -1.11     |
|    learning_rate   | 9.81e-05  |
|    n_updates       | 3869798   |
----------------------------------
Eval num_timesteps=200000, episode_reward=-122729.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 200000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.888     |
|    ent_coef        | 0.0222    |
|    ent_coef_loss   | 0.257     |
|    learning_rate   | 9.8e-05   |
|    n_updates       | 3879798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 40        |
|    fps             | 14        |
|    time_elapsed    | 13433     |
|    total_timesteps | 200000    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-122678.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 210000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 4.13      |
|    ent_coef        | 0.0203    |
|    ent_coef_loss   | 0.578     |
|    learning_rate   | 9.79e-05  |
|    n_updates       | 3889798   |
----------------------------------
Eval num_timesteps=220000, episode_reward=-122692.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 220000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.354     |
|    ent_coef        | 0.0305    |
|    ent_coef_loss   | 0.146     |
|    learning_rate   | 9.78e-05  |
|    n_updates       | 3899798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 44        |
|    fps             | 14        |
|    time_elapsed    | 14822     |
|    total_timesteps | 220000    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-122722.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 230000    |
| train/             |           |
|    actor_loss      | 2.44e+03  |
|    critic_loss     | 0.969     |
|    ent_coef        | 0.0563    |
|    ent_coef_loss   | 0.791     |
|    learning_rate   | 9.77e-05  |
|    n_updates       | 3909798   |
----------------------------------
Eval num_timesteps=240000, episode_reward=-122743.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 240000    |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 0.815     |
|    ent_coef        | 0.0536    |
|    ent_coef_loss   | -0.519    |
|    learning_rate   | 9.76e-05  |
|    n_updates       | 3919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 48        |
|    fps             | 14        |
|    time_elapsed    | 16168     |
|    total_timesteps | 240000    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-122694.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 250000    |
| train/             |           |
|    actor_loss      | 2.44e+03  |
|    critic_loss     | 0.239     |
|    ent_coef        | 0.0304    |
|    ent_coef_loss   | -2.7      |
|    learning_rate   | 9.75e-05  |
|    n_updates       | 3929798   |
----------------------------------
Eval num_timesteps=260000, episode_reward=-122763.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 260000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.205     |
|    ent_coef        | 0.0188    |
|    ent_coef_loss   | 0.894     |
|    learning_rate   | 9.74e-05  |
|    n_updates       | 3939798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 52        |
|    fps             | 14        |
|    time_elapsed    | 17515     |
|    total_timesteps | 260000    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-122698.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 270000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.35      |
|    ent_coef        | 0.019     |
|    ent_coef_loss   | -3.36     |
|    learning_rate   | 9.73e-05  |
|    n_updates       | 3949798   |
----------------------------------
Eval num_timesteps=280000, episode_reward=-122718.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 280000    |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.1       |
|    ent_coef        | 0.00904   |
|    ent_coef_loss   | -2.48     |
|    learning_rate   | 9.72e-05  |
|    n_updates       | 3959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 56        |
|    fps             | 14        |
|    time_elapsed    | 18857     |
|    total_timesteps | 280000    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-122719.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 290000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.518     |
|    ent_coef        | 0.0118    |
|    ent_coef_loss   | 0.639     |
|    learning_rate   | 9.71e-05  |
|    n_updates       | 3969798   |
----------------------------------
Eval num_timesteps=300000, episode_reward=-122764.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 300000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.437     |
|    ent_coef        | 0.0125    |
|    ent_coef_loss   | 0.973     |
|    learning_rate   | 9.7e-05   |
|    n_updates       | 3979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 60        |
|    fps             | 14        |
|    time_elapsed    | 20188     |
|    total_timesteps | 300000    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-122763.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 310000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.16      |
|    ent_coef        | 0.0195    |
|    ent_coef_loss   | -0.992    |
|    learning_rate   | 9.69e-05  |
|    n_updates       | 3989798   |
----------------------------------
Eval num_timesteps=320000, episode_reward=-122741.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 320000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.6       |
|    ent_coef        | 0.0182    |
|    ent_coef_loss   | 0.133     |
|    learning_rate   | 9.68e-05  |
|    n_updates       | 3999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 64        |
|    fps             | 14        |
|    time_elapsed    | 21515     |
|    total_timesteps | 320000    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-122675.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 330000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.955     |
|    ent_coef        | 0.0185    |
|    ent_coef_loss   | -0.997    |
|    learning_rate   | 9.67e-05  |
|    n_updates       | 4009798   |
----------------------------------
Eval num_timesteps=340000, episode_reward=-122677.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 340000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.238     |
|    ent_coef        | 0.0222    |
|    ent_coef_loss   | -2.86     |
|    learning_rate   | 9.66e-05  |
|    n_updates       | 4019798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 68        |
|    fps             | 14        |
|    time_elapsed    | 22836     |
|    total_timesteps | 340000    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-122573.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 350000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.612     |
|    ent_coef        | 0.0184    |
|    ent_coef_loss   | -1.01     |
|    learning_rate   | 9.65e-05  |
|    n_updates       | 4029798   |
----------------------------------
New best mean reward!
Eval num_timesteps=360000, episode_reward=-122717.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 360000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.627     |
|    ent_coef        | 0.0188    |
|    ent_coef_loss   | 1.13      |
|    learning_rate   | 9.64e-05  |
|    n_updates       | 4039798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 72        |
|    fps             | 14        |
|    time_elapsed    | 24153     |
|    total_timesteps | 360000    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-122709.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 370000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.154     |
|    ent_coef        | 0.015     |
|    ent_coef_loss   | -3.05     |
|    learning_rate   | 9.63e-05  |
|    n_updates       | 4049798   |
----------------------------------
Eval num_timesteps=380000, episode_reward=-122711.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 380000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.362     |
|    ent_coef        | 0.023     |
|    ent_coef_loss   | 0.645     |
|    learning_rate   | 9.62e-05  |
|    n_updates       | 4059798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 76        |
|    fps             | 14        |
|    time_elapsed    | 25482     |
|    total_timesteps | 380000    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-122755.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 390000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.915     |
|    ent_coef        | 0.0115    |
|    ent_coef_loss   | -5.62     |
|    learning_rate   | 9.61e-05  |
|    n_updates       | 4069798   |
----------------------------------
Eval num_timesteps=400000, episode_reward=-122755.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 400000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.221     |
|    ent_coef        | 0.00851   |
|    ent_coef_loss   | 1.18      |
|    learning_rate   | 9.6e-05   |
|    n_updates       | 4079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 80        |
|    fps             | 14        |
|    time_elapsed    | 26816     |
|    total_timesteps | 400000    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-122741.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 410000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.301     |
|    ent_coef        | 0.00893   |
|    ent_coef_loss   | -1.29     |
|    learning_rate   | 9.59e-05  |
|    n_updates       | 4089798   |
----------------------------------
Eval num_timesteps=420000, episode_reward=-122742.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 420000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.779     |
|    ent_coef        | 0.00934   |
|    ent_coef_loss   | -2.32     |
|    learning_rate   | 9.58e-05  |
|    n_updates       | 4099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 84        |
|    fps             | 14        |
|    time_elapsed    | 28159     |
|    total_timesteps | 420000    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-122758.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 430000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 2.35      |
|    ent_coef        | 0.00753   |
|    ent_coef_loss   | 1.66      |
|    learning_rate   | 9.57e-05  |
|    n_updates       | 4109798   |
----------------------------------
Eval num_timesteps=440000, episode_reward=-122764.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 440000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0972    |
|    ent_coef        | 0.0083    |
|    ent_coef_loss   | -5.8      |
|    learning_rate   | 9.56e-05  |
|    n_updates       | 4119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 88        |
|    fps             | 14        |
|    time_elapsed    | 29535     |
|    total_timesteps | 440000    |
----------------------------------
Eval num_timesteps=450000, episode_reward=-122755.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 450000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.113     |
|    ent_coef        | 0.00861   |
|    ent_coef_loss   | 0.0534    |
|    learning_rate   | 9.55e-05  |
|    n_updates       | 4129798   |
----------------------------------
Eval num_timesteps=460000, episode_reward=-122629.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 460000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.221     |
|    ent_coef        | 0.0104    |
|    ent_coef_loss   | -0.154    |
|    learning_rate   | 9.54e-05  |
|    n_updates       | 4139798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 92        |
|    fps             | 14        |
|    time_elapsed    | 30885     |
|    total_timesteps | 460000    |
----------------------------------
Eval num_timesteps=470000, episode_reward=-122644.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 470000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.107     |
|    ent_coef        | 0.00709   |
|    ent_coef_loss   | -1.67     |
|    learning_rate   | 9.53e-05  |
|    n_updates       | 4149798   |
----------------------------------
Eval num_timesteps=480000, episode_reward=-122693.04 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 480000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 6.54      |
|    ent_coef        | 0.00709   |
|    ent_coef_loss   | -3.83     |
|    learning_rate   | 9.52e-05  |
|    n_updates       | 4159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 96        |
|    fps             | 14        |
|    time_elapsed    | 32217     |
|    total_timesteps | 480000    |
----------------------------------
Eval num_timesteps=490000, episode_reward=-122681.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 490000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.76      |
|    ent_coef        | 0.00452   |
|    ent_coef_loss   | -0.379    |
|    learning_rate   | 9.51e-05  |
|    n_updates       | 4169798   |
----------------------------------
Eval num_timesteps=500000, episode_reward=-122734.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 500000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.162     |
|    ent_coef        | 0.00455   |
|    ent_coef_loss   | 6.13      |
|    learning_rate   | 9.5e-05   |
|    n_updates       | 4179798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 100       |
|    fps             | 14        |
|    time_elapsed    | 33562     |
|    total_timesteps | 500000    |
----------------------------------
Eval num_timesteps=510000, episode_reward=-122693.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 510000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.163     |
|    ent_coef        | 0.00512   |
|    ent_coef_loss   | -0.561    |
|    learning_rate   | 9.49e-05  |
|    n_updates       | 4189798   |
----------------------------------
Eval num_timesteps=520000, episode_reward=-122716.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 520000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.515     |
|    ent_coef        | 0.00506   |
|    ent_coef_loss   | 2.83      |
|    learning_rate   | 9.48e-05  |
|    n_updates       | 4199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 104       |
|    fps             | 14        |
|    time_elapsed    | 34886     |
|    total_timesteps | 520000    |
----------------------------------
Eval num_timesteps=530000, episode_reward=-122706.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 530000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0924    |
|    ent_coef        | 0.00386   |
|    ent_coef_loss   | -2.48     |
|    learning_rate   | 9.47e-05  |
|    n_updates       | 4209798   |
----------------------------------
Eval num_timesteps=540000, episode_reward=-122702.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 540000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.653     |
|    ent_coef        | 0.0105    |
|    ent_coef_loss   | 9.49      |
|    learning_rate   | 9.46e-05  |
|    n_updates       | 4219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 108       |
|    fps             | 14        |
|    time_elapsed    | 36208     |
|    total_timesteps | 540000    |
----------------------------------
Eval num_timesteps=550000, episode_reward=-122768.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 550000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.248     |
|    ent_coef        | 0.00654   |
|    ent_coef_loss   | -0.383    |
|    learning_rate   | 9.45e-05  |
|    n_updates       | 4229798   |
----------------------------------
Eval num_timesteps=560000, episode_reward=-122704.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 560000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0756    |
|    ent_coef        | 0.0055    |
|    ent_coef_loss   | -2.93     |
|    learning_rate   | 9.44e-05  |
|    n_updates       | 4239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 112       |
|    fps             | 14        |
|    time_elapsed    | 37531     |
|    total_timesteps | 560000    |
----------------------------------
Eval num_timesteps=570000, episode_reward=-122717.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 570000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.291     |
|    ent_coef        | 0.00447   |
|    ent_coef_loss   | 1.33      |
|    learning_rate   | 9.43e-05  |
|    n_updates       | 4249798   |
----------------------------------
Eval num_timesteps=580000, episode_reward=-122721.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 580000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 16.5      |
|    ent_coef        | 0.00569   |
|    ent_coef_loss   | 2.13      |
|    learning_rate   | 9.42e-05  |
|    n_updates       | 4259798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 116       |
|    fps             | 14        |
|    time_elapsed    | 38857     |
|    total_timesteps | 580000    |
----------------------------------
Eval num_timesteps=590000, episode_reward=-122702.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 590000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.399     |
|    ent_coef        | 0.00884   |
|    ent_coef_loss   | -0.662    |
|    learning_rate   | 9.41e-05  |
|    n_updates       | 4269798   |
----------------------------------
Eval num_timesteps=600000, episode_reward=-122685.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 600000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.341     |
|    ent_coef        | 0.00645   |
|    ent_coef_loss   | -1.1      |
|    learning_rate   | 9.4e-05   |
|    n_updates       | 4279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 120       |
|    fps             | 14        |
|    time_elapsed    | 40196     |
|    total_timesteps | 600000    |
----------------------------------
Eval num_timesteps=610000, episode_reward=-122709.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 610000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 8.29      |
|    ent_coef        | 0.00404   |
|    ent_coef_loss   | 0.231     |
|    learning_rate   | 9.39e-05  |
|    n_updates       | 4289798   |
----------------------------------
Eval num_timesteps=620000, episode_reward=-122707.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 620000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.699     |
|    ent_coef        | 0.00398   |
|    ent_coef_loss   | 20        |
|    learning_rate   | 9.38e-05  |
|    n_updates       | 4299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 124       |
|    fps             | 14        |
|    time_elapsed    | 41542     |
|    total_timesteps | 620000    |
----------------------------------
Eval num_timesteps=630000, episode_reward=-122715.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 630000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.351     |
|    ent_coef        | 0.00938   |
|    ent_coef_loss   | 9.21      |
|    learning_rate   | 9.37e-05  |
|    n_updates       | 4309798   |
----------------------------------
Eval num_timesteps=640000, episode_reward=-122700.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 640000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.177     |
|    ent_coef        | 0.0144    |
|    ent_coef_loss   | 0.506     |
|    learning_rate   | 9.36e-05  |
|    n_updates       | 4319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 128       |
|    fps             | 14        |
|    time_elapsed    | 42895     |
|    total_timesteps | 640000    |
----------------------------------
Eval num_timesteps=650000, episode_reward=-122693.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 650000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.478     |
|    ent_coef        | 0.00839   |
|    ent_coef_loss   | -1.51     |
|    learning_rate   | 9.35e-05  |
|    n_updates       | 4329798   |
----------------------------------
Eval num_timesteps=660000, episode_reward=-122712.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 660000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0521    |
|    ent_coef        | 0.00979   |
|    ent_coef_loss   | 4.5       |
|    learning_rate   | 9.34e-05  |
|    n_updates       | 4339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 132       |
|    fps             | 14        |
|    time_elapsed    | 44246     |
|    total_timesteps | 660000    |
----------------------------------
Eval num_timesteps=670000, episode_reward=-122715.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 670000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.405     |
|    ent_coef        | 0.00784   |
|    ent_coef_loss   | 4.47      |
|    learning_rate   | 9.33e-05  |
|    n_updates       | 4349798   |
----------------------------------
Eval num_timesteps=680000, episode_reward=-122727.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 680000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.117     |
|    ent_coef        | 0.0134    |
|    ent_coef_loss   | -0.0334   |
|    learning_rate   | 9.32e-05  |
|    n_updates       | 4359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 136       |
|    fps             | 14        |
|    time_elapsed    | 45593     |
|    total_timesteps | 680000    |
----------------------------------
Eval num_timesteps=690000, episode_reward=-122718.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 690000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0936    |
|    ent_coef        | 0.0155    |
|    ent_coef_loss   | -3.55     |
|    learning_rate   | 9.31e-05  |
|    n_updates       | 4369798   |
----------------------------------
Eval num_timesteps=700000, episode_reward=-122725.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 700000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0708    |
|    ent_coef        | 0.0083    |
|    ent_coef_loss   | -2.09     |
|    learning_rate   | 9.3e-05   |
|    n_updates       | 4379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 140       |
|    fps             | 14        |
|    time_elapsed    | 46933     |
|    total_timesteps | 700000    |
----------------------------------
Eval num_timesteps=710000, episode_reward=-122727.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 710000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.561     |
|    ent_coef        | 0.00657   |
|    ent_coef_loss   | 3.4       |
|    learning_rate   | 9.29e-05  |
|    n_updates       | 4389798   |
----------------------------------
Eval num_timesteps=720000, episode_reward=-122724.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 720000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0424    |
|    ent_coef        | 0.0108    |
|    ent_coef_loss   | -1.8      |
|    learning_rate   | 9.28e-05  |
|    n_updates       | 4399798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 144       |
|    fps             | 14        |
|    time_elapsed    | 48266     |
|    total_timesteps | 720000    |
----------------------------------
Eval num_timesteps=730000, episode_reward=-122718.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 730000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.601     |
|    ent_coef        | 0.0131    |
|    ent_coef_loss   | -0.126    |
|    learning_rate   | 9.27e-05  |
|    n_updates       | 4409798   |
----------------------------------
Eval num_timesteps=740000, episode_reward=-122755.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 740000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0639    |
|    ent_coef        | 0.00807   |
|    ent_coef_loss   | -1.05     |
|    learning_rate   | 9.26e-05  |
|    n_updates       | 4419798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 148       |
|    fps             | 14        |
|    time_elapsed    | 49606     |
|    total_timesteps | 740000    |
----------------------------------
Eval num_timesteps=750000, episode_reward=-122710.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 750000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.052     |
|    ent_coef        | 0.00537   |
|    ent_coef_loss   | 2.36      |
|    learning_rate   | 9.25e-05  |
|    n_updates       | 4429798   |
----------------------------------
Eval num_timesteps=760000, episode_reward=-122745.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 760000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0511    |
|    ent_coef        | 0.00349   |
|    ent_coef_loss   | 1.55      |
|    learning_rate   | 9.24e-05  |
|    n_updates       | 4439798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 152       |
|    fps             | 14        |
|    time_elapsed    | 50885     |
|    total_timesteps | 760000    |
----------------------------------
Eval num_timesteps=770000, episode_reward=-122716.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 770000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.84      |
|    ent_coef        | 0.00896   |
|    ent_coef_loss   | 6.59      |
|    learning_rate   | 9.23e-05  |
|    n_updates       | 4449798   |
----------------------------------
Eval num_timesteps=780000, episode_reward=-122730.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 780000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.202     |
|    ent_coef        | 0.0109    |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 9.22e-05  |
|    n_updates       | 4459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 156       |
|    fps             | 14        |
|    time_elapsed    | 52162     |
|    total_timesteps | 780000    |
----------------------------------
Eval num_timesteps=790000, episode_reward=-122718.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 790000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.468     |
|    ent_coef        | 0.0117    |
|    ent_coef_loss   | -0.772    |
|    learning_rate   | 9.21e-05  |
|    n_updates       | 4469798   |
----------------------------------
Eval num_timesteps=800000, episode_reward=-122708.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 800000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.031     |
|    ent_coef        | 0.0101    |
|    ent_coef_loss   | -1.09     |
|    learning_rate   | 9.2e-05   |
|    n_updates       | 4479798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 160       |
|    fps             | 14        |
|    time_elapsed    | 53447     |
|    total_timesteps | 800000    |
----------------------------------
Eval num_timesteps=810000, episode_reward=-122695.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 810000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.868     |
|    ent_coef        | 0.00716   |
|    ent_coef_loss   | 0.207     |
|    learning_rate   | 9.19e-05  |
|    n_updates       | 4489798   |
----------------------------------
Eval num_timesteps=820000, episode_reward=-122709.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 820000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.912     |
|    ent_coef        | 0.00589   |
|    ent_coef_loss   | -1.35     |
|    learning_rate   | 9.18e-05  |
|    n_updates       | 4499798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 164       |
|    fps             | 14        |
|    time_elapsed    | 54742     |
|    total_timesteps | 820000    |
----------------------------------
Eval num_timesteps=830000, episode_reward=-122709.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 830000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.116     |
|    ent_coef        | 0.00524   |
|    ent_coef_loss   | 0.644     |
|    learning_rate   | 9.17e-05  |
|    n_updates       | 4509798   |
----------------------------------
Eval num_timesteps=840000, episode_reward=-122697.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 840000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.106     |
|    ent_coef        | 0.0036    |
|    ent_coef_loss   | -0.114    |
|    learning_rate   | 9.16e-05  |
|    n_updates       | 4519798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 168       |
|    fps             | 14        |
|    time_elapsed    | 56039     |
|    total_timesteps | 840000    |
----------------------------------
Eval num_timesteps=850000, episode_reward=-122708.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 850000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.13      |
|    ent_coef        | 0.00351   |
|    ent_coef_loss   | -1.1      |
|    learning_rate   | 9.15e-05  |
|    n_updates       | 4529798   |
----------------------------------
Eval num_timesteps=860000, episode_reward=-122708.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 860000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0717    |
|    ent_coef        | 0.00376   |
|    ent_coef_loss   | 2.73      |
|    learning_rate   | 9.14e-05  |
|    n_updates       | 4539798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 172       |
|    fps             | 14        |
|    time_elapsed    | 57339     |
|    total_timesteps | 860000    |
----------------------------------
Eval num_timesteps=870000, episode_reward=-122707.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 870000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0369    |
|    ent_coef        | 0.00391   |
|    ent_coef_loss   | -4.03     |
|    learning_rate   | 9.13e-05  |
|    n_updates       | 4549798   |
----------------------------------
Eval num_timesteps=880000, episode_reward=-122689.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 880000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.026     |
|    ent_coef        | 0.00491   |
|    ent_coef_loss   | 0.256     |
|    learning_rate   | 9.12e-05  |
|    n_updates       | 4559798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 176       |
|    fps             | 15        |
|    time_elapsed    | 58634     |
|    total_timesteps | 880000    |
----------------------------------
Eval num_timesteps=890000, episode_reward=-122678.45 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 890000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0283    |
|    ent_coef        | 0.0038    |
|    ent_coef_loss   | -2.17     |
|    learning_rate   | 9.11e-05  |
|    n_updates       | 4569798   |
----------------------------------
Eval num_timesteps=900000, episode_reward=-122675.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 900000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0588    |
|    ent_coef        | 0.00323   |
|    ent_coef_loss   | -2.71     |
|    learning_rate   | 9.1e-05   |
|    n_updates       | 4579798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 180       |
|    fps             | 15        |
|    time_elapsed    | 59917     |
|    total_timesteps | 900000    |
----------------------------------
Eval num_timesteps=910000, episode_reward=-122695.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 910000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0929    |
|    ent_coef        | 0.00347   |
|    ent_coef_loss   | 4         |
|    learning_rate   | 9.09e-05  |
|    n_updates       | 4589798   |
----------------------------------
Eval num_timesteps=920000, episode_reward=-122665.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 920000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.505     |
|    ent_coef        | 0.00284   |
|    ent_coef_loss   | 3.03      |
|    learning_rate   | 9.08e-05  |
|    n_updates       | 4599798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 184       |
|    fps             | 15        |
|    time_elapsed    | 61193     |
|    total_timesteps | 920000    |
----------------------------------
Eval num_timesteps=930000, episode_reward=-122667.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 930000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.105     |
|    ent_coef        | 0.00204   |
|    ent_coef_loss   | -1.54     |
|    learning_rate   | 9.07e-05  |
|    n_updates       | 4609798   |
----------------------------------
Eval num_timesteps=940000, episode_reward=-122680.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 940000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0598    |
|    ent_coef        | 0.0027    |
|    ent_coef_loss   | -1.26     |
|    learning_rate   | 9.06e-05  |
|    n_updates       | 4619798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 188       |
|    fps             | 15        |
|    time_elapsed    | 62466     |
|    total_timesteps | 940000    |
----------------------------------
Eval num_timesteps=950000, episode_reward=-122669.01 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 950000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0661    |
|    ent_coef        | 0.00324   |
|    ent_coef_loss   | -0.945    |
|    learning_rate   | 9.05e-05  |
|    n_updates       | 4629798   |
----------------------------------
Eval num_timesteps=960000, episode_reward=-122669.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 960000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0398    |
|    ent_coef        | 0.00261   |
|    ent_coef_loss   | -4.69     |
|    learning_rate   | 9.04e-05  |
|    n_updates       | 4639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 192       |
|    fps             | 15        |
|    time_elapsed    | 63728     |
|    total_timesteps | 960000    |
----------------------------------
Eval num_timesteps=970000, episode_reward=-122654.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 970000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.174     |
|    ent_coef        | 0.00263   |
|    ent_coef_loss   | 0.501     |
|    learning_rate   | 9.03e-05  |
|    n_updates       | 4649798   |
----------------------------------
Eval num_timesteps=980000, episode_reward=-122689.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 980000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0653    |
|    ent_coef        | 0.00215   |
|    ent_coef_loss   | 1.3       |
|    learning_rate   | 9.02e-05  |
|    n_updates       | 4659798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 196       |
|    fps             | 15        |
|    time_elapsed    | 65015     |
|    total_timesteps | 980000    |
----------------------------------
Eval num_timesteps=990000, episode_reward=-122624.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 990000    |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.136     |
|    ent_coef        | 0.00383   |
|    ent_coef_loss   | 13.2      |
|    learning_rate   | 9.01e-05  |
|    n_updates       | 4669798   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=-122649.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1000000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0132    |
|    ent_coef        | 0.00395   |
|    ent_coef_loss   | -0.902    |
|    learning_rate   | 9e-05     |
|    n_updates       | 4679798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 200       |
|    fps             | 15        |
|    time_elapsed    | 66315     |
|    total_timesteps | 1000000   |
----------------------------------
Eval num_timesteps=1010000, episode_reward=-122637.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1010000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.166     |
|    ent_coef        | 0.00406   |
|    ent_coef_loss   | 3.45      |
|    learning_rate   | 8.99e-05  |
|    n_updates       | 4689798   |
----------------------------------
Eval num_timesteps=1020000, episode_reward=-122656.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1020000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0153    |
|    ent_coef        | 0.00291   |
|    ent_coef_loss   | -7.17     |
|    learning_rate   | 8.98e-05  |
|    n_updates       | 4699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 204       |
|    fps             | 15        |
|    time_elapsed    | 67601     |
|    total_timesteps | 1020000   |
----------------------------------
Eval num_timesteps=1030000, episode_reward=-122633.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1030000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0828    |
|    ent_coef        | 0.00213   |
|    ent_coef_loss   | 9.03      |
|    learning_rate   | 8.97e-05  |
|    n_updates       | 4709798   |
----------------------------------
Eval num_timesteps=1040000, episode_reward=-122658.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1040000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.547     |
|    ent_coef        | 0.00211   |
|    ent_coef_loss   | 0.388     |
|    learning_rate   | 8.96e-05  |
|    n_updates       | 4719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 208       |
|    fps             | 15        |
|    time_elapsed    | 68896     |
|    total_timesteps | 1040000   |
----------------------------------
Eval num_timesteps=1050000, episode_reward=-122643.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1050000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.225     |
|    ent_coef        | 0.00192   |
|    ent_coef_loss   | -3.42     |
|    learning_rate   | 8.95e-05  |
|    n_updates       | 4729798   |
----------------------------------
Eval num_timesteps=1060000, episode_reward=-122641.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1060000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0311    |
|    ent_coef        | 0.00162   |
|    ent_coef_loss   | -0.105    |
|    learning_rate   | 8.94e-05  |
|    n_updates       | 4739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 212       |
|    fps             | 15        |
|    time_elapsed    | 70196     |
|    total_timesteps | 1060000   |
----------------------------------
Eval num_timesteps=1070000, episode_reward=-122649.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1070000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.435     |
|    ent_coef        | 0.00185   |
|    ent_coef_loss   | -2.93     |
|    learning_rate   | 8.93e-05  |
|    n_updates       | 4749798   |
----------------------------------
Eval num_timesteps=1080000, episode_reward=-122644.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1080000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0315    |
|    ent_coef        | 0.00247   |
|    ent_coef_loss   | -3.06     |
|    learning_rate   | 8.92e-05  |
|    n_updates       | 4759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 216       |
|    fps             | 15        |
|    time_elapsed    | 71488     |
|    total_timesteps | 1080000   |
----------------------------------
Eval num_timesteps=1090000, episode_reward=-122640.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1090000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0374    |
|    ent_coef        | 0.00302   |
|    ent_coef_loss   | -1.43     |
|    learning_rate   | 8.91e-05  |
|    n_updates       | 4769798   |
----------------------------------
Eval num_timesteps=1100000, episode_reward=-122637.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1100000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.132     |
|    ent_coef        | 0.00264   |
|    ent_coef_loss   | 2.43      |
|    learning_rate   | 8.9e-05   |
|    n_updates       | 4779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 220       |
|    fps             | 15        |
|    time_elapsed    | 72770     |
|    total_timesteps | 1100000   |
----------------------------------
Eval num_timesteps=1110000, episode_reward=-122624.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1110000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.265     |
|    ent_coef        | 0.00248   |
|    ent_coef_loss   | 5.43      |
|    learning_rate   | 8.89e-05  |
|    n_updates       | 4789798   |
----------------------------------
Eval num_timesteps=1120000, episode_reward=-122649.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1120000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.119     |
|    ent_coef        | 0.00286   |
|    ent_coef_loss   | -2.72     |
|    learning_rate   | 8.88e-05  |
|    n_updates       | 4799798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 224       |
|    fps             | 15        |
|    time_elapsed    | 74055     |
|    total_timesteps | 1120000   |
----------------------------------
Eval num_timesteps=1130000, episode_reward=-122635.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1130000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.103     |
|    ent_coef        | 0.00263   |
|    ent_coef_loss   | -3.54     |
|    learning_rate   | 8.87e-05  |
|    n_updates       | 4809798   |
----------------------------------
Eval num_timesteps=1140000, episode_reward=-122636.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1140000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.04      |
|    ent_coef        | 0.00324   |
|    ent_coef_loss   | 1.77      |
|    learning_rate   | 8.86e-05  |
|    n_updates       | 4819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 228       |
|    fps             | 15        |
|    time_elapsed    | 75319     |
|    total_timesteps | 1140000   |
----------------------------------
Eval num_timesteps=1150000, episode_reward=-122634.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1150000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0818    |
|    ent_coef        | 0.00225   |
|    ent_coef_loss   | -2.17     |
|    learning_rate   | 8.85e-05  |
|    n_updates       | 4829798   |
----------------------------------
Eval num_timesteps=1160000, episode_reward=-122638.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1160000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0936    |
|    ent_coef        | 0.00171   |
|    ent_coef_loss   | 6.42      |
|    learning_rate   | 8.84e-05  |
|    n_updates       | 4839798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 232       |
|    fps             | 15        |
|    time_elapsed    | 76581     |
|    total_timesteps | 1160000   |
----------------------------------
Eval num_timesteps=1170000, episode_reward=-122656.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1170000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.133     |
|    ent_coef        | 0.00168   |
|    ent_coef_loss   | -2.96     |
|    learning_rate   | 8.83e-05  |
|    n_updates       | 4849798   |
----------------------------------
Eval num_timesteps=1180000, episode_reward=-122623.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1180000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0579    |
|    ent_coef        | 0.00155   |
|    ent_coef_loss   | 1.63      |
|    learning_rate   | 8.82e-05  |
|    n_updates       | 4859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 236       |
|    fps             | 15        |
|    time_elapsed    | 77860     |
|    total_timesteps | 1180000   |
----------------------------------
Eval num_timesteps=1190000, episode_reward=-122651.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1190000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.23      |
|    ent_coef        | 0.00204   |
|    ent_coef_loss   | -0.528    |
|    learning_rate   | 8.81e-05  |
|    n_updates       | 4869798   |
----------------------------------
Eval num_timesteps=1200000, episode_reward=-122692.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1200000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.991     |
|    ent_coef        | 0.00382   |
|    ent_coef_loss   | 4.35      |
|    learning_rate   | 8.8e-05   |
|    n_updates       | 4879798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 240       |
|    fps             | 15        |
|    time_elapsed    | 79131     |
|    total_timesteps | 1200000   |
----------------------------------
Eval num_timesteps=1210000, episode_reward=-122663.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1210000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.223     |
|    ent_coef        | 0.00307   |
|    ent_coef_loss   | 1.58      |
|    learning_rate   | 8.79e-05  |
|    n_updates       | 4889798   |
----------------------------------
Eval num_timesteps=1220000, episode_reward=-122638.97 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1220000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.24      |
|    ent_coef        | 0.00311   |
|    ent_coef_loss   | -0.0586   |
|    learning_rate   | 8.78e-05  |
|    n_updates       | 4899798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 244       |
|    fps             | 15        |
|    time_elapsed    | 80403     |
|    total_timesteps | 1220000   |
----------------------------------
Eval num_timesteps=1230000, episode_reward=-122645.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1230000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.506     |
|    ent_coef        | 0.00256   |
|    ent_coef_loss   | 1.14      |
|    learning_rate   | 8.77e-05  |
|    n_updates       | 4909798   |
----------------------------------
Eval num_timesteps=1240000, episode_reward=-122656.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1240000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.045     |
|    ent_coef        | 0.00202   |
|    ent_coef_loss   | -0.539    |
|    learning_rate   | 8.76e-05  |
|    n_updates       | 4919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 248       |
|    fps             | 15        |
|    time_elapsed    | 81699     |
|    total_timesteps | 1240000   |
----------------------------------
Eval num_timesteps=1250000, episode_reward=-122663.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1250000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 14.1      |
|    ent_coef        | 0.00186   |
|    ent_coef_loss   | 1.78      |
|    learning_rate   | 8.75e-05  |
|    n_updates       | 4929798   |
----------------------------------
Eval num_timesteps=1260000, episode_reward=-122683.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1260000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0424    |
|    ent_coef        | 0.00223   |
|    ent_coef_loss   | -1.45     |
|    learning_rate   | 8.74e-05  |
|    n_updates       | 4939798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 252       |
|    fps             | 15        |
|    time_elapsed    | 83001     |
|    total_timesteps | 1260000   |
----------------------------------
Eval num_timesteps=1270000, episode_reward=-122665.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1270000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0794    |
|    ent_coef        | 0.00227   |
|    ent_coef_loss   | -2.3      |
|    learning_rate   | 8.73e-05  |
|    n_updates       | 4949798   |
----------------------------------
Eval num_timesteps=1280000, episode_reward=-122662.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1280000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.085     |
|    ent_coef        | 0.00198   |
|    ent_coef_loss   | -0.798    |
|    learning_rate   | 8.72e-05  |
|    n_updates       | 4959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 256       |
|    fps             | 15        |
|    time_elapsed    | 84284     |
|    total_timesteps | 1280000   |
----------------------------------
Eval num_timesteps=1290000, episode_reward=-122663.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1290000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.221     |
|    ent_coef        | 0.00208   |
|    ent_coef_loss   | -4.55     |
|    learning_rate   | 8.71e-05  |
|    n_updates       | 4969798   |
----------------------------------
Eval num_timesteps=1300000, episode_reward=-122670.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1300000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0168    |
|    ent_coef        | 0.0017    |
|    ent_coef_loss   | -5.83     |
|    learning_rate   | 8.7e-05   |
|    n_updates       | 4979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 260       |
|    fps             | 15        |
|    time_elapsed    | 85652     |
|    total_timesteps | 1300000   |
----------------------------------
Eval num_timesteps=1310000, episode_reward=-122660.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1310000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.642     |
|    ent_coef        | 0.00158   |
|    ent_coef_loss   | -7.29     |
|    learning_rate   | 8.69e-05  |
|    n_updates       | 4989798   |
----------------------------------
Eval num_timesteps=1320000, episode_reward=-122641.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1320000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.012     |
|    ent_coef        | 0.00187   |
|    ent_coef_loss   | -0.659    |
|    learning_rate   | 8.68e-05  |
|    n_updates       | 4999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 264       |
|    fps             | 15        |
|    time_elapsed    | 87001     |
|    total_timesteps | 1320000   |
----------------------------------
Eval num_timesteps=1330000, episode_reward=-122672.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1330000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.22      |
|    ent_coef        | 0.00121   |
|    ent_coef_loss   | -1.95     |
|    learning_rate   | 8.67e-05  |
|    n_updates       | 5009798   |
----------------------------------
Eval num_timesteps=1340000, episode_reward=-122638.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1340000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0415    |
|    ent_coef        | 0.000818  |
|    ent_coef_loss   | -0.889    |
|    learning_rate   | 8.66e-05  |
|    n_updates       | 5019798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 268       |
|    fps             | 15        |
|    time_elapsed    | 88344     |
|    total_timesteps | 1340000   |
----------------------------------
Eval num_timesteps=1350000, episode_reward=-122668.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1350000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.136     |
|    ent_coef        | 0.000873  |
|    ent_coef_loss   | 1.57      |
|    learning_rate   | 8.65e-05  |
|    n_updates       | 5029798   |
----------------------------------
Eval num_timesteps=1360000, episode_reward=-122651.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1360000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.105     |
|    ent_coef        | 0.000736  |
|    ent_coef_loss   | -3.9      |
|    learning_rate   | 8.64e-05  |
|    n_updates       | 5039798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 272       |
|    fps             | 15        |
|    time_elapsed    | 89697     |
|    total_timesteps | 1360000   |
----------------------------------
Eval num_timesteps=1370000, episode_reward=-122654.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1370000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.00533   |
|    ent_coef        | 0.000871  |
|    ent_coef_loss   | 0.599     |
|    learning_rate   | 8.63e-05  |
|    n_updates       | 5049798   |
----------------------------------
Eval num_timesteps=1380000, episode_reward=-122639.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1380000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.00838   |
|    ent_coef        | 0.00112   |
|    ent_coef_loss   | 4.6       |
|    learning_rate   | 8.62e-05  |
|    n_updates       | 5059798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 276       |
|    fps             | 15        |
|    time_elapsed    | 91029     |
|    total_timesteps | 1380000   |
----------------------------------
Eval num_timesteps=1390000, episode_reward=-122637.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1390000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.249     |
|    ent_coef        | 0.00133   |
|    ent_coef_loss   | 5.02      |
|    learning_rate   | 8.61e-05  |
|    n_updates       | 5069798   |
----------------------------------
Eval num_timesteps=1400000, episode_reward=-122653.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1400000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0228    |
|    ent_coef        | 0.00103   |
|    ent_coef_loss   | -5.04     |
|    learning_rate   | 8.6e-05   |
|    n_updates       | 5079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 280       |
|    fps             | 15        |
|    time_elapsed    | 92370     |
|    total_timesteps | 1400000   |
----------------------------------
Eval num_timesteps=1410000, episode_reward=-122665.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1410000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0389    |
|    ent_coef        | 0.00103   |
|    ent_coef_loss   | 2.88      |
|    learning_rate   | 8.59e-05  |
|    n_updates       | 5089798   |
----------------------------------
Eval num_timesteps=1420000, episode_reward=-122665.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1420000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.259     |
|    ent_coef        | 0.00107   |
|    ent_coef_loss   | 1.08      |
|    learning_rate   | 8.58e-05  |
|    n_updates       | 5099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 284       |
|    fps             | 15        |
|    time_elapsed    | 93722     |
|    total_timesteps | 1420000   |
----------------------------------
Eval num_timesteps=1430000, episode_reward=-122652.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1430000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0125    |
|    ent_coef        | 0.000998  |
|    ent_coef_loss   | 6.7       |
|    learning_rate   | 8.57e-05  |
|    n_updates       | 5109798   |
----------------------------------
Eval num_timesteps=1440000, episode_reward=-122677.31 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1440000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.16      |
|    ent_coef        | 0.000991  |
|    ent_coef_loss   | -8.88     |
|    learning_rate   | 8.56e-05  |
|    n_updates       | 5119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 288       |
|    fps             | 15        |
|    time_elapsed    | 95079     |
|    total_timesteps | 1440000   |
----------------------------------
Eval num_timesteps=1450000, episode_reward=-122652.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1450000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.152     |
|    ent_coef        | 0.000812  |
|    ent_coef_loss   | 7.36      |
|    learning_rate   | 8.55e-05  |
|    n_updates       | 5129798   |
----------------------------------
Eval num_timesteps=1460000, episode_reward=-122690.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1460000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.138     |
|    ent_coef        | 0.000814  |
|    ent_coef_loss   | -4.43     |
|    learning_rate   | 8.54e-05  |
|    n_updates       | 5139798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 292       |
|    fps             | 15        |
|    time_elapsed    | 96435     |
|    total_timesteps | 1460000   |
----------------------------------
Eval num_timesteps=1470000, episode_reward=-122723.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1470000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.409     |
|    ent_coef        | 0.00105   |
|    ent_coef_loss   | 12.9      |
|    learning_rate   | 8.53e-05  |
|    n_updates       | 5149798   |
----------------------------------
Eval num_timesteps=1480000, episode_reward=-122665.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1480000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.113     |
|    ent_coef        | 0.00104   |
|    ent_coef_loss   | -1.47     |
|    learning_rate   | 8.52e-05  |
|    n_updates       | 5159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 296       |
|    fps             | 15        |
|    time_elapsed    | 97782     |
|    total_timesteps | 1480000   |
----------------------------------
Eval num_timesteps=1490000, episode_reward=-122677.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1490000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0122    |
|    ent_coef        | 0.000867  |
|    ent_coef_loss   | -1.47     |
|    learning_rate   | 8.51e-05  |
|    n_updates       | 5169798   |
----------------------------------
Eval num_timesteps=1500000, episode_reward=-122676.57 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1500000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0159    |
|    ent_coef        | 0.000962  |
|    ent_coef_loss   | -0.423    |
|    learning_rate   | 8.5e-05   |
|    n_updates       | 5179798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 300       |
|    fps             | 15        |
|    time_elapsed    | 99122     |
|    total_timesteps | 1500000   |
----------------------------------
Eval num_timesteps=1510000, episode_reward=-122666.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1510000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.483     |
|    ent_coef        | 0.000978  |
|    ent_coef_loss   | -4.71     |
|    learning_rate   | 8.49e-05  |
|    n_updates       | 5189798   |
----------------------------------
Eval num_timesteps=1520000, episode_reward=-122680.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1520000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0683    |
|    ent_coef        | 0.00137   |
|    ent_coef_loss   | -5.92     |
|    learning_rate   | 8.48e-05  |
|    n_updates       | 5199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 304       |
|    fps             | 15        |
|    time_elapsed    | 100507    |
|    total_timesteps | 1520000   |
----------------------------------
Eval num_timesteps=1530000, episode_reward=-122666.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1530000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.105     |
|    ent_coef        | 0.00129   |
|    ent_coef_loss   | -3.1      |
|    learning_rate   | 8.47e-05  |
|    n_updates       | 5209798   |
----------------------------------
Eval num_timesteps=1540000, episode_reward=-122651.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1540000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0579    |
|    ent_coef        | 0.00119   |
|    ent_coef_loss   | 0.547     |
|    learning_rate   | 8.46e-05  |
|    n_updates       | 5219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 308       |
|    fps             | 15        |
|    time_elapsed    | 101839    |
|    total_timesteps | 1540000   |
----------------------------------
Eval num_timesteps=1550000, episode_reward=-122656.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1550000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.128     |
|    ent_coef        | 0.001     |
|    ent_coef_loss   | -5.68     |
|    learning_rate   | 8.45e-05  |
|    n_updates       | 5229798   |
----------------------------------
Eval num_timesteps=1560000, episode_reward=-122658.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1560000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0318    |
|    ent_coef        | 0.00106   |
|    ent_coef_loss   | -7.84     |
|    learning_rate   | 8.44e-05  |
|    n_updates       | 5239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 312       |
|    fps             | 15        |
|    time_elapsed    | 103173    |
|    total_timesteps | 1560000   |
----------------------------------
Eval num_timesteps=1570000, episode_reward=-122651.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1570000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0168    |
|    ent_coef        | 0.00105   |
|    ent_coef_loss   | -5.42     |
|    learning_rate   | 8.43e-05  |
|    n_updates       | 5249798   |
----------------------------------
Eval num_timesteps=1580000, episode_reward=-122655.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1580000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0553    |
|    ent_coef        | 0.000996  |
|    ent_coef_loss   | -11.2     |
|    learning_rate   | 8.42e-05  |
|    n_updates       | 5259798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 316       |
|    fps             | 15        |
|    time_elapsed    | 104524    |
|    total_timesteps | 1580000   |
----------------------------------
Eval num_timesteps=1590000, episode_reward=-122639.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1590000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.95      |
|    ent_coef        | 0.00131   |
|    ent_coef_loss   | 12.4      |
|    learning_rate   | 8.41e-05  |
|    n_updates       | 5269798   |
----------------------------------
Eval num_timesteps=1600000, episode_reward=-122638.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1600000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0181    |
|    ent_coef        | 0.00147   |
|    ent_coef_loss   | -5.97     |
|    learning_rate   | 8.4e-05   |
|    n_updates       | 5279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 320       |
|    fps             | 15        |
|    time_elapsed    | 105924    |
|    total_timesteps | 1600000   |
----------------------------------
Eval num_timesteps=1610000, episode_reward=-122639.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1610000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.552     |
|    ent_coef        | 0.00113   |
|    ent_coef_loss   | -7.3      |
|    learning_rate   | 8.39e-05  |
|    n_updates       | 5289798   |
----------------------------------
Eval num_timesteps=1620000, episode_reward=-122656.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1620000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0792    |
|    ent_coef        | 0.00142   |
|    ent_coef_loss   | -11.1     |
|    learning_rate   | 8.38e-05  |
|    n_updates       | 5299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 324       |
|    fps             | 15        |
|    time_elapsed    | 107276    |
|    total_timesteps | 1620000   |
----------------------------------
Eval num_timesteps=1630000, episode_reward=-122653.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1630000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.266     |
|    ent_coef        | 0.00139   |
|    ent_coef_loss   | 4.92      |
|    learning_rate   | 8.37e-05  |
|    n_updates       | 5309798   |
----------------------------------
Eval num_timesteps=1640000, episode_reward=-122639.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1640000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.047     |
|    ent_coef        | 0.00131   |
|    ent_coef_loss   | -2.65     |
|    learning_rate   | 8.36e-05  |
|    n_updates       | 5319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 328       |
|    fps             | 15        |
|    time_elapsed    | 108579    |
|    total_timesteps | 1640000   |
----------------------------------
Eval num_timesteps=1650000, episode_reward=-122628.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1650000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0594    |
|    ent_coef        | 0.00144   |
|    ent_coef_loss   | 9.22      |
|    learning_rate   | 8.35e-05  |
|    n_updates       | 5329798   |
----------------------------------
Eval num_timesteps=1660000, episode_reward=-122653.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1660000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.00603   |
|    ent_coef        | 0.00128   |
|    ent_coef_loss   | -0.169    |
|    learning_rate   | 8.34e-05  |
|    n_updates       | 5339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 332       |
|    fps             | 15        |
|    time_elapsed    | 109885    |
|    total_timesteps | 1660000   |
----------------------------------
Eval num_timesteps=1670000, episode_reward=-122654.02 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1670000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0244    |
|    ent_coef        | 0.00102   |
|    ent_coef_loss   | 0.552     |
|    learning_rate   | 8.33e-05  |
|    n_updates       | 5349798   |
----------------------------------
Eval num_timesteps=1680000, episode_reward=-122639.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1680000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.238     |
|    ent_coef        | 0.00109   |
|    ent_coef_loss   | -4.43     |
|    learning_rate   | 8.32e-05  |
|    n_updates       | 5359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 336       |
|    fps             | 15        |
|    time_elapsed    | 111188    |
|    total_timesteps | 1680000   |
----------------------------------
Eval num_timesteps=1690000, episode_reward=-122615.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1690000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.133     |
|    ent_coef        | 0.00093   |
|    ent_coef_loss   | 3.04      |
|    learning_rate   | 8.31e-05  |
|    n_updates       | 5369798   |
----------------------------------
Eval num_timesteps=1700000, episode_reward=-122629.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1700000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.148     |
|    ent_coef        | 0.00111   |
|    ent_coef_loss   | -2.63     |
|    learning_rate   | 8.3e-05   |
|    n_updates       | 5379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 340       |
|    fps             | 15        |
|    time_elapsed    | 112488    |
|    total_timesteps | 1700000   |
----------------------------------
Eval num_timesteps=1710000, episode_reward=-122654.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1710000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.119     |
|    ent_coef        | 0.00105   |
|    ent_coef_loss   | -6.09     |
|    learning_rate   | 8.29e-05  |
|    n_updates       | 5389798   |
----------------------------------
Eval num_timesteps=1720000, episode_reward=-122626.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1720000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.00765   |
|    ent_coef        | 0.00122   |
|    ent_coef_loss   | 8.31      |
|    learning_rate   | 8.28e-05  |
|    n_updates       | 5399798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 344       |
|    fps             | 15        |
|    time_elapsed    | 113798    |
|    total_timesteps | 1720000   |
----------------------------------
Eval num_timesteps=1730000, episode_reward=-122647.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1730000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.511     |
|    ent_coef        | 0.00118   |
|    ent_coef_loss   | -11.6     |
|    learning_rate   | 8.27e-05  |
|    n_updates       | 5409798   |
----------------------------------
Eval num_timesteps=1740000, episode_reward=-122640.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1740000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.287     |
|    ent_coef        | 0.00121   |
|    ent_coef_loss   | 10.1      |
|    learning_rate   | 8.26e-05  |
|    n_updates       | 5419798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 348       |
|    fps             | 15        |
|    time_elapsed    | 115106    |
|    total_timesteps | 1740000   |
----------------------------------
Eval num_timesteps=1750000, episode_reward=-122640.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1750000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.00869   |
|    ent_coef        | 0.00161   |
|    ent_coef_loss   | 4.5       |
|    learning_rate   | 8.25e-05  |
|    n_updates       | 5429798   |
----------------------------------
Eval num_timesteps=1760000, episode_reward=-122628.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1760000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.256     |
|    ent_coef        | 0.00175   |
|    ent_coef_loss   | 11.7      |
|    learning_rate   | 8.24e-05  |
|    n_updates       | 5439798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 352       |
|    fps             | 15        |
|    time_elapsed    | 116422    |
|    total_timesteps | 1760000   |
----------------------------------
Eval num_timesteps=1770000, episode_reward=-122661.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1770000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0303    |
|    ent_coef        | 0.00164   |
|    ent_coef_loss   | 1.56      |
|    learning_rate   | 8.23e-05  |
|    n_updates       | 5449798   |
----------------------------------
Eval num_timesteps=1780000, episode_reward=-122740.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1780000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0553    |
|    ent_coef        | 0.00199   |
|    ent_coef_loss   | 3.94      |
|    learning_rate   | 8.22e-05  |
|    n_updates       | 5459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 356       |
|    fps             | 15        |
|    time_elapsed    | 117775    |
|    total_timesteps | 1780000   |
----------------------------------
Eval num_timesteps=1790000, episode_reward=-122756.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1790000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.112     |
|    ent_coef        | 0.00432   |
|    ent_coef_loss   | 6.49      |
|    learning_rate   | 8.21e-05  |
|    n_updates       | 5469798   |
----------------------------------
Eval num_timesteps=1800000, episode_reward=-122640.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1800000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.321     |
|    ent_coef        | 0.00898   |
|    ent_coef_loss   | 2.3       |
|    learning_rate   | 8.2e-05   |
|    n_updates       | 5479798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 360       |
|    fps             | 15        |
|    time_elapsed    | 119093    |
|    total_timesteps | 1800000   |
----------------------------------
Eval num_timesteps=1810000, episode_reward=-122612.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1810000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.18      |
|    ent_coef        | 0.00956   |
|    ent_coef_loss   | -1.76     |
|    learning_rate   | 8.19e-05  |
|    n_updates       | 5489798   |
----------------------------------
Eval num_timesteps=1820000, episode_reward=-122594.35 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1820000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.137     |
|    ent_coef        | 0.00852   |
|    ent_coef_loss   | 1.27      |
|    learning_rate   | 8.18e-05  |
|    n_updates       | 5499798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 364       |
|    fps             | 15        |
|    time_elapsed    | 120402    |
|    total_timesteps | 1820000   |
----------------------------------
Eval num_timesteps=1830000, episode_reward=-122638.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1830000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.115     |
|    ent_coef        | 0.0104    |
|    ent_coef_loss   | 2.05      |
|    learning_rate   | 8.17e-05  |
|    n_updates       | 5509798   |
----------------------------------
Eval num_timesteps=1840000, episode_reward=-122647.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1840000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.0574    |
|    ent_coef        | 0.0118    |
|    ent_coef_loss   | 2.52      |
|    learning_rate   | 8.16e-05  |
|    n_updates       | 5519798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 368       |
|    fps             | 15        |
|    time_elapsed    | 121715    |
|    total_timesteps | 1840000   |
----------------------------------
Eval num_timesteps=1850000, episode_reward=-122642.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1850000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.0757    |
|    ent_coef        | 0.0204    |
|    ent_coef_loss   | 1.22      |
|    learning_rate   | 8.15e-05  |
|    n_updates       | 5529798   |
----------------------------------
Eval num_timesteps=1860000, episode_reward=-122636.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1860000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.0532    |
|    ent_coef        | 0.0165    |
|    ent_coef_loss   | -1.96     |
|    learning_rate   | 8.14e-05  |
|    n_updates       | 5539798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 372       |
|    fps             | 15        |
|    time_elapsed    | 123021    |
|    total_timesteps | 1860000   |
----------------------------------
Eval num_timesteps=1870000, episode_reward=-122630.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1870000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.0644    |
|    ent_coef        | 0.01      |
|    ent_coef_loss   | -0.0564   |
|    learning_rate   | 8.13e-05  |
|    n_updates       | 5549798   |
----------------------------------
Eval num_timesteps=1880000, episode_reward=-122617.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1880000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.0376    |
|    ent_coef        | 0.00482   |
|    ent_coef_loss   | -3.14     |
|    learning_rate   | 8.12e-05  |
|    n_updates       | 5559798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 376       |
|    fps             | 15        |
|    time_elapsed    | 124327    |
|    total_timesteps | 1880000   |
----------------------------------
Eval num_timesteps=1890000, episode_reward=-122607.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1890000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0883    |
|    ent_coef        | 0.00231   |
|    ent_coef_loss   | -9.21     |
|    learning_rate   | 8.11e-05  |
|    n_updates       | 5569798   |
----------------------------------
Eval num_timesteps=1900000, episode_reward=-122608.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1900000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0359    |
|    ent_coef        | 0.00123   |
|    ent_coef_loss   | -1.56     |
|    learning_rate   | 8.1e-05   |
|    n_updates       | 5579798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 380       |
|    fps             | 15        |
|    time_elapsed    | 125641    |
|    total_timesteps | 1900000   |
----------------------------------
Eval num_timesteps=1910000, episode_reward=-122587.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1910000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0214    |
|    ent_coef        | 0.00114   |
|    ent_coef_loss   | -2.47     |
|    learning_rate   | 8.09e-05  |
|    n_updates       | 5589798   |
----------------------------------
Eval num_timesteps=1920000, episode_reward=-122542.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1920000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.363     |
|    ent_coef        | 0.00141   |
|    ent_coef_loss   | 16.2      |
|    learning_rate   | 8.08e-05  |
|    n_updates       | 5599798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 384       |
|    fps             | 15        |
|    time_elapsed    | 126958    |
|    total_timesteps | 1920000   |
----------------------------------
Eval num_timesteps=1930000, episode_reward=-122582.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1930000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.24      |
|    ent_coef        | 0.00185   |
|    ent_coef_loss   | -6.64     |
|    learning_rate   | 8.07e-05  |
|    n_updates       | 5609798   |
----------------------------------
Eval num_timesteps=1940000, episode_reward=-122589.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1940000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0641    |
|    ent_coef        | 0.00153   |
|    ent_coef_loss   | 0.294     |
|    learning_rate   | 8.06e-05  |
|    n_updates       | 5619798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 388       |
|    fps             | 15        |
|    time_elapsed    | 128278    |
|    total_timesteps | 1940000   |
----------------------------------
Eval num_timesteps=1950000, episode_reward=-122590.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1950000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.21      |
|    ent_coef        | 0.00152   |
|    ent_coef_loss   | -3.36     |
|    learning_rate   | 8.05e-05  |
|    n_updates       | 5629798   |
----------------------------------
Eval num_timesteps=1960000, episode_reward=-122602.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1960000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0907    |
|    ent_coef        | 0.00207   |
|    ent_coef_loss   | -1.27     |
|    learning_rate   | 8.04e-05  |
|    n_updates       | 5639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 392       |
|    fps             | 15        |
|    time_elapsed    | 129598    |
|    total_timesteps | 1960000   |
----------------------------------
Eval num_timesteps=1970000, episode_reward=-122602.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1970000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.104     |
|    ent_coef        | 0.00196   |
|    ent_coef_loss   | 1.43      |
|    learning_rate   | 8.03e-05  |
|    n_updates       | 5649798   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=-122601.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1980000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.197     |
|    ent_coef        | 0.00223   |
|    ent_coef_loss   | -6.03     |
|    learning_rate   | 8.02e-05  |
|    n_updates       | 5659798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 396       |
|    fps             | 15        |
|    time_elapsed    | 130919    |
|    total_timesteps | 1980000   |
----------------------------------
Eval num_timesteps=1990000, episode_reward=-122557.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 1990000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0363    |
|    ent_coef        | 0.00277   |
|    ent_coef_loss   | -2.41     |
|    learning_rate   | 8.01e-05  |
|    n_updates       | 5669798   |
----------------------------------
Eval num_timesteps=2000000, episode_reward=-122551.72 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2000000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.096     |
|    ent_coef        | 0.00342   |
|    ent_coef_loss   | 17.2      |
|    learning_rate   | 8e-05     |
|    n_updates       | 5679798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 400       |
|    fps             | 15        |
|    time_elapsed    | 132232    |
|    total_timesteps | 2000000   |
----------------------------------
Eval num_timesteps=2010000, episode_reward=-122573.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2010000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.413     |
|    ent_coef        | 0.00732   |
|    ent_coef_loss   | 9.24      |
|    learning_rate   | 7.99e-05  |
|    n_updates       | 5689798   |
----------------------------------
Eval num_timesteps=2020000, episode_reward=-122618.56 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2020000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.303     |
|    ent_coef        | 0.00733   |
|    ent_coef_loss   | -1.07     |
|    learning_rate   | 7.98e-05  |
|    n_updates       | 5699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 404       |
|    fps             | 15        |
|    time_elapsed    | 133549    |
|    total_timesteps | 2020000   |
----------------------------------
Eval num_timesteps=2030000, episode_reward=-122593.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2030000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.245     |
|    ent_coef        | 0.00425   |
|    ent_coef_loss   | -2.63     |
|    learning_rate   | 7.97e-05  |
|    n_updates       | 5709798   |
----------------------------------
Eval num_timesteps=2040000, episode_reward=-122597.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2040000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0173    |
|    ent_coef        | 0.00296   |
|    ent_coef_loss   | -6.25     |
|    learning_rate   | 7.96e-05  |
|    n_updates       | 5719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 408       |
|    fps             | 15        |
|    time_elapsed    | 134859    |
|    total_timesteps | 2040000   |
----------------------------------
Eval num_timesteps=2050000, episode_reward=-122586.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2050000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0446    |
|    ent_coef        | 0.00206   |
|    ent_coef_loss   | 11.9      |
|    learning_rate   | 7.95e-05  |
|    n_updates       | 5729798   |
----------------------------------
Eval num_timesteps=2060000, episode_reward=-122555.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2060000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.258     |
|    ent_coef        | 0.00173   |
|    ent_coef_loss   | -3.61     |
|    learning_rate   | 7.94e-05  |
|    n_updates       | 5739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 412       |
|    fps             | 15        |
|    time_elapsed    | 136162    |
|    total_timesteps | 2060000   |
----------------------------------
Eval num_timesteps=2070000, episode_reward=-122514.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2070000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0685    |
|    ent_coef        | 0.00166   |
|    ent_coef_loss   | -1.35     |
|    learning_rate   | 7.93e-05  |
|    n_updates       | 5749798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2080000, episode_reward=-122540.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2080000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.268     |
|    ent_coef        | 0.00172   |
|    ent_coef_loss   | 0.842     |
|    learning_rate   | 7.92e-05  |
|    n_updates       | 5759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 416       |
|    fps             | 15        |
|    time_elapsed    | 137389    |
|    total_timesteps | 2080000   |
----------------------------------
Eval num_timesteps=2090000, episode_reward=-122545.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2090000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0922    |
|    ent_coef        | 0.0017    |
|    ent_coef_loss   | -5.33     |
|    learning_rate   | 7.91e-05  |
|    n_updates       | 5769798   |
----------------------------------
Eval num_timesteps=2100000, episode_reward=-122446.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2100000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0669    |
|    ent_coef        | 0.00241   |
|    ent_coef_loss   | 1.38      |
|    learning_rate   | 7.9e-05   |
|    n_updates       | 5779798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 420       |
|    fps             | 15        |
|    time_elapsed    | 138622    |
|    total_timesteps | 2100000   |
----------------------------------
Eval num_timesteps=2110000, episode_reward=-122466.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2110000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0868    |
|    ent_coef        | 0.00272   |
|    ent_coef_loss   | -12.6     |
|    learning_rate   | 7.89e-05  |
|    n_updates       | 5789798   |
----------------------------------
Eval num_timesteps=2120000, episode_reward=-122476.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2120000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0351    |
|    ent_coef        | 0.00171   |
|    ent_coef_loss   | 2.72      |
|    learning_rate   | 7.88e-05  |
|    n_updates       | 5799798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 424       |
|    fps             | 15        |
|    time_elapsed    | 139850    |
|    total_timesteps | 2120000   |
----------------------------------
Eval num_timesteps=2130000, episode_reward=-122553.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2130000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0328    |
|    ent_coef        | 0.00134   |
|    ent_coef_loss   | -10.3     |
|    learning_rate   | 7.87e-05  |
|    n_updates       | 5809798   |
----------------------------------
Eval num_timesteps=2140000, episode_reward=-122474.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2140000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0164    |
|    ent_coef        | 0.000928  |
|    ent_coef_loss   | 3.18      |
|    learning_rate   | 7.86e-05  |
|    n_updates       | 5819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 428       |
|    fps             | 15        |
|    time_elapsed    | 141077    |
|    total_timesteps | 2140000   |
----------------------------------
Eval num_timesteps=2150000, episode_reward=-122526.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2150000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.113     |
|    ent_coef        | 0.000709  |
|    ent_coef_loss   | -12.5     |
|    learning_rate   | 7.85e-05  |
|    n_updates       | 5829798   |
----------------------------------
Eval num_timesteps=2160000, episode_reward=-122558.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2160000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.263     |
|    ent_coef        | 0.000609  |
|    ent_coef_loss   | -9.58     |
|    learning_rate   | 7.84e-05  |
|    n_updates       | 5839798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 432       |
|    fps             | 15        |
|    time_elapsed    | 142305    |
|    total_timesteps | 2160000   |
----------------------------------
Eval num_timesteps=2170000, episode_reward=-122476.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2170000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.472     |
|    ent_coef        | 0.000528  |
|    ent_coef_loss   | -6.94     |
|    learning_rate   | 7.83e-05  |
|    n_updates       | 5849798   |
----------------------------------
Eval num_timesteps=2180000, episode_reward=-122557.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2180000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0459    |
|    ent_coef        | 0.00111   |
|    ent_coef_loss   | 15.1      |
|    learning_rate   | 7.82e-05  |
|    n_updates       | 5859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 436       |
|    fps             | 15        |
|    time_elapsed    | 143541    |
|    total_timesteps | 2180000   |
----------------------------------
Eval num_timesteps=2190000, episode_reward=-122572.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2190000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0779    |
|    ent_coef        | 0.00228   |
|    ent_coef_loss   | 5.97      |
|    learning_rate   | 7.81e-05  |
|    n_updates       | 5869798   |
----------------------------------
Eval num_timesteps=2200000, episode_reward=-122443.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2200000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.486     |
|    ent_coef        | 0.00258   |
|    ent_coef_loss   | -1.83     |
|    learning_rate   | 7.8e-05   |
|    n_updates       | 5879798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 440       |
|    fps             | 15        |
|    time_elapsed    | 144784    |
|    total_timesteps | 2200000   |
----------------------------------
Eval num_timesteps=2210000, episode_reward=-122594.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2210000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0967    |
|    ent_coef        | 0.00224   |
|    ent_coef_loss   | 4.5       |
|    learning_rate   | 7.79e-05  |
|    n_updates       | 5889798   |
----------------------------------
Eval num_timesteps=2220000, episode_reward=-122424.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2220000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0611    |
|    ent_coef        | 0.00298   |
|    ent_coef_loss   | -1.34     |
|    learning_rate   | 7.78e-05  |
|    n_updates       | 5899798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 444       |
|    fps             | 15        |
|    time_elapsed    | 146034    |
|    total_timesteps | 2220000   |
----------------------------------
Eval num_timesteps=2230000, episode_reward=-122450.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2230000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.246     |
|    ent_coef        | 0.0044    |
|    ent_coef_loss   | 2.98      |
|    learning_rate   | 7.77e-05  |
|    n_updates       | 5909798   |
----------------------------------
Eval num_timesteps=2240000, episode_reward=-122541.38 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2240000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.367     |
|    ent_coef        | 0.00758   |
|    ent_coef_loss   | 3.56      |
|    learning_rate   | 7.76e-05  |
|    n_updates       | 5919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 448       |
|    fps             | 15        |
|    time_elapsed    | 147273    |
|    total_timesteps | 2240000   |
----------------------------------
Eval num_timesteps=2250000, episode_reward=-122423.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2250000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0172    |
|    ent_coef        | 0.00611   |
|    ent_coef_loss   | -1.66     |
|    learning_rate   | 7.75e-05  |
|    n_updates       | 5929798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2260000, episode_reward=-122350.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2260000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.109     |
|    ent_coef        | 0.00408   |
|    ent_coef_loss   | -6.46     |
|    learning_rate   | 7.74e-05  |
|    n_updates       | 5939798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 452       |
|    fps             | 15        |
|    time_elapsed    | 148504    |
|    total_timesteps | 2260000   |
----------------------------------
Eval num_timesteps=2270000, episode_reward=-122246.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2270000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.59      |
|    ent_coef        | 0.00411   |
|    ent_coef_loss   | 4.87      |
|    learning_rate   | 7.73e-05  |
|    n_updates       | 5949798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2280000, episode_reward=-122412.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2280000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0214    |
|    ent_coef        | 0.0046    |
|    ent_coef_loss   | 7.93      |
|    learning_rate   | 7.72e-05  |
|    n_updates       | 5959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 456       |
|    fps             | 15        |
|    time_elapsed    | 149725    |
|    total_timesteps | 2280000   |
----------------------------------
Eval num_timesteps=2290000, episode_reward=-122257.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2290000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.067     |
|    ent_coef        | 0.00605   |
|    ent_coef_loss   | -5.55     |
|    learning_rate   | 7.71e-05  |
|    n_updates       | 5969798   |
----------------------------------
Eval num_timesteps=2300000, episode_reward=-122452.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2300000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0239    |
|    ent_coef        | 0.00501   |
|    ent_coef_loss   | -2.95     |
|    learning_rate   | 7.7e-05   |
|    n_updates       | 5979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 460       |
|    fps             | 15        |
|    time_elapsed    | 150945    |
|    total_timesteps | 2300000   |
----------------------------------
Eval num_timesteps=2310000, episode_reward=-122530.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2310000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.383     |
|    ent_coef        | 0.00674   |
|    ent_coef_loss   | -1.09     |
|    learning_rate   | 7.69e-05  |
|    n_updates       | 5989798   |
----------------------------------
Eval num_timesteps=2320000, episode_reward=-122419.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2320000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0269    |
|    ent_coef        | 0.00673   |
|    ent_coef_loss   | 10.4      |
|    learning_rate   | 7.68e-05  |
|    n_updates       | 5999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 464       |
|    fps             | 15        |
|    time_elapsed    | 152166    |
|    total_timesteps | 2320000   |
----------------------------------
Eval num_timesteps=2330000, episode_reward=-122322.73 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2330000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.191     |
|    ent_coef        | 0.00589   |
|    ent_coef_loss   | -5.22     |
|    learning_rate   | 7.67e-05  |
|    n_updates       | 6009798   |
----------------------------------
Eval num_timesteps=2340000, episode_reward=-122316.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2340000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.18      |
|    ent_coef        | 0.00737   |
|    ent_coef_loss   | 2.48      |
|    learning_rate   | 7.66e-05  |
|    n_updates       | 6019798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 468       |
|    fps             | 15        |
|    time_elapsed    | 153394    |
|    total_timesteps | 2340000   |
----------------------------------
Eval num_timesteps=2350000, episode_reward=-122305.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2350000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.125     |
|    ent_coef        | 0.0146    |
|    ent_coef_loss   | 5.32      |
|    learning_rate   | 7.65e-05  |
|    n_updates       | 6029798   |
----------------------------------
Eval num_timesteps=2360000, episode_reward=-122376.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2360000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.117     |
|    ent_coef        | 0.00966   |
|    ent_coef_loss   | -1.21     |
|    learning_rate   | 7.64e-05  |
|    n_updates       | 6039798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 472       |
|    fps             | 15        |
|    time_elapsed    | 154625    |
|    total_timesteps | 2360000   |
----------------------------------
Eval num_timesteps=2370000, episode_reward=-122355.81 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2370000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.491     |
|    ent_coef        | 0.0104    |
|    ent_coef_loss   | -2.09     |
|    learning_rate   | 7.63e-05  |
|    n_updates       | 6049798   |
----------------------------------
Eval num_timesteps=2380000, episode_reward=-122362.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2380000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0958    |
|    ent_coef        | 0.0119    |
|    ent_coef_loss   | 0.72      |
|    learning_rate   | 7.62e-05  |
|    n_updates       | 6059798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 476       |
|    fps             | 15        |
|    time_elapsed    | 155857    |
|    total_timesteps | 2380000   |
----------------------------------
Eval num_timesteps=2390000, episode_reward=-122370.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2390000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.182     |
|    ent_coef        | 0.0109    |
|    ent_coef_loss   | -1.53     |
|    learning_rate   | 7.61e-05  |
|    n_updates       | 6069798   |
----------------------------------
Eval num_timesteps=2400000, episode_reward=-122366.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2400000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.2       |
|    ent_coef        | 0.0109    |
|    ent_coef_loss   | 1.34      |
|    learning_rate   | 7.6e-05   |
|    n_updates       | 6079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    episodes        | 480       |
|    fps             | 15        |
|    time_elapsed    | 157094    |
|    total_timesteps | 2400000   |
----------------------------------
Eval num_timesteps=2410000, episode_reward=-122370.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2410000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0896    |
|    ent_coef        | 0.00994   |
|    ent_coef_loss   | -1.9      |
|    learning_rate   | 7.59e-05  |
|    n_updates       | 6089798   |
----------------------------------
Eval num_timesteps=2420000, episode_reward=-122418.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2420000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.368     |
|    ent_coef        | 0.0101    |
|    ent_coef_loss   | 0.735     |
|    learning_rate   | 7.58e-05  |
|    n_updates       | 6099798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 484       |
|    fps             | 15        |
|    time_elapsed    | 158332    |
|    total_timesteps | 2420000   |
----------------------------------
Eval num_timesteps=2430000, episode_reward=-122468.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2430000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.115     |
|    ent_coef        | 0.00773   |
|    ent_coef_loss   | -2.28     |
|    learning_rate   | 7.57e-05  |
|    n_updates       | 6109798   |
----------------------------------
Eval num_timesteps=2440000, episode_reward=-122329.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2440000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.237     |
|    ent_coef        | 0.00618   |
|    ent_coef_loss   | -2.36     |
|    learning_rate   | 7.56e-05  |
|    n_updates       | 6119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 488       |
|    fps             | 15        |
|    time_elapsed    | 159568    |
|    total_timesteps | 2440000   |
----------------------------------
Eval num_timesteps=2450000, episode_reward=-122311.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2450000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0669    |
|    ent_coef        | 0.00503   |
|    ent_coef_loss   | -1.09     |
|    learning_rate   | 7.55e-05  |
|    n_updates       | 6129798   |
----------------------------------
Eval num_timesteps=2460000, episode_reward=-122430.28 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2460000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.03      |
|    ent_coef        | 0.00801   |
|    ent_coef_loss   | -3.63     |
|    learning_rate   | 7.54e-05  |
|    n_updates       | 6139798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 492       |
|    fps             | 15        |
|    time_elapsed    | 160797    |
|    total_timesteps | 2460000   |
----------------------------------
Eval num_timesteps=2470000, episode_reward=-122448.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2470000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.252     |
|    ent_coef        | 0.00529   |
|    ent_coef_loss   | -3.48     |
|    learning_rate   | 7.53e-05  |
|    n_updates       | 6149798   |
----------------------------------
Eval num_timesteps=2480000, episode_reward=-122434.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2480000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.305     |
|    ent_coef        | 0.00411   |
|    ent_coef_loss   | 0.267     |
|    learning_rate   | 7.52e-05  |
|    n_updates       | 6159798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 496       |
|    fps             | 15        |
|    time_elapsed    | 162015    |
|    total_timesteps | 2480000   |
----------------------------------
Eval num_timesteps=2490000, episode_reward=-122422.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2490000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.089     |
|    ent_coef        | 0.00555   |
|    ent_coef_loss   | -0.989    |
|    learning_rate   | 7.51e-05  |
|    n_updates       | 6169798   |
----------------------------------
Eval num_timesteps=2500000, episode_reward=-122216.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2500000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.454     |
|    ent_coef        | 0.00588   |
|    ent_coef_loss   | -0.571    |
|    learning_rate   | 7.5e-05   |
|    n_updates       | 6179798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 500       |
|    fps             | 15        |
|    time_elapsed    | 163228    |
|    total_timesteps | 2500000   |
----------------------------------
Eval num_timesteps=2510000, episode_reward=-122584.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.23e+05 |
| time/              |           |
|    total_timesteps | 2510000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.28      |
|    ent_coef        | 0.0061    |
|    ent_coef_loss   | 1.72      |
|    learning_rate   | 7.49e-05  |
|    n_updates       | 6189798   |
----------------------------------
Eval num_timesteps=2520000, episode_reward=-122307.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2520000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.106     |
|    ent_coef        | 0.00755   |
|    ent_coef_loss   | 3.54      |
|    learning_rate   | 7.48e-05  |
|    n_updates       | 6199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 504       |
|    fps             | 15        |
|    time_elapsed    | 164442    |
|    total_timesteps | 2520000   |
----------------------------------
Eval num_timesteps=2530000, episode_reward=-122380.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2530000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0714    |
|    ent_coef        | 0.00741   |
|    ent_coef_loss   | -1.51     |
|    learning_rate   | 7.47e-05  |
|    n_updates       | 6209798   |
----------------------------------
Eval num_timesteps=2540000, episode_reward=-122397.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2540000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0618    |
|    ent_coef        | 0.00683   |
|    ent_coef_loss   | 0.229     |
|    learning_rate   | 7.46e-05  |
|    n_updates       | 6219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 508       |
|    fps             | 15        |
|    time_elapsed    | 165720    |
|    total_timesteps | 2540000   |
----------------------------------
Eval num_timesteps=2550000, episode_reward=-122424.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2550000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.3       |
|    ent_coef        | 0.00684   |
|    ent_coef_loss   | 2.16      |
|    learning_rate   | 7.45e-05  |
|    n_updates       | 6229798   |
----------------------------------
Eval num_timesteps=2560000, episode_reward=-122401.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2560000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.252     |
|    ent_coef        | 0.00615   |
|    ent_coef_loss   | 0.83      |
|    learning_rate   | 7.44e-05  |
|    n_updates       | 6239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 512       |
|    fps             | 15        |
|    time_elapsed    | 167019    |
|    total_timesteps | 2560000   |
----------------------------------
Eval num_timesteps=2570000, episode_reward=-122398.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2570000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.924     |
|    ent_coef        | 0.00842   |
|    ent_coef_loss   | -1.26     |
|    learning_rate   | 7.43e-05  |
|    n_updates       | 6249798   |
----------------------------------
Eval num_timesteps=2580000, episode_reward=-122298.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2580000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0445    |
|    ent_coef        | 0.00897   |
|    ent_coef_loss   | 0.469     |
|    learning_rate   | 7.42e-05  |
|    n_updates       | 6259798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 516       |
|    fps             | 15        |
|    time_elapsed    | 168518    |
|    total_timesteps | 2580000   |
----------------------------------
Eval num_timesteps=2590000, episode_reward=-122359.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2590000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0927    |
|    ent_coef        | 0.0103    |
|    ent_coef_loss   | 0.409     |
|    learning_rate   | 7.41e-05  |
|    n_updates       | 6269798   |
----------------------------------
Eval num_timesteps=2600000, episode_reward=-122452.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2600000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.208     |
|    ent_coef        | 0.01      |
|    ent_coef_loss   | 2.3       |
|    learning_rate   | 7.4e-05   |
|    n_updates       | 6279798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 520       |
|    fps             | 15        |
|    time_elapsed    | 169805    |
|    total_timesteps | 2600000   |
----------------------------------
Eval num_timesteps=2610000, episode_reward=-122411.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2610000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0984    |
|    ent_coef        | 0.0142    |
|    ent_coef_loss   | 0.872     |
|    learning_rate   | 7.39e-05  |
|    n_updates       | 6289798   |
----------------------------------
Eval num_timesteps=2620000, episode_reward=-122424.64 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2620000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.802     |
|    ent_coef        | 0.0122    |
|    ent_coef_loss   | -2.46     |
|    learning_rate   | 7.38e-05  |
|    n_updates       | 6299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 524       |
|    fps             | 15        |
|    time_elapsed    | 171050    |
|    total_timesteps | 2620000   |
----------------------------------
Eval num_timesteps=2630000, episode_reward=-122376.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2630000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.94      |
|    ent_coef        | 0.00996   |
|    ent_coef_loss   | -1.13     |
|    learning_rate   | 7.37e-05  |
|    n_updates       | 6309798   |
----------------------------------
Eval num_timesteps=2640000, episode_reward=-122343.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2640000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 0.378     |
|    ent_coef        | 0.0131    |
|    ent_coef_loss   | 0.224     |
|    learning_rate   | 7.36e-05  |
|    n_updates       | 6319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 528       |
|    fps             | 15        |
|    time_elapsed    | 172290    |
|    total_timesteps | 2640000   |
----------------------------------
Eval num_timesteps=2650000, episode_reward=-122369.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2650000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.462     |
|    ent_coef        | 0.0211    |
|    ent_coef_loss   | 0.712     |
|    learning_rate   | 7.35e-05  |
|    n_updates       | 6329798   |
----------------------------------
Eval num_timesteps=2660000, episode_reward=-122339.74 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2660000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.33      |
|    ent_coef        | 0.0267    |
|    ent_coef_loss   | 3.9       |
|    learning_rate   | 7.34e-05  |
|    n_updates       | 6339798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 532       |
|    fps             | 15        |
|    time_elapsed    | 173538    |
|    total_timesteps | 2660000   |
----------------------------------
Eval num_timesteps=2670000, episode_reward=-122336.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2670000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.14      |
|    ent_coef        | 0.0207    |
|    ent_coef_loss   | -2.48     |
|    learning_rate   | 7.33e-05  |
|    n_updates       | 6349798   |
----------------------------------
Eval num_timesteps=2680000, episode_reward=-122347.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2680000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.443     |
|    ent_coef        | 0.0185    |
|    ent_coef_loss   | 1.89      |
|    learning_rate   | 7.32e-05  |
|    n_updates       | 6359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 536       |
|    fps             | 15        |
|    time_elapsed    | 174795    |
|    total_timesteps | 2680000   |
----------------------------------
Eval num_timesteps=2690000, episode_reward=-122320.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2690000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.37      |
|    ent_coef        | 0.0196    |
|    ent_coef_loss   | -2.27     |
|    learning_rate   | 7.31e-05  |
|    n_updates       | 6369798   |
----------------------------------
Eval num_timesteps=2700000, episode_reward=-122318.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2700000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.222     |
|    ent_coef        | 0.0152    |
|    ent_coef_loss   | -2.61     |
|    learning_rate   | 7.3e-05   |
|    n_updates       | 6379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 540       |
|    fps             | 15        |
|    time_elapsed    | 176051    |
|    total_timesteps | 2700000   |
----------------------------------
Eval num_timesteps=2710000, episode_reward=-122321.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2710000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.135     |
|    ent_coef        | 0.0151    |
|    ent_coef_loss   | 11.6      |
|    learning_rate   | 7.29e-05  |
|    n_updates       | 6389798   |
----------------------------------
Eval num_timesteps=2720000, episode_reward=-122305.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2720000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.0602    |
|    ent_coef        | 0.0112    |
|    ent_coef_loss   | -2.25     |
|    learning_rate   | 7.28e-05  |
|    n_updates       | 6399798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 544       |
|    fps             | 15        |
|    time_elapsed    | 177318    |
|    total_timesteps | 2720000   |
----------------------------------
Eval num_timesteps=2730000, episode_reward=-122290.13 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2730000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.935     |
|    ent_coef        | 0.0102    |
|    ent_coef_loss   | -1.43     |
|    learning_rate   | 7.27e-05  |
|    n_updates       | 6409798   |
----------------------------------
Eval num_timesteps=2740000, episode_reward=-122273.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2740000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.638     |
|    ent_coef        | 0.0141    |
|    ent_coef_loss   | 1.61      |
|    learning_rate   | 7.26e-05  |
|    n_updates       | 6419798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 548       |
|    fps             | 15        |
|    time_elapsed    | 178580    |
|    total_timesteps | 2740000   |
----------------------------------
Eval num_timesteps=2750000, episode_reward=-122287.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2750000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.206     |
|    ent_coef        | 0.0153    |
|    ent_coef_loss   | -1.14     |
|    learning_rate   | 7.25e-05  |
|    n_updates       | 6429798   |
----------------------------------
Eval num_timesteps=2760000, episode_reward=-122302.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2760000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 2.68      |
|    ent_coef        | 0.013     |
|    ent_coef_loss   | -0.291    |
|    learning_rate   | 7.24e-05  |
|    n_updates       | 6439798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 552       |
|    fps             | 15        |
|    time_elapsed    | 179822    |
|    total_timesteps | 2760000   |
----------------------------------
Eval num_timesteps=2770000, episode_reward=-122259.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2770000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 1.11      |
|    ent_coef        | 0.0136    |
|    ent_coef_loss   | -0.0271   |
|    learning_rate   | 7.23e-05  |
|    n_updates       | 6449798   |
----------------------------------
Eval num_timesteps=2780000, episode_reward=-122244.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2780000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 6.13      |
|    ent_coef        | 0.0182    |
|    ent_coef_loss   | -0.246    |
|    learning_rate   | 7.22e-05  |
|    n_updates       | 6459798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 556       |
|    fps             | 15        |
|    time_elapsed    | 181065    |
|    total_timesteps | 2780000   |
----------------------------------
Eval num_timesteps=2790000, episode_reward=-122242.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2790000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.326     |
|    ent_coef        | 0.016     |
|    ent_coef_loss   | 0.983     |
|    learning_rate   | 7.21e-05  |
|    n_updates       | 6469798   |
----------------------------------
Eval num_timesteps=2800000, episode_reward=-122261.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2800000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.14      |
|    ent_coef        | 0.0129    |
|    ent_coef_loss   | -0.388    |
|    learning_rate   | 7.2e-05   |
|    n_updates       | 6479798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 560       |
|    fps             | 15        |
|    time_elapsed    | 182305    |
|    total_timesteps | 2800000   |
----------------------------------
Eval num_timesteps=2810000, episode_reward=-122216.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2810000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.97      |
|    ent_coef        | 0.0114    |
|    ent_coef_loss   | -1.82     |
|    learning_rate   | 7.19e-05  |
|    n_updates       | 6489798   |
----------------------------------
Eval num_timesteps=2820000, episode_reward=-122203.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2820000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 4.35      |
|    ent_coef        | 0.00943   |
|    ent_coef_loss   | 0.893     |
|    learning_rate   | 7.18e-05  |
|    n_updates       | 6499798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 564       |
|    fps             | 15        |
|    time_elapsed    | 183576    |
|    total_timesteps | 2820000   |
----------------------------------
Eval num_timesteps=2830000, episode_reward=-122173.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2830000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.132     |
|    ent_coef        | 0.012     |
|    ent_coef_loss   | 2.11      |
|    learning_rate   | 7.17e-05  |
|    n_updates       | 6509798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2840000, episode_reward=-122176.47 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2840000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.403     |
|    ent_coef        | 0.0182    |
|    ent_coef_loss   | 0.716     |
|    learning_rate   | 7.16e-05  |
|    n_updates       | 6519798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 568       |
|    fps             | 15        |
|    time_elapsed    | 184836    |
|    total_timesteps | 2840000   |
----------------------------------
Eval num_timesteps=2850000, episode_reward=-122145.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2850000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.352     |
|    ent_coef        | 0.0157    |
|    ent_coef_loss   | 0.397     |
|    learning_rate   | 7.15e-05  |
|    n_updates       | 6529798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2860000, episode_reward=-122069.98 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2860000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 1.8       |
|    ent_coef        | 0.0135    |
|    ent_coef_loss   | -2.62     |
|    learning_rate   | 7.14e-05  |
|    n_updates       | 6539798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 572       |
|    fps             | 15        |
|    time_elapsed    | 186089    |
|    total_timesteps | 2860000   |
----------------------------------
Eval num_timesteps=2870000, episode_reward=-121947.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2870000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.373     |
|    ent_coef        | 0.00883   |
|    ent_coef_loss   | 2.42      |
|    learning_rate   | 7.13e-05  |
|    n_updates       | 6549798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2880000, episode_reward=-121991.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 2880000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.796     |
|    ent_coef        | 0.0151    |
|    ent_coef_loss   | 0.304     |
|    learning_rate   | 7.12e-05  |
|    n_updates       | 6559798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 576       |
|    fps             | 15        |
|    time_elapsed    | 187308    |
|    total_timesteps | 2880000   |
----------------------------------
Eval num_timesteps=2890000, episode_reward=-119525.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+05 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | 2.46e+03 |
|    critic_loss     | 1.44     |
|    ent_coef        | 0.0188   |
|    ent_coef_loss   | -0.326   |
|    learning_rate   | 7.11e-05 |
|    n_updates       | 6569798  |
---------------------------------
New best mean reward!
Eval num_timesteps=2900000, episode_reward=-119516.50 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -1.2e+05 |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | 2.47e+03 |
|    critic_loss     | 0.284    |
|    ent_coef        | 0.0227   |
|    ent_coef_loss   | 0.419    |
|    learning_rate   | 7.1e-05  |
|    n_updates       | 6579798  |
---------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 580       |
|    fps             | 15        |
|    time_elapsed    | 188578    |
|    total_timesteps | 2900000   |
----------------------------------
Eval num_timesteps=2910000, episode_reward=-57525.21 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.75e+04 |
| time/              |           |
|    total_timesteps | 2910000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 0.284     |
|    ent_coef        | 0.0228    |
|    ent_coef_loss   | 0.706     |
|    learning_rate   | 7.09e-05  |
|    n_updates       | 6589798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2920000, episode_reward=-55114.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.51e+04 |
| time/              |           |
|    total_timesteps | 2920000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 4.11      |
|    ent_coef        | 0.0261    |
|    ent_coef_loss   | -0.149    |
|    learning_rate   | 7.08e-05  |
|    n_updates       | 6599798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    episodes        | 584       |
|    fps             | 15        |
|    time_elapsed    | 189830    |
|    total_timesteps | 2920000   |
----------------------------------
Eval num_timesteps=2930000, episode_reward=-49388.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.94e+04 |
| time/              |           |
|    total_timesteps | 2930000   |
| train/             |           |
|    actor_loss      | 2.48e+03  |
|    critic_loss     | 3.14e+04  |
|    ent_coef        | 0.0298    |
|    ent_coef_loss   | 0.132     |
|    learning_rate   | 7.07e-05  |
|    n_updates       | 6609798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2940000, episode_reward=-43417.27 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.34e+04 |
| time/              |           |
|    total_timesteps | 2940000   |
| train/             |           |
|    actor_loss      | 2.47e+03  |
|    critic_loss     | 115       |
|    ent_coef        | 0.044     |
|    ent_coef_loss   | 1.36      |
|    learning_rate   | 7.06e-05  |
|    n_updates       | 6619798   |
----------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.2e+05 |
| time/              |          |
|    episodes        | 588      |
|    fps             | 15       |
|    time_elapsed    | 191077   |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2950000, episode_reward=-119060.18 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 2950000   |
| train/             |           |
|    actor_loss      | 2.46e+03  |
|    critic_loss     | 29.4      |
|    ent_coef        | 0.0517    |
|    ent_coef_loss   | 1.8       |
|    learning_rate   | 7.05e-05  |
|    n_updates       | 6629798   |
----------------------------------
Eval num_timesteps=2960000, episode_reward=-119451.77 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 2960000   |
| train/             |           |
|    actor_loss      | 2.44e+03  |
|    critic_loss     | 28.1      |
|    ent_coef        | 0.0967    |
|    ent_coef_loss   | 2.22      |
|    learning_rate   | 7.04e-05  |
|    n_updates       | 6639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.19e+05 |
| time/              |           |
|    episodes        | 592       |
|    fps             | 15        |
|    time_elapsed    | 192298    |
|    total_timesteps | 2960000   |
----------------------------------
Eval num_timesteps=2970000, episode_reward=-43061.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.31e+04 |
| time/              |           |
|    total_timesteps | 2970000   |
| train/             |           |
|    actor_loss      | 2.45e+03  |
|    critic_loss     | 7.3       |
|    ent_coef        | 0.086     |
|    ent_coef_loss   | -0.608    |
|    learning_rate   | 7.03e-05  |
|    n_updates       | 6649798   |
----------------------------------
New best mean reward!
Eval num_timesteps=2980000, episode_reward=-35718.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.57e+04 |
| time/              |           |
|    total_timesteps | 2980000   |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 721       |
|    ent_coef        | 0.0788    |
|    ent_coef_loss   | 0.808     |
|    learning_rate   | 7.02e-05  |
|    n_updates       | 6659798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.17e+05 |
| time/              |           |
|    episodes        | 596       |
|    fps             | 15        |
|    time_elapsed    | 193517    |
|    total_timesteps | 2980000   |
----------------------------------
Eval num_timesteps=2990000, episode_reward=-119462.48 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 2990000   |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 23.9      |
|    ent_coef        | 0.115     |
|    ent_coef_loss   | -0.0191   |
|    learning_rate   | 7.01e-05  |
|    n_updates       | 6669798   |
----------------------------------
Eval num_timesteps=3000000, episode_reward=-116030.20 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.16e+05 |
| time/              |           |
|    total_timesteps | 3000000   |
| train/             |           |
|    actor_loss      | 2.41e+03  |
|    critic_loss     | 42.8      |
|    ent_coef        | 0.0934    |
|    ent_coef_loss   | 0.172     |
|    learning_rate   | 7e-05     |
|    n_updates       | 6679798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    episodes        | 600       |
|    fps             | 15        |
|    time_elapsed    | 194715    |
|    total_timesteps | 3000000   |
----------------------------------
Eval num_timesteps=3010000, episode_reward=-37991.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.8e+04 |
| time/              |          |
|    total_timesteps | 3010000  |
| train/             |          |
|    actor_loss      | 2.42e+03 |
|    critic_loss     | 56.3     |
|    ent_coef        | 0.0808   |
|    ent_coef_loss   | 0.767    |
|    learning_rate   | 6.99e-05 |
|    n_updates       | 6689798  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=-41921.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.19e+04 |
| time/              |           |
|    total_timesteps | 3020000   |
| train/             |           |
|    actor_loss      | 2.4e+03   |
|    critic_loss     | 28.5      |
|    ent_coef        | 0.102     |
|    ent_coef_loss   | 1.06      |
|    learning_rate   | 6.98e-05  |
|    n_updates       | 6699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    episodes        | 604       |
|    fps             | 15        |
|    time_elapsed    | 195917    |
|    total_timesteps | 3020000   |
----------------------------------
Eval num_timesteps=3030000, episode_reward=-115427.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.15e+05 |
| time/              |           |
|    total_timesteps | 3030000   |
| train/             |           |
|    actor_loss      | 2.42e+03  |
|    critic_loss     | 39.8      |
|    ent_coef        | 0.096     |
|    ent_coef_loss   | 0.408     |
|    learning_rate   | 6.97e-05  |
|    n_updates       | 6709798   |
----------------------------------
Eval num_timesteps=3040000, episode_reward=-119103.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 3040000   |
| train/             |           |
|    actor_loss      | 2.42e+03  |
|    critic_loss     | 32.8      |
|    ent_coef        | 0.0906    |
|    ent_coef_loss   | -0.142    |
|    learning_rate   | 6.96e-05  |
|    n_updates       | 6719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    episodes        | 608       |
|    fps             | 15        |
|    time_elapsed    | 197107    |
|    total_timesteps | 3040000   |
----------------------------------
Eval num_timesteps=3050000, episode_reward=-119231.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 3050000   |
| train/             |           |
|    actor_loss      | 2.42e+03  |
|    critic_loss     | 55.3      |
|    ent_coef        | 0.0647    |
|    ent_coef_loss   | -0.534    |
|    learning_rate   | 6.95e-05  |
|    n_updates       | 6729798   |
----------------------------------
Eval num_timesteps=3060000, episode_reward=-119257.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.19e+05 |
| time/              |           |
|    total_timesteps | 3060000   |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 27.2      |
|    ent_coef        | 0.0737    |
|    ent_coef_loss   | -0.321    |
|    learning_rate   | 6.94e-05  |
|    n_updates       | 6739798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    episodes        | 612       |
|    fps             | 15        |
|    time_elapsed    | 198298    |
|    total_timesteps | 3060000   |
----------------------------------
Eval num_timesteps=3070000, episode_reward=-37137.94 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.71e+04 |
| time/              |           |
|    total_timesteps | 3070000   |
| train/             |           |
|    actor_loss      | 2.43e+03  |
|    critic_loss     | 48.4      |
|    ent_coef        | 0.0835    |
|    ent_coef_loss   | 0.645     |
|    learning_rate   | 6.93e-05  |
|    n_updates       | 6749798   |
----------------------------------
Eval num_timesteps=3080000, episode_reward=-36546.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.65e+04 |
| time/              |           |
|    total_timesteps | 3080000   |
| train/             |           |
|    actor_loss      | 2.42e+03  |
|    critic_loss     | 14.4      |
|    ent_coef        | 0.109     |
|    ent_coef_loss   | -0.602    |
|    learning_rate   | 6.92e-05  |
|    n_updates       | 6759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.13e+05 |
| time/              |           |
|    episodes        | 616       |
|    fps             | 15        |
|    time_elapsed    | 199500    |
|    total_timesteps | 3080000   |
----------------------------------
Eval num_timesteps=3090000, episode_reward=-39395.93 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.94e+04 |
| time/              |           |
|    total_timesteps | 3090000   |
| train/             |           |
|    actor_loss      | 2.4e+03   |
|    critic_loss     | 21        |
|    ent_coef        | 0.123     |
|    ent_coef_loss   | 0.47      |
|    learning_rate   | 6.91e-05  |
|    n_updates       | 6769798   |
----------------------------------
Eval num_timesteps=3100000, episode_reward=-36279.62 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.63e+04 |
| time/              |           |
|    total_timesteps | 3100000   |
| train/             |           |
|    actor_loss      | 2.39e+03  |
|    critic_loss     | 81.7      |
|    ent_coef        | 0.143     |
|    ent_coef_loss   | -0.252    |
|    learning_rate   | 6.9e-05   |
|    n_updates       | 6779798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -1.1e+05 |
| time/              |          |
|    episodes        | 620      |
|    fps             | 15       |
|    time_elapsed    | 200701   |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=-43490.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.35e+04 |
| time/              |           |
|    total_timesteps | 3110000   |
| train/             |           |
|    actor_loss      | 2.37e+03  |
|    critic_loss     | 73.5      |
|    ent_coef        | 0.14      |
|    ent_coef_loss   | 0.593     |
|    learning_rate   | 6.89e-05  |
|    n_updates       | 6789798   |
----------------------------------
Eval num_timesteps=3120000, episode_reward=-35136.63 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.51e+04 |
| time/              |           |
|    total_timesteps | 3120000   |
| train/             |           |
|    actor_loss      | 2.34e+03  |
|    critic_loss     | 29.9      |
|    ent_coef        | 0.144     |
|    ent_coef_loss   | 1.24      |
|    learning_rate   | 6.88e-05  |
|    n_updates       | 6799798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.07e+05 |
| time/              |           |
|    episodes        | 624       |
|    fps             | 15        |
|    time_elapsed    | 201907    |
|    total_timesteps | 3120000   |
----------------------------------
Eval num_timesteps=3130000, episode_reward=-34658.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.47e+04 |
| time/              |           |
|    total_timesteps | 3130000   |
| train/             |           |
|    actor_loss      | 2.35e+03  |
|    critic_loss     | 48        |
|    ent_coef        | 0.152     |
|    ent_coef_loss   | 0.478     |
|    learning_rate   | 6.87e-05  |
|    n_updates       | 6809798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3140000, episode_reward=-37809.51 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.78e+04 |
| time/              |           |
|    total_timesteps | 3140000   |
| train/             |           |
|    actor_loss      | 2.38e+03  |
|    critic_loss     | 18.9      |
|    ent_coef        | 0.173     |
|    ent_coef_loss   | -0.379    |
|    learning_rate   | 6.86e-05  |
|    n_updates       | 6819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.04e+05 |
| time/              |           |
|    episodes        | 628       |
|    fps             | 15        |
|    time_elapsed    | 203111    |
|    total_timesteps | 3140000   |
----------------------------------
Eval num_timesteps=3150000, episode_reward=-36675.90 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.67e+04 |
| time/              |           |
|    total_timesteps | 3150000   |
| train/             |           |
|    actor_loss      | 2.37e+03  |
|    critic_loss     | 33        |
|    ent_coef        | 0.148     |
|    ent_coef_loss   | -1.02     |
|    learning_rate   | 6.85e-05  |
|    n_updates       | 6829798   |
----------------------------------
Eval num_timesteps=3160000, episode_reward=-33345.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.33e+04 |
| time/              |           |
|    total_timesteps | 3160000   |
| train/             |           |
|    actor_loss      | 2.36e+03  |
|    critic_loss     | 18        |
|    ent_coef        | 0.119     |
|    ent_coef_loss   | -1.65     |
|    learning_rate   | 6.84e-05  |
|    n_updates       | 6839798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -1.01e+05 |
| time/              |           |
|    episodes        | 632       |
|    fps             | 15        |
|    time_elapsed    | 204312    |
|    total_timesteps | 3160000   |
----------------------------------
Eval num_timesteps=3170000, episode_reward=-36538.91 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.65e+04 |
| time/              |           |
|    total_timesteps | 3170000   |
| train/             |           |
|    actor_loss      | 2.28e+03  |
|    critic_loss     | 104       |
|    ent_coef        | 0.192     |
|    ent_coef_loss   | 1.92      |
|    learning_rate   | 6.83e-05  |
|    n_updates       | 6849798   |
----------------------------------
Eval num_timesteps=3180000, episode_reward=-34511.08 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.45e+04 |
| time/              |           |
|    total_timesteps | 3180000   |
| train/             |           |
|    actor_loss      | 2.27e+03  |
|    critic_loss     | 36.8      |
|    ent_coef        | 0.194     |
|    ent_coef_loss   | -0.479    |
|    learning_rate   | 6.82e-05  |
|    n_updates       | 6859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -9.71e+04 |
| time/              |           |
|    episodes        | 636       |
|    fps             | 15        |
|    time_elapsed    | 205509    |
|    total_timesteps | 3180000   |
----------------------------------
Eval num_timesteps=3190000, episode_reward=-35385.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.54e+04 |
| time/              |           |
|    total_timesteps | 3190000   |
| train/             |           |
|    actor_loss      | 2.28e+03  |
|    critic_loss     | 32.7      |
|    ent_coef        | 0.143     |
|    ent_coef_loss   | 0.894     |
|    learning_rate   | 6.81e-05  |
|    n_updates       | 6869798   |
----------------------------------
Eval num_timesteps=3200000, episode_reward=-35288.12 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.53e+04 |
| time/              |           |
|    total_timesteps | 3200000   |
| train/             |           |
|    actor_loss      | 2.3e+03   |
|    critic_loss     | 65.7      |
|    ent_coef        | 0.131     |
|    ent_coef_loss   | -0.83     |
|    learning_rate   | 6.8e-05   |
|    n_updates       | 6879798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -9.36e+04 |
| time/              |           |
|    episodes        | 640       |
|    fps             | 15        |
|    time_elapsed    | 206704    |
|    total_timesteps | 3200000   |
----------------------------------
Eval num_timesteps=3210000, episode_reward=-35581.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.56e+04 |
| time/              |           |
|    total_timesteps | 3210000   |
| train/             |           |
|    actor_loss      | 2.26e+03  |
|    critic_loss     | 45.2      |
|    ent_coef        | 0.137     |
|    ent_coef_loss   | -0.37     |
|    learning_rate   | 6.79e-05  |
|    n_updates       | 6889798   |
----------------------------------
Eval num_timesteps=3220000, episode_reward=-34582.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.46e+04 |
| time/              |           |
|    total_timesteps | 3220000   |
| train/             |           |
|    actor_loss      | 2.25e+03  |
|    critic_loss     | 33.2      |
|    ent_coef        | 0.166     |
|    ent_coef_loss   | 0.0146    |
|    learning_rate   | 6.78e-05  |
|    n_updates       | 6899798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -9.01e+04 |
| time/              |           |
|    episodes        | 644       |
|    fps             | 15        |
|    time_elapsed    | 207905    |
|    total_timesteps | 3220000   |
----------------------------------
Eval num_timesteps=3230000, episode_reward=-34322.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.43e+04 |
| time/              |           |
|    total_timesteps | 3230000   |
| train/             |           |
|    actor_loss      | 2.22e+03  |
|    critic_loss     | 51.2      |
|    ent_coef        | 0.162     |
|    ent_coef_loss   | 0.446     |
|    learning_rate   | 6.77e-05  |
|    n_updates       | 6909798   |
----------------------------------
Eval num_timesteps=3240000, episode_reward=-34910.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.49e+04 |
| time/              |           |
|    total_timesteps | 3240000   |
| train/             |           |
|    actor_loss      | 2.19e+03  |
|    critic_loss     | 61.8      |
|    ent_coef        | 0.211     |
|    ent_coef_loss   | -0.117    |
|    learning_rate   | 6.76e-05  |
|    n_updates       | 6919798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -8.65e+04 |
| time/              |           |
|    episodes        | 648       |
|    fps             | 15        |
|    time_elapsed    | 209121    |
|    total_timesteps | 3240000   |
----------------------------------
Eval num_timesteps=3250000, episode_reward=-33162.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.32e+04 |
| time/              |           |
|    total_timesteps | 3250000   |
| train/             |           |
|    actor_loss      | 2.2e+03   |
|    critic_loss     | 29.7      |
|    ent_coef        | 0.268     |
|    ent_coef_loss   | 0.671     |
|    learning_rate   | 6.75e-05  |
|    n_updates       | 6929798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3260000, episode_reward=-33833.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.38e+04 |
| time/              |           |
|    total_timesteps | 3260000   |
| train/             |           |
|    actor_loss      | 2.19e+03  |
|    critic_loss     | 49.7      |
|    ent_coef        | 0.232     |
|    ent_coef_loss   | 0.104     |
|    learning_rate   | 6.74e-05  |
|    n_updates       | 6939798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -8.3e+04 |
| time/              |          |
|    episodes        | 652      |
|    fps             | 15       |
|    time_elapsed    | 210325   |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=-32080.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.21e+04 |
| time/              |           |
|    total_timesteps | 3270000   |
| train/             |           |
|    actor_loss      | 2.15e+03  |
|    critic_loss     | 25.7      |
|    ent_coef        | 0.324     |
|    ent_coef_loss   | 0.309     |
|    learning_rate   | 6.73e-05  |
|    n_updates       | 6949798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3280000, episode_reward=-33149.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.31e+04 |
| time/              |           |
|    total_timesteps | 3280000   |
| train/             |           |
|    actor_loss      | 2.14e+03  |
|    critic_loss     | 29.2      |
|    ent_coef        | 0.252     |
|    ent_coef_loss   | -0.253    |
|    learning_rate   | 6.72e-05  |
|    n_updates       | 6959798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -7.94e+04 |
| time/              |           |
|    episodes        | 656       |
|    fps             | 15        |
|    time_elapsed    | 211529    |
|    total_timesteps | 3280000   |
----------------------------------
Eval num_timesteps=3290000, episode_reward=-34565.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.46e+04 |
| time/              |           |
|    total_timesteps | 3290000   |
| train/             |           |
|    actor_loss      | 2.16e+03  |
|    critic_loss     | 58.5      |
|    ent_coef        | 0.22      |
|    ent_coef_loss   | -0.663    |
|    learning_rate   | 6.71e-05  |
|    n_updates       | 6969798   |
----------------------------------
Eval num_timesteps=3300000, episode_reward=-32386.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.24e+04 |
| time/              |           |
|    total_timesteps | 3300000   |
| train/             |           |
|    actor_loss      | 2.09e+03  |
|    critic_loss     | 242       |
|    ent_coef        | 0.321     |
|    ent_coef_loss   | -0.625    |
|    learning_rate   | 6.7e-05   |
|    n_updates       | 6979798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -7.59e+04 |
| time/              |           |
|    episodes        | 660       |
|    fps             | 15        |
|    time_elapsed    | 212735    |
|    total_timesteps | 3300000   |
----------------------------------
Eval num_timesteps=3310000, episode_reward=-36073.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.61e+04 |
| time/              |           |
|    total_timesteps | 3310000   |
| train/             |           |
|    actor_loss      | 2.09e+03  |
|    critic_loss     | 294       |
|    ent_coef        | 0.252     |
|    ent_coef_loss   | 0.0827    |
|    learning_rate   | 6.69e-05  |
|    n_updates       | 6989798   |
----------------------------------
Eval num_timesteps=3320000, episode_reward=-34474.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.45e+04 |
| time/              |           |
|    total_timesteps | 3320000   |
| train/             |           |
|    actor_loss      | 2.08e+03  |
|    critic_loss     | 47.8      |
|    ent_coef        | 0.295     |
|    ent_coef_loss   | 0.0772    |
|    learning_rate   | 6.68e-05  |
|    n_updates       | 6999798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -7.29e+04 |
| time/              |           |
|    episodes        | 664       |
|    fps             | 15        |
|    time_elapsed    | 213934    |
|    total_timesteps | 3320000   |
----------------------------------
Eval num_timesteps=3330000, episode_reward=-35193.80 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.52e+04 |
| time/              |           |
|    total_timesteps | 3330000   |
| train/             |           |
|    actor_loss      | 1.99e+03  |
|    critic_loss     | 35        |
|    ent_coef        | 0.411     |
|    ent_coef_loss   | 0.0456    |
|    learning_rate   | 6.67e-05  |
|    n_updates       | 7009798   |
----------------------------------
Eval num_timesteps=3340000, episode_reward=-38650.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.87e+04 |
| time/              |           |
|    total_timesteps | 3340000   |
| train/             |           |
|    actor_loss      | 2.01e+03  |
|    critic_loss     | 133       |
|    ent_coef        | 0.303     |
|    ent_coef_loss   | 0.21      |
|    learning_rate   | 6.66e-05  |
|    n_updates       | 7019798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -6.94e+04 |
| time/              |           |
|    episodes        | 668       |
|    fps             | 15        |
|    time_elapsed    | 215131    |
|    total_timesteps | 3340000   |
----------------------------------
Eval num_timesteps=3350000, episode_reward=-37237.78 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.72e+04 |
| time/              |           |
|    total_timesteps | 3350000   |
| train/             |           |
|    actor_loss      | 1.96e+03  |
|    critic_loss     | 52.9      |
|    ent_coef        | 0.35      |
|    ent_coef_loss   | 0.0438    |
|    learning_rate   | 6.65e-05  |
|    n_updates       | 7029798   |
----------------------------------
Eval num_timesteps=3360000, episode_reward=-37338.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.73e+04 |
| time/              |           |
|    total_timesteps | 3360000   |
| train/             |           |
|    actor_loss      | 1.92e+03  |
|    critic_loss     | 130       |
|    ent_coef        | 0.36      |
|    ent_coef_loss   | 0.486     |
|    learning_rate   | 6.64e-05  |
|    n_updates       | 7039798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -6.6e+04 |
| time/              |          |
|    episodes        | 672      |
|    fps             | 15       |
|    time_elapsed    | 216332   |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=-41498.29 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.15e+04 |
| time/              |           |
|    total_timesteps | 3370000   |
| train/             |           |
|    actor_loss      | 1.91e+03  |
|    critic_loss     | 152       |
|    ent_coef        | 0.41      |
|    ent_coef_loss   | -0.27     |
|    learning_rate   | 6.63e-05  |
|    n_updates       | 7049798   |
----------------------------------
Eval num_timesteps=3380000, episode_reward=-32820.76 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.28e+04 |
| time/              |           |
|    total_timesteps | 3380000   |
| train/             |           |
|    actor_loss      | 1.86e+03  |
|    critic_loss     | 52.8      |
|    ent_coef        | 0.295     |
|    ent_coef_loss   | -0.0749   |
|    learning_rate   | 6.62e-05  |
|    n_updates       | 7059798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -6.26e+04 |
| time/              |           |
|    episodes        | 676       |
|    fps             | 15        |
|    time_elapsed    | 217537    |
|    total_timesteps | 3380000   |
----------------------------------
Eval num_timesteps=3390000, episode_reward=-37335.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.73e+04 |
| time/              |           |
|    total_timesteps | 3390000   |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 309       |
|    ent_coef        | 0.358     |
|    ent_coef_loss   | -0.00854  |
|    learning_rate   | 6.61e-05  |
|    n_updates       | 7069798   |
----------------------------------
Eval num_timesteps=3400000, episode_reward=-36866.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.69e+04 |
| time/              |           |
|    total_timesteps | 3400000   |
| train/             |           |
|    actor_loss      | 1.88e+03  |
|    critic_loss     | 37.9      |
|    ent_coef        | 0.296     |
|    ent_coef_loss   | -0.174    |
|    learning_rate   | 6.6e-05   |
|    n_updates       | 7079798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.92e+04 |
| time/              |           |
|    episodes        | 680       |
|    fps             | 15        |
|    time_elapsed    | 218742    |
|    total_timesteps | 3400000   |
----------------------------------
Eval num_timesteps=3410000, episode_reward=-35934.88 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.59e+04 |
| time/              |           |
|    total_timesteps | 3410000   |
| train/             |           |
|    actor_loss      | 1.85e+03  |
|    critic_loss     | 90.4      |
|    ent_coef        | 0.341     |
|    ent_coef_loss   | 0.357     |
|    learning_rate   | 6.59e-05  |
|    n_updates       | 7089798   |
----------------------------------
Eval num_timesteps=3420000, episode_reward=-35687.75 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.57e+04 |
| time/              |           |
|    total_timesteps | 3420000   |
| train/             |           |
|    actor_loss      | 1.83e+03  |
|    critic_loss     | 55        |
|    ent_coef        | 0.4       |
|    ent_coef_loss   | -0.307    |
|    learning_rate   | 6.58e-05  |
|    n_updates       | 7099798   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5e+03    |
|    ep_rew_mean     | -5.6e+04 |
| time/              |          |
|    episodes        | 684      |
|    fps             | 15       |
|    time_elapsed    | 219949   |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=-35639.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.56e+04 |
| time/              |           |
|    total_timesteps | 3430000   |
| train/             |           |
|    actor_loss      | 1.8e+03   |
|    critic_loss     | 123       |
|    ent_coef        | 0.34      |
|    ent_coef_loss   | -0.132    |
|    learning_rate   | 6.57e-05  |
|    n_updates       | 7109798   |
----------------------------------
Eval num_timesteps=3440000, episode_reward=-34898.41 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.49e+04 |
| time/              |           |
|    total_timesteps | 3440000   |
| train/             |           |
|    actor_loss      | 1.8e+03   |
|    critic_loss     | 48.4      |
|    ent_coef        | 0.381     |
|    ent_coef_loss   | -0.342    |
|    learning_rate   | 6.56e-05  |
|    n_updates       | 7119798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.42e+04 |
| time/              |           |
|    episodes        | 688       |
|    fps             | 15        |
|    time_elapsed    | 221170    |
|    total_timesteps | 3440000   |
----------------------------------
Eval num_timesteps=3450000, episode_reward=-34383.69 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.44e+04 |
| time/              |           |
|    total_timesteps | 3450000   |
| train/             |           |
|    actor_loss      | 1.78e+03  |
|    critic_loss     | 82.7      |
|    ent_coef        | 0.331     |
|    ent_coef_loss   | -0.0283   |
|    learning_rate   | 6.55e-05  |
|    n_updates       | 7129798   |
----------------------------------
Eval num_timesteps=3460000, episode_reward=-34100.06 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.41e+04 |
| time/              |           |
|    total_timesteps | 3460000   |
| train/             |           |
|    actor_loss      | 1.69e+03  |
|    critic_loss     | 42.7      |
|    ent_coef        | 0.36      |
|    ent_coef_loss   | 0.42      |
|    learning_rate   | 6.54e-05  |
|    n_updates       | 7139798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.16e+04 |
| time/              |           |
|    episodes        | 692       |
|    fps             | 15        |
|    time_elapsed    | 222397    |
|    total_timesteps | 3460000   |
----------------------------------
Eval num_timesteps=3470000, episode_reward=-33410.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.34e+04 |
| time/              |           |
|    total_timesteps | 3470000   |
| train/             |           |
|    actor_loss      | 1.69e+03  |
|    critic_loss     | 48        |
|    ent_coef        | 0.38      |
|    ent_coef_loss   | -0.02     |
|    learning_rate   | 6.53e-05  |
|    n_updates       | 7149798   |
----------------------------------
Eval num_timesteps=3480000, episode_reward=-33046.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.3e+04 |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | 1.79e+03 |
|    critic_loss     | 26       |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | 0.0622   |
|    learning_rate   | 6.52e-05 |
|    n_updates       | 7159798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -5.05e+04 |
| time/              |           |
|    episodes        | 696       |
|    fps             | 15        |
|    time_elapsed    | 223614    |
|    total_timesteps | 3480000   |
----------------------------------
Eval num_timesteps=3490000, episode_reward=-33180.15 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.32e+04 |
| time/              |           |
|    total_timesteps | 3490000   |
| train/             |           |
|    actor_loss      | 1.64e+03  |
|    critic_loss     | 117       |
|    ent_coef        | 0.385     |
|    ent_coef_loss   | 0.258     |
|    learning_rate   | 6.51e-05  |
|    n_updates       | 7169798   |
----------------------------------
Eval num_timesteps=3500000, episode_reward=-32611.92 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.26e+04 |
| time/              |           |
|    total_timesteps | 3500000   |
| train/             |           |
|    actor_loss      | 1.67e+03  |
|    critic_loss     | 155       |
|    ent_coef        | 0.393     |
|    ent_coef_loss   | -0.0417   |
|    learning_rate   | 6.5e-05   |
|    n_updates       | 7179798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.76e+04 |
| time/              |           |
|    episodes        | 700       |
|    fps             | 15        |
|    time_elapsed    | 224836    |
|    total_timesteps | 3500000   |
----------------------------------
Eval num_timesteps=3510000, episode_reward=-33139.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.31e+04 |
| time/              |           |
|    total_timesteps | 3510000   |
| train/             |           |
|    actor_loss      | 1.61e+03  |
|    critic_loss     | 55.8      |
|    ent_coef        | 0.397     |
|    ent_coef_loss   | -0.121    |
|    learning_rate   | 6.49e-05  |
|    n_updates       | 7189798   |
----------------------------------
Eval num_timesteps=3520000, episode_reward=-32683.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.27e+04 |
| time/              |           |
|    total_timesteps | 3520000   |
| train/             |           |
|    actor_loss      | 1.54e+03  |
|    critic_loss     | 74.8      |
|    ent_coef        | 0.422     |
|    ent_coef_loss   | 0.203     |
|    learning_rate   | 6.48e-05  |
|    n_updates       | 7199798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.49e+04 |
| time/              |           |
|    episodes        | 704       |
|    fps             | 15        |
|    time_elapsed    | 226077    |
|    total_timesteps | 3520000   |
----------------------------------
Eval num_timesteps=3530000, episode_reward=-33159.82 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.32e+04 |
| time/              |           |
|    total_timesteps | 3530000   |
| train/             |           |
|    actor_loss      | 1.59e+03  |
|    critic_loss     | 24.5      |
|    ent_coef        | 0.423     |
|    ent_coef_loss   | 0.0738    |
|    learning_rate   | 6.47e-05  |
|    n_updates       | 7209798   |
----------------------------------
Eval num_timesteps=3540000, episode_reward=-32472.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.25e+04 |
| time/              |           |
|    total_timesteps | 3540000   |
| train/             |           |
|    actor_loss      | 1.53e+03  |
|    critic_loss     | 44.1      |
|    ent_coef        | 0.423     |
|    ent_coef_loss   | 0.00759   |
|    learning_rate   | 6.46e-05  |
|    n_updates       | 7219798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -4.18e+04 |
| time/              |           |
|    episodes        | 708       |
|    fps             | 15        |
|    time_elapsed    | 227295    |
|    total_timesteps | 3540000   |
----------------------------------
Eval num_timesteps=3550000, episode_reward=-32725.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.27e+04 |
| time/              |           |
|    total_timesteps | 3550000   |
| train/             |           |
|    actor_loss      | 1.54e+03  |
|    critic_loss     | 58.4      |
|    ent_coef        | 0.437     |
|    ent_coef_loss   | 0.201     |
|    learning_rate   | 6.45e-05  |
|    n_updates       | 7229798   |
----------------------------------
Eval num_timesteps=3560000, episode_reward=-32162.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+04 |
| time/              |           |
|    total_timesteps | 3560000   |
| train/             |           |
|    actor_loss      | 1.47e+03  |
|    critic_loss     | 29.7      |
|    ent_coef        | 0.444     |
|    ent_coef_loss   | -0.0262   |
|    learning_rate   | 6.44e-05  |
|    n_updates       | 7239798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.83e+04 |
| time/              |           |
|    episodes        | 712       |
|    fps             | 15        |
|    time_elapsed    | 228513    |
|    total_timesteps | 3560000   |
----------------------------------
Eval num_timesteps=3570000, episode_reward=-32110.11 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.21e+04 |
| time/              |           |
|    total_timesteps | 3570000   |
| train/             |           |
|    actor_loss      | 1.46e+03  |
|    critic_loss     | 30.4      |
|    ent_coef        | 0.447     |
|    ent_coef_loss   | 0.0231    |
|    learning_rate   | 6.43e-05  |
|    n_updates       | 7249798   |
----------------------------------
Eval num_timesteps=3580000, episode_reward=-32151.42 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.22e+04 |
| time/              |           |
|    total_timesteps | 3580000   |
| train/             |           |
|    actor_loss      | 1.46e+03  |
|    critic_loss     | 212       |
|    ent_coef        | 0.444     |
|    ent_coef_loss   | -0.0143   |
|    learning_rate   | 6.42e-05  |
|    n_updates       | 7259798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.64e+04 |
| time/              |           |
|    episodes        | 716       |
|    fps             | 15        |
|    time_elapsed    | 229733    |
|    total_timesteps | 3580000   |
----------------------------------
Eval num_timesteps=3590000, episode_reward=-32007.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+04 |
| time/              |          |
|    total_timesteps | 3590000  |
| train/             |          |
|    actor_loss      | 1.48e+03 |
|    critic_loss     | 185      |
|    ent_coef        | 0.434    |
|    ent_coef_loss   | 0.127    |
|    learning_rate   | 6.41e-05 |
|    n_updates       | 7269798  |
---------------------------------
New best mean reward!
Eval num_timesteps=3600000, episode_reward=-31827.36 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+04 |
| time/              |           |
|    total_timesteps | 3600000   |
| train/             |           |
|    actor_loss      | 1.42e+03  |
|    critic_loss     | 52.1      |
|    ent_coef        | 0.408     |
|    ent_coef_loss   | 0.32      |
|    learning_rate   | 6.4e-05   |
|    n_updates       | 7279798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.62e+04 |
| time/              |           |
|    episodes        | 720       |
|    fps             | 15        |
|    time_elapsed    | 230955    |
|    total_timesteps | 3600000   |
----------------------------------
Eval num_timesteps=3610000, episode_reward=-31535.14 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.15e+04 |
| time/              |           |
|    total_timesteps | 3610000   |
| train/             |           |
|    actor_loss      | 1.37e+03  |
|    critic_loss     | 85.7      |
|    ent_coef        | 0.409     |
|    ent_coef_loss   | -0.0256   |
|    learning_rate   | 6.39e-05  |
|    n_updates       | 7289798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3620000, episode_reward=-32406.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.24e+04 |
| time/              |           |
|    total_timesteps | 3620000   |
| train/             |           |
|    actor_loss      | 1.39e+03  |
|    critic_loss     | 41.8      |
|    ent_coef        | 0.39      |
|    ent_coef_loss   | -0.189    |
|    learning_rate   | 6.38e-05  |
|    n_updates       | 7299798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.49e+04 |
| time/              |           |
|    episodes        | 724       |
|    fps             | 15        |
|    time_elapsed    | 232176    |
|    total_timesteps | 3620000   |
----------------------------------
Eval num_timesteps=3630000, episode_reward=-32022.70 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3.2e+04 |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | 1.37e+03 |
|    critic_loss     | 54.9     |
|    ent_coef        | 0.372    |
|    ent_coef_loss   | -0.105   |
|    learning_rate   | 6.37e-05 |
|    n_updates       | 7309798  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=-31886.95 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.19e+04 |
| time/              |           |
|    total_timesteps | 3640000   |
| train/             |           |
|    actor_loss      | 1.36e+03  |
|    critic_loss     | 37        |
|    ent_coef        | 0.364     |
|    ent_coef_loss   | 0.137     |
|    learning_rate   | 6.36e-05  |
|    n_updates       | 7319798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.47e+04 |
| time/              |           |
|    episodes        | 728       |
|    fps             | 15        |
|    time_elapsed    | 233396    |
|    total_timesteps | 3640000   |
----------------------------------
Eval num_timesteps=3650000, episode_reward=-31832.87 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.18e+04 |
| time/              |           |
|    total_timesteps | 3650000   |
| train/             |           |
|    actor_loss      | 1.34e+03  |
|    critic_loss     | 18.8      |
|    ent_coef        | 0.348     |
|    ent_coef_loss   | 0.0401    |
|    learning_rate   | 6.35e-05  |
|    n_updates       | 7329798   |
----------------------------------
Eval num_timesteps=3660000, episode_reward=-31088.05 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.11e+04 |
| time/              |           |
|    total_timesteps | 3660000   |
| train/             |           |
|    actor_loss      | 1.36e+03  |
|    critic_loss     | 69.7      |
|    ent_coef        | 0.346     |
|    ent_coef_loss   | 0.0976    |
|    learning_rate   | 6.34e-05  |
|    n_updates       | 7339798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.45e+04 |
| time/              |           |
|    episodes        | 732       |
|    fps             | 15        |
|    time_elapsed    | 234619    |
|    total_timesteps | 3660000   |
----------------------------------
Eval num_timesteps=3670000, episode_reward=-31551.54 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.16e+04 |
| time/              |           |
|    total_timesteps | 3670000   |
| train/             |           |
|    actor_loss      | 1.3e+03   |
|    critic_loss     | 94.3      |
|    ent_coef        | 0.365     |
|    ent_coef_loss   | 0.263     |
|    learning_rate   | 6.33e-05  |
|    n_updates       | 7349798   |
----------------------------------
Eval num_timesteps=3680000, episode_reward=-31234.26 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.12e+04 |
| time/              |           |
|    total_timesteps | 3680000   |
| train/             |           |
|    actor_loss      | 1.34e+03  |
|    critic_loss     | 34.3      |
|    ent_coef        | 0.368     |
|    ent_coef_loss   | 0.325     |
|    learning_rate   | 6.32e-05  |
|    n_updates       | 7359798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.44e+04 |
| time/              |           |
|    episodes        | 736       |
|    fps             | 15        |
|    time_elapsed    | 235840    |
|    total_timesteps | 3680000   |
----------------------------------
Eval num_timesteps=3690000, episode_reward=-31144.53 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.11e+04 |
| time/              |           |
|    total_timesteps | 3690000   |
| train/             |           |
|    actor_loss      | 1.28e+03  |
|    critic_loss     | 48.9      |
|    ent_coef        | 0.366     |
|    ent_coef_loss   | -0.31     |
|    learning_rate   | 6.31e-05  |
|    n_updates       | 7369798   |
----------------------------------
Eval num_timesteps=3700000, episode_reward=-31240.59 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.12e+04 |
| time/              |           |
|    total_timesteps | 3700000   |
| train/             |           |
|    actor_loss      | 1.31e+03  |
|    critic_loss     | 40.9      |
|    ent_coef        | 0.376     |
|    ent_coef_loss   | 0.251     |
|    learning_rate   | 6.3e-05   |
|    n_updates       | 7379798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.42e+04 |
| time/              |           |
|    episodes        | 740       |
|    fps             | 15        |
|    time_elapsed    | 237061    |
|    total_timesteps | 3700000   |
----------------------------------
Eval num_timesteps=3710000, episode_reward=-30934.39 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.09e+04 |
| time/              |           |
|    total_timesteps | 3710000   |
| train/             |           |
|    actor_loss      | 1.27e+03  |
|    critic_loss     | 73.5      |
|    ent_coef        | 0.359     |
|    ent_coef_loss   | -0.148    |
|    learning_rate   | 6.29e-05  |
|    n_updates       | 7389798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3720000, episode_reward=-30861.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.09e+04 |
| time/              |           |
|    total_timesteps | 3720000   |
| train/             |           |
|    actor_loss      | 1.25e+03  |
|    critic_loss     | 46.7      |
|    ent_coef        | 0.365     |
|    ent_coef_loss   | 0.387     |
|    learning_rate   | 6.28e-05  |
|    n_updates       | 7399798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.41e+04 |
| time/              |           |
|    episodes        | 744       |
|    fps             | 15        |
|    time_elapsed    | 238279    |
|    total_timesteps | 3720000   |
----------------------------------
Eval num_timesteps=3730000, episode_reward=-30646.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.06e+04 |
| time/              |           |
|    total_timesteps | 3730000   |
| train/             |           |
|    actor_loss      | 1.21e+03  |
|    critic_loss     | 55.7      |
|    ent_coef        | 0.351     |
|    ent_coef_loss   | -0.31     |
|    learning_rate   | 6.27e-05  |
|    n_updates       | 7409798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3740000, episode_reward=-30612.24 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.06e+04 |
| time/              |           |
|    total_timesteps | 3740000   |
| train/             |           |
|    actor_loss      | 1.2e+03   |
|    critic_loss     | 89.3      |
|    ent_coef        | 0.352     |
|    ent_coef_loss   | 0.0208    |
|    learning_rate   | 6.26e-05  |
|    n_updates       | 7419798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.39e+04 |
| time/              |           |
|    episodes        | 748       |
|    fps             | 15        |
|    time_elapsed    | 239496    |
|    total_timesteps | 3740000   |
----------------------------------
Eval num_timesteps=3750000, episode_reward=-30497.60 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.05e+04 |
| time/              |           |
|    total_timesteps | 3750000   |
| train/             |           |
|    actor_loss      | 1.2e+03   |
|    critic_loss     | 32.7      |
|    ent_coef        | 0.355     |
|    ent_coef_loss   | -0.107    |
|    learning_rate   | 6.25e-05  |
|    n_updates       | 7429798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3760000, episode_reward=-30355.43 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.04e+04 |
| time/              |           |
|    total_timesteps | 3760000   |
| train/             |           |
|    actor_loss      | 1.22e+03  |
|    critic_loss     | 46.3      |
|    ent_coef        | 0.337     |
|    ent_coef_loss   | 0.101     |
|    learning_rate   | 6.24e-05  |
|    n_updates       | 7439798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.38e+04 |
| time/              |           |
|    episodes        | 752       |
|    fps             | 15        |
|    time_elapsed    | 240721    |
|    total_timesteps | 3760000   |
----------------------------------
Eval num_timesteps=3770000, episode_reward=-30273.58 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.03e+04 |
| time/              |           |
|    total_timesteps | 3770000   |
| train/             |           |
|    actor_loss      | 1.14e+03  |
|    critic_loss     | 85.2      |
|    ent_coef        | 0.33      |
|    ent_coef_loss   | 0.0554    |
|    learning_rate   | 6.23e-05  |
|    n_updates       | 7449798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3780000, episode_reward=-30189.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.02e+04 |
| time/              |           |
|    total_timesteps | 3780000   |
| train/             |           |
|    actor_loss      | 1.19e+03  |
|    critic_loss     | 128       |
|    ent_coef        | 0.341     |
|    ent_coef_loss   | 0.0976    |
|    learning_rate   | 6.22e-05  |
|    n_updates       | 7459798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.37e+04 |
| time/              |           |
|    episodes        | 756       |
|    fps             | 15        |
|    time_elapsed    | 241939    |
|    total_timesteps | 3780000   |
----------------------------------
Eval num_timesteps=3790000, episode_reward=-30133.07 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.01e+04 |
| time/              |           |
|    total_timesteps | 3790000   |
| train/             |           |
|    actor_loss      | 1.12e+03  |
|    critic_loss     | 101       |
|    ent_coef        | 0.323     |
|    ent_coef_loss   | -0.143    |
|    learning_rate   | 6.21e-05  |
|    n_updates       | 7469798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3800000, episode_reward=-29959.52 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3e+04   |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | 1.15e+03 |
|    critic_loss     | 49.2     |
|    ent_coef        | 0.322    |
|    ent_coef_loss   | -0.0756  |
|    learning_rate   | 6.2e-05  |
|    n_updates       | 7479798  |
---------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.36e+04 |
| time/              |           |
|    episodes        | 760       |
|    fps             | 15        |
|    time_elapsed    | 243155    |
|    total_timesteps | 3800000   |
----------------------------------
Eval num_timesteps=3810000, episode_reward=-29938.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3810000   |
| train/             |           |
|    actor_loss      | 1.15e+03  |
|    critic_loss     | 79.1      |
|    ent_coef        | 0.312     |
|    ent_coef_loss   | 0.207     |
|    learning_rate   | 6.19e-05  |
|    n_updates       | 7489798   |
----------------------------------
New best mean reward!
Eval num_timesteps=3820000, episode_reward=-29768.96 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 3820000   |
| train/             |           |
|    actor_loss      | 1.11e+03  |
|    critic_loss     | 59.6      |
|    ent_coef        | 0.296     |
|    ent_coef_loss   | -0.179    |
|    learning_rate   | 6.18e-05  |
|    n_updates       | 7499798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.29e+04 |
| time/              |           |
|    episodes        | 764       |
|    fps             | 15        |
|    time_elapsed    | 244377    |
|    total_timesteps | 3820000   |
----------------------------------
Eval num_timesteps=3830000, episode_reward=-29782.61 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 3830000   |
| train/             |           |
|    actor_loss      | 1.05e+03  |
|    critic_loss     | 49.7      |
|    ent_coef        | 0.295     |
|    ent_coef_loss   | -0.402    |
|    learning_rate   | 6.17e-05  |
|    n_updates       | 7509798   |
----------------------------------
Eval num_timesteps=3840000, episode_reward=-30023.66 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3e+04   |
| time/              |          |
|    total_timesteps | 3840000  |
| train/             |          |
|    actor_loss      | 1.13e+03 |
|    critic_loss     | 27       |
|    ent_coef        | 0.301    |
|    ent_coef_loss   | -0.227   |
|    learning_rate   | 6.16e-05 |
|    n_updates       | 7519798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.27e+04 |
| time/              |           |
|    episodes        | 768       |
|    fps             | 15        |
|    time_elapsed    | 245598    |
|    total_timesteps | 3840000   |
----------------------------------
Eval num_timesteps=3850000, episode_reward=-29923.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3850000   |
| train/             |           |
|    actor_loss      | 1.07e+03  |
|    critic_loss     | 127       |
|    ent_coef        | 0.307     |
|    ent_coef_loss   | -0.193    |
|    learning_rate   | 6.15e-05  |
|    n_updates       | 7529798   |
----------------------------------
Eval num_timesteps=3860000, episode_reward=-30062.32 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.01e+04 |
| time/              |           |
|    total_timesteps | 3860000   |
| train/             |           |
|    actor_loss      | 1.03e+03  |
|    critic_loss     | 68.8      |
|    ent_coef        | 0.319     |
|    ent_coef_loss   | -0.022    |
|    learning_rate   | 6.14e-05  |
|    n_updates       | 7539798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.24e+04 |
| time/              |           |
|    episodes        | 772       |
|    fps             | 15        |
|    time_elapsed    | 246873    |
|    total_timesteps | 3860000   |
----------------------------------
Eval num_timesteps=3870000, episode_reward=-30169.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.02e+04 |
| time/              |           |
|    total_timesteps | 3870000   |
| train/             |           |
|    actor_loss      | 1.05e+03  |
|    critic_loss     | 80.2      |
|    ent_coef        | 0.359     |
|    ent_coef_loss   | -0.0654   |
|    learning_rate   | 6.13e-05  |
|    n_updates       | 7549798   |
----------------------------------
Eval num_timesteps=3880000, episode_reward=-29933.86 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3880000   |
| train/             |           |
|    actor_loss      | 1.02e+03  |
|    critic_loss     | 60.7      |
|    ent_coef        | 0.355     |
|    ent_coef_loss   | 0.113     |
|    learning_rate   | 6.12e-05  |
|    n_updates       | 7559798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.22e+04 |
| time/              |           |
|    episodes        | 776       |
|    fps             | 15        |
|    time_elapsed    | 248111    |
|    total_timesteps | 3880000   |
----------------------------------
Eval num_timesteps=3890000, episode_reward=-29922.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3890000   |
| train/             |           |
|    actor_loss      | 999       |
|    critic_loss     | 266       |
|    ent_coef        | 0.357     |
|    ent_coef_loss   | 0.0645    |
|    learning_rate   | 6.11e-05  |
|    n_updates       | 7569798   |
----------------------------------
Eval num_timesteps=3900000, episode_reward=-30052.71 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.01e+04 |
| time/              |           |
|    total_timesteps | 3900000   |
| train/             |           |
|    actor_loss      | 983       |
|    critic_loss     | 41        |
|    ent_coef        | 0.352     |
|    ent_coef_loss   | 0.123     |
|    learning_rate   | 6.1e-05   |
|    n_updates       | 7579798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.19e+04 |
| time/              |           |
|    episodes        | 780       |
|    fps             | 15        |
|    time_elapsed    | 249341    |
|    total_timesteps | 3900000   |
----------------------------------
Eval num_timesteps=3910000, episode_reward=-30216.65 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.02e+04 |
| time/              |           |
|    total_timesteps | 3910000   |
| train/             |           |
|    actor_loss      | 960       |
|    critic_loss     | 67        |
|    ent_coef        | 0.338     |
|    ent_coef_loss   | -0.0327   |
|    learning_rate   | 6.09e-05  |
|    n_updates       | 7589798   |
----------------------------------
Eval num_timesteps=3920000, episode_reward=-30244.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.02e+04 |
| time/              |           |
|    total_timesteps | 3920000   |
| train/             |           |
|    actor_loss      | 951       |
|    critic_loss     | 95.3      |
|    ent_coef        | 0.337     |
|    ent_coef_loss   | -0.138    |
|    learning_rate   | 6.08e-05  |
|    n_updates       | 7599798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.17e+04 |
| time/              |           |
|    episodes        | 784       |
|    fps             | 15        |
|    time_elapsed    | 250564    |
|    total_timesteps | 3920000   |
----------------------------------
Eval num_timesteps=3930000, episode_reward=-30316.10 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.03e+04 |
| time/              |           |
|    total_timesteps | 3930000   |
| train/             |           |
|    actor_loss      | 919       |
|    critic_loss     | 49.8      |
|    ent_coef        | 0.314     |
|    ent_coef_loss   | -0.356    |
|    learning_rate   | 6.07e-05  |
|    n_updates       | 7609798   |
----------------------------------
Eval num_timesteps=3940000, episode_reward=-29948.83 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3940000   |
| train/             |           |
|    actor_loss      | 921       |
|    critic_loss     | 64.1      |
|    ent_coef        | 0.319     |
|    ent_coef_loss   | 0.079     |
|    learning_rate   | 6.06e-05  |
|    n_updates       | 7619798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.15e+04 |
| time/              |           |
|    episodes        | 788       |
|    fps             | 15        |
|    time_elapsed    | 251778    |
|    total_timesteps | 3940000   |
----------------------------------
Eval num_timesteps=3950000, episode_reward=-29947.37 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3950000   |
| train/             |           |
|    actor_loss      | 928       |
|    critic_loss     | 74.6      |
|    ent_coef        | 0.312     |
|    ent_coef_loss   | 0.00594   |
|    learning_rate   | 6.05e-05  |
|    n_updates       | 7629798   |
----------------------------------
Eval num_timesteps=3960000, episode_reward=-29915.44 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3960000   |
| train/             |           |
|    actor_loss      | 908       |
|    critic_loss     | 28.5      |
|    ent_coef        | 0.321     |
|    ent_coef_loss   | -0.15     |
|    learning_rate   | 6.04e-05  |
|    n_updates       | 7639798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.13e+04 |
| time/              |           |
|    episodes        | 792       |
|    fps             | 15        |
|    time_elapsed    | 252993    |
|    total_timesteps | 3960000   |
----------------------------------
Eval num_timesteps=3970000, episode_reward=-29884.49 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 3970000   |
| train/             |           |
|    actor_loss      | 906       |
|    critic_loss     | 106       |
|    ent_coef        | 0.308     |
|    ent_coef_loss   | 0.188     |
|    learning_rate   | 6.03e-05  |
|    n_updates       | 7649798   |
----------------------------------
Eval num_timesteps=3980000, episode_reward=-29759.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 3980000   |
| train/             |           |
|    actor_loss      | 904       |
|    critic_loss     | 55.9      |
|    ent_coef        | 0.307     |
|    ent_coef_loss   | -0.0485   |
|    learning_rate   | 6.02e-05  |
|    n_updates       | 7659798   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.12e+04 |
| time/              |           |
|    episodes        | 796       |
|    fps             | 15        |
|    time_elapsed    | 254215    |
|    total_timesteps | 3980000   |
----------------------------------
Eval num_timesteps=3990000, episode_reward=-29843.89 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 3990000   |
| train/             |           |
|    actor_loss      | 914       |
|    critic_loss     | 39.1      |
|    ent_coef        | 0.305     |
|    ent_coef_loss   | -0.246    |
|    learning_rate   | 6.01e-05  |
|    n_updates       | 7669798   |
----------------------------------
Eval num_timesteps=4000000, episode_reward=-29812.85 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4000000   |
| train/             |           |
|    actor_loss      | 864       |
|    critic_loss     | 51.2      |
|    ent_coef        | 0.293     |
|    ent_coef_loss   | -0.239    |
|    learning_rate   | 6e-05     |
|    n_updates       | 7679798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.11e+04 |
| time/              |           |
|    episodes        | 800       |
|    fps             | 15        |
|    time_elapsed    | 255446    |
|    total_timesteps | 4000000   |
----------------------------------
Eval num_timesteps=4010000, episode_reward=-29665.19 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.97e+04 |
| time/              |           |
|    total_timesteps | 4010000   |
| train/             |           |
|    actor_loss      | 891       |
|    critic_loss     | 73.4      |
|    ent_coef        | 0.29      |
|    ent_coef_loss   | -0.0876   |
|    learning_rate   | 5.99e-05  |
|    n_updates       | 7689798   |
----------------------------------
New best mean reward!
Eval num_timesteps=4020000, episode_reward=-29674.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.97e+04 |
| time/              |           |
|    total_timesteps | 4020000   |
| train/             |           |
|    actor_loss      | 854       |
|    critic_loss     | 66.6      |
|    ent_coef        | 0.302     |
|    ent_coef_loss   | 0.0275    |
|    learning_rate   | 5.98e-05  |
|    n_updates       | 7699798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.09e+04 |
| time/              |           |
|    episodes        | 804       |
|    fps             | 15        |
|    time_elapsed    | 256672    |
|    total_timesteps | 4020000   |
----------------------------------
Eval num_timesteps=4030000, episode_reward=-29857.99 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4030000   |
| train/             |           |
|    actor_loss      | 865       |
|    critic_loss     | 117       |
|    ent_coef        | 0.294     |
|    ent_coef_loss   | -0.164    |
|    learning_rate   | 5.97e-05  |
|    n_updates       | 7709798   |
----------------------------------
Eval num_timesteps=4040000, episode_reward=-29762.03 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4040000   |
| train/             |           |
|    actor_loss      | 807       |
|    critic_loss     | 98.3      |
|    ent_coef        | 0.272     |
|    ent_coef_loss   | -0.123    |
|    learning_rate   | 5.96e-05  |
|    n_updates       | 7719798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.08e+04 |
| time/              |           |
|    episodes        | 808       |
|    fps             | 15        |
|    time_elapsed    | 257943    |
|    total_timesteps | 4040000   |
----------------------------------
Eval num_timesteps=4050000, episode_reward=-29846.30 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4050000   |
| train/             |           |
|    actor_loss      | 821       |
|    critic_loss     | 55.8      |
|    ent_coef        | 0.275     |
|    ent_coef_loss   | -0.0172   |
|    learning_rate   | 5.95e-05  |
|    n_updates       | 7729798   |
----------------------------------
Eval num_timesteps=4060000, episode_reward=-30016.22 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3e+04   |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | 797      |
|    critic_loss     | 114      |
|    ent_coef        | 0.269    |
|    ent_coef_loss   | -0.278   |
|    learning_rate   | 5.94e-05 |
|    n_updates       | 7739798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.07e+04 |
| time/              |           |
|    episodes        | 812       |
|    fps             | 15        |
|    time_elapsed    | 259006    |
|    total_timesteps | 4060000   |
----------------------------------
Eval num_timesteps=4070000, episode_reward=-29888.23 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4070000   |
| train/             |           |
|    actor_loss      | 794       |
|    critic_loss     | 27.8      |
|    ent_coef        | 0.285     |
|    ent_coef_loss   | -0.316    |
|    learning_rate   | 5.93e-05  |
|    n_updates       | 7749798   |
----------------------------------
Eval num_timesteps=4080000, episode_reward=-29922.55 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4080000   |
| train/             |           |
|    actor_loss      | 792       |
|    critic_loss     | 43.9      |
|    ent_coef        | 0.273     |
|    ent_coef_loss   | 0.121     |
|    learning_rate   | 5.92e-05  |
|    n_updates       | 7759798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.06e+04 |
| time/              |           |
|    episodes        | 816       |
|    fps             | 15        |
|    time_elapsed    | 260216    |
|    total_timesteps | 4080000   |
----------------------------------
Eval num_timesteps=4090000, episode_reward=-29964.34 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3e+04   |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | 770      |
|    critic_loss     | 46.5     |
|    ent_coef        | 0.263    |
|    ent_coef_loss   | -0.23    |
|    learning_rate   | 5.91e-05 |
|    n_updates       | 7769798  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=-29919.79 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4100000   |
| train/             |           |
|    actor_loss      | 769       |
|    critic_loss     | 30.3      |
|    ent_coef        | 0.266     |
|    ent_coef_loss   | -0.298    |
|    learning_rate   | 5.9e-05   |
|    n_updates       | 7779798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.05e+04 |
| time/              |           |
|    episodes        | 820       |
|    fps             | 15        |
|    time_elapsed    | 261492    |
|    total_timesteps | 4100000   |
----------------------------------
Eval num_timesteps=4110000, episode_reward=-29934.25 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4110000   |
| train/             |           |
|    actor_loss      | 781       |
|    critic_loss     | 31.7      |
|    ent_coef        | 0.263     |
|    ent_coef_loss   | 0.251     |
|    learning_rate   | 5.89e-05  |
|    n_updates       | 7789798   |
----------------------------------
Eval num_timesteps=4120000, episode_reward=-29958.40 +/- 0.00
Episode length: 5000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 5e+03    |
|    mean_reward     | -3e+04   |
| time/              |          |
|    total_timesteps | 4120000  |
| train/             |          |
|    actor_loss      | 771      |
|    critic_loss     | 42       |
|    ent_coef        | 0.24     |
|    ent_coef_loss   | 0.133    |
|    learning_rate   | 5.88e-05 |
|    n_updates       | 7799798  |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.05e+04 |
| time/              |           |
|    episodes        | 824       |
|    fps             | 15        |
|    time_elapsed    | 262773    |
|    total_timesteps | 4120000   |
----------------------------------
Eval num_timesteps=4130000, episode_reward=-29827.09 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4130000   |
| train/             |           |
|    actor_loss      | 750       |
|    critic_loss     | 18.4      |
|    ent_coef        | 0.226     |
|    ent_coef_loss   | -0.158    |
|    learning_rate   | 5.87e-05  |
|    n_updates       | 7809798   |
----------------------------------
Eval num_timesteps=4140000, episode_reward=-29908.17 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4140000   |
| train/             |           |
|    actor_loss      | 761       |
|    critic_loss     | 20.1      |
|    ent_coef        | 0.222     |
|    ent_coef_loss   | -0.0392   |
|    learning_rate   | 5.86e-05  |
|    n_updates       | 7819798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.04e+04 |
| time/              |           |
|    episodes        | 828       |
|    fps             | 15        |
|    time_elapsed    | 264040    |
|    total_timesteps | 4140000   |
----------------------------------
Eval num_timesteps=4150000, episode_reward=-29795.46 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4150000   |
| train/             |           |
|    actor_loss      | 750       |
|    critic_loss     | 39.7      |
|    ent_coef        | 0.213     |
|    ent_coef_loss   | 0.118     |
|    learning_rate   | 5.85e-05  |
|    n_updates       | 7829798   |
----------------------------------
Eval num_timesteps=4160000, episode_reward=-29944.84 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.99e+04 |
| time/              |           |
|    total_timesteps | 4160000   |
| train/             |           |
|    actor_loss      | 746       |
|    critic_loss     | 59        |
|    ent_coef        | 0.205     |
|    ent_coef_loss   | 0.304     |
|    learning_rate   | 5.84e-05  |
|    n_updates       | 7839798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.03e+04 |
| time/              |           |
|    episodes        | 832       |
|    fps             | 15        |
|    time_elapsed    | 265328    |
|    total_timesteps | 4160000   |
----------------------------------
Eval num_timesteps=4170000, episode_reward=-29778.68 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4170000   |
| train/             |           |
|    actor_loss      | 731       |
|    critic_loss     | 32.3      |
|    ent_coef        | 0.212     |
|    ent_coef_loss   | -0.216    |
|    learning_rate   | 5.83e-05  |
|    n_updates       | 7849798   |
----------------------------------
Eval num_timesteps=4180000, episode_reward=-29772.33 +/- 0.00
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.98e+04 |
| time/              |           |
|    total_timesteps | 4180000   |
| train/             |           |
|    actor_loss      | 732       |
|    critic_loss     | 52.2      |
|    ent_coef        | 0.19      |
|    ent_coef_loss   | -0.0797   |
|    learning_rate   | 5.82e-05  |
|    n_updates       | 7859798   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5e+03     |
|    ep_rew_mean     | -3.03e+04 |
| time/              |           |
|    episodes        | 836       |
|    fps             | 15        |
|    time_elapsed    | 266590    |
|    total_timesteps | 4180000   |
